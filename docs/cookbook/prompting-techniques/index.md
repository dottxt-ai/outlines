# Prompting Techniques

This index provides links to various prompting techniques as featured in [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608).

Each is a simple example of the technique using Outlines. Read more about each technique in detail in [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608) and try out simple code examples below.

## The Techniques
- [Analogical Prompting](analogical-prompting.md) - Uses analogies to guide the model's reasoning.
- [AutoPrompt](autoprompt.md) - Automatically generates prompts for specific tasks.
- [Chain of Thought (CoT) Prompting](chain-of-thought-cot-prompting.md) - Encourages the model to show its reasoning step-by-step.
- [Consistency-Based Self-Adaptive Prompting (CoSP)](consistency-based-self-adaptive-prompting-cosp.md) - Adapts prompts based on consistency of model outputs.
- [Contrastive CoT Prompting](contrastive-cot-prompting.md) - Uses contrasting examples to improve chain of thought reasoning.
- [Cumulative Reasoning](cumulative-reasoning.md) - Builds upon previous reasoning steps to reach a conclusion.
- [Decomposed Prompting (DeComp)](decomposed-prompting-decomp.md) - Breaks down complex tasks into smaller, manageable steps.
- [Demonstration Ensembling (DENSE)](demonstration-ensembling-dense.md) - Combines multiple demonstrations to improve performance.
- [Dialogue-Comprised Policy Gradient-Based Discrete Prompt Optimization (DP2O)](dialogue-comprised-policy-gradient-based-discrete-prompt-optimization-dp2o.md) - Optimizes prompts through dialogue-based interactions.
- [Diverse Diversity-Focused Self-Consistency](diverse-diversity-focused-self-consistency.md) - Promotes diverse outputs while maintaining consistency.
- [Emotion Prompting](emotion-prompting.md) - Incorporates emotional context into prompts.
- [Max Mutual Information Method](max-mutual-information-method.md) - Maximizes mutual information between prompts and desired outputs.
- [Meta Prompting](meta-prompting.md) - Uses prompts to generate or improve other prompts.
- [Prompt Mining](prompt-mining.md) - Extracts effective prompts from existing data or model outputs.
- [Re-Reading (Re2)](re-reading-re2.md) - Encourages the model to review and refine its own outputs.
- [Reversing Chain of Thought (RCoT)](reversing-chain-of-thought-rcot.md) - Applies chain of thought reasoning in reverse order.
- [Self-Ask](self-ask.md) - Prompts the model to ask and answer its own follow-up questions.
- [Self-Calibration](self-calibration.md) - Helps the model adjust its own confidence and accuracy.
- [Self-Consistency](self-consistency.md) - Generates multiple outputs and selects the most consistent one.
- [Self-Generated In-Context Learning (SG-ICL)](self-generated-in-context-learning-sg-icl.md) - Uses the model to generate its own in-context learning examples.
- [Self-Refine](self-refine.md) - Allows the model to iteratively improve its own outputs.
- [Simulation Theory of Mind (SimToM)](simtom-simulation-theory-of-mind.md) - Simulates different perspectives or thought processes.
- [Skeleton of Thought](skeleton-of-thought.md) - Provides a structural framework for the model's reasoning.
- [System 2 Attention (S2A)](system-2-attention-s2a.md) - Mimics human-like deliberate thinking processes.
- [Universal Self-Adaptive Prompting (USP)](universal-self-adaptive-prompting-usp.md) - Adapts prompts across different tasks and domains.
- [Zero-Shot Chain of Thought (CoT)](zero-shot-chain-of-thought-cot.md) - Applies chain of thought reasoning without specific examples.
- [Zero-Shot Prompting](zero-shot-prompting.md) - Generates answers without any task-specific examples or fine-tuning.