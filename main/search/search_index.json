{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Outlines!","text":"<p>LLMs are powerful but their outputs are unpredictable. Most solutions attempt to fix bad outputs after generation using parsing, regex, or fragile code that breaks easily.</p> <p>Outlines guarantees structured outputs during generation \u2014 directly from any LLM.</p> <ul> <li>Works with any model - Same code runs across OpenAI, Ollama, vLLM, and more</li> <li>Simple integration - Just pass your desired output type: <code>model(prompt, output_type)</code></li> <li>Guaranteed valid structure - No more parsing headaches or broken JSON</li> <li>Provider independence - Switch models without changing code</li> <li>Rich structure definition - Use Json Schema, regular expressions or context-free grammars</li> </ul> <p>Get Started View Examples API Reference GitHub</p>"},{"location":"#building-the-future-of-structured-generation","title":"\ud83d\ude80 Building the future of structured generation","text":"<p>We're working with select partners to develop new interfaces to structured generation.</p> <p>Need XML, FHIR, custom schemas or grammars? Let's talk.</p> <p>Become a design partner</p>"},{"location":"#see-it-in-action","title":"See it in action","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Literal\nimport outlines\nimport openai\n\nclass Customer(BaseModel):\n    name: str\n    urgency: Literal[\"high\", \"medium\", \"low\"]\n    issue: str\n\nclient = openai.OpenAI()\nmodel = outlines.from_openai(client, \"gpt-4o\")\n\ncustomer = model(\n    \"Alice needs help with login issues ASAP\",\n    Customer\n)\n# \u2713 Always returns valid Customer object\n# \u2713 No parsing, no errors, no retries\n</code></pre>"},{"location":"#quick-install","title":"Quick install","text":"<pre><code>pip install outlines\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li> Reliable - Guaranteed schema compliance -- always valid JSON.</li> <li> Feature-rich - Supports a large proportion of the JSON Schema spec, along with regex and context-free grammars.</li> <li> Fast - Microseconds of overhead vs seconds of retries. Compilation happens once, not every request.</li> <li> Simple - Outlines is a low-abstraction library. Write code the way you normally do with LLMs. No agent frameworks needed.</li> </ul>"},{"location":"#supported-inference-apis-libraries-servers","title":"Supported inference APIs, libraries &amp; servers","text":"<ul> <li>vLLM</li> <li>vLLM offline</li> <li>Transformers</li> <li>llama.cpp</li> <li>Ollama</li> <li>MLX-LM</li> <li>SgLang</li> <li>TGI</li> <li>OpenAI</li> <li>Anthropic</li> <li>Gemini</li> <li>Dottxt</li> </ul>"},{"location":"#who-is-using-outlines","title":"Who is using Outlines?","text":"<p>Hundreds of organisations and the main LLM serving frameworks (vLLM, TGI, LoRAX, xinference, SGLang) use Outlines. Prominent companies and organizations that use Outlines include:</p> <p>Organizations are included either because they use Outlines as a dependency in a public repository, or because of direct communication between members of the Outlines team and employees at these organizations.</p> <p>Still not convinced, read what people say about us. And make sure to take a look at what the community is building!</p>"},{"location":"#outlines-people","title":"Outlines people","text":"<p>Outlines would not be what it is today without a community of dedicated developers:</p> <p> </p>"},{"location":"#about-txt","title":"About .txt","text":"<p>Outlines is built with \u2764\ufe0f by .txt.</p> <p>.txt solves the critical problem of reliable structured output generation for large language models. Our commercially-licensed libraries ensure 100% compliance with JSON Schema, regular expressions and context-free grammars while adding only microseconds of latency. Unlike open-source alternatives, we offer superior reliability, performance, and enterprise support.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard. It is now maintained by .txt.</p>"},{"location":"core_concepts/","title":"Core concepts","text":"<p>Coming soon. This will document various concepts at a high level, so users can understand Outlines before diving into specific implementations.</p> <ol> <li>Constrained decoding, tokens, and the basics of logit biasing</li> <li>Different ways to define output structure (regex, JSON schema, Pydantic models, context-free grammars)</li> <li>How finite state machines are used to guarantee output structure</li> <li><code>Generator</code>, <code>Application</code>, <code>Template</code>,</li> <li>Prompt engineering vs. structured generation</li> </ol>"},{"location":"api_reference/","title":"outlines","text":"<p>Outlines is a Generative Model Programming Framework.</p>"},{"location":"api_reference/#outlines.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/#outlines.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/#outlines.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/#outlines.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/#outlines.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/#outlines.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/#outlines.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/#outlines.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/#outlines.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/#outlines.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/#outlines.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/#outlines.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/#outlines.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/#outlines.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/#outlines.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/#outlines.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/#outlines.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/#outlines.applications","title":"<code>applications</code>","text":"<p>Encapsulate a prompt template and an output type into a reusable object.</p>"},{"location":"api_reference/#outlines.applications.Application","title":"<code>Application</code>","text":"<p>Application is a class that encapsulates a prompt template and an output type. It can be called to generate a response by providing a model, the values to be substituted in the template in a dictionary and optional inference parameters.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[Template, Callable]</code> <p>A callable that takes arguments and returns a prompt string.</p> required <code>output_type</code> <code>Any</code> <p>The expected output type of the generated response.</p> <code>None</code> <p>Examples:</p> <pre><code>from pydantic import BaseModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines import models, Application\nfrom outlines.types import JsonType\nfrom outlines.templates import Template\n\nclass OutputModel(BaseModel):\n    result: int\n\nmodel = models.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\ntemplate_string = \"What is 2 times {{ num }}?\"\ntemplate = Template.from_string(template_string)\n\napplication = Application(template, JsonType(OutputModel))\n\nresult = application(model, {\"num\": 3}, max_new_tokens=20)\nprint(result)  # Expected output: { \"result\" : 6 }\n</code></pre> Source code in <code>outlines/applications.py</code> <pre><code>class Application:\n    \"\"\"\n    Application is a class that encapsulates a prompt template and an\n    output type. It can be called to generate a response by providing a\n    model, the values to be substituted in the template in a dictionary\n    and optional inference parameters.\n\n    Parameters\n    ----------\n    template : Union[Template, Callable]\n        A callable that takes arguments and returns a prompt string.\n    output_type : Any\n        The expected output type of the generated response.\n\n    Examples\n    --------\n    ```python\n    from pydantic import BaseModel\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    from outlines import models, Application\n    from outlines.types import JsonType\n    from outlines.templates import Template\n\n    class OutputModel(BaseModel):\n        result: int\n\n    model = models.from_transformers(\n        AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n        AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n    )\n\n    template_string = \"What is 2 times {{ num }}?\"\n    template = Template.from_string(template_string)\n\n    application = Application(template, JsonType(OutputModel))\n\n    result = application(model, {\"num\": 3}, max_new_tokens=20)\n    print(result)  # Expected output: { \"result\" : 6 }\n    ```\n\n    \"\"\"\n    def __init__(\n        self,\n        template: Union[Template, Callable],\n        output_type: Optional[Any] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        template\n            The template to use to build the prompt.\n        output_type\n            The output type provided to the generator.\n\n        \"\"\"\n        self.template = template\n        self.output_type = output_type\n        self.generator: Optional[Union[\n            BlackBoxGenerator, SteerableGenerator\n        ]] = None\n        self.model: Optional[Model] = None\n\n    def __call__(\n        self,\n        model: Model,\n        template_vars: Dict[str, Any],\n        **inference_kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The model to use to generate the response.\n        template_vars\n            The variables to be substituted in the template.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n        Returns\n        -------\n        Any\n            The generated response.\n        \"\"\"\n        if model is None:\n            raise ValueError(\"you must provide a model\")\n        # We save the generator to avoid creating a new one for each call.\n        # If the model has changed since the last call, we create a new\n        # generator.\n        if model != self.model:\n            self.model = model\n            self.generator = Generator(model, self.output_type)  # type: ignore\n\n        prompt = self.template(**template_vars)\n        assert self.generator is not None\n        return self.generator(prompt, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.applications.Application.__call__","title":"<code>__call__(model, template_vars, **inference_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use to generate the response.</p> required <code>template_vars</code> <code>Dict[str, Any]</code> <p>The variables to be substituted in the template.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The generated response.</p> Source code in <code>outlines/applications.py</code> <pre><code>def __call__(\n    self,\n    model: Model,\n    template_vars: Dict[str, Any],\n    **inference_kwargs\n) -&gt; Any:\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The model to use to generate the response.\n    template_vars\n        The variables to be substituted in the template.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n    Returns\n    -------\n    Any\n        The generated response.\n    \"\"\"\n    if model is None:\n        raise ValueError(\"you must provide a model\")\n    # We save the generator to avoid creating a new one for each call.\n    # If the model has changed since the last call, we create a new\n    # generator.\n    if model != self.model:\n        self.model = model\n        self.generator = Generator(model, self.output_type)  # type: ignore\n\n    prompt = self.template(**template_vars)\n    assert self.generator is not None\n    return self.generator(prompt, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.applications.Application.__init__","title":"<code>__init__(template, output_type=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[Template, Callable]</code> <p>The template to use to build the prompt.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided to the generator.</p> <code>None</code> Source code in <code>outlines/applications.py</code> <pre><code>def __init__(\n    self,\n    template: Union[Template, Callable],\n    output_type: Optional[Any] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    template\n        The template to use to build the prompt.\n    output_type\n        The output type provided to the generator.\n\n    \"\"\"\n    self.template = template\n    self.output_type = output_type\n    self.generator: Optional[Union[\n        BlackBoxGenerator, SteerableGenerator\n    ]] = None\n    self.model: Optional[Model] = None\n</code></pre>"},{"location":"api_reference/#outlines.backends","title":"<code>backends</code>","text":"<p>Module to define the backends in charge of creating logits processors.</p>"},{"location":"api_reference/#outlines.backends.BaseBackend","title":"<code>BaseBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all backends.</p> <p>The subclasses must implement methods that create a logits processor from a JSON schema, regex or CFG.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>class BaseBackend(ABC):\n    \"\"\"Base class for all backends.\n\n    The subclasses must implement methods that create a logits processor\n    from a JSON schema, regex or CFG.\n\n    \"\"\"\n\n    @abstractmethod\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.BaseBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.BaseBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.BaseBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.LLGuidanceBackend","title":"<code>LLGuidanceBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for LLGuidance.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceBackend(BaseBackend):\n    \"\"\"Backend for LLGuidance.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import llguidance as llg\n\n        self.llg = llg\n        self.tensor_library_name = model.tensor_library_name\n        self.llg_tokenizer = self._create_llg_tokenizer(model)\n\n    def _create_llg_tokenizer(self, model: SteerableModel) -&gt; \"LLGTokenizer\":\n        \"\"\"Create an llg tokenizer from the Outlines model's tokenizer.\n\n        Parameters\n        ----------\n        model: Model\n            The Outlines model.\n\n        Returns\n        -------\n        LLGTokenizer\n            The llg tokenizer.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(model.hf_tokenizer)\n\n        elif isinstance(model, LlamaCpp):\n            import llama_cpp\n            import llguidance.llamacpp\n\n            vocab = llama_cpp.llama_model_get_vocab(model.model.model)\n            return llguidance.llamacpp.lltokenizer_from_vocab(vocab)\n\n        elif isinstance(model, MLXLM): # pragma: no cover\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(\n                model.mlx_tokenizer._tokenizer\n            )\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported model type: {type(model)}. \"\n                \"Llguidance only supports LlamaCpp, MLXLM \"\n                \"and Transformers models.\"\n            )\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"regex\", regex)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        # We try both lark and ebnf\n        try:\n            grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n        except ValueError:\n            grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/#outlines.backends.LLGuidanceBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import llguidance as llg\n\n    self.llg = llg\n    self.tensor_library_name = model.tensor_library_name\n    self.llg_tokenizer = self._create_llg_tokenizer(model)\n</code></pre>"},{"location":"api_reference/#outlines.backends.LLGuidanceBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    # We try both lark and ebnf\n    try:\n        grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n    except ValueError:\n        grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.LLGuidanceBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.LLGuidanceBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"regex\", regex)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.OutlinesCoreBackend","title":"<code>OutlinesCoreBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreBackend(BaseBackend):\n    \"\"\"Backend for Outlines Core.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            tokenizer = model.tokenizer\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, LlamaCpp):\n            tokenizer = model.tokenizer # type: ignore\n            vocabulary = tokenizer.vocabulary\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer # type: ignore\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        self.eos_token_id = eos_token_id\n        self.vocabulary = self.create_outlines_core_vocabulary(\n            vocabulary, eos_token_id, eos_token, token_to_str\n        )\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ):\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n        return self.get_regex_logits_processor(regex)\n\n    def get_regex_logits_processor(self, regex: str):\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        index = Index(regex, self.vocabulary)\n        return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n\n    def get_cfg_logits_processor(self, grammar):\n        raise NotImplementedError(\n            \"Outlines Core does not support context-free grammar.\"\n        )\n\n    @staticmethod\n    def create_outlines_core_vocabulary(\n        vocab: Dict[str, int],\n        eos_token_id: int,\n        eos_token: str,\n        token_to_str: Callable[[str], str]\n    ) -&gt; Vocabulary:\n        \"\"\"Create an Outlines Core Vocabulary instance.\n\n        Parameters\n        ----------\n        vocab: Dict[str, int]\n            The vocabulary to create an Outlines Core vocabulary from.\n        eos_token_id: int\n            The EOS token ID.\n        eos_token: str\n            The EOS token.\n        token_to_str: Callable[[str], str]\n            The function to convert a token to a string.\n\n        Returns\n        -------\n        Vocabulary\n            The Outlines Core Vocabulary instance.\n\n        \"\"\"\n        formatted_vocab = {}\n        for token, token_id in vocab.items():\n            # This step is necessary to transform special tokens into their\n            # string representation, in particular for spacing. We need those\n            # string representations as outlines core first builds an FSM from\n            # the regex provided that only contains regular strings.\n            token_as_str = token_to_str(token)\n            formatted_vocab[token_as_str] = [token_id]\n        formatted_vocab.pop(eos_token)\n        return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/#outlines.backends.OutlinesCoreBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    if isinstance(model, Transformers):\n        tokenizer = model.tokenizer\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, LlamaCpp):\n        tokenizer = model.tokenizer # type: ignore\n        vocabulary = tokenizer.vocabulary\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer # type: ignore\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n    else: # pragma: no cover\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    self.eos_token_id = eos_token_id\n    self.vocabulary = self.create_outlines_core_vocabulary(\n        vocabulary, eos_token_id, eos_token, token_to_str\n    )\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/#outlines.backends.OutlinesCoreBackend.create_outlines_core_vocabulary","title":"<code>create_outlines_core_vocabulary(vocab, eos_token_id, eos_token, token_to_str)</code>  <code>staticmethod</code>","text":"<p>Create an Outlines Core Vocabulary instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Dict[str, int]</code> <p>The vocabulary to create an Outlines Core vocabulary from.</p> required <code>eos_token_id</code> <code>int</code> <p>The EOS token ID.</p> required <code>eos_token</code> <code>str</code> <p>The EOS token.</p> required <code>token_to_str</code> <code>Callable[[str], str]</code> <p>The function to convert a token to a string.</p> required <p>Returns:</p> Type Description <code>Vocabulary</code> <p>The Outlines Core Vocabulary instance.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>@staticmethod\ndef create_outlines_core_vocabulary(\n    vocab: Dict[str, int],\n    eos_token_id: int,\n    eos_token: str,\n    token_to_str: Callable[[str], str]\n) -&gt; Vocabulary:\n    \"\"\"Create an Outlines Core Vocabulary instance.\n\n    Parameters\n    ----------\n    vocab: Dict[str, int]\n        The vocabulary to create an Outlines Core vocabulary from.\n    eos_token_id: int\n        The EOS token ID.\n    eos_token: str\n        The EOS token.\n    token_to_str: Callable[[str], str]\n        The function to convert a token to a string.\n\n    Returns\n    -------\n    Vocabulary\n        The Outlines Core Vocabulary instance.\n\n    \"\"\"\n    formatted_vocab = {}\n    for token, token_id in vocab.items():\n        # This step is necessary to transform special tokens into their\n        # string representation, in particular for spacing. We need those\n        # string representations as outlines core first builds an FSM from\n        # the regex provided that only contains regular strings.\n        token_as_str = token_to_str(token)\n        formatted_vocab[token_as_str] = [token_id]\n    formatted_vocab.pop(eos_token)\n    return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/#outlines.backends.OutlinesCoreBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n):\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n    return self.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/#outlines.backends.OutlinesCoreBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_regex_logits_processor(self, regex: str):\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    index = Index(regex, self.vocabulary)\n    return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n</code></pre>"},{"location":"api_reference/#outlines.backends.XGrammarBackend","title":"<code>XGrammarBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarBackend(BaseBackend):\n    \"\"\"Backend for XGrammar.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import xgrammar as xgr\n\n        if isinstance(model, Transformers):\n            tokenizer = model.hf_tokenizer\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer._tokenizer\n        else: # pragma: no cover\n            raise ValueError(\n                \"The xgrammar backend only supports Transformers and \"\n                + \"MLXLM models\"\n            )\n\n        tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n            tokenizer,\n            vocab_size=len(tokenizer.get_vocab())\n        )\n        self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_json_schema(\n            json_schema\n        )\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_regex(regex)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/#outlines.backends.XGrammarBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import xgrammar as xgr\n\n    if isinstance(model, Transformers):\n        tokenizer = model.hf_tokenizer\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer._tokenizer\n    else: # pragma: no cover\n        raise ValueError(\n            \"The xgrammar backend only supports Transformers and \"\n            + \"MLXLM models\"\n        )\n\n    tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n        tokenizer,\n        vocab_size=len(tokenizer.get_vocab())\n    )\n    self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/#outlines.backends.XGrammarBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.XGrammarBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_json_schema(\n        json_schema\n    )\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.XGrammarBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_regex(regex)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(backend_name, model, grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_cfg_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    grammar: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or CFG_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_cfg_logits_processor(grammar)\n</code></pre>"},{"location":"api_reference/#outlines.backends.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(backend_name, model, json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_json_schema_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    json_schema: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or JSON_SCHEMA_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_json_schema_logits_processor(json_schema)\n</code></pre>"},{"location":"api_reference/#outlines.backends.get_regex_logits_processor","title":"<code>get_regex_logits_processor(backend_name, model, regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_regex_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    regex: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or REGEX_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/#outlines.backends.base","title":"<code>base</code>","text":"<p>Base class for all backends.</p>"},{"location":"api_reference/#outlines.backends.base.BaseBackend","title":"<code>BaseBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all backends.</p> <p>The subclasses must implement methods that create a logits processor from a JSON schema, regex or CFG.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>class BaseBackend(ABC):\n    \"\"\"Base class for all backends.\n\n    The subclasses must implement methods that create a logits processor\n    from a JSON schema, regex or CFG.\n\n    \"\"\"\n\n    @abstractmethod\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.base.BaseBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.base.BaseBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.base.BaseBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance","title":"<code>llguidance</code>","text":"<p>Backend class for LLGuidance.</p>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceBackend","title":"<code>LLGuidanceBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for LLGuidance.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceBackend(BaseBackend):\n    \"\"\"Backend for LLGuidance.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import llguidance as llg\n\n        self.llg = llg\n        self.tensor_library_name = model.tensor_library_name\n        self.llg_tokenizer = self._create_llg_tokenizer(model)\n\n    def _create_llg_tokenizer(self, model: SteerableModel) -&gt; \"LLGTokenizer\":\n        \"\"\"Create an llg tokenizer from the Outlines model's tokenizer.\n\n        Parameters\n        ----------\n        model: Model\n            The Outlines model.\n\n        Returns\n        -------\n        LLGTokenizer\n            The llg tokenizer.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(model.hf_tokenizer)\n\n        elif isinstance(model, LlamaCpp):\n            import llama_cpp\n            import llguidance.llamacpp\n\n            vocab = llama_cpp.llama_model_get_vocab(model.model.model)\n            return llguidance.llamacpp.lltokenizer_from_vocab(vocab)\n\n        elif isinstance(model, MLXLM): # pragma: no cover\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(\n                model.mlx_tokenizer._tokenizer\n            )\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported model type: {type(model)}. \"\n                \"Llguidance only supports LlamaCpp, MLXLM \"\n                \"and Transformers models.\"\n            )\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"regex\", regex)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        # We try both lark and ebnf\n        try:\n            grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n        except ValueError:\n            grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import llguidance as llg\n\n    self.llg = llg\n    self.tensor_library_name = model.tensor_library_name\n    self.llg_tokenizer = self._create_llg_tokenizer(model)\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    # We try both lark and ebnf\n    try:\n        grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n    except ValueError:\n        grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"regex\", regex)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceLogitsProcessor","title":"<code>LLGuidanceLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits Processor for the LLGuidance backend.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits Processor for the LLGuidance backend.\"\"\"\n\n    def __init__(\n        self,\n        grammar: str,\n        llg_tokenizer,\n        tensor_library_name: str,\n    ) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        grammar: str\n            The grammar spec to use to create the LLMatcher\n        llg_tokenizer: LLTokenizer\n            The LLGuidance tokenizer\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        self.is_first_token = True\n        self.grammar = grammar\n        self.llg_tokenizer = llg_tokenizer\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int) -&gt; None:\n        \"\"\"Setup the LLMatchers, the bitmask and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size of the input\n\n        \"\"\"\n        from llguidance import LLMatcher\n\n        self.ll_matchers = [\n            LLMatcher(self.llg_tokenizer, self.grammar)\n            for _ in range(batch_size)\n        ]\n\n        # we must adapt the bitmask creation and the bias function to the\n        # tensor library used by the model\n        if self.tensor_library_name == \"torch\":\n            import llguidance.torch\n\n            self.bitmask = llguidance.torch.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"numpy\":\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_numpy\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported tensor library: {self.tensor_library_name}\")\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the MLX backend.\"\"\"\n        import llguidance.mlx\n        import llguidance.numpy\n\n        biased_logits_array = []\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            biased_logits = llguidance.mlx.apply_token_bitmask(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Torch backend.\"\"\"\n        import llguidance.torch\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.torch.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                self.tensor_adapter.get_device(logits)\n            )\n            llguidance.torch.apply_token_bitmask_inplace(\n                logits[i], # type: ignore\n                self.bitmask[i]\n            )\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Numpy backend.\"\"\"\n        import llguidance.numpy\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            llguidance.numpy.apply_token_bitmask_inplace(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the instances of LLMatcher to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        if self.is_first_token:\n            self._setup(self.tensor_adapter.shape(input_ids)[0])\n            self.is_first_token = False\n\n        # we do not make the matchers consume the last token during the first\n        # generation step because no tokens have been generated yet\n        else:\n            for i in range(self.tensor_adapter.shape(input_ids)[0]):\n                sequence = input_ids[i] # type: ignore\n                last_token = sequence[-1].item()\n                self.ll_matchers[i].consume_token(last_token)\n                error = self.ll_matchers[i].get_error()\n                if error:\n                    warnings.warn(f\"Error in LLMatcher: {error}\")\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.__init__","title":"<code>__init__(grammar, llg_tokenizer, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The grammar spec to use to create the LLMatcher</p> required <code>llg_tokenizer</code> <p>The LLGuidance tokenizer</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(\n    self,\n    grammar: str,\n    llg_tokenizer,\n    tensor_library_name: str,\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    grammar: str\n        The grammar spec to use to create the LLMatcher\n    llg_tokenizer: LLTokenizer\n        The LLGuidance tokenizer\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    self.is_first_token = True\n    self.grammar = grammar\n    self.llg_tokenizer = llg_tokenizer\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the instances of LLMatcher to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the instances of LLMatcher to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    if self.is_first_token:\n        self._setup(self.tensor_adapter.shape(input_ids)[0])\n        self.is_first_token = False\n\n    # we do not make the matchers consume the last token during the first\n    # generation step because no tokens have been generated yet\n    else:\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            sequence = input_ids[i] # type: ignore\n            last_token = sequence[-1].item()\n            self.ll_matchers[i].consume_token(last_token)\n            error = self.ll_matchers[i].get_error()\n            if error:\n                warnings.warn(f\"Error in LLMatcher: {error}\")\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core","title":"<code>outlines_core</code>","text":"<p>Backend class for Outlines Core.</p>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreBackend","title":"<code>OutlinesCoreBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreBackend(BaseBackend):\n    \"\"\"Backend for Outlines Core.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            tokenizer = model.tokenizer\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, LlamaCpp):\n            tokenizer = model.tokenizer # type: ignore\n            vocabulary = tokenizer.vocabulary\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer # type: ignore\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        self.eos_token_id = eos_token_id\n        self.vocabulary = self.create_outlines_core_vocabulary(\n            vocabulary, eos_token_id, eos_token, token_to_str\n        )\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ):\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n        return self.get_regex_logits_processor(regex)\n\n    def get_regex_logits_processor(self, regex: str):\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        index = Index(regex, self.vocabulary)\n        return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n\n    def get_cfg_logits_processor(self, grammar):\n        raise NotImplementedError(\n            \"Outlines Core does not support context-free grammar.\"\n        )\n\n    @staticmethod\n    def create_outlines_core_vocabulary(\n        vocab: Dict[str, int],\n        eos_token_id: int,\n        eos_token: str,\n        token_to_str: Callable[[str], str]\n    ) -&gt; Vocabulary:\n        \"\"\"Create an Outlines Core Vocabulary instance.\n\n        Parameters\n        ----------\n        vocab: Dict[str, int]\n            The vocabulary to create an Outlines Core vocabulary from.\n        eos_token_id: int\n            The EOS token ID.\n        eos_token: str\n            The EOS token.\n        token_to_str: Callable[[str], str]\n            The function to convert a token to a string.\n\n        Returns\n        -------\n        Vocabulary\n            The Outlines Core Vocabulary instance.\n\n        \"\"\"\n        formatted_vocab = {}\n        for token, token_id in vocab.items():\n            # This step is necessary to transform special tokens into their\n            # string representation, in particular for spacing. We need those\n            # string representations as outlines core first builds an FSM from\n            # the regex provided that only contains regular strings.\n            token_as_str = token_to_str(token)\n            formatted_vocab[token_as_str] = [token_id]\n        formatted_vocab.pop(eos_token)\n        return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    if isinstance(model, Transformers):\n        tokenizer = model.tokenizer\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, LlamaCpp):\n        tokenizer = model.tokenizer # type: ignore\n        vocabulary = tokenizer.vocabulary\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer # type: ignore\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n    else: # pragma: no cover\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    self.eos_token_id = eos_token_id\n    self.vocabulary = self.create_outlines_core_vocabulary(\n        vocabulary, eos_token_id, eos_token, token_to_str\n    )\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreBackend.create_outlines_core_vocabulary","title":"<code>create_outlines_core_vocabulary(vocab, eos_token_id, eos_token, token_to_str)</code>  <code>staticmethod</code>","text":"<p>Create an Outlines Core Vocabulary instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Dict[str, int]</code> <p>The vocabulary to create an Outlines Core vocabulary from.</p> required <code>eos_token_id</code> <code>int</code> <p>The EOS token ID.</p> required <code>eos_token</code> <code>str</code> <p>The EOS token.</p> required <code>token_to_str</code> <code>Callable[[str], str]</code> <p>The function to convert a token to a string.</p> required <p>Returns:</p> Type Description <code>Vocabulary</code> <p>The Outlines Core Vocabulary instance.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>@staticmethod\ndef create_outlines_core_vocabulary(\n    vocab: Dict[str, int],\n    eos_token_id: int,\n    eos_token: str,\n    token_to_str: Callable[[str], str]\n) -&gt; Vocabulary:\n    \"\"\"Create an Outlines Core Vocabulary instance.\n\n    Parameters\n    ----------\n    vocab: Dict[str, int]\n        The vocabulary to create an Outlines Core vocabulary from.\n    eos_token_id: int\n        The EOS token ID.\n    eos_token: str\n        The EOS token.\n    token_to_str: Callable[[str], str]\n        The function to convert a token to a string.\n\n    Returns\n    -------\n    Vocabulary\n        The Outlines Core Vocabulary instance.\n\n    \"\"\"\n    formatted_vocab = {}\n    for token, token_id in vocab.items():\n        # This step is necessary to transform special tokens into their\n        # string representation, in particular for spacing. We need those\n        # string representations as outlines core first builds an FSM from\n        # the regex provided that only contains regular strings.\n        token_as_str = token_to_str(token)\n        formatted_vocab[token_as_str] = [token_id]\n    formatted_vocab.pop(eos_token)\n    return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n):\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n    return self.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_regex_logits_processor(self, regex: str):\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    index = Index(regex, self.vocabulary)\n    return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor","title":"<code>OutlinesCoreLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for Outlines Core.\"\"\"\n\n    def __init__(\n        self, index: Index, tensor_library_name: str\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        index: Index\n            The Outlines Core `Index` instance to use to create the Outlines\n            Core `Guide` instances that will be used to bias the logits\n        tensor_library_name: str\n            The tensor library name to use for the logits processor.\n\n        \"\"\"\n        self.index = index\n        self.tensor_library_name = tensor_library_name\n        self.is_first_token = True\n        super().__init__(tensor_library_name)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the logits processor.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Set the guides, bitmasks and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size and the device\n        of the logits.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size.\n        vocab_size: int\n            The vocabulary size.\n\n        \"\"\"\n        if self.tensor_library_name == \"torch\":\n            from outlines_core.kernels.torch import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_torch\n\n        elif self.tensor_library_name == \"numpy\":\n            from outlines_core.kernels.numpy import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_numpy\n\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            from outlines_core.kernels.mlx import (\n                allocate_token_bitmask\n            )\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_mlx\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._guides = [Guide(self.index) for _ in range(batch_size)]\n        self._bitmasks = [\n            self.allocate_token_bitmask(vocab_size)\n            for _ in range(batch_size)\n        ]\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        from outlines_core.kernels.mlx import (\n            apply_token_bitmask,\n            fill_next_token_bitmask\n        )\n\n        biased_logits_array = []\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            biased_logits = apply_token_bitmask(\n                self.tensor_adapter.unsqueeze(logits[i]), self._bitmasks[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        from outlines_core.kernels.torch import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                self.tensor_adapter.get_device(logits)\n            )\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Numpy tensors.\"\"\"\n        from outlines_core.kernels.numpy import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the guides to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n                # This circumvents issue #227 in outlines_core\n                # Ideally, we would be able to advance all the times as the final\n                # state would accept the eos token leading to itself\n                if (\n                    not self._guides[i].is_finished()\n                    or self._guides[i].accepts_tokens([last_token_id])\n                ):\n                    self._guides[i].advance(\n                        token_id=last_token_id,\n                        return_tokens=False\n                    )\n\n        return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.__init__","title":"<code>__init__(index, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The Outlines Core <code>Index</code> instance to use to create the Outlines Core <code>Guide</code> instances that will be used to bias the logits</p> required <code>tensor_library_name</code> <code>str</code> <p>The tensor library name to use for the logits processor.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(\n    self, index: Index, tensor_library_name: str\n):\n    \"\"\"\n    Parameters\n    ----------\n    index: Index\n        The Outlines Core `Index` instance to use to create the Outlines\n        Core `Guide` instances that will be used to bias the logits\n    tensor_library_name: str\n        The tensor library name to use for the logits processor.\n\n    \"\"\"\n    self.index = index\n    self.tensor_library_name = tensor_library_name\n    self.is_first_token = True\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the guides to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the guides to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n            # This circumvents issue #227 in outlines_core\n            # Ideally, we would be able to advance all the times as the final\n            # state would accept the eos token leading to itself\n            if (\n                not self._guides[i].is_finished()\n                or self._guides[i].accepts_tokens([last_token_id])\n            ):\n                self._guides[i].advance(\n                    token_id=last_token_id,\n                    return_tokens=False\n                )\n\n    return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the logits processor.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar","title":"<code>xgrammar</code>","text":"<p>Backend class for XGrammar.</p>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarBackend","title":"<code>XGrammarBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarBackend(BaseBackend):\n    \"\"\"Backend for XGrammar.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import xgrammar as xgr\n\n        if isinstance(model, Transformers):\n            tokenizer = model.hf_tokenizer\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer._tokenizer\n        else: # pragma: no cover\n            raise ValueError(\n                \"The xgrammar backend only supports Transformers and \"\n                + \"MLXLM models\"\n            )\n\n        tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n            tokenizer,\n            vocab_size=len(tokenizer.get_vocab())\n        )\n        self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_json_schema(\n            json_schema\n        )\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_regex(regex)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import xgrammar as xgr\n\n    if isinstance(model, Transformers):\n        tokenizer = model.hf_tokenizer\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer._tokenizer\n    else: # pragma: no cover\n        raise ValueError(\n            \"The xgrammar backend only supports Transformers and \"\n            + \"MLXLM models\"\n        )\n\n    tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n        tokenizer,\n        vocab_size=len(tokenizer.get_vocab())\n    )\n    self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_json_schema(\n        json_schema\n    )\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_regex(regex)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarLogitsProcessor","title":"<code>XGrammarLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for XGrammar.\"\"\"\n\n    def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n        \"\"\"\n        Parameters\n        ----------\n        compiled_grammar: str\n            The compiled grammar to use to create the logits processor.\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        import xgrammar as xgr\n\n        self.xgr = xgr\n        self.is_first_token = True\n        self.compiled_grammar = compiled_grammar\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Setup the logits processor for a new generation.\"\"\"\n        if self.tensor_library_name == \"torch\":\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._matchers = [\n            self.xgr.GrammarMatcher(self.compiled_grammar)\n            for _ in range(batch_size)\n        ]\n        self._bitmask = self.xgr.allocate_token_bitmask(batch_size, vocab_size)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            self.tensor_adapter.get_device(logits)\n        )\n        self.xgr.apply_token_bitmask_inplace(logits, self._bitmask)\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            \"cpu\"\n        )\n\n        return logits\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        import mlx.core as mx\n        from xgrammar.kernels.apply_token_bitmask_mlx import apply_token_bitmask_mlx\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        biased_logits = apply_token_bitmask_mlx(\n            mx.array(self._bitmask.numpy()), logits, self.tensor_adapter.shape(logits)[1]\n        )\n\n        return biased_logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                if not self._matchers[i].is_terminated(): # pragma: no cover\n                    last_token_id = self.tensor_adapter.to_scalar(\n                        input_ids[i][-1] # type: ignore\n                    )\n                    assert self._matchers[i].accept_token(last_token_id)\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarLogitsProcessor.__init__","title":"<code>__init__(compiled_grammar, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>compiled_grammar</code> <code>str</code> <p>The compiled grammar to use to create the logits processor.</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n    \"\"\"\n    Parameters\n    ----------\n    compiled_grammar: str\n        The compiled grammar to use to create the logits processor.\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    import xgrammar as xgr\n\n    self.xgr = xgr\n    self.is_first_token = True\n    self.compiled_grammar = compiled_grammar\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the XGrammar matchers to bias the logits.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            if not self._matchers[i].is_terminated(): # pragma: no cover\n                last_token_id = self.tensor_adapter.to_scalar(\n                    input_ids[i][-1] # type: ignore\n                )\n                assert self._matchers[i].accept_token(last_token_id)\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/#outlines.backends.xgrammar.XGrammarLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/#outlines.caching","title":"<code>caching</code>","text":"<p>Caching and memoization of function calls.</p>"},{"location":"api_reference/#outlines.caching.cache","title":"<code>cache(expire=None, typed=False, ignore=())</code>","text":"<p>Caching decorator for memoizing function calls.</p> <p>The cache key is created based on the values returned by the key_function callable if provided or based on the arguments of the decorated function directly otherwise</p> <p>This is based on <code>diskcache</code>'s <code>memoize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>expire</code> <code>Optional[float]</code> <p>Seconds until arguments expire.</p> <code>None</code> <code>typed</code> <p>Cache different types separately.</p> <code>False</code> <code>ignore</code> <p>Positional or keyword arguments to ignore.</p> <code>()</code> <p>Returns:</p> Type Description <code>    A decorator function that can be applied to other functions.</code> Source code in <code>outlines/caching.py</code> <pre><code>def cache(expire: Optional[float] = None, typed=False, ignore=()):\n    \"\"\"Caching decorator for memoizing function calls.\n\n    The cache key is created based on the values returned by the key_function callable\n    if provided or based on the arguments of the decorated function directly otherwise\n\n    This is based on `diskcache`'s `memoize`.\n\n    Parameters\n    ----------\n    expire\n        Seconds until arguments expire.\n    typed\n        Cache different types separately.\n    ignore\n        Positional or keyword arguments to ignore.\n\n    Returns\n    -------\n        A decorator function that can be applied to other functions.\n    \"\"\"\n\n    def decorator(cached_function: Callable):\n        memory = get_cache()\n\n        base = (full_name(cached_function),)\n\n        if asyncio.iscoroutinefunction(cached_function):  # pragma: no cover\n\n            async def wrapper(*args, **kwargs):\n                if not _caching_enabled:\n                    return await cached_function(*args, **kwargs)\n\n                cache_key = wrapper.__cache_key__(*args, **kwargs)\n                result = wrapper.__memory__.get(cache_key, default=ENOVAL, retry=True)\n\n                if result is ENOVAL:\n                    result = await cached_function(*args, **kwargs)\n                    wrapper.__memory__.set(cache_key, result, expire, retry=True)\n\n                return result\n\n        else:\n\n            def wrapper(*args, **kwargs):\n                if not _caching_enabled:\n                    return cached_function(*args, **kwargs)\n\n                cache_key = wrapper.__cache_key__(*args, **kwargs)\n                result = wrapper.__memory__.get(cache_key, default=ENOVAL, retry=True)\n\n                if result is ENOVAL:\n                    result = cached_function(*args, **kwargs)\n                    wrapper.__memory__.set(cache_key, result, expire, retry=True)\n\n                return result\n\n        def __cache_key__(*args, **kwargs):\n            \"\"\"Make key for cache given function arguments.\"\"\"\n            return args_to_key(base, args, kwargs, typed, ignore)\n\n        wrapper.__cache_key__ = __cache_key__  # type: ignore\n        wrapper.__memory__ = memory  # type: ignore\n        wrapper.__wrapped__ = cached_function  # type: ignore\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api_reference/#outlines.caching.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Erase the cache completely.</p> Source code in <code>outlines/caching.py</code> <pre><code>def clear_cache():\n    \"\"\"Erase the cache completely.\"\"\"\n    memory = get_cache()\n    memory.clear()\n</code></pre>"},{"location":"api_reference/#outlines.caching.disable_cache","title":"<code>disable_cache()</code>","text":"<p>Disable the cache for this session.</p> <p>Generative models output different results each time they are called when sampling. This can be a desirable property for some workflows, in which case one can call <code>outlines.call.disable</code> to disable the cache for the session.</p> <p>This function does not delete the cache, call <code>outlines.cache.clear</code> instead. It also does not overwrite the cache with the values returned during the session.</p> Example <p><code>outlines.cache.disable</code> should be called right after importing outlines:</p> <p>import outlines.caching as cache cache.disable_cache()</p> Source code in <code>outlines/caching.py</code> <pre><code>def disable_cache():\n    \"\"\"Disable the cache for this session.\n\n    Generative models output different results each time they are called when\n    sampling. This can be a desirable property for some workflows, in which case\n    one can call `outlines.call.disable` to disable the cache for the session.\n\n    This function does not delete the cache, call `outlines.cache.clear`\n    instead. It also does not overwrite the cache with the values returned\n    during the session.\n\n    Example\n    -------\n\n    `outlines.cache.disable` should be called right after importing outlines:\n\n    &gt;&gt;&gt; import outlines.caching as cache\n    &gt;&gt;&gt; cache.disable_cache()\n\n    \"\"\"\n    global _caching_enabled\n    _caching_enabled = False\n</code></pre>"},{"location":"api_reference/#outlines.caching.get_cache","title":"<code>get_cache()</code>  <code>cached</code>","text":"<p>Get the context object that contains previously-computed return values.</p> <p>The cache is used to avoid unnecessary computations and API calls, which can be long and expensive for large models.</p> <p>The cache directory defaults to <code>HOMEDIR/.cache/outlines</code>, but this choice can be overridden by the user by setting the value of the <code>OUTLINES_CACHE_DIR</code> environment variable.</p> Source code in <code>outlines/caching.py</code> <pre><code>@functools.lru_cache(1)\ndef get_cache():\n    \"\"\"Get the context object that contains previously-computed return values.\n\n    The cache is used to avoid unnecessary computations and API calls, which can\n    be long and expensive for large models.\n\n    The cache directory defaults to `HOMEDIR/.cache/outlines`, but this choice\n    can be overridden by the user by setting the value of the `OUTLINES_CACHE_DIR`\n    environment variable.\n\n    \"\"\"\n    from outlines._version import __version__ as outlines_version  # type: ignore\n\n    outlines_cache_dir = os.environ.get(\"OUTLINES_CACHE_DIR\")\n    xdg_cache_home = os.environ.get(\"XDG_CACHE_HOME\")\n    home_dir = os.path.normpath(os.path.expanduser(\"~\"))\n    if outlines_cache_dir:\n        # OUTLINES_CACHE_DIR takes precedence\n        cache_dir = outlines_cache_dir\n    elif xdg_cache_home:  # pragma: no cover\n        cache_dir = os.path.join(xdg_cache_home, \".cache\", \"outlines\")\n    elif home_dir != \"/\": # pragma: no cover\n        cache_dir = os.path.join(home_dir, \".cache\", \"outlines\")\n    else:  # pragma: no cover\n        # home_dir may be / inside a docker container without existing user\n        tempdir = tempfile.gettempdir()\n        cache_dir = os.path.join(tempdir, \".cache\", \"outlines\")\n\n    memory = Cache(\n        cache_dir,\n        eviction_policy=\"none\",\n        cull_limit=0,\n        disk=CloudpickleDisk,\n    )\n\n    # ensure if version upgrade occurs, old cache is pruned\n    if outlines_version != memory.get(\"__version__\"):\n        memory.clear()\n    memory[\"__version__\"] = outlines_version\n\n    return memory\n</code></pre>"},{"location":"api_reference/#outlines.generator","title":"<code>generator</code>","text":"<p>Encapsulate a model and an output type into a reusable object.</p>"},{"location":"api_reference/#outlines.generator.AsyncBlackBoxGenerator","title":"<code>AsyncBlackBoxGenerator</code>","text":"<p>Asynchronous generator for which we don't control constrained generation.</p> <p>The output type provided is not compiled into a logits processor, but is instead directly passed on to the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>class AsyncBlackBoxGenerator:\n    \"\"\"Asynchronous generator for which we don't control constrained\n    generation.\n\n    The output type provided is not compiled into a logits processor, but is\n    instead directly passed on to the model.\n\n    \"\"\"\n    output_type: Optional[Any]\n\n    def __init__(self, model: AsyncBlackBoxModel, output_type: Optional[Any]):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type that will be used to constrain the generation.\n\n        \"\"\"\n        self.model = model\n        self.output_type = output_type\n\n    async def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return await self.model.generate(\n            prompt, self.output_type, **inference_kwargs\n        )\n\n    async def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        return await self.model.generate_batch(\n            prompts, self.output_type, **inference_kwargs\n        )\n\n    async def stream(self, prompt: Any, **inference_kwargs) -&gt; AsyncIterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        async for chunk in self.model.generate_stream(  # pragma: no cover\n            prompt, self.output_type, **inference_kwargs\n        ):\n            yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.generator.AsyncBlackBoxGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return await self.model.generate(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.AsyncBlackBoxGenerator.__init__","title":"<code>__init__(model, output_type)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncBlackBoxModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type that will be used to constrain the generation.</p> required Source code in <code>outlines/generator.py</code> <pre><code>def __init__(self, model: AsyncBlackBoxModel, output_type: Optional[Any]):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type that will be used to constrain the generation.\n\n    \"\"\"\n    self.model = model\n    self.output_type = output_type\n</code></pre>"},{"location":"api_reference/#outlines.generator.AsyncBlackBoxGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    return await self.model.generate_batch(\n        prompts, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.AsyncBlackBoxGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def stream(self, prompt: Any, **inference_kwargs) -&gt; AsyncIterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    async for chunk in self.model.generate_stream(  # pragma: no cover\n        prompt, self.output_type, **inference_kwargs\n    ):\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.generator.BlackBoxGenerator","title":"<code>BlackBoxGenerator</code>","text":"<p>Synchronous generator for which we don't control constrained generation.</p> <p>The output type provided is not compiled into a logits processor, but is instead directly passed on to the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>class BlackBoxGenerator:\n    \"\"\"Synchronous generator for which we don't control constrained\n    generation.\n\n    The output type provided is not compiled into a logits processor, but is\n    instead directly passed on to the model.\n\n    \"\"\"\n    output_type: Optional[Any]\n\n    def __init__(self, model: BlackBoxModel, output_type: Optional[Any]):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type that will be used to constrain the generation.\n\n        \"\"\"\n        self.model = model\n        self.output_type = output_type\n\n    def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return self.model.generate(\n            prompt, self.output_type, **inference_kwargs\n        )\n\n    def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        return self.model.generate_batch(\n            prompts, self.output_type, **inference_kwargs\n        )\n\n    def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return self.model.generate_stream(\n            prompt, self.output_type, **inference_kwargs\n        )\n</code></pre>"},{"location":"api_reference/#outlines.generator.BlackBoxGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return self.model.generate(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.BlackBoxGenerator.__init__","title":"<code>__init__(model, output_type)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>BlackBoxModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type that will be used to constrain the generation.</p> required Source code in <code>outlines/generator.py</code> <pre><code>def __init__(self, model: BlackBoxModel, output_type: Optional[Any]):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type that will be used to constrain the generation.\n\n    \"\"\"\n    self.model = model\n    self.output_type = output_type\n</code></pre>"},{"location":"api_reference/#outlines.generator.BlackBoxGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    return self.model.generate_batch(\n        prompts, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.BlackBoxGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return self.model.generate_stream(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator","title":"<code>SteerableGenerator</code>","text":"<p>Represents a generator for which we control constrained generation.</p> <p>The generator is responsible for building and storing the logits processor (which can be quite expensive to build), and then passing it to the model when the generator is called.</p> <p>The argument defining constrained generation can be of 2 types associated to different methods to create an instance of the generator: - <code>output_type</code> (through <code>__init__</code>): an output type as defined in the   <code>outlines.types</code> module - <code>processor</code> (through <code>from_processor</code>): an already built logits processor    as defined in the <code>outlines.processors</code> module</p> <p>The 2 parameters are mutually exclusive.</p> Source code in <code>outlines/generator.py</code> <pre><code>class SteerableGenerator:\n    \"\"\"Represents a generator for which we control constrained generation.\n\n    The generator is responsible for building and storing the logits processor\n    (which can be quite expensive to build), and then passing it to the model\n    when the generator is called.\n\n    The argument defining constrained generation can be of 2 types associated\n    to different methods to create an instance of the generator:\n    - `output_type` (through `__init__`): an output type as defined in the\n      `outlines.types` module\n    - `processor` (through `from_processor`): an already built logits processor\n       as defined in the `outlines.processors` module\n\n    The 2 parameters are mutually exclusive.\n\n    \"\"\"\n    logits_processor: Optional[LogitsProcessorType]\n\n    def __init__(\n        self,\n        model: SteerableModel,\n        output_type: Optional[Any],\n        backend_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type expressed as a Python type\n        backend_name\n            The name of the backend to use to create the logits processor.\n\n        \"\"\"\n        self.model = model\n        if output_type is None:\n            self.logits_processor = None\n        else:\n            term = python_types_to_terms(output_type)\n            if isinstance(term, CFG):\n                cfg_string = term.definition\n                self.logits_processor = get_cfg_logits_processor(\n                    backend_name,\n                    model,\n                    cfg_string,\n                )\n            elif isinstance(term, JsonSchema):\n                self.logits_processor = get_json_schema_logits_processor(\n                    backend_name,\n                    model,\n                    term.schema,\n                )\n            else:\n                regex_string = to_regex(term)\n                self.logits_processor = get_regex_logits_processor(\n                    backend_name,\n                    model,\n                    regex_string,\n                )\n\n    @classmethod\n    def from_processor(\n        cls, model: SteerableModel, processor: LogitsProcessorType\n    ):\n        \"\"\"Create a generator from a logits processor.\n\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        processor\n            An instance of a logits processor.\n\n        \"\"\"\n        instance = cls.__new__(cls)\n        instance.model = model\n        instance.logits_processor = processor\n\n        return instance\n\n    def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate(\n            prompt, self.logits_processor, **inference_kwargs\n        )\n\n    def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate_batch(\n            prompts, self.logits_processor, **inference_kwargs\n        )\n\n    def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate_stream(\n            prompt, self.logits_processor, **inference_kwargs\n        )\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate(\n        prompt, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator.__init__","title":"<code>__init__(model, output_type, backend_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type expressed as a Python type</p> required <code>backend_name</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor.</p> <code>None</code> Source code in <code>outlines/generator.py</code> <pre><code>def __init__(\n    self,\n    model: SteerableModel,\n    output_type: Optional[Any],\n    backend_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type expressed as a Python type\n    backend_name\n        The name of the backend to use to create the logits processor.\n\n    \"\"\"\n    self.model = model\n    if output_type is None:\n        self.logits_processor = None\n    else:\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            cfg_string = term.definition\n            self.logits_processor = get_cfg_logits_processor(\n                backend_name,\n                model,\n                cfg_string,\n            )\n        elif isinstance(term, JsonSchema):\n            self.logits_processor = get_json_schema_logits_processor(\n                backend_name,\n                model,\n                term.schema,\n            )\n        else:\n            regex_string = to_regex(term)\n            self.logits_processor = get_regex_logits_processor(\n                backend_name,\n                model,\n                regex_string,\n            )\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate_batch(\n        prompts, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator.from_processor","title":"<code>from_processor(model, processor)</code>  <code>classmethod</code>","text":"<p>Create a generator from a logits processor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>An instance of an Outlines model.</p> required <code>processor</code> <code>LogitsProcessorType</code> <p>An instance of a logits processor.</p> required Source code in <code>outlines/generator.py</code> <pre><code>@classmethod\ndef from_processor(\n    cls, model: SteerableModel, processor: LogitsProcessorType\n):\n    \"\"\"Create a generator from a logits processor.\n\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    processor\n        An instance of a logits processor.\n\n    \"\"\"\n    instance = cls.__new__(cls)\n    instance.model = model\n    instance.logits_processor = processor\n\n    return instance\n</code></pre>"},{"location":"api_reference/#outlines.generator.SteerableGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate_stream(\n        prompt, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/#outlines.generator.Generator","title":"<code>Generator(model, output_type=None, backend=None, *, processor=None)</code>","text":"<p>Create a generator for the given model and output parameters.</p> <p>The 2 parameters output_type and processor are mutually exclusive. The parameters processor is only supported for SteerableModel instances (typically local models) and is intended to be only used by advanced users.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Model, AsyncModel]</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type expressed as a Python type or a type defined in the outlines.types.dsl module.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor. Only used for steerable models if there is an output type and <code>processor</code> is not provided.</p> <code>None</code> <code>processor</code> <code>Optional[LogitsProcessorType]</code> <p>An instance of a logits processor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]</code> <p>A generator instance.</p> Source code in <code>outlines/generator.py</code> <pre><code>def Generator(\n    model: Union[Model, AsyncModel],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    *,\n    processor: Optional[LogitsProcessorType] = None,\n) -&gt; Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]:\n    \"\"\"Create a generator for the given model and output parameters.\n\n    The 2 parameters output_type and processor are mutually exclusive. The\n    parameters processor is only supported for SteerableModel instances\n    (typically local models) and is intended to be only used by advanced users.\n\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type expressed as a Python type or a type defined in the\n        outlines.types.dsl module.\n    backend\n        The name of the backend to use to create the logits processor. Only\n        used for steerable models if there is an output type and `processor` is\n        not provided.\n    processor\n        An instance of a logits processor.\n\n    Returns\n    -------\n    Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]\n        A generator instance.\n\n    \"\"\"\n    provided_output_params = sum(\n        param is not None\n        for param in [output_type, processor]\n    )\n    if provided_output_params &gt; 1:\n        raise ValueError(\n            \"At most one of output_type or processor can be provided\"\n        )\n\n    if isinstance(model, SteerableModel): # type: ignore\n        if processor is not None:\n            return SteerableGenerator.from_processor(model, processor) # type: ignore\n        else:\n            return SteerableGenerator(model, output_type, backend) # type: ignore\n    else:\n        if processor is not None:\n            raise NotImplementedError(\n                \"This model does not support logits processors\"\n            )\n        if isinstance(model, AsyncBlackBoxModel): # type: ignore\n            return AsyncBlackBoxGenerator(model, output_type) # type: ignore\n        elif isinstance(model, BlackBoxModel): # type: ignore\n            return BlackBoxGenerator(model, output_type) # type: ignore\n        else:\n            raise ValueError(\n                \"The model argument must be an instance of \"\n                \"SteerableModel, BlackBoxModel or AsyncBlackBoxModel\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.grammars","title":"<code>grammars</code>","text":"<p>A few common Lark grammars.</p>"},{"location":"api_reference/#outlines.grammars.read_grammar","title":"<code>read_grammar(grammar_file_name, base_grammar_path=GRAMMAR_PATH)</code>","text":"<p>Read grammar file from default grammar path.</p> <p>Parameters:</p> Name Type Description Default <code>grammar_file_name</code> <code>str</code> <p>The name of the grammar file to read.</p> required <code>base_grammar_path</code> <code>Path</code> <p>The path to the directory containing the grammar file.</p> <code>GRAMMAR_PATH</code> <p>Returns:</p> Type Description <code>str</code> <p>The contents of the grammar file.</p> Source code in <code>outlines/grammars.py</code> <pre><code>def read_grammar(\n    grammar_file_name: str,\n    base_grammar_path: Path = GRAMMAR_PATH,\n) -&gt; str:\n    \"\"\"Read grammar file from default grammar path.\n\n    Parameters\n    ----------\n    grammar_file_name\n        The name of the grammar file to read.\n    base_grammar_path\n        The path to the directory containing the grammar file.\n\n    Returns\n    -------\n    str\n        The contents of the grammar file.\n\n    \"\"\"\n    full_path = base_grammar_path / grammar_file_name\n    with open(full_path) as file:\n        return file.read()\n</code></pre>"},{"location":"api_reference/#outlines.inputs","title":"<code>inputs</code>","text":"<p>Contain classes used to define the inputs of a model.</p>"},{"location":"api_reference/#outlines.inputs.Audio","title":"<code>Audio</code>  <code>dataclass</code>","text":"<p>Contains an audio that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports audio processing.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Any</code> <p>The audio to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Audio:\n    \"\"\"Contains an audio that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports audio\n    processing.\n\n    Parameters\n    ----------\n    audio\n        The audio to use in the text generation.\n\n    \"\"\"\n    audio: Any\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat","title":"<code>Chat</code>  <code>dataclass</code>","text":"<p>Contains the input for a chat model.</p> <p>Provide an instance of this class as the <code>model_input</code> argument to a model that supports chat.</p> <p>Each message contained in the messages list must be a dict with 'role' and 'content' keys. The role can be 'user', 'assistant', or 'system'. The content supports either: - a text string, - a list containing text and assets (e.g., [\"Describe...\", Image(...)]), - only for HuggingFace transformers models, a list of dict items with explicit types (e.g.,   [{\"type\": \"text\", \"text\": \"Describe...\"}, {\"type\": \"image\", \"image\": Image(...)}])</p> <p>Examples:</p> <pre><code># Initialize the chat with a system message.\nchat_prompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n])\n\n# Add a user message with an image and call the model (not shown here).\nchat_prompt.add_user_message([\"Describe the image below\", Image(image)])\n\n# Add as an assistant message the response from the model.\nchat_prompt.add_assistant_message(\"There is a black cat sitting on a couch.\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>The list of messages that will be provided to the model.</p> <code>None</code> Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Chat:\n    \"\"\"Contains the input for a chat model.\n\n    Provide an instance of this class as the `model_input` argument to a model\n    that supports chat.\n\n    Each message contained in the messages list must be a dict with 'role' and\n    'content' keys. The role can be 'user', 'assistant', or 'system'. The content\n    supports either:\n    - a text string,\n    - a list containing text and assets (e.g., [\"Describe...\", Image(...)]),\n    - only for HuggingFace transformers models, a list of dict items with explicit types (e.g.,\n      [{\"type\": \"text\", \"text\": \"Describe...\"}, {\"type\": \"image\", \"image\": Image(...)}])\n\n    Examples\n    --------\n    ```python\n    # Initialize the chat with a system message.\n    chat_prompt = Chat([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    ])\n\n    # Add a user message with an image and call the model (not shown here).\n    chat_prompt.add_user_message([\"Describe the image below\", Image(image)])\n\n    # Add as an assistant message the response from the model.\n    chat_prompt.add_assistant_message(\"There is a black cat sitting on a couch.\")\n    ```\n\n    Parameters\n    ----------\n    messages\n        The list of messages that will be provided to the model.\n\n    \"\"\"\n    messages: List[Dict[str, Any]] = None # type: ignore\n\n    def __post_init__(self):\n        if self.messages is None:\n            self.messages = []\n\n    def append(self, message: Dict[str, Any]):\n        \"\"\"Add a message to the chat.\n\n        Parameters\n        ----------\n        message\n            The message to add to the chat.\n\n        \"\"\"\n        self.messages.append(message)\n\n    def extend(self, messages: List[Dict[str, Any]]):\n        \"\"\"Add a list of messages to the chat.\n\n        Parameters\n        ----------\n        messages\n            The list of messages to add to the chat.\n\n        \"\"\"\n        self.messages.extend(messages)\n\n    def pop(self) -&gt; Dict[str, Any]:\n        \"\"\"Remove the last message from the chat.\n\n        Returns\n        -------\n        message\n            The removed message.\n\n        \"\"\"\n        return self.messages.pop()\n\n    def add_system_message(self, content: str | List[Any]):\n        \"\"\"Add a system message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the system message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"system\", \"content\": content})\n\n    def add_user_message(self, content: str | List[Any]):\n        \"\"\"Add a user message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the user message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"user\", \"content\": content})\n\n    def add_assistant_message(self, content: str | List[Any]):\n        \"\"\"Add an assistant message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the assistant message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"assistant\", \"content\": content})\n\n    def __str__(self):\n        return \"\\n\".join(str(message) for message in self.messages)\n\n    def __repr__(self):\n        return f\"Chat(messages={self.messages})\"\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.add_assistant_message","title":"<code>add_assistant_message(content)</code>","text":"<p>Add an assistant message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the assistant message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_assistant_message(self, content: str | List[Any]):\n    \"\"\"Add an assistant message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the assistant message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"assistant\", \"content\": content})\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.add_system_message","title":"<code>add_system_message(content)</code>","text":"<p>Add a system message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the system message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_system_message(self, content: str | List[Any]):\n    \"\"\"Add a system message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the system message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"system\", \"content\": content})\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.add_user_message","title":"<code>add_user_message(content)</code>","text":"<p>Add a user message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the user message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_user_message(self, content: str | List[Any]):\n    \"\"\"Add a user message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the user message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"user\", \"content\": content})\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.append","title":"<code>append(message)</code>","text":"<p>Add a message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Dict[str, Any]</code> <p>The message to add to the chat.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def append(self, message: Dict[str, Any]):\n    \"\"\"Add a message to the chat.\n\n    Parameters\n    ----------\n    message\n        The message to add to the chat.\n\n    \"\"\"\n    self.messages.append(message)\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.extend","title":"<code>extend(messages)</code>","text":"<p>Add a list of messages to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>The list of messages to add to the chat.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def extend(self, messages: List[Dict[str, Any]]):\n    \"\"\"Add a list of messages to the chat.\n\n    Parameters\n    ----------\n    messages\n        The list of messages to add to the chat.\n\n    \"\"\"\n    self.messages.extend(messages)\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Chat.pop","title":"<code>pop()</code>","text":"<p>Remove the last message from the chat.</p> <p>Returns:</p> Type Description <code>message</code> <p>The removed message.</p> Source code in <code>outlines/inputs.py</code> <pre><code>def pop(self) -&gt; Dict[str, Any]:\n    \"\"\"Remove the last message from the chat.\n\n    Returns\n    -------\n    message\n        The removed message.\n\n    \"\"\"\n    return self.messages.pop()\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Image","title":"<code>Image</code>  <code>dataclass</code>","text":"<p>Contains an image that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports vision.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Image:\n    \"\"\"Contains an image that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports vision.\n\n    Parameters\n    ----------\n    image\n        The image to use in the text generation.\n\n    \"\"\"\n    image: PILImage.Image\n\n    def __post_init__(self):\n        image = self.image\n\n        if not image.format:\n            raise TypeError(\n                \"Could not read the format of the image passed to the model.\"\n            )\n\n        buffer = BytesIO()\n        image.save(buffer, format=image.format)\n        self.image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        self.image_format = f\"image/{image.format.lower()}\"\n</code></pre>"},{"location":"api_reference/#outlines.inputs.Video","title":"<code>Video</code>  <code>dataclass</code>","text":"<p>Contains a video that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports video processing.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Any</code> <p>The video to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Video:\n    \"\"\"Contains a video that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports video\n    processing.\n\n    Parameters\n    ----------\n    video\n        The video to use in the text generation.\n\n    \"\"\"\n    video: Any\n</code></pre>"},{"location":"api_reference/#outlines.models","title":"<code>models</code>","text":"<p>Module that contains all the models integrated in outlines.</p> <p>We group the models in submodules by provider instead of theme (completion, chat completion, diffusers, etc.) and use routing functions everywhere else in the codebase.</p>"},{"location":"api_reference/#outlines.models.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.models.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/#outlines.models.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/#outlines.models.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.models.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/#outlines.models.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.models.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.models.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/#outlines.models.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.models.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.models.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.models.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/#outlines.models.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.models.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.models.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.models.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/#outlines.models.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.models.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/#outlines.models.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.models.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/#outlines.models.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.models.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.models.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/#outlines.models.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/#outlines.models.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.models.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic","title":"<code>anthropic</code>","text":"<p>Integration with Anthropic's API.</p>"},{"location":"api_reference/#outlines.models.anthropic.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.AnthropicTypeAdapter","title":"<code>AnthropicTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Anthropic</code> model.</p> <p><code>AnthropicTypeAdapter</code> is responsible for preparing the arguments to Anthropic's <code>messages.create</code> method: the input (prompt and possibly image). Anthropic does not support defining the output type, so <code>format_output_type</code> is not implemented.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class AnthropicTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Anthropic` model.\n\n    `AnthropicTypeAdapter` is responsible for preparing the arguments to\n    Anthropic's `messages.create` method: the input (prompt and possibly\n    image).\n    Anthropic does not support defining the output type, so\n    `format_output_type` is not implemented.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\n            \"messages\": [self._create_message(\"user\", model_input)]\n        }\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"messages\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `messages` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"messages\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_content_messages = [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": image.image_format,\n                        \"data\": image.image_str,\n                    },\n                }\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    *image_content_messages,\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(self, output_type):\n        \"\"\"Not implemented for Anthropic.\"\"\"\n        if output_type is None:\n            return {}\n        else:\n            raise NotImplementedError(\n                f\"The output type {output_type} is not available with \"\n                \"Anthropic.\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.AnthropicTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `messages` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"messages\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.AnthropicTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.AnthropicTypeAdapter.format_output_type","title":"<code>format_output_type(output_type)</code>","text":"<p>Not implemented for Anthropic.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def format_output_type(self, output_type):\n    \"\"\"Not implemented for Anthropic.\"\"\"\n    if output_type is None:\n        return {}\n    else:\n        raise NotImplementedError(\n            f\"The output type {output_type} is not available with \"\n            \"Anthropic.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.anthropic.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.base","title":"<code>base</code>","text":"<p>Base classes for all models and model type adapters.</p>"},{"location":"api_reference/#outlines.models.base.AsyncModel","title":"<code>AsyncModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all asynchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class AsyncModel(ABC):\n    \"\"\"Base class for all asynchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    async def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator(\"prompt\")\n        ```\n        and\n        ```python\n        await model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator(model_input, **inference_kwargs)\n\n    async def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        await model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    async def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        async for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        async for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n\n        async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n            yield chunk\n\n    @abstractmethod\n    async def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A coroutine that will produce an async iterator of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator(\"prompt\")\n</code></pre> and <pre><code>await model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator(\"prompt\")\n    ```\n    and\n    ```python\n    await model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>await model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    await model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A coroutine that will produce an async iterator of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A coroutine that will produce an async iterator of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.AsyncModel.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nasync for chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>async for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    async for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    async for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n\n    async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.models.base.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.base.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt","title":"<code>dottxt</code>","text":"<p>Integration with Dottxt's API.</p>"},{"location":"api_reference/#outlines.models.dottxt.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.DottxtTypeAdapter","title":"<code>DottxtTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Dottxt</code> model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class DottxtTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Dottxt` model.\"\"\"\n\n    def format_input(self, model_input: str) -&gt; str:\n        \"\"\"Format the prompt to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The input to pass to the client.\n\n        \"\"\"\n        if isinstance(model_input, str):\n            return model_input\n        raise TypeError(\n            f\"The input type {model_input} is not available with Dottxt. \"\n            \"The only available type is `str`.\"\n        )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        str\n            The output type to pass to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if output_type is None:\n            raise TypeError(\n                \"You must provide an output type. Dottxt only supports \"\n                \"constrained generation.\"\n            )\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Dottxt. \"\n                \"Consider using a local mode instead.\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.DottxtTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Format the prompt to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_input(self, model_input: str) -&gt; str:\n    \"\"\"Format the prompt to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The input to pass to the client.\n\n    \"\"\"\n    if isinstance(model_input, str):\n        return model_input\n    raise TypeError(\n        f\"The input type {model_input} is not available with Dottxt. \"\n        \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.DottxtTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The output type to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    str\n        The output type to pass to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if output_type is None:\n        raise TypeError(\n            \"You must provide an output type. Dottxt only supports \"\n            \"constrained generation.\"\n        )\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Dottxt. \"\n            \"Consider using a local mode instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.dottxt.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini","title":"<code>gemini</code>","text":"<p>Integration with Gemini's API.</p>"},{"location":"api_reference/#outlines.models.gemini.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.GeminiTypeAdapter","title":"<code>GeminiTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Gemini</code> model.</p> <p><code>GeminiTypeAdapter</code> is responsible for preparing the arguments to Gemini's client <code>models.generate_content</code> method: the input (prompt and possibly image), as well as the output type (either JSON or multiple choice).</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class GeminiTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Gemini` model.\n\n    `GeminiTypeAdapter` is responsible for preparing the arguments to Gemini's\n    client `models.generate_content` method: the input (prompt and possibly\n    image), as well as the output type (either JSON or multiple choice).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `contents` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `contents` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Gemini. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\"contents\": [self._create_text_part(model_input)]}\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"contents\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `contents` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"contents\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        # Gemini uses \"model\" instead of \"assistant\"\n        if role == \"assistant\":\n            role = \"model\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"parts\": [self._create_text_part(content)],\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_part(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"parts\": [\n                    self._create_text_part(prompt),\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n        return {\"contents\": [prompt, *image_parts]}\n\n\n    def _create_text_part(self, text: str) -&gt; dict:\n        \"\"\"Create a text input part for a message.\"\"\"\n        return {\n            \"text\": text,\n        }\n\n    def _create_img_part(self, image: Image) -&gt; dict:\n        \"\"\"Create an image input part for a message.\"\"\"\n        return {\n            \"inline_data\": {\n                \"mime_type\": image.image_format,\n                \"data\": image.image_str,\n            }\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `generation_config` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `generation_config` argument to pass to the client.\n\n        \"\"\"\n\n        # Unsupported output pytes\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` \"\n                \"keyword in Json Schema are available with Gemini. Use an \"\n                \"open source model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Gemini. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                JsonSchema.convert_to(\n                    output_type,\n                    [\"dataclass\", \"typeddict\", \"pydantic\"]\n                )\n            )\n\n        # List of structured types\n        elif is_typing_list(output_type):\n            return self.format_list_output_type(output_type)\n\n        # Multiple choice types\n        elif is_enum(output_type):\n            return self.format_enum_output_type(output_type)\n        elif is_literal(output_type):\n            enum = get_enum_from_literal(output_type)\n            return self.format_enum_output_type(enum)\n        elif isinstance(output_type, Choice):\n            enum = get_enum_from_choice(output_type)\n            return self.format_enum_output_type(enum)\n\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Gemini. \"\n                \"Consider using a local model or dottxt instead.\"\n            )\n\n    def format_enum_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"text/x.enum\",\n            \"response_schema\": output_type,\n        }\n\n    def format_json_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"application/json\",\n            \"response_schema\": output_type,\n        }\n\n    def format_list_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        args = get_args(output_type)\n\n        if len(args) == 1:\n            item_type = args[0]\n\n            if JsonSchema.is_json_schema(item_type):\n                return {\n                    \"response_mime_type\": \"application/json\",\n                    \"response_schema\": list[  # type: ignore\n                        JsonSchema.convert_to(\n                            item_type,\n                            [\"dataclass\", \"typeddict\", \"pydantic\"]\n                        )\n                    ],\n                }\n            else:\n                raise TypeError(\n                    \"The list items output type must contain a JSON schema \"\n                    \"type.\"\n                )\n\n        raise TypeError(\n            f\"Gemini only supports homogeneous lists: \"\n            \"list[BaseModel], list[TypedDict] or list[dataclass]. \"\n            f\"Got {output_type} instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.GeminiTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `contents` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"contents\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.GeminiTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>contents</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `contents` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `contents` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Gemini. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.GeminiTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>generation_config</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>generation_config</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `generation_config` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `generation_config` argument to pass to the client.\n\n    \"\"\"\n\n    # Unsupported output pytes\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` \"\n            \"keyword in Json Schema are available with Gemini. Use an \"\n            \"open source model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Gemini. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            JsonSchema.convert_to(\n                output_type,\n                [\"dataclass\", \"typeddict\", \"pydantic\"]\n            )\n        )\n\n    # List of structured types\n    elif is_typing_list(output_type):\n        return self.format_list_output_type(output_type)\n\n    # Multiple choice types\n    elif is_enum(output_type):\n        return self.format_enum_output_type(output_type)\n    elif is_literal(output_type):\n        enum = get_enum_from_literal(output_type)\n        return self.format_enum_output_type(enum)\n    elif isinstance(output_type, Choice):\n        enum = get_enum_from_choice(output_type)\n        return self.format_enum_output_type(enum)\n\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Gemini. \"\n            \"Consider using a local model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.gemini.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp","title":"<code>llamacpp</code>","text":"<p>Integration with the <code>llama-cpp-python</code> library.</p>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCppTokenizer","title":"<code>LlamaCppTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTokenizer(Tokenizer):\n    def __init__(self, model: \"Llama\"):\n        self.tokenizer = model.tokenizer()\n        self.special_tokens: Set[str] = set()\n        self.vocabulary: Dict[str, int] = dict()\n\n        # TODO: Remove when https://github.com/ggerganov/llama.cpp/pull/5613\n        # is resolved\n        self._hf_tokenizer = None\n        if (\n            hasattr(model, \"tokenizer_\")\n            and hasattr(model.tokenizer_, \"hf_tokenizer\")\n        ):\n            self._hf_tokenizer = model.tokenizer_.hf_tokenizer\n            self.eos_token_id = self._hf_tokenizer.eos_token_id\n            self.eos_token = self._hf_tokenizer.eos_token\n            self.vocabulary = self._hf_tokenizer.get_vocab()\n        else:\n            from llama_cpp import (\n                llama_model_get_vocab,\n                llama_token_to_piece,\n            )\n\n            self.eos_token_id = model.token_eos()\n            size = 32\n            buffer = (ctypes.c_char * size)()\n            for i in range(model.n_vocab()):\n                n = llama_token_to_piece(\n                    llama_model_get_vocab(model.model),\n                    i,\n                    buffer,\n                    size,\n                    0,\n                    True\n                )\n                token_piece = buffer[:n].decode(\"utf-8\", errors=\"replace\") # type: ignore\n                self.vocabulary[token_piece] = i\n                if i == self.eos_token_id:\n                    self.eos_token = token_piece\n\n        self.pad_token_id = self.eos_token_id\n        # ensure stable ordering of vocabulary\n        self.vocabulary = {\n            tok: tok_id\n            for tok, tok_id\n            in sorted(self.vocabulary.items(), key=lambda x: x[1])\n        }\n        self._hash = None\n\n    def decode(self, token_ids: List[int]) -&gt; List[str]:\n        decoded_bytes = self.tokenizer.detokenize(token_ids)\n        return [decoded_bytes.decode(\"utf-8\", errors=\"ignore\")]\n\n    def encode(\n        self,\n        prompt: Union[str, List[str]],\n        add_bos: bool = True,\n        special: bool = True,\n    ) -&gt; Tuple[List[int], List[int]]:\n        if isinstance(prompt, list):\n            raise NotImplementedError(\n                \"llama-cpp-python tokenizer doesn't support batch tokenization\"\n            )\n        token_ids = self.tokenizer.tokenize(\n            prompt.encode(\"utf-8\", errors=\"ignore\"),\n            add_bos=add_bos,\n            special=special,\n        )\n        # generate attention mask, missing from llama-cpp-python\n        attention_mask = [\n            1 if token_id != self.pad_token_id else 0 for token_id in token_ids\n        ]\n        return token_ids, attention_mask\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        if self._hf_tokenizer is not None:\n            from transformers.file_utils import SPIECE_UNDERLINE\n\n            token_str = self._hf_tokenizer.convert_tokens_to_string([token])\n            if (\n                token.startswith(SPIECE_UNDERLINE)\n                or token == \"&lt;0x20&gt;\"\n            ):  # pragma: no cover\n                token_str = \" \" + token_str\n            return token_str\n        else:\n            return token\n\n    def __eq__(self, other):\n        if not isinstance(other, LlamaCppTokenizer):\n            return False\n        return self.__getstate__() == other.__getstate__()\n\n    def __hash__(self):\n        # We create a custom hash as pickle.dumps(self) is not stable\n        if self._hash is None:\n            self._hash = hash((\n                tuple(sorted(self.vocabulary.items())),\n                self.eos_token_id,\n                self.eos_token,\n                self.pad_token_id,\n                tuple(sorted(self.special_tokens)),\n            ))\n        return self._hash\n\n    def __getstate__(self):\n        \"\"\"Create a stable representation for outlines.caching\"\"\"\n        return (\n            self.vocabulary,\n            self.eos_token_id,\n            self.eos_token,\n            self.pad_token_id,\n            sorted(self.special_tokens),\n        )\n\n    def __setstate__(self, state):\n        raise NotImplementedError(\"Cannot load a pickled llamacpp tokenizer\")\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCppTokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a stable representation for outlines.caching</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Create a stable representation for outlines.caching\"\"\"\n    return (\n        self.vocabulary,\n        self.eos_token_id,\n        self.eos_token,\n        self.pad_token_id,\n        sorted(self.special_tokens),\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCppTypeAdapter","title":"<code>LlamaCppTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>LlamaCpp</code> model.</p> <p><code>LlamaCppTypeAdapter</code> is responsible for preparing the arguments to the <code>Llama</code> object text generation methods.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `LlamaCpp` model.\n\n    `LlamaCppTypeAdapter` is responsible for preparing the arguments to\n    the `Llama` object text generation methods.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"LlamaCpp. The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; list:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"LlamaCpp does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return  [\n            {\n                \"role\": message[\"role\"],\n                \"content\": message[\"content\"],\n            }\n            for message in model_input.messages\n        ]\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        LogitsProcessorList\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from llama_cpp import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"LlamaCpp. The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>LogitsProcessorList</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    LogitsProcessorList\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from llama_cpp import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/#outlines.models.llamacpp.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral","title":"<code>mistral</code>","text":"<p>Integration with Mistral AI API.</p>"},{"location":"api_reference/#outlines.models.mistral.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter","title":"<code>MistralTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Mistral</code> model.</p> <p>Prepares arguments for Mistral's client <code>chat.complete</code>, <code>chat.complete_async</code>, or <code>chat.stream</code> methods. Handles input (prompt or chat messages) and output type (JSON schema types).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class MistralTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Mistral` model.\n\n    Prepares arguments for Mistral's client `chat.complete`,\n    `chat.complete_async`, or `chat.stream` methods. Handles input (prompt or\n    chat messages) and output type (JSON schema types).\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Format a string input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : str\n            The input string prompt.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=model_input)]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Format a list input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : list\n            The input list, containing a string prompt and optionally Image\n            objects (vision models only).\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=self._create_message_content(model_input))]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a Chat input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : Chat\n            The Chat object containing a list of message dictionaries.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n        messages = []\n\n        for message in model_input.messages:\n            role = message[\"role\"]\n            content = message[\"content\"]\n            if role == \"user\":\n                messages.append(\n                    UserMessage(content=self._create_message_content(content))\n                )\n            elif role == \"assistant\":\n                messages.append(AssistantMessage(content=content))\n            elif role == \"system\":\n                messages.append(SystemMessage(content=content))\n            else:\n                raise ValueError(f\"Unsupported role: {role}\")\n\n        return messages\n\n    def _create_message_content(\n        self, content: Union[str, list]\n    ) -&gt; Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]:\n        \"\"\"Create message content from an input.\n\n        Parameters\n        ----------\n        content : Union[str, list]\n            The content to format, either a string or a list containing a\n            string and optionally Image objects.\n\n        Returns\n        -------\n        Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]\n            The formatted content, either a string or a list of content parts\n            (text and image URLs).\n\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list):\n            if not content:\n                raise ValueError(\"Content list cannot be empty.\")\n            if not isinstance(content[0], str):\n                raise ValueError(\n                    \"The first item in the list should be a string.\"\n                )\n            if len(content) == 1:\n                return content[0]\n            content_parts: List[Dict[str, Union[str, Dict[str, str]]]] = [\n                {\"type\": \"text\", \"text\": content[0]}\n            ]\n            for item in content[1:]:\n                if isinstance(item, Image):\n                    data_url = f\"data:{item.image_format};base64,{item.image_str}\"\n                    content_parts.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": data_url}\n                    })\n                else:\n                    raise ValueError(\n                        f\"Invalid item type in content list: {type(item)}. \"\n                        + \"Expected Image objects after the first string.\"\n                    )\n            return content_parts\n        else:\n            raise TypeError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"Content must be a string or a list starting with a string \"\n                + \"followed by optional Image objects.\"\n            )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type : Optional[Any]\n            The desired output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `response_format` dict to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif is_pydantic_model(output_type):\n            schema = output_type.model_json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_dataclass(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_typed_dict(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_genson_schema_builder(output_type):\n            schema = json.loads(output_type.to_json())\n            return self.format_json_schema_type(schema)\n        elif isinstance(output_type, JsonSchema):\n            return self.format_json_schema_type(json.loads(output_type.schema))\n\n        # Json mode\n        elif is_native_dict(output_type):\n            return {\"type\": \"json_object\"}\n\n        # Unsupported types\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not available with \"\n                \"Mistral.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Mistral.\"\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", str(output_type))\n            raise TypeError(\n                f\"The type {type_name} is not available with Mistral.\"\n            )\n\n    def format_json_schema_type(\n        self, schema: dict, schema_name: str = \"default\"\n    ) -&gt; dict:\n        \"\"\"Create the `response_format` argument to pass to the client from a\n        JSON schema dictionary.\n\n        Parameters\n        ----------\n        schema : dict\n            The JSON schema to format.\n        schema_name : str\n            The name of the schema.\n\n        Returns\n        -------\n        dict\n            The value of the `response_format` argument to pass to the client.\n\n        \"\"\"\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"schema\": schema,\n                \"name\": schema_name.lower(),\n                \"strict\": True\n            }\n        }\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Format a Chat input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat</code> <p>The Chat object containing a list of message dictionaries.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a Chat input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : Chat\n        The Chat object containing a list of message dictionaries.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n    messages = []\n\n    for message in model_input.messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        if role == \"user\":\n            messages.append(\n                UserMessage(content=self._create_message_content(content))\n            )\n        elif role == \"assistant\":\n            messages.append(AssistantMessage(content=content))\n        elif role == \"system\":\n            messages.append(SystemMessage(content=content))\n        else:\n            raise ValueError(f\"Unsupported role: {role}\")\n\n    return messages\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_json_schema_type","title":"<code>format_json_schema_type(schema, schema_name='default')</code>","text":"<p>Create the <code>response_format</code> argument to pass to the client from a JSON schema dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to format.</p> required <code>schema_name</code> <code>str</code> <p>The name of the schema.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The value of the <code>response_format</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_json_schema_type(\n    self, schema: dict, schema_name: str = \"default\"\n) -&gt; dict:\n    \"\"\"Create the `response_format` argument to pass to the client from a\n    JSON schema dictionary.\n\n    Parameters\n    ----------\n    schema : dict\n        The JSON schema to format.\n    schema_name : str\n        The name of the schema.\n\n    Returns\n    -------\n    dict\n        The value of the `response_format` argument to pass to the client.\n\n    \"\"\"\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"schema\": schema,\n            \"name\": schema_name.lower(),\n            \"strict\": True\n        }\n    }\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Format a list input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list</code> <p>The input list, containing a string prompt and optionally Image objects (vision models only).</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Format a list input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : list\n        The input list, containing a string prompt and optionally Image\n        objects (vision models only).\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=self._create_message_content(model_input))]\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The desired output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>response_format</code> dict to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type : Optional[Any]\n        The desired output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `response_format` dict to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif is_pydantic_model(output_type):\n        schema = output_type.model_json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_dataclass(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_typed_dict(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_genson_schema_builder(output_type):\n        schema = json.loads(output_type.to_json())\n        return self.format_json_schema_type(schema)\n    elif isinstance(output_type, JsonSchema):\n        return self.format_json_schema_type(json.loads(output_type.schema))\n\n    # Json mode\n    elif is_native_dict(output_type):\n        return {\"type\": \"json_object\"}\n\n    # Unsupported types\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not available with \"\n            \"Mistral.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Mistral.\"\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", str(output_type))\n        raise TypeError(\n            f\"The type {type_name} is not available with Mistral.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.MistralTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Format a string input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input string prompt.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Format a string input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : str\n        The input string prompt.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=model_input)]\n</code></pre>"},{"location":"api_reference/#outlines.models.mistral.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm","title":"<code>mlxlm</code>","text":"<p>Integration with the <code>mlx_lm</code> library.</p>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLMTypeAdapter","title":"<code>MLXLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>MLXLM</code> model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `MLXLM` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"mlx-lm. The available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str):\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"mlx-lm does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[list[OutlinesLogitsProcessor]]\n            The logits processor argument to be passed to the model.\n\n        \"\"\"\n        if not output_type:\n            return None\n        return [output_type]\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"mlx-lm. The available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.MLXLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[list[OutlinesLogitsProcessor]]</code> <p>The logits processor argument to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[list[OutlinesLogitsProcessor]]\n        The logits processor argument to be passed to the model.\n\n    \"\"\"\n    if not output_type:\n        return None\n    return [output_type]\n</code></pre>"},{"location":"api_reference/#outlines.models.mlxlm.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama","title":"<code>ollama</code>","text":"<p>Integration with the <code>ollama</code> library.</p>"},{"location":"api_reference/#outlines.models.ollama.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter","title":"<code>OllamaTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Ollama</code> model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class OllamaTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Ollama` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The formatted value of the `messages` argument to be passed to\n            the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            return {\n                \"role\": role,\n                \"content\": prompt,\n                \"image\": [image.image_str for image in images],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(\n        self, output_type: Optional[Any] = None\n    ) -&gt; Optional[dict]:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Optional[str]\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        if output_type is None:\n            return None\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Ollama. \"\n                \"Consider using a local model instead.\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted value of the <code>messages</code> argument to be passed to the client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The formatted value of the `messages` argument to be passed to\n        the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[Any] = None\n) -&gt; Optional[dict]:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Optional[str]\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    if output_type is None:\n        return None\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Ollama. \"\n            \"Consider using a local model instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.OllamaTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.ollama.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.openai","title":"<code>openai</code>","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api_reference/#outlines.models.openai.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter","title":"<code>OpenAITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>OpenAI</code> model.</p> <p><code>OpenAITypeAdapter</code> is responsible for preparing the arguments to OpenAI's <code>completions.create</code> methods: the input (prompt and possibly image), as well as the output type (only JSON).</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `OpenAI` model.\n\n    `OpenAITypeAdapter` is responsible for preparing the arguments to OpenAI's\n    `completions.create` methods: the input (prompt and possibly image), as\n    well as the output type (only JSON).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_content(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def _create_img_content(self, image: Image) -&gt; dict:\n        \"\"\"Create the content for an image input.\"\"\"\n        return {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:{image.image_format};base64,{image.image_str}\"  # noqa: E702\n            },\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client based on the\n        output type specified by the user.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` keyword \"\n                \"in Json Schema are available with OpenAI. Use an open source \"\n                \"model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n        elif is_native_dict(output_type):\n            return self.format_json_mode_type()\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n    def format_json_output_type(self, schema: dict) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified a `Json` output type.\n\n        \"\"\"\n        # OpenAI requires `additionalProperties` to be set to False\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"response_format\": {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"name\": \"default\",\n                    \"strict\": True,\n                    \"schema\": schema,\n                },\n            }\n        }\n\n    def format_json_mode_type(self) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified the output type should be a JSON but without specifying the\n        schema (also called \"JSON mode\").\n\n        \"\"\"\n        return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_json_mode_type","title":"<code>format_json_mode_type()</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified the output type should be a JSON but without specifying the schema (also called \"JSON mode\").</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_mode_type(self) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified the output type should be a JSON but without specifying the\n    schema (also called \"JSON mode\").\n\n    \"\"\"\n    return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_json_output_type","title":"<code>format_json_output_type(schema)</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified a <code>Json</code> output type.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_output_type(self, schema: dict) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified a `Json` output type.\n\n    \"\"\"\n    # OpenAI requires `additionalProperties` to be set to False\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"default\",\n                \"strict\": True,\n                \"schema\": schema,\n            },\n        }\n    }\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to the client based on the output type specified by the user.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client based on the\n    output type specified by the user.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` keyword \"\n            \"in Json Schema are available with OpenAI. Use an open source \"\n            \"model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n    elif is_native_dict(output_type):\n        return self.format_json_mode_type()\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.OpenAITypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/#outlines.models.openai.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang","title":"<code>sglang</code>","text":"<p>Integration with an SGLang server.</p>"},{"location":"api_reference/#outlines.models.sglang.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLangTypeAdapter","title":"<code>SGLangTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>SGLang</code> and <code>AsyncSGLang</code> models.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLangTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `SGLang` and `AsyncSGLang` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the sglang\n        server expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            warnings.warn(\n                \"SGLang grammar-based structured outputs expects an EBNF \"\n                \"grammar instead of a Lark grammar as is generally used in \"\n                \"Outlines. The grammar cannot be used as a structured output \"\n                \"type with an outlines backend, it is only compatible with \"\n                \"the sglang and llguidance backends.\"\n            )\n            return {\"extra_body\": {\"ebnf\": term.definition}}\n        elif isinstance(term, JsonSchema):\n            return OpenAITypeAdapter().format_json_output_type(\n                json.loads(term.schema)\n            )\n        else:\n            return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLangTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the sglang server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the sglang\n    server expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.SGLangTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        warnings.warn(\n            \"SGLang grammar-based structured outputs expects an EBNF \"\n            \"grammar instead of a Lark grammar as is generally used in \"\n            \"Outlines. The grammar cannot be used as a structured output \"\n            \"type with an outlines backend, it is only compatible with \"\n            \"the sglang and llguidance backends.\"\n        )\n        return {\"extra_body\": {\"ebnf\": term.definition}}\n    elif isinstance(term, JsonSchema):\n        return OpenAITypeAdapter().format_json_output_type(\n            json.loads(term.schema)\n        )\n    else:\n        return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/#outlines.models.sglang.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi","title":"<code>tgi</code>","text":"<p>Integration with a TGI server.</p>"},{"location":"api_reference/#outlines.models.tgi.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGITypeAdapter","title":"<code>TGITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>TGI</code> and <code>AsyncTGI</code> models.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `TGI` and `AsyncTGI` models.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the client.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {input} is not available with TGI. \"\n            + \"The only available type is `str`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Argument\n        --------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            raise NotImplementedError(\n                \"TGI does not support CFG-based structured outputs.\"\n            )\n        elif isinstance(term, JsonSchema):\n            return {\n                \"grammar\": {\n                    \"type\": \"json\",\n                    \"value\": json.loads(term.schema),\n                }\n            }\n        else:\n            return {\n                \"grammar\": {\n                    \"type\": \"regex\",\n                    \"value\": to_regex(term),\n                }\n            }\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the client.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the client.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {input} is not available with TGI. \"\n        + \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.TGITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> Argument <p>output_type     The structured output type provided.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Argument\n    --------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        raise NotImplementedError(\n            \"TGI does not support CFG-based structured outputs.\"\n        )\n    elif isinstance(term, JsonSchema):\n        return {\n            \"grammar\": {\n                \"type\": \"json\",\n                \"value\": json.loads(term.schema),\n            }\n        }\n    else:\n        return {\n            \"grammar\": {\n                \"type\": \"regex\",\n                \"value\": to_regex(term),\n            }\n        }\n</code></pre>"},{"location":"api_reference/#outlines.models.tgi.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.tokenizer","title":"<code>tokenizer</code>","text":""},{"location":"api_reference/#outlines.models.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Hashable</code>, <code>Protocol</code></p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>class Tokenizer(Hashable, Protocol):\n    eos_token: str\n    eos_token_id: int\n    pad_token_id: int\n    vocabulary: Dict[str, int]\n    special_tokens: Set[str]\n\n    def encode(\n        self, prompt: Union[str, List[str]]\n    ) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n        \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n        ...\n\n    def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n        \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n        ...\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        \"\"\"Convert a token to its equivalent string.\n\n        This is for instance useful for BPE tokenizers where whitespaces are\n        represented by the special characted `\u0120`. This prevents matching a raw\n        token that includes `\u0120` with a string.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.models.tokenizer.Tokenizer.convert_token_to_string","title":"<code>convert_token_to_string(token)</code>","text":"<p>Convert a token to its equivalent string.</p> <p>This is for instance useful for BPE tokenizers where whitespaces are represented by the special characted <code>\u0120</code>. This prevents matching a raw token that includes <code>\u0120</code> with a string.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def convert_token_to_string(self, token: str) -&gt; str:\n    \"\"\"Convert a token to its equivalent string.\n\n    This is for instance useful for BPE tokenizers where whitespaces are\n    represented by the special characted `\u0120`. This prevents matching a raw\n    token that includes `\u0120` with a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids)</code>","text":"<p>Translate an array of token ids to a string or list of strings.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n    \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.tokenizer.Tokenizer.encode","title":"<code>encode(prompt)</code>","text":"<p>Translate the input prompts into arrays of token ids and attention mask.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def encode(\n    self, prompt: Union[str, List[str]]\n) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n    \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers","title":"<code>transformers</code>","text":"<p>Integration with the <code>transformers</code> library.</p>"},{"location":"api_reference/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersMultiModalTypeAdapter","title":"<code>TransformersMultiModalTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for <code>TransformersMultiModal</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModalTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for `TransformersMultiModal` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Fomat the prompt arguments to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available. Please \"\n            + \"provide a list containing a text prompt and assets \"\n            + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n            + \"model or a `Chat` instance.\"\n        )\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; dict:\n        conversation = []\n        assets = []\n\n        # process each message, convert if needed to standardized multimodal chat template format\n        # and collect assets for HF processor\n        for message in model_input.messages:\n            processed_message, message_assets = self._prepare_message(\n                message[\"role\"], message[\"content\"]\n            )\n            conversation.append(processed_message)\n            assets.extend(message_assets)\n\n        formatted_prompt = self.tokenizer.apply_chat_template(\n            conversation,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        # use the formatted prompt and the assets to format the input\n        return self.format_list_input([formatted_prompt, *assets])\n\n    def _prepare_message(self, role: str, content: str | list) -&gt; tuple[dict, list]:\n        \"\"\"Create a message.\"\"\"\n        if isinstance(content, str):\n            return {\"role\": role, \"content\": content}, []\n\n        elif isinstance(content, list):\n            if all(isinstance(item, dict) for item in content): # HF multimodal chat template\n                return {\"role\": role, \"content\": content}, self._extract_assets_from_content(content)\n            else: # list of string + assets\n                prompt = content[0]\n                assets = content[1:]\n                assets_dict = [self._format_asset_for_template(asset) for asset in assets]\n\n                return {\"role\": role, \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *assets_dict\n                ]}, assets\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"The content must be a string or a list containing text and assets \"\n                + \"or a list of dict items with explicit types.\"\n            )\n\n    def _extract_assets_from_content(self, content: list) -&gt; list:\n        \"\"\"Process a list of dict items.\"\"\"\n        assets = []\n\n        for item in content:\n            if len(item) &gt; 2:\n                raise ValueError(\n                    f\"Found item with multiple keys: {item}. \"\n                    + \"Each item in the content list must be a dictionary with a 'type' key and a single asset key. \"\n                    + \"To include multiple assets, use separate dictionary items. \"\n                    + \"For example: [{{'type': 'image', 'image': image1}}, {{'type': 'image', 'image': image2}}]. \"\n                )\n\n            if \"type\" not in item:\n                raise ValueError(\n                    \"Each item in the content list must be a dictionary with a 'type' key. \"\n                    + \"Valid types are 'text', 'image', 'video', or 'audio'. \"\n                    + \"For instance {{'type': 'text', 'text': 'your message'}}. \"\n                    + f\"Found item without 'type' key: {item}\"\n                )\n            if item[\"type\"] == \"text\":\n                continue\n            elif item[\"type\"] in [\"image\", \"video\", \"audio\"]:\n                asset_key = item[\"type\"]\n                if asset_key not in item:\n                    raise ValueError(\n                        f\"Item with type '{asset_key}' must contain a '{asset_key}' key. \"\n                        + f\"Found item: {item}\"\n                    )\n                if isinstance(item[asset_key], (Image, Video, Audio)):\n                    assets.append(item[asset_key])\n                else:\n                    raise ValueError(\n                        \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                        + f\"Unsupported asset type: {type(item[asset_key])}\"\n                    )\n            else:\n                raise ValueError(\n                    \"Content must be 'text', 'image', 'video' or 'audio'. \"\n                    + f\"Unsupported content type: {item['type']}\")\n        return assets\n\n    def _format_asset_for_template(self, asset: Image | Video | Audio) -&gt; dict:\n        \"\"\"Process an asset.\"\"\"\n        if isinstance(asset, Image):\n            return {\"type\": \"image\", \"image\": asset}\n        elif isinstance(asset, Video):\n            return {\"type\": \"video\", \"video\": asset}\n        elif isinstance(asset, Audio):\n            return {\"type\": \"audio\", \"audio\": asset}\n        else:\n            raise ValueError(\n                \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                + f\"Unsupported asset type: {type(asset)}\"\n            )\n\n    @format_input.register(list)\n    def format_list_input(self, model_input: list) -&gt; dict:\n        prompt = model_input[0]\n        assets = model_input[1:]\n\n        if not assets:  # handle empty assets case\n            return {\"text\": prompt}\n\n        asset_types = set(type(asset) for asset in assets)\n        if len(asset_types) &gt; 1:\n            raise ValueError(\n                \"All assets must be of the same type. \"\n                + f\"Found types: {asset_types}\"\n            )\n        asset_type = asset_types.pop()\n\n        if asset_type == Image:\n            return {\n                \"text\": prompt,\n                \"images\": [asset.image for asset in assets]\n            }\n        elif asset_type == Audio: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"audio\": [asset.audio for asset in assets]\n            }\n        elif asset_type == Video: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"videos\": [asset.video for asset in assets]\n            }\n        else:\n            raise ValueError(f\"Unsupported asset type: {asset_type}\")\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Argument\n        --------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Fomat the prompt arguments to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Fomat the prompt arguments to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available. Please \"\n        + \"provide a list containing a text prompt and assets \"\n        + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n        + \"model or a `Chat` instance.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> Argument <p>output_type     The logits processor provided.</p> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Argument\n    --------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersTypeAdapter","title":"<code>TransformersTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Transformers` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available.\"\n            \"The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available.\"\n        \"The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.TransformersTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#outlines.models.utils.set_additional_properties_false_json_schema","title":"<code>set_additional_properties_false_json_schema(schema)</code>","text":"<p>Set additionalProperties to False to all objects in the schema using jsonpath.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to modify</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The modified schema with additionalProperties set to False</p> Source code in <code>outlines/models/utils.py</code> <pre><code>def set_additional_properties_false_json_schema(schema: dict) -&gt; dict:\n    \"\"\"Set additionalProperties to False to all objects in the schema using jsonpath.\n\n    Parameters\n    ----------\n    schema\n        The JSON schema to modify\n\n    Returns\n    -------\n    dict\n        The modified schema with additionalProperties set to False\n    \"\"\"\n    # Get all nodes\n    jsonpath_expr = jsonpath_ng.parse('$..*')\n    matches = jsonpath_expr.find(schema)\n\n    # Go over all nodes and set additionalProperties to False if it's an object\n    for match in matches:\n        if match.value == 'object':\n            if 'additionalProperties' not in match.context.value:\n                match.context.value['additionalProperties'] = False\n\n    return schema\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm","title":"<code>vllm</code>","text":"<p>Integration with a vLLM server.</p>"},{"location":"api_reference/#outlines.models.vllm.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLMTypeAdapter","title":"<code>VLLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLM</code> and <code>AsyncVLLM</code> models.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLM` and `AsyncVLLM` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the vLLM server\n        expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the model.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"guided_grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            extra_body = {\"guided_json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n            return extra_body\n        else:\n            return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the vLLM server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the vLLM server\n    expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.VLLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the model.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"guided_grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        extra_body = {\"guided_json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n        return extra_body\n    else:\n        return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline","title":"<code>vllm_offline</code>","text":"<p>Integration with the <code>vllm</code> library (offline mode).</p>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter","title":"<code>VLLMOfflineTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLMOffline</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOfflineTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLMOffline` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"VLLM offline. The only available types are `str` and \"\n            \"`Chat` (containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_input_str(self, model_input: str) -&gt; str:\n        \"\"\"Format a `str` input.\n\n        \"\"\"\n        return model_input\n\n    @format_input.register(Chat)\n    def format_input_chat(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a `Chat` input.\n\n        \"\"\"\n        for message in model_input.messages:\n            content = message[\"content\"]\n            if isinstance(content, list):\n                raise ValueError(\n                    \"Assets are not supported for vLLM offline.\"\n                    \"Please only use text content in the `Chat` input.\"\n                )\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the model.\n\n        For vLLM, the structured output definition is set in the\n        `GuidedDecodingParams` constructor that is provided as a value to the\n        `guided_decoding` parameter of the `SamplingParams` constructor, itself\n        provided as a value to the `sampling_params` parameter of the `generate`\n        method.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The arguments to provide to the `GuidedDecodingParams` constructor.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            guided_decoding_params = {\"json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n            return guided_decoding_params\n        else:\n            return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"VLLM offline. The only available types are `str` and \"\n        \"`Chat` (containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_chat","title":"<code>format_input_chat(model_input)</code>","text":"<p>Format a <code>Chat</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(Chat)\ndef format_input_chat(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a `Chat` input.\n\n    \"\"\"\n    for message in model_input.messages:\n        content = message[\"content\"]\n        if isinstance(content, list):\n            raise ValueError(\n                \"Assets are not supported for vLLM offline.\"\n                \"Please only use text content in the `Chat` input.\"\n            )\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_str","title":"<code>format_input_str(model_input)</code>","text":"<p>Format a <code>str</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(str)\ndef format_input_str(self, model_input: str) -&gt; str:\n    \"\"\"Format a `str` input.\n\n    \"\"\"\n    return model_input\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the model.</p> <p>For vLLM, the structured output definition is set in the <code>GuidedDecodingParams</code> constructor that is provided as a value to the <code>guided_decoding</code> parameter of the <code>SamplingParams</code> constructor, itself provided as a value to the <code>sampling_params</code> parameter of the <code>generate</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments to provide to the <code>GuidedDecodingParams</code> constructor.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the model.\n\n    For vLLM, the structured output definition is set in the\n    `GuidedDecodingParams` constructor that is provided as a value to the\n    `guided_decoding` parameter of the `SamplingParams` constructor, itself\n    provided as a value to the `sampling_params` parameter of the `generate`\n    method.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The arguments to provide to the `GuidedDecodingParams` constructor.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        guided_decoding_params = {\"json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n        return guided_decoding_params\n    else:\n        return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/#outlines.models.vllm_offline.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/#outlines.processors","title":"<code>processors</code>","text":"<p>Processors to control generation in steerable models.</p>"},{"location":"api_reference/#outlines.processors.OutlinesLogitsProcessor","title":"<code>OutlinesLogitsProcessor</code>","text":"<p>Base class for logits processors. This class implements a shared <code>__call__</code> method is called by the models and returns the processed logits. It relies on the <code>process_logits</code> method that must be implemented by the subclasses to do the actual processing. The <code>tensor_adapter</code> attribute, created at initialization based on the tensor library name specified in the constructor, is used to manipulate the tensors using the appropriate library for the model (numpy, torch...).</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>class OutlinesLogitsProcessor:\n    \"\"\"Base class for logits processors.\n    This class implements a shared `__call__` method is called by the models\n    and returns the processed logits. It relies on the `process_logits` method\n    that must be implemented by the subclasses to do the actual processing. The\n    `tensor_adapter` attribute, created at initialization based on the\n    tensor library name specified in the constructor, is used to manipulate the\n    tensors using the appropriate library for the model (numpy, torch...).\n    \"\"\"\n    tensor_adapter: TensorAdapterImplementation\n\n    def __init__(self, tensor_library_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        tensor_library_name\n            The name of the library to use to manipulate tensors. Possible\n            values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n            that your model is using.\n        \"\"\"\n        # Temporary fix as torch raises a warning that can cause can an error\n        # with python 3.12.\n        if tensor_library_name == \"torch\":\n            import torch._dynamo\n\n            torch._dynamo.config.suppress_errors = True\n\n        tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n        if tensor_adapter_class is None:\n            raise NotImplementedError(\n                f\"Library {tensor_library_name} is not available\"\n            )\n        self.tensor_adapter = tensor_adapter_class()  # type: ignore\n\n    def reset(self):\n        \"\"\"Reset the logits processor for a new generation\n\n        Only implement this method in subclasses if the logits processor\n        needs to be reset for a new generation.\n\n        \"\"\"\n        pass # pragma: no cover\n\n    @abstractmethod\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Main method to implement for logits processors subclasses.\n        This method applies a mask on the logits to bias the generation.\n        It is called by the `__call__` method that standardizes the shape of\n        `input_ids` and `logits` to ensure they are 2D tensors.\n        Elements to keep in mind when designing universal logits processors:\n        - logits processors are only used once and never re-applied for a new\n        sequence generator\n        - Some models only pass output_ids, some models such as llamacpp and\n        transformers prefix with input_ids\n        - Some sampling methods, such as beam search, result in unstable\n        sequence ordering in models like vLLM\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a 2D tensor.\n        logits\n            The logits for the current generation step in a 2D tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a 2D tensor.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Entrypoint for logits processors, this is the method that is\n        called by the model.\n        Because different models use different structures to store the\n        input_ids and logits, we standardize their format to 2D tensors\n        before calling the `process_logits` method. After processing, the\n        logits are cast back to the original array library type before being\n        returned.\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a tensor.\n        logits\n            The logits for the current generation step in a tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a tensor.\n        \"\"\"\n        # if input_ids is 1D and logits is 2D with a single sequence,\n        # reshape input_ids to 2D (needed for mlx-lm)\n        if (\n            len(self.tensor_adapter.shape(input_ids)) == 1\n            and len(self.tensor_adapter.shape(logits)) == 2\n            and self.tensor_adapter.shape(logits)[0] == 1\n        ):\n            input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n        assert (\n            self.tensor_adapter.shape(logits)[:-1]\n            == self.tensor_adapter.shape(input_ids)[:-1]\n        )\n\n        # Guarantee passed as 2D Tensors, then covert back to original\n        # (1D or 2D) shape\n        if len(self.tensor_adapter.shape(logits)) == 2:\n            processed_logits = self.process_logits(input_ids, logits)\n        elif len(self.tensor_adapter.shape(logits)) == 1:\n            processed_logits = self.tensor_adapter.squeeze(\n                self.process_logits(\n                    self.tensor_adapter.unsqueeze(input_ids),\n                    self.tensor_adapter.unsqueeze(logits),\n                ),\n            )\n        else:\n            raise ValueError(\n                f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n                + \"supported\"\n            )\n\n        return processed_logits\n</code></pre>"},{"location":"api_reference/#outlines.processors.OutlinesLogitsProcessor.__call__","title":"<code>__call__(input_ids, logits)</code>","text":"<p>Entrypoint for logits processors, this is the method that is called by the model. Because different models use different structures to store the input_ids and logits, we standardize their format to 2D tensors before calling the <code>process_logits</code> method. After processing, the logits are cast back to the original array library type before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __call__(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Entrypoint for logits processors, this is the method that is\n    called by the model.\n    Because different models use different structures to store the\n    input_ids and logits, we standardize their format to 2D tensors\n    before calling the `process_logits` method. After processing, the\n    logits are cast back to the original array library type before being\n    returned.\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a tensor.\n    logits\n        The logits for the current generation step in a tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a tensor.\n    \"\"\"\n    # if input_ids is 1D and logits is 2D with a single sequence,\n    # reshape input_ids to 2D (needed for mlx-lm)\n    if (\n        len(self.tensor_adapter.shape(input_ids)) == 1\n        and len(self.tensor_adapter.shape(logits)) == 2\n        and self.tensor_adapter.shape(logits)[0] == 1\n    ):\n        input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n    assert (\n        self.tensor_adapter.shape(logits)[:-1]\n        == self.tensor_adapter.shape(input_ids)[:-1]\n    )\n\n    # Guarantee passed as 2D Tensors, then covert back to original\n    # (1D or 2D) shape\n    if len(self.tensor_adapter.shape(logits)) == 2:\n        processed_logits = self.process_logits(input_ids, logits)\n    elif len(self.tensor_adapter.shape(logits)) == 1:\n        processed_logits = self.tensor_adapter.squeeze(\n            self.process_logits(\n                self.tensor_adapter.unsqueeze(input_ids),\n                self.tensor_adapter.unsqueeze(logits),\n            ),\n        )\n    else:\n        raise ValueError(\n            f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n            + \"supported\"\n        )\n\n    return processed_logits\n</code></pre>"},{"location":"api_reference/#outlines.processors.OutlinesLogitsProcessor.__init__","title":"<code>__init__(tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor_library_name</code> <code>str</code> <p>The name of the library to use to manipulate tensors. Possible values are \"mlx\", \"numpy\" and \"torch\". You must choose the library that your model is using.</p> required Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __init__(self, tensor_library_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    tensor_library_name\n        The name of the library to use to manipulate tensors. Possible\n        values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n        that your model is using.\n    \"\"\"\n    # Temporary fix as torch raises a warning that can cause can an error\n    # with python 3.12.\n    if tensor_library_name == \"torch\":\n        import torch._dynamo\n\n        torch._dynamo.config.suppress_errors = True\n\n    tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n    if tensor_adapter_class is None:\n        raise NotImplementedError(\n            f\"Library {tensor_library_name} is not available\"\n        )\n    self.tensor_adapter = tensor_adapter_class()  # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.processors.OutlinesLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>  <code>abstractmethod</code>","text":"<p>Main method to implement for logits processors subclasses. This method applies a mask on the logits to bias the generation. It is called by the <code>__call__</code> method that standardizes the shape of <code>input_ids</code> and <code>logits</code> to ensure they are 2D tensors. Elements to keep in mind when designing universal logits processors: - logits processors are only used once and never re-applied for a new sequence generator - Some models only pass output_ids, some models such as llamacpp and transformers prefix with input_ids - Some sampling methods, such as beam search, result in unstable sequence ordering in models like vLLM</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a 2D tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a 2D tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a 2D tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>@abstractmethod\ndef process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Main method to implement for logits processors subclasses.\n    This method applies a mask on the logits to bias the generation.\n    It is called by the `__call__` method that standardizes the shape of\n    `input_ids` and `logits` to ensure they are 2D tensors.\n    Elements to keep in mind when designing universal logits processors:\n    - logits processors are only used once and never re-applied for a new\n    sequence generator\n    - Some models only pass output_ids, some models such as llamacpp and\n    transformers prefix with input_ids\n    - Some sampling methods, such as beam search, result in unstable\n    sequence ordering in models like vLLM\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a 2D tensor.\n    logits\n        The logits for the current generation step in a 2D tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a 2D tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.OutlinesLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor for a new generation</p> <p>Only implement this method in subclasses if the logits processor needs to be reset for a new generation.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the logits processor for a new generation\n\n    Only implement this method in subclasses if the logits processor\n    needs to be reset for a new generation.\n\n    \"\"\"\n    pass # pragma: no cover\n</code></pre>"},{"location":"api_reference/#outlines.processors.base_logits_processor","title":"<code>base_logits_processor</code>","text":"<p>Base class for logits processors.</p>"},{"location":"api_reference/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor","title":"<code>OutlinesLogitsProcessor</code>","text":"<p>Base class for logits processors. This class implements a shared <code>__call__</code> method is called by the models and returns the processed logits. It relies on the <code>process_logits</code> method that must be implemented by the subclasses to do the actual processing. The <code>tensor_adapter</code> attribute, created at initialization based on the tensor library name specified in the constructor, is used to manipulate the tensors using the appropriate library for the model (numpy, torch...).</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>class OutlinesLogitsProcessor:\n    \"\"\"Base class for logits processors.\n    This class implements a shared `__call__` method is called by the models\n    and returns the processed logits. It relies on the `process_logits` method\n    that must be implemented by the subclasses to do the actual processing. The\n    `tensor_adapter` attribute, created at initialization based on the\n    tensor library name specified in the constructor, is used to manipulate the\n    tensors using the appropriate library for the model (numpy, torch...).\n    \"\"\"\n    tensor_adapter: TensorAdapterImplementation\n\n    def __init__(self, tensor_library_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        tensor_library_name\n            The name of the library to use to manipulate tensors. Possible\n            values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n            that your model is using.\n        \"\"\"\n        # Temporary fix as torch raises a warning that can cause can an error\n        # with python 3.12.\n        if tensor_library_name == \"torch\":\n            import torch._dynamo\n\n            torch._dynamo.config.suppress_errors = True\n\n        tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n        if tensor_adapter_class is None:\n            raise NotImplementedError(\n                f\"Library {tensor_library_name} is not available\"\n            )\n        self.tensor_adapter = tensor_adapter_class()  # type: ignore\n\n    def reset(self):\n        \"\"\"Reset the logits processor for a new generation\n\n        Only implement this method in subclasses if the logits processor\n        needs to be reset for a new generation.\n\n        \"\"\"\n        pass # pragma: no cover\n\n    @abstractmethod\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Main method to implement for logits processors subclasses.\n        This method applies a mask on the logits to bias the generation.\n        It is called by the `__call__` method that standardizes the shape of\n        `input_ids` and `logits` to ensure they are 2D tensors.\n        Elements to keep in mind when designing universal logits processors:\n        - logits processors are only used once and never re-applied for a new\n        sequence generator\n        - Some models only pass output_ids, some models such as llamacpp and\n        transformers prefix with input_ids\n        - Some sampling methods, such as beam search, result in unstable\n        sequence ordering in models like vLLM\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a 2D tensor.\n        logits\n            The logits for the current generation step in a 2D tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a 2D tensor.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Entrypoint for logits processors, this is the method that is\n        called by the model.\n        Because different models use different structures to store the\n        input_ids and logits, we standardize their format to 2D tensors\n        before calling the `process_logits` method. After processing, the\n        logits are cast back to the original array library type before being\n        returned.\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a tensor.\n        logits\n            The logits for the current generation step in a tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a tensor.\n        \"\"\"\n        # if input_ids is 1D and logits is 2D with a single sequence,\n        # reshape input_ids to 2D (needed for mlx-lm)\n        if (\n            len(self.tensor_adapter.shape(input_ids)) == 1\n            and len(self.tensor_adapter.shape(logits)) == 2\n            and self.tensor_adapter.shape(logits)[0] == 1\n        ):\n            input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n        assert (\n            self.tensor_adapter.shape(logits)[:-1]\n            == self.tensor_adapter.shape(input_ids)[:-1]\n        )\n\n        # Guarantee passed as 2D Tensors, then covert back to original\n        # (1D or 2D) shape\n        if len(self.tensor_adapter.shape(logits)) == 2:\n            processed_logits = self.process_logits(input_ids, logits)\n        elif len(self.tensor_adapter.shape(logits)) == 1:\n            processed_logits = self.tensor_adapter.squeeze(\n                self.process_logits(\n                    self.tensor_adapter.unsqueeze(input_ids),\n                    self.tensor_adapter.unsqueeze(logits),\n                ),\n            )\n        else:\n            raise ValueError(\n                f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n                + \"supported\"\n            )\n\n        return processed_logits\n</code></pre>"},{"location":"api_reference/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__call__","title":"<code>__call__(input_ids, logits)</code>","text":"<p>Entrypoint for logits processors, this is the method that is called by the model. Because different models use different structures to store the input_ids and logits, we standardize their format to 2D tensors before calling the <code>process_logits</code> method. After processing, the logits are cast back to the original array library type before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __call__(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Entrypoint for logits processors, this is the method that is\n    called by the model.\n    Because different models use different structures to store the\n    input_ids and logits, we standardize their format to 2D tensors\n    before calling the `process_logits` method. After processing, the\n    logits are cast back to the original array library type before being\n    returned.\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a tensor.\n    logits\n        The logits for the current generation step in a tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a tensor.\n    \"\"\"\n    # if input_ids is 1D and logits is 2D with a single sequence,\n    # reshape input_ids to 2D (needed for mlx-lm)\n    if (\n        len(self.tensor_adapter.shape(input_ids)) == 1\n        and len(self.tensor_adapter.shape(logits)) == 2\n        and self.tensor_adapter.shape(logits)[0] == 1\n    ):\n        input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n    assert (\n        self.tensor_adapter.shape(logits)[:-1]\n        == self.tensor_adapter.shape(input_ids)[:-1]\n    )\n\n    # Guarantee passed as 2D Tensors, then covert back to original\n    # (1D or 2D) shape\n    if len(self.tensor_adapter.shape(logits)) == 2:\n        processed_logits = self.process_logits(input_ids, logits)\n    elif len(self.tensor_adapter.shape(logits)) == 1:\n        processed_logits = self.tensor_adapter.squeeze(\n            self.process_logits(\n                self.tensor_adapter.unsqueeze(input_ids),\n                self.tensor_adapter.unsqueeze(logits),\n            ),\n        )\n    else:\n        raise ValueError(\n            f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n            + \"supported\"\n        )\n\n    return processed_logits\n</code></pre>"},{"location":"api_reference/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__init__","title":"<code>__init__(tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor_library_name</code> <code>str</code> <p>The name of the library to use to manipulate tensors. Possible values are \"mlx\", \"numpy\" and \"torch\". You must choose the library that your model is using.</p> required Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __init__(self, tensor_library_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    tensor_library_name\n        The name of the library to use to manipulate tensors. Possible\n        values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n        that your model is using.\n    \"\"\"\n    # Temporary fix as torch raises a warning that can cause can an error\n    # with python 3.12.\n    if tensor_library_name == \"torch\":\n        import torch._dynamo\n\n        torch._dynamo.config.suppress_errors = True\n\n    tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n    if tensor_adapter_class is None:\n        raise NotImplementedError(\n            f\"Library {tensor_library_name} is not available\"\n        )\n    self.tensor_adapter = tensor_adapter_class()  # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>  <code>abstractmethod</code>","text":"<p>Main method to implement for logits processors subclasses. This method applies a mask on the logits to bias the generation. It is called by the <code>__call__</code> method that standardizes the shape of <code>input_ids</code> and <code>logits</code> to ensure they are 2D tensors. Elements to keep in mind when designing universal logits processors: - logits processors are only used once and never re-applied for a new sequence generator - Some models only pass output_ids, some models such as llamacpp and transformers prefix with input_ids - Some sampling methods, such as beam search, result in unstable sequence ordering in models like vLLM</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a 2D tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a 2D tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a 2D tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>@abstractmethod\ndef process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Main method to implement for logits processors subclasses.\n    This method applies a mask on the logits to bias the generation.\n    It is called by the `__call__` method that standardizes the shape of\n    `input_ids` and `logits` to ensure they are 2D tensors.\n    Elements to keep in mind when designing universal logits processors:\n    - logits processors are only used once and never re-applied for a new\n    sequence generator\n    - Some models only pass output_ids, some models such as llamacpp and\n    transformers prefix with input_ids\n    - Some sampling methods, such as beam search, result in unstable\n    sequence ordering in models like vLLM\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a 2D tensor.\n    logits\n        The logits for the current generation step in a 2D tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a 2D tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor for a new generation</p> <p>Only implement this method in subclasses if the logits processor needs to be reset for a new generation.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the logits processor for a new generation\n\n    Only implement this method in subclasses if the logits processor\n    needs to be reset for a new generation.\n\n    \"\"\"\n    pass # pragma: no cover\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters","title":"<code>tensor_adapters</code>","text":"<p>Library specific objects to manipulate tensors.</p>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base","title":"<code>base</code>","text":"<p>Base class for tensor adapters.</p>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter","title":"<code>TensorAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tensor adapters.</p> <p>This class defines the interface for tensor adapters that are used to manipulate tensors in different libraries. Concrete implementations of this class should provide specific implementations for each method as well as providing a <code>library_name</code> attribute.</p> <p>TODO: Update the version of outlines-core used to receive plain arrays instead of torch tensors. In the meantime, implementations of this class must make sure that their <code>full_like</code> and <code>concatenate</code> methods can handle torch tensors.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>class TensorAdapter(ABC):\n    \"\"\"Abstract base class for tensor adapters.\n\n    This class defines the interface for tensor adapters that are used to\n    manipulate tensors in different libraries. Concrete implementations of\n    this class should provide specific implementations for each method as\n    well as providing a `library_name` attribute.\n\n    TODO: Update the version of outlines-core used to receive plain arrays\n    instead of torch tensors. In the meantime, implementations of this class\n    must make sure that their `full_like` and `concatenate` methods can\n    handle torch tensors.\n\n    \"\"\"\n    library_name: str\n\n    @abstractmethod\n    def shape(self, tensor: TensorType) -&gt; list[int]:\n        \"\"\"Get the shape of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the shape of.\n\n        Returns\n        -------\n        list[int]\n            The shape of the tensor. The list contains as many elements as\n            there are dimensions in the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Add a dimension to the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to add a dimension to.\n\n        Returns\n        -------\n        TensorType\n            The tensor with an additional dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def squeeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Remove a dimension from the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to remove a dimension from.\n\n        Returns\n        -------\n        TensorType\n            The tensor with one less dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_list(self, tensor: TensorType) -&gt; list:\n        \"\"\"Convert the tensor to a list.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to convert to a list.\n\n        Returns\n        -------\n        list\n            The tensor as a list.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_scalar(self, tensor: TensorType) -&gt; Any:\n        \"\"\"Return the only element of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to return the only element of.\n\n        Returns\n        -------\n        Any\n            The only element of the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n        \"\"\"Create a tensor with the same shape as the input tensor filled\n        with a scalar value.\n\n        ATTENTION: This method receives a torch tensor regardless of the\n        library used.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a new tensor with the same shape.\n        fill_value\n            The value to fill the new tensor with.\n\n        Returns\n        -------\n        TensorType\n            A tensor with the same shape as the input tensor filled with the\n            specified value.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def concatenate(\n        self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n    ) -&gt; TensorType:\n        \"\"\"Concatenate a list of tensors along axis 0.\n\n        ATTENTION: This method can either receive a list of torch tensors or\n        a list of tensors from the library used.\n\n        Parameters\n        ----------\n        tensors\n            The list of tensors to concatenate.\n\n        Returns\n        -------\n        TensorType\n            The concatenated tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_device(self, tensor: TensorType) -&gt; str:\n        \"\"\"Get the name of the tensor's device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the device of.\n\n        Returns\n        -------\n        str\n            The name of the tensor's device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n        \"\"\"Move the tensor to a specified device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to move to a specified device.\n        device\n            The name of the device to move the tensor to.\n\n        Returns\n        -------\n        TensorType\n            The tensor moved to the specified device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Create a boolean ones tensor with the same shape as the input\n        tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a boolean ones tensor with the same shape.\n\n        Returns\n        -------\n        TensorType\n            A boolean ones tensor with the same shape as the input tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply_mask(\n        self, tensor: TensorType, mask: TensorType, value: Any\n    ) -&gt; TensorType:\n        \"\"\"Fill the elements of the tensor where the mask is True with the\n        specified value.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to fill.\n        mask\n            The mask to apply to the tensor.\n        value\n            The value to fill the tensor with.\n\n        Returns\n        -------\n        TensorType\n            The tensor with the mask applied.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def argsort_descending(\n        self, tensor: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Return the indices that would sort the tensor in descending order\n        along axis -1.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to sort.\n\n        Returns\n        -------\n        TensorType\n            The indices that would sort the tensor in descending order along\n            axis -1.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.apply_mask","title":"<code>apply_mask(tensor, mask, value)</code>  <code>abstractmethod</code>","text":"<p>Fill the elements of the tensor where the mask is True with the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to fill.</p> required <code>mask</code> <code>TensorType</code> <p>The mask to apply to the tensor.</p> required <code>value</code> <code>Any</code> <p>The value to fill the tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with the mask applied.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef apply_mask(\n    self, tensor: TensorType, mask: TensorType, value: Any\n) -&gt; TensorType:\n    \"\"\"Fill the elements of the tensor where the mask is True with the\n    specified value.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to fill.\n    mask\n        The mask to apply to the tensor.\n    value\n        The value to fill the tensor with.\n\n    Returns\n    -------\n    TensorType\n        The tensor with the mask applied.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.argsort_descending","title":"<code>argsort_descending(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the indices that would sort the tensor in descending order along axis -1.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to sort.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The indices that would sort the tensor in descending order along axis -1.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef argsort_descending(\n    self, tensor: TensorType\n) -&gt; TensorType:\n    \"\"\"Return the indices that would sort the tensor in descending order\n    along axis -1.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to sort.\n\n    Returns\n    -------\n    TensorType\n        The indices that would sort the tensor in descending order along\n        axis -1.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.boolean_ones_like","title":"<code>boolean_ones_like(tensor)</code>  <code>abstractmethod</code>","text":"<p>Create a boolean ones tensor with the same shape as the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to create a boolean ones tensor with the same shape.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A boolean ones tensor with the same shape as the input tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Create a boolean ones tensor with the same shape as the input\n    tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a boolean ones tensor with the same shape.\n\n    Returns\n    -------\n    TensorType\n        A boolean ones tensor with the same shape as the input tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.concatenate","title":"<code>concatenate(tensors)</code>  <code>abstractmethod</code>","text":"<p>Concatenate a list of tensors along axis 0.</p> <p>ATTENTION: This method can either receive a list of torch tensors or a list of tensors from the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Union[Tensor, TensorType]]</code> <p>The list of tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The concatenated tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef concatenate(\n    self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n) -&gt; TensorType:\n    \"\"\"Concatenate a list of tensors along axis 0.\n\n    ATTENTION: This method can either receive a list of torch tensors or\n    a list of tensors from the library used.\n\n    Parameters\n    ----------\n    tensors\n        The list of tensors to concatenate.\n\n    Returns\n    -------\n    TensorType\n        The concatenated tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.full_like","title":"<code>full_like(tensor, fill_value)</code>  <code>abstractmethod</code>","text":"<p>Create a tensor with the same shape as the input tensor filled with a scalar value.</p> <p>ATTENTION: This method receives a torch tensor regardless of the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to create a new tensor with the same shape.</p> required <code>fill_value</code> <code>Any</code> <p>The value to fill the new tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A tensor with the same shape as the input tensor filled with the specified value.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n    \"\"\"Create a tensor with the same shape as the input tensor filled\n    with a scalar value.\n\n    ATTENTION: This method receives a torch tensor regardless of the\n    library used.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a new tensor with the same shape.\n    fill_value\n        The value to fill the new tensor with.\n\n    Returns\n    -------\n    TensorType\n        A tensor with the same shape as the input tensor filled with the\n        specified value.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.get_device","title":"<code>get_device(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the name of the tensor's device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the device of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the tensor's device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef get_device(self, tensor: TensorType) -&gt; str:\n    \"\"\"Get the name of the tensor's device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the device of.\n\n    Returns\n    -------\n    str\n        The name of the tensor's device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.shape","title":"<code>shape(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the shape of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the shape of.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The shape of the tensor. The list contains as many elements as there are dimensions in the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef shape(self, tensor: TensorType) -&gt; list[int]:\n    \"\"\"Get the shape of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the shape of.\n\n    Returns\n    -------\n    list[int]\n        The shape of the tensor. The list contains as many elements as\n        there are dimensions in the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.squeeze","title":"<code>squeeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Remove a dimension from the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to remove a dimension from.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with one less dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef squeeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Remove a dimension from the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to remove a dimension from.\n\n    Returns\n    -------\n    TensorType\n        The tensor with one less dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.to_device","title":"<code>to_device(tensor, device)</code>  <code>abstractmethod</code>","text":"<p>Move the tensor to a specified device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to move to a specified device.</p> required <code>device</code> <code>str</code> <p>The name of the device to move the tensor to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor moved to the specified device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n    \"\"\"Move the tensor to a specified device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to move to a specified device.\n    device\n        The name of the device to move the tensor to.\n\n    Returns\n    -------\n    TensorType\n        The tensor moved to the specified device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.to_list","title":"<code>to_list(tensor)</code>  <code>abstractmethod</code>","text":"<p>Convert the tensor to a list.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to convert to a list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The tensor as a list.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_list(self, tensor: TensorType) -&gt; list:\n    \"\"\"Convert the tensor to a list.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to convert to a list.\n\n    Returns\n    -------\n    list\n        The tensor as a list.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.to_scalar","title":"<code>to_scalar(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the only element of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to return the only element of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The only element of the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_scalar(self, tensor: TensorType) -&gt; Any:\n    \"\"\"Return the only element of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to return the only element of.\n\n    Returns\n    -------\n    Any\n        The only element of the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.base.TensorAdapter.unsqueeze","title":"<code>unsqueeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Add a dimension to the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to add a dimension to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with an additional dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Add a dimension to the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to add a dimension to.\n\n    Returns\n    -------\n    TensorType\n        The tensor with an additional dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#outlines.processors.tensor_adapters.mlx","title":"<code>mlx</code>","text":"<p>Tensor adapter for the <code>mlx</code> library.</p>"},{"location":"api_reference/#outlines.processors.tensor_adapters.numpy","title":"<code>numpy</code>","text":"<p>Tensor adapter for the <code>numpy</code> library.</p>"},{"location":"api_reference/#outlines.processors.tensor_adapters.torch","title":"<code>torch</code>","text":"<p>Tensor adapter for the <code>torch</code> library.</p>"},{"location":"api_reference/#outlines.templates","title":"<code>templates</code>","text":"<p>Create templates to easily build prompts.</p>"},{"location":"api_reference/#outlines.templates.Template","title":"<code>Template</code>  <code>dataclass</code>","text":"<p>Represents a prompt template.</p> <p>We return a <code>Template</code> class instead of a simple function so the template can be accessed by callers.</p> Source code in <code>outlines/templates.py</code> <pre><code>@dataclass\nclass Template:\n    \"\"\"Represents a prompt template.\n\n    We return a `Template` class instead of a simple function so the\n    template can be accessed by callers.\n\n    \"\"\"\n    template: jinja2.Template\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        str\n            The rendered template as a Python string.\n\n        \"\"\"\n        return self.template.render(**kwargs)\n\n    @classmethod\n    def from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a string containing a Jinja\n        template.\n\n        Parameters\n        ----------\n        content : str\n            The string content to be converted into a template.\n\n        Returns\n        -------\n        Template\n            An instance of the class with the provided content as a template.\n\n        \"\"\"\n        return cls(build_template_from_string(content, filters))\n\n    @classmethod\n    def from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a file containing a Jinja\n        template.\n\n        Note: This method does not allow to include and inheritance to\n        reference files that are outside the folder or subfolders of the file\n        given to `from_file`.\n\n        Parameters\n        ----------\n        path : Path\n            The path to the file containing the Jinja template.\n\n        Returns\n        -------\n        Template\n            An instance of the Template class with the template loaded from the\n            file.\n\n        \"\"\"\n        # We don't use a `Signature` here because it seems not feasible to\n        # infer one from a Jinja2 environment that is\n        # split across multiple files (since e.g. we support features like\n        # Jinja2 includes and template inheritance)\n        return cls(build_template_from_file(path, filters))\n</code></pre>"},{"location":"api_reference/#outlines.templates.Template.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a Python string.</p> Source code in <code>outlines/templates.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    str\n        The rendered template as a Python string.\n\n    \"\"\"\n    return self.template.render(**kwargs)\n</code></pre>"},{"location":"api_reference/#outlines.templates.Template.from_file","title":"<code>from_file(path, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a file containing a Jinja template.</p> <p>Note: This method does not allow to include and inheritance to reference files that are outside the folder or subfolders of the file given to <code>from_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file containing the Jinja template.</p> required <p>Returns:</p> Type Description <code>Template</code> <p>An instance of the Template class with the template loaded from the file.</p> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a file containing a Jinja\n    template.\n\n    Note: This method does not allow to include and inheritance to\n    reference files that are outside the folder or subfolders of the file\n    given to `from_file`.\n\n    Parameters\n    ----------\n    path : Path\n        The path to the file containing the Jinja template.\n\n    Returns\n    -------\n    Template\n        An instance of the Template class with the template loaded from the\n        file.\n\n    \"\"\"\n    # We don't use a `Signature` here because it seems not feasible to\n    # infer one from a Jinja2 environment that is\n    # split across multiple files (since e.g. we support features like\n    # Jinja2 includes and template inheritance)\n    return cls(build_template_from_file(path, filters))\n</code></pre>"},{"location":"api_reference/#outlines.templates.Template.from_string","title":"<code>from_string(content, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a string containing a Jinja template.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The string content to be converted into a template.</p> required <p>Returns:</p> Type Description <code>Template</code> <p>An instance of the class with the provided content as a template.</p> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a string containing a Jinja\n    template.\n\n    Parameters\n    ----------\n    content : str\n        The string content to be converted into a template.\n\n    Returns\n    -------\n    Template\n        An instance of the class with the provided content as a template.\n\n    \"\"\"\n    return cls(build_template_from_string(content, filters))\n</code></pre>"},{"location":"api_reference/#outlines.templates.Vision","title":"<code>Vision(prompt, image)</code>","text":"<p>This factory function replaces the deprecated <code>Vision</code> class until it is fully removed in outlines v1.2.0.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to use to generate the response.</p> required <code>image</code> <code>Image</code> <p>The image to use to generate the response.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list containing the prompt and Image instance.</p> Source code in <code>outlines/templates.py</code> <pre><code>def Vision(prompt: str, image: PILImage.Image) -&gt; list:\n    \"\"\"This factory function replaces the deprecated `Vision` class until it is\n    fully removed in outlines v1.2.0.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate the response.\n    image\n        The image to use to generate the response.\n\n    Returns\n    -------\n    list\n        A list containing the prompt and Image instance.\n    \"\"\"\n    warnings.warn(\"\"\"\n        The Vision function is deprecated and will be removed in outlines 1.2.0.\n        Instead of using Vision, please use a prompt along with an\n        outlines.inputs.Image instance.\n        For instance:\n        ```python\n        import openai\n        from outlines import Image, from_openai\n        model = from_openai(\"gpt-4o\")\n        response = model(\n            [\"A beautiful image of a cat\", Image(my_image)],\n            max_tokens=100\n        )\n        ```\n        \"\"\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return [prompt, Image(image)]\n</code></pre>"},{"location":"api_reference/#outlines.templates.create_jinja_env","title":"<code>create_jinja_env(loader, filters)</code>","text":"<p>Create a new Jinja environment.</p> <p>The Jinja environment is loaded with a set of pre-defined filters: - <code>name</code>: get the name of a function - <code>description</code>: get a function's docstring - <code>source</code>: get a function's source code - <code>signature</code>: get a function's signature - <code>args</code>: get a function's arguments - <code>schema</code>: display a JSON Schema</p> <p>Users may pass additional filters, and/or override existing ones.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>Optional[BaseLoader]</code> <p>An optional <code>BaseLoader</code> instance</p> required <code>filters</code> <code>Dict[str, Callable]</code> <p>A dictionary of filters, map between the filter's name and the corresponding function.</p> required Source code in <code>outlines/templates.py</code> <pre><code>def create_jinja_env(\n    loader: Optional[jinja2.BaseLoader], filters: Dict[str, Callable]\n) -&gt; jinja2.Environment:\n    \"\"\"Create a new Jinja environment.\n\n    The Jinja environment is loaded with a set of pre-defined filters:\n    - `name`: get the name of a function\n    - `description`: get a function's docstring\n    - `source`: get a function's source code\n    - `signature`: get a function's signature\n    - `args`: get a function's arguments\n    - `schema`: display a JSON Schema\n\n    Users may pass additional filters, and/or override existing ones.\n\n    Parameters\n    ----------\n    loader\n       An optional `BaseLoader` instance\n    filters\n       A dictionary of filters, map between the filter's name and the\n       corresponding function.\n\n    \"\"\"\n    env = jinja2.Environment(\n        loader=loader,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=jinja2.StrictUndefined,\n    )\n\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n    env.filters[\"args\"] = get_fn_args\n\n    # The filters passed by the user may override the\n    # pre-defined filters.\n    for name, filter_fn in filters.items():\n        env.filters[name] = filter_fn\n\n    return env\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_fn_args","title":"<code>get_fn_args(fn)</code>","text":"<p>Returns the arguments of a function with annotations and default values if provided.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_args(fn: Callable):\n    \"\"\"Returns the arguments of a function with annotations and default values if provided.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `args` filter only applies to callables.\")\n\n    arg_str_list = []\n    signature = inspect.signature(fn)\n    arg_str_list = [str(param) for param in signature.parameters.values()]\n    arg_str = \", \".join(arg_str_list)\n    return arg_str\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:  # pragma: no cover\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:  # pragma: no cover\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api_reference/#outlines.templates.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api_reference/#outlines.templates.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/templates.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value: # pragma: no cover\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api_reference/#outlines.types","title":"<code>types</code>","text":"<p>Output types for structured generation and regex DSL.</p>"},{"location":"api_reference/#outlines.types.AirportImportError","title":"<code>AirportImportError</code>","text":"<p>Dummy module that raises an error when accessed.</p> Source code in <code>outlines/types/__init__.py</code> <pre><code>class AirportImportError:\n    \"\"\"Dummy module that raises an error when accessed.\"\"\"\n    def __getattr__(self, name):\n        raise ImportError(\n            \"The 'airportsdata' package is required to use airport types. \"\n            \"Install it with: pip install 'outlines[airports]'\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.types.CFG","title":"<code>CFG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>str</code> <p>The definition of the context-free grammar as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass CFG(Term):\n    \"\"\"Class representing a context-free grammar.\n\n    Parameters\n    ----------\n    definition\n        The definition of the context-free grammar as a string.\n\n    \"\"\"\n    definition: str\n\n    def _display_node(self) -&gt; str:\n        return f\"CFG('{self.definition}')\"\n\n    def __repr__(self):\n        return f\"CFG(definition='{self.definition}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, CFG):\n            return False\n        return self.definition == other.definition\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"CFG\":\n        \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n        Parameters\n        ----------\n        path : str\n            The path to the file containing the CFG definition.\n        Returns\n        -------\n        CFG\n            A CFG instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            definition = f.read()\n        return cls(definition)\n</code></pre>"},{"location":"api_reference/#outlines.types.CFG.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a CFG instance from a file containing a CFG definition.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the CFG definition.</p> required <p>Returns:</p> Type Description <code>CFG</code> <p>A CFG instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"CFG\":\n    \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n    Parameters\n    ----------\n    path : str\n        The path to the file containing the CFG definition.\n    Returns\n    -------\n    CFG\n        A CFG instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        definition = f.read()\n    return cls(definition)\n</code></pre>"},{"location":"api_reference/#outlines.types.Choice","title":"<code>Choice</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a choice between different items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>The items to choose from.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Choice(Term):\n    \"\"\"Class representing a choice between different items.\n\n    Parameters\n    ----------\n    items\n        The items to choose from.\n\n    \"\"\"\n    items: List[Any]\n\n    def _display_node(self) -&gt; str:\n        return f\"Choice({repr(self.items)})\"\n\n    def __repr__(self):\n        return f\"Choice(items={repr(self.items)})\"\n</code></pre>"},{"location":"api_reference/#outlines.types.CountryImportError","title":"<code>CountryImportError</code>","text":"<p>Dummy module that raises an error when accessed.</p> Source code in <code>outlines/types/__init__.py</code> <pre><code>class CountryImportError:\n    \"\"\"Dummy module that raises an error when accessed.\"\"\"\n    def __getattr__(self, name):\n        raise ImportError(\n            \"The 'iso3166' package is required to use country types. \"\n            \"Install it with: pip install 'outlines[countries]'\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.types.JsonSchema","title":"<code>JsonSchema</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a JSON schema.</p> <p>The JSON schema object from which to instantiate the class can be a dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a genSON schema builder.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>class JsonSchema(Term):\n    \"\"\"Class representing a JSON schema.\n\n    The JSON schema object from which to instantiate the class can be a\n    dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a\n    genSON schema builder.\n\n    \"\"\"\n    schema: str\n    whitespace_pattern: OptionalType[str]\n\n    def __init__(\n        self,\n        schema: Union[\n            dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n        ],\n        whitespace_pattern: OptionalType[str] = None,\n        ensure_ascii: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        schema\n            The object containing the JSON schema.\n        whitespace_pattern\n            The pattern to use to match whitespace characters.\n        ensure_ascii\n            Whether to ensure the schema is ASCII-only.\n\n        \"\"\"\n        schema_str: str\n\n        if is_dict_instance(schema):\n            schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n        elif is_str_instance(schema):\n            schema_str = str(schema)\n        elif is_pydantic_model(schema):\n            schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n        elif is_typed_dict(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_dataclass(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_genson_schema_builder(schema):\n            schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {schema}. The schema must be either \"\n                + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n                + \"builder or a string or dict that contains the JSON schema \"\n                + \"specification\"\n            )\n\n        self.schema = schema_str\n        self.whitespace_pattern = whitespace_pattern\n\n    def __post_init__(self):\n        jsonschema.Draft7Validator.check_schema(json.loads(self.schema))\n\n    @classmethod\n    def is_json_schema(cls, obj: Any) -&gt; bool:\n        \"\"\"Check if the object provided is a JSON schema type.\n\n        Parameters\n        ----------\n        obj: Any\n            The object to check\n\n        Returns\n        -------\n        bool\n            True if the object is a JSON schema type, False otherwise\n\n        \"\"\"\n        return (\n            isinstance(obj, cls)\n            or is_pydantic_model(obj)\n            or is_typed_dict(obj)\n            or is_dataclass(obj)\n            or is_genson_schema_builder(obj)\n        )\n\n    @classmethod\n    def convert_to(\n        cls,\n        schema: Union[\n            \"JsonSchema\",\n            type[BaseModel],\n            _TypedDictMeta,\n            type,\n            SchemaBuilder,\n        ],\n        target_types: List[Literal[\n            \"str\",\n            \"dict\",\n            \"pydantic\",\n            \"typeddict\",\n            \"dataclass\",\n            \"genson\",\n        ]],\n    ) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n        \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n        If the schema provided is already of a type in the target_types, return\n        it unchanged.\n\n        Parameters\n        ----------\n        schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n            The schema to convert\n        target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n            The target types to convert to\n\n        \"\"\"\n        # If the schema provided is already of a type in the target_types,\n        # just return it\n        if isinstance(schema, cls):\n            if \"str\" in target_types:\n                return schema.schema\n            elif \"dict\" in target_types:\n                return json.loads(schema.schema)\n        elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n            return schema\n        elif is_typed_dict(schema) and \"typeddict\" in target_types:\n            return schema\n        elif is_dataclass(schema) and \"dataclass\" in target_types:\n            return schema\n        elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n            return schema\n\n        # Convert the schema to a JSON schema string/dict\n        if isinstance(schema, cls):\n            schema_str = schema.schema\n        else:\n            schema_str = cls(schema).schema\n        schema_dict = json.loads(schema_str)\n\n        for target_type in target_types:\n            try:\n                # Convert the JSON schema string to the target type\n                if target_type == \"str\":\n                    return schema_str\n                elif target_type == \"dict\":\n                    return schema_dict\n                elif target_type == \"pydantic\":\n                    return json_schema_dict_to_pydantic(schema_dict)\n                elif target_type == \"typeddict\":\n                    return json_schema_dict_to_typeddict(schema_dict)\n                elif target_type == \"dataclass\":\n                    return json_schema_dict_to_dataclass(schema_dict)\n                # No conversion available for genson\n            except Exception as e:  # pragma: no cover\n                warnings.warn(\n                    f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n                )\n                continue\n\n        raise ValueError(\n            f\"Cannot convert schema type {type(schema)} to any of the target \"\n            f\"types {target_types}\"\n        )\n\n    def _display_node(self) -&gt; str:\n        return f\"JsonSchema('{self.schema}')\"\n\n    def __repr__(self):\n        return f\"JsonSchema(schema='{self.schema}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, JsonSchema):\n            return False\n        try:\n            self_dict = json.loads(self.schema)\n            other_dict = json.loads(other.schema)\n            return self_dict == other_dict\n        except json.JSONDecodeError:  # pragma: no cover\n            return self.schema == other.schema\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"JsonSchema\":\n        \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n        schema.\n\n        Parameters\n        ----------\n        path:\n            The path to the file containing the JSON schema.\n        Returns\n        -------\n        JsonSchema\n            A JsonSchema instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            schema = json.load(f)\n        return cls(schema)\n</code></pre>"},{"location":"api_reference/#outlines.types.JsonSchema.__init__","title":"<code>__init__(schema, whitespace_pattern=None, ensure_ascii=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The object containing the JSON schema.</p> required <code>whitespace_pattern</code> <code>Optional[str]</code> <p>The pattern to use to match whitespace characters.</p> <code>None</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to ensure the schema is ASCII-only.</p> <code>True</code> Source code in <code>outlines/types/dsl.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[\n        dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n    ],\n    whitespace_pattern: OptionalType[str] = None,\n    ensure_ascii: bool = True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    schema\n        The object containing the JSON schema.\n    whitespace_pattern\n        The pattern to use to match whitespace characters.\n    ensure_ascii\n        Whether to ensure the schema is ASCII-only.\n\n    \"\"\"\n    schema_str: str\n\n    if is_dict_instance(schema):\n        schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n    elif is_str_instance(schema):\n        schema_str = str(schema)\n    elif is_pydantic_model(schema):\n        schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n    elif is_typed_dict(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_dataclass(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_genson_schema_builder(schema):\n        schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n            + \"builder or a string or dict that contains the JSON schema \"\n            + \"specification\"\n        )\n\n    self.schema = schema_str\n    self.whitespace_pattern = whitespace_pattern\n</code></pre>"},{"location":"api_reference/#outlines.types.JsonSchema.convert_to","title":"<code>convert_to(schema, target_types)</code>  <code>classmethod</code>","text":"<p>Convert a JSON schema type to a different JSON schema type.</p> <p>If the schema provided is already of a type in the target_types, return it unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The schema to convert</p> required <code>target_types</code> <code>List[Literal['str', 'dict', 'pydantic', 'typeddict', 'dataclass', 'genson']]</code> <p>The target types to convert to</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef convert_to(\n    cls,\n    schema: Union[\n        \"JsonSchema\",\n        type[BaseModel],\n        _TypedDictMeta,\n        type,\n        SchemaBuilder,\n    ],\n    target_types: List[Literal[\n        \"str\",\n        \"dict\",\n        \"pydantic\",\n        \"typeddict\",\n        \"dataclass\",\n        \"genson\",\n    ]],\n) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n    \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n    If the schema provided is already of a type in the target_types, return\n    it unchanged.\n\n    Parameters\n    ----------\n    schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n        The schema to convert\n    target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n        The target types to convert to\n\n    \"\"\"\n    # If the schema provided is already of a type in the target_types,\n    # just return it\n    if isinstance(schema, cls):\n        if \"str\" in target_types:\n            return schema.schema\n        elif \"dict\" in target_types:\n            return json.loads(schema.schema)\n    elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n        return schema\n    elif is_typed_dict(schema) and \"typeddict\" in target_types:\n        return schema\n    elif is_dataclass(schema) and \"dataclass\" in target_types:\n        return schema\n    elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n        return schema\n\n    # Convert the schema to a JSON schema string/dict\n    if isinstance(schema, cls):\n        schema_str = schema.schema\n    else:\n        schema_str = cls(schema).schema\n    schema_dict = json.loads(schema_str)\n\n    for target_type in target_types:\n        try:\n            # Convert the JSON schema string to the target type\n            if target_type == \"str\":\n                return schema_str\n            elif target_type == \"dict\":\n                return schema_dict\n            elif target_type == \"pydantic\":\n                return json_schema_dict_to_pydantic(schema_dict)\n            elif target_type == \"typeddict\":\n                return json_schema_dict_to_typeddict(schema_dict)\n            elif target_type == \"dataclass\":\n                return json_schema_dict_to_dataclass(schema_dict)\n            # No conversion available for genson\n        except Exception as e:  # pragma: no cover\n            warnings.warn(\n                f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n            )\n            continue\n\n    raise ValueError(\n        f\"Cannot convert schema type {type(schema)} to any of the target \"\n        f\"types {target_types}\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.types.JsonSchema.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a JsonSchema instance from a .json file containing a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the JSON schema.</p> required <p>Returns:</p> Type Description <code>JsonSchema</code> <p>A JsonSchema instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"JsonSchema\":\n    \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n    schema.\n\n    Parameters\n    ----------\n    path:\n        The path to the file containing the JSON schema.\n    Returns\n    -------\n    JsonSchema\n        A JsonSchema instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        schema = json.load(f)\n    return cls(schema)\n</code></pre>"},{"location":"api_reference/#outlines.types.JsonSchema.is_json_schema","title":"<code>is_json_schema(obj)</code>  <code>classmethod</code>","text":"<p>Check if the object provided is a JSON schema type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a JSON schema type, False otherwise</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef is_json_schema(cls, obj: Any) -&gt; bool:\n    \"\"\"Check if the object provided is a JSON schema type.\n\n    Parameters\n    ----------\n    obj: Any\n        The object to check\n\n    Returns\n    -------\n    bool\n        True if the object is a JSON schema type, False otherwise\n\n    \"\"\"\n    return (\n        isinstance(obj, cls)\n        or is_pydantic_model(obj)\n        or is_typed_dict(obj)\n        or is_dataclass(obj)\n        or is_genson_schema_builder(obj)\n    )\n</code></pre>"},{"location":"api_reference/#outlines.types.Regex","title":"<code>Regex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Regex(Term):\n    \"\"\"Class representing a regular expression.\n\n    Parameters\n    ----------\n    pattern\n        The regular expression as a string.\n\n    \"\"\"\n    pattern: str\n\n    def _display_node(self) -&gt; str:\n        return f\"Regex('{self.pattern}')\"\n\n    def __repr__(self):\n        return f\"Regex(pattern='{self.pattern}')\"\n</code></pre>"},{"location":"api_reference/#outlines.types.at_least","title":"<code>at_least(count, term)</code>","text":"<p>Repeat the term at least <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_least(count: int, term: Union[Term, str]) -&gt; QuantifyMinimum:\n    \"\"\"Repeat the term at least `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMinimum(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.at_most","title":"<code>at_most(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_most(count: int, term: Union[Term, str]) -&gt; QuantifyMaximum:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMaximum(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.either","title":"<code>either(*terms)</code>","text":"<p>Represents an alternative between different terms or strings.</p> <p>This factory function automatically translates string arguments into <code>String</code> objects.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def either(*terms: Union[str, Term]):\n    \"\"\"Represents an alternative between different terms or strings.\n\n    This factory function automatically translates string arguments\n    into `String` objects.\n\n    \"\"\"\n    terms = [String(arg) if isinstance(arg, str) else arg for arg in terms]\n    return Alternatives(terms)\n</code></pre>"},{"location":"api_reference/#outlines.types.exactly","title":"<code>exactly(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def exactly(count: int, term: Union[Term, str]) -&gt; QuantifyExact:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyExact(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.airports","title":"<code>airports</code>","text":"<p>Generate valid airport codes.</p>"},{"location":"api_reference/#outlines.types.countries","title":"<code>countries</code>","text":"<p>Generate valid country codes and names.</p>"},{"location":"api_reference/#outlines.types.countries.get_country_flags","title":"<code>get_country_flags()</code>","text":"<p>Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.</p> Source code in <code>outlines/types/countries.py</code> <pre><code>def get_country_flags():\n    \"\"\"Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.\"\"\"\n    base = ord(\"\ud83c\udde6\")\n    return {\n        code.name: chr(base + ord(code.name[0]) - ord(\"A\"))\n        + chr(base + ord(code.name[1]) - ord(\"A\"))\n        for code in Alpha2\n    }\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl","title":"<code>dsl</code>","text":"<p>Regular expression DSL and output types for structured generation.</p> <p>This module contains elements related to three logical steps in the use of output types for structured generation:</p> <ol> <li>Definition of <code>Term</code> classes that contain output type definitions. That    includes both terms intended to be used by themselves such as <code>JsonSchema</code>    or <code>CFG</code> and terms that are part of the regular expression DSL such as    <code>Alternatives</code> or <code>KleeneStar</code> (and the related functions).</li> <li>Conversion of Python types into <code>Term</code> instances (<code>python_types_to_terms</code>).</li> <li>Conversion of a <code>Term</code> instance into a regular expression (<code>to_regex</code>).</li> </ol>"},{"location":"api_reference/#outlines.types.dsl.CFG","title":"<code>CFG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>str</code> <p>The definition of the context-free grammar as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass CFG(Term):\n    \"\"\"Class representing a context-free grammar.\n\n    Parameters\n    ----------\n    definition\n        The definition of the context-free grammar as a string.\n\n    \"\"\"\n    definition: str\n\n    def _display_node(self) -&gt; str:\n        return f\"CFG('{self.definition}')\"\n\n    def __repr__(self):\n        return f\"CFG(definition='{self.definition}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, CFG):\n            return False\n        return self.definition == other.definition\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"CFG\":\n        \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n        Parameters\n        ----------\n        path : str\n            The path to the file containing the CFG definition.\n        Returns\n        -------\n        CFG\n            A CFG instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            definition = f.read()\n        return cls(definition)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.CFG.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a CFG instance from a file containing a CFG definition.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the CFG definition.</p> required <p>Returns:</p> Type Description <code>CFG</code> <p>A CFG instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"CFG\":\n    \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n    Parameters\n    ----------\n    path : str\n        The path to the file containing the CFG definition.\n    Returns\n    -------\n    CFG\n        A CFG instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        definition = f.read()\n    return cls(definition)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.Choice","title":"<code>Choice</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a choice between different items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>The items to choose from.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Choice(Term):\n    \"\"\"Class representing a choice between different items.\n\n    Parameters\n    ----------\n    items\n        The items to choose from.\n\n    \"\"\"\n    items: List[Any]\n\n    def _display_node(self) -&gt; str:\n        return f\"Choice({repr(self.items)})\"\n\n    def __repr__(self):\n        return f\"Choice(items={repr(self.items)})\"\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.JsonSchema","title":"<code>JsonSchema</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a JSON schema.</p> <p>The JSON schema object from which to instantiate the class can be a dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a genSON schema builder.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>class JsonSchema(Term):\n    \"\"\"Class representing a JSON schema.\n\n    The JSON schema object from which to instantiate the class can be a\n    dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a\n    genSON schema builder.\n\n    \"\"\"\n    schema: str\n    whitespace_pattern: OptionalType[str]\n\n    def __init__(\n        self,\n        schema: Union[\n            dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n        ],\n        whitespace_pattern: OptionalType[str] = None,\n        ensure_ascii: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        schema\n            The object containing the JSON schema.\n        whitespace_pattern\n            The pattern to use to match whitespace characters.\n        ensure_ascii\n            Whether to ensure the schema is ASCII-only.\n\n        \"\"\"\n        schema_str: str\n\n        if is_dict_instance(schema):\n            schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n        elif is_str_instance(schema):\n            schema_str = str(schema)\n        elif is_pydantic_model(schema):\n            schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n        elif is_typed_dict(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_dataclass(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_genson_schema_builder(schema):\n            schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {schema}. The schema must be either \"\n                + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n                + \"builder or a string or dict that contains the JSON schema \"\n                + \"specification\"\n            )\n\n        self.schema = schema_str\n        self.whitespace_pattern = whitespace_pattern\n\n    def __post_init__(self):\n        jsonschema.Draft7Validator.check_schema(json.loads(self.schema))\n\n    @classmethod\n    def is_json_schema(cls, obj: Any) -&gt; bool:\n        \"\"\"Check if the object provided is a JSON schema type.\n\n        Parameters\n        ----------\n        obj: Any\n            The object to check\n\n        Returns\n        -------\n        bool\n            True if the object is a JSON schema type, False otherwise\n\n        \"\"\"\n        return (\n            isinstance(obj, cls)\n            or is_pydantic_model(obj)\n            or is_typed_dict(obj)\n            or is_dataclass(obj)\n            or is_genson_schema_builder(obj)\n        )\n\n    @classmethod\n    def convert_to(\n        cls,\n        schema: Union[\n            \"JsonSchema\",\n            type[BaseModel],\n            _TypedDictMeta,\n            type,\n            SchemaBuilder,\n        ],\n        target_types: List[Literal[\n            \"str\",\n            \"dict\",\n            \"pydantic\",\n            \"typeddict\",\n            \"dataclass\",\n            \"genson\",\n        ]],\n    ) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n        \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n        If the schema provided is already of a type in the target_types, return\n        it unchanged.\n\n        Parameters\n        ----------\n        schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n            The schema to convert\n        target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n            The target types to convert to\n\n        \"\"\"\n        # If the schema provided is already of a type in the target_types,\n        # just return it\n        if isinstance(schema, cls):\n            if \"str\" in target_types:\n                return schema.schema\n            elif \"dict\" in target_types:\n                return json.loads(schema.schema)\n        elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n            return schema\n        elif is_typed_dict(schema) and \"typeddict\" in target_types:\n            return schema\n        elif is_dataclass(schema) and \"dataclass\" in target_types:\n            return schema\n        elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n            return schema\n\n        # Convert the schema to a JSON schema string/dict\n        if isinstance(schema, cls):\n            schema_str = schema.schema\n        else:\n            schema_str = cls(schema).schema\n        schema_dict = json.loads(schema_str)\n\n        for target_type in target_types:\n            try:\n                # Convert the JSON schema string to the target type\n                if target_type == \"str\":\n                    return schema_str\n                elif target_type == \"dict\":\n                    return schema_dict\n                elif target_type == \"pydantic\":\n                    return json_schema_dict_to_pydantic(schema_dict)\n                elif target_type == \"typeddict\":\n                    return json_schema_dict_to_typeddict(schema_dict)\n                elif target_type == \"dataclass\":\n                    return json_schema_dict_to_dataclass(schema_dict)\n                # No conversion available for genson\n            except Exception as e:  # pragma: no cover\n                warnings.warn(\n                    f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n                )\n                continue\n\n        raise ValueError(\n            f\"Cannot convert schema type {type(schema)} to any of the target \"\n            f\"types {target_types}\"\n        )\n\n    def _display_node(self) -&gt; str:\n        return f\"JsonSchema('{self.schema}')\"\n\n    def __repr__(self):\n        return f\"JsonSchema(schema='{self.schema}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, JsonSchema):\n            return False\n        try:\n            self_dict = json.loads(self.schema)\n            other_dict = json.loads(other.schema)\n            return self_dict == other_dict\n        except json.JSONDecodeError:  # pragma: no cover\n            return self.schema == other.schema\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"JsonSchema\":\n        \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n        schema.\n\n        Parameters\n        ----------\n        path:\n            The path to the file containing the JSON schema.\n        Returns\n        -------\n        JsonSchema\n            A JsonSchema instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            schema = json.load(f)\n        return cls(schema)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.JsonSchema.__init__","title":"<code>__init__(schema, whitespace_pattern=None, ensure_ascii=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The object containing the JSON schema.</p> required <code>whitespace_pattern</code> <code>Optional[str]</code> <p>The pattern to use to match whitespace characters.</p> <code>None</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to ensure the schema is ASCII-only.</p> <code>True</code> Source code in <code>outlines/types/dsl.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[\n        dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n    ],\n    whitespace_pattern: OptionalType[str] = None,\n    ensure_ascii: bool = True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    schema\n        The object containing the JSON schema.\n    whitespace_pattern\n        The pattern to use to match whitespace characters.\n    ensure_ascii\n        Whether to ensure the schema is ASCII-only.\n\n    \"\"\"\n    schema_str: str\n\n    if is_dict_instance(schema):\n        schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n    elif is_str_instance(schema):\n        schema_str = str(schema)\n    elif is_pydantic_model(schema):\n        schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n    elif is_typed_dict(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_dataclass(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_genson_schema_builder(schema):\n        schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n            + \"builder or a string or dict that contains the JSON schema \"\n            + \"specification\"\n        )\n\n    self.schema = schema_str\n    self.whitespace_pattern = whitespace_pattern\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.JsonSchema.convert_to","title":"<code>convert_to(schema, target_types)</code>  <code>classmethod</code>","text":"<p>Convert a JSON schema type to a different JSON schema type.</p> <p>If the schema provided is already of a type in the target_types, return it unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The schema to convert</p> required <code>target_types</code> <code>List[Literal['str', 'dict', 'pydantic', 'typeddict', 'dataclass', 'genson']]</code> <p>The target types to convert to</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef convert_to(\n    cls,\n    schema: Union[\n        \"JsonSchema\",\n        type[BaseModel],\n        _TypedDictMeta,\n        type,\n        SchemaBuilder,\n    ],\n    target_types: List[Literal[\n        \"str\",\n        \"dict\",\n        \"pydantic\",\n        \"typeddict\",\n        \"dataclass\",\n        \"genson\",\n    ]],\n) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n    \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n    If the schema provided is already of a type in the target_types, return\n    it unchanged.\n\n    Parameters\n    ----------\n    schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n        The schema to convert\n    target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n        The target types to convert to\n\n    \"\"\"\n    # If the schema provided is already of a type in the target_types,\n    # just return it\n    if isinstance(schema, cls):\n        if \"str\" in target_types:\n            return schema.schema\n        elif \"dict\" in target_types:\n            return json.loads(schema.schema)\n    elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n        return schema\n    elif is_typed_dict(schema) and \"typeddict\" in target_types:\n        return schema\n    elif is_dataclass(schema) and \"dataclass\" in target_types:\n        return schema\n    elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n        return schema\n\n    # Convert the schema to a JSON schema string/dict\n    if isinstance(schema, cls):\n        schema_str = schema.schema\n    else:\n        schema_str = cls(schema).schema\n    schema_dict = json.loads(schema_str)\n\n    for target_type in target_types:\n        try:\n            # Convert the JSON schema string to the target type\n            if target_type == \"str\":\n                return schema_str\n            elif target_type == \"dict\":\n                return schema_dict\n            elif target_type == \"pydantic\":\n                return json_schema_dict_to_pydantic(schema_dict)\n            elif target_type == \"typeddict\":\n                return json_schema_dict_to_typeddict(schema_dict)\n            elif target_type == \"dataclass\":\n                return json_schema_dict_to_dataclass(schema_dict)\n            # No conversion available for genson\n        except Exception as e:  # pragma: no cover\n            warnings.warn(\n                f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n            )\n            continue\n\n    raise ValueError(\n        f\"Cannot convert schema type {type(schema)} to any of the target \"\n        f\"types {target_types}\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.JsonSchema.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a JsonSchema instance from a .json file containing a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the JSON schema.</p> required <p>Returns:</p> Type Description <code>JsonSchema</code> <p>A JsonSchema instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"JsonSchema\":\n    \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n    schema.\n\n    Parameters\n    ----------\n    path:\n        The path to the file containing the JSON schema.\n    Returns\n    -------\n    JsonSchema\n        A JsonSchema instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        schema = json.load(f)\n    return cls(schema)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.JsonSchema.is_json_schema","title":"<code>is_json_schema(obj)</code>  <code>classmethod</code>","text":"<p>Check if the object provided is a JSON schema type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a JSON schema type, False otherwise</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef is_json_schema(cls, obj: Any) -&gt; bool:\n    \"\"\"Check if the object provided is a JSON schema type.\n\n    Parameters\n    ----------\n    obj: Any\n        The object to check\n\n    Returns\n    -------\n    bool\n        True if the object is a JSON schema type, False otherwise\n\n    \"\"\"\n    return (\n        isinstance(obj, cls)\n        or is_pydantic_model(obj)\n        or is_typed_dict(obj)\n        or is_dataclass(obj)\n        or is_genson_schema_builder(obj)\n    )\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.Regex","title":"<code>Regex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Regex(Term):\n    \"\"\"Class representing a regular expression.\n\n    Parameters\n    ----------\n    pattern\n        The regular expression as a string.\n\n    \"\"\"\n    pattern: str\n\n    def _display_node(self) -&gt; str:\n        return f\"Regex('{self.pattern}')\"\n\n    def __repr__(self):\n        return f\"Regex(pattern='{self.pattern}')\"\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.Term","title":"<code>Term</code>","text":"<p>Represents types defined with a regular expression.</p> <p><code>Regex</code> instances can be used as a type in a Pydantic model definittion. They will be translated to JSON Schema as a \"string\" field with the \"pattern\" keyword set to the regular expression this class represents. The class also handles validation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from outlines.types import Regex\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n&gt;&gt;&gt;     name: str\n&gt;&gt;&gt;     age: age_type\n</code></pre> Source code in <code>outlines/types/dsl.py</code> <pre><code>class Term:\n    \"\"\"Represents types defined with a regular expression.\n\n    `Regex` instances can be used as a type in a Pydantic model definittion.\n    They will be translated to JSON Schema as a \"string\" field with the\n    \"pattern\" keyword set to the regular expression this class represents. The\n    class also handles validation.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from outlines.types import Regex\n    &gt;&gt;&gt; from pydantic import BaseModel\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; class User(BaseModel):\n    &gt;&gt;&gt;     name: str\n    &gt;&gt;&gt;     age: age_type\n\n    \"\"\"\n\n    def __add__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([self, other])\n\n    def __radd__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([other, self])\n\n    def __or__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([self, other])\n\n    def __ror__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([other, self])\n\n    def __get_validator__(self, _core_schema):\n        def validate(input_value):\n            return self.validate(input_value)\n\n        return validate\n\n    def __get_pydantic_core_schema__(\n        self, source_type: Any, handler: GetCoreSchemaHandler\n    ) -&gt; cs.CoreSchema:\n        return cs.no_info_plain_validator_function(lambda value: self.validate(value))\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: cs.CoreSchema, handler: GetJsonSchemaHandler\n    ) -&gt; JsonSchemaValue:\n        return {\"type\": \"string\", \"pattern\": to_regex(self)}\n\n    def validate(self, value: str) -&gt; str:\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if not compiled.fullmatch(str(value)):\n            raise ValueError(\n                f\"Input should be in the language of the regular expression {pattern}\"\n            )\n        return value\n\n    def matches(self, value: str) -&gt; bool:\n        \"\"\"Check that a given value is in the language defined by the Term.\n\n        We make the assumption that the language defined by the term can\n        be defined with a regular expression.\n\n        \"\"\"\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if compiled.fullmatch(str(value)):\n            return True\n        return False\n\n    def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n        \"\"\"Display the regex tree in ASCII format.\"\"\"\n        branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n        result = indent + branch + self._display_node() + \"\\n\"\n\n        # Calculate the new indent for children\n        new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n        # Let each subclass handle its children\n        result += self._display_children(new_indent)\n        return result\n\n    def _display_node(self):\n        raise NotImplementedError\n\n    def _display_children(self, indent: str) -&gt; str:\n        \"\"\"Display the children of this node. Override in subclasses with children.\"\"\"\n        return \"\"\n\n    def __str__(self):\n        return self.display_ascii_tree()\n\n    def optional(self) -&gt; \"Optional\":\n        return optional(self)\n\n    def exactly(self, count: int) -&gt; \"QuantifyExact\":\n        return exactly(count, self)\n\n    def at_least(self, count: int) -&gt; \"QuantifyMinimum\":\n        return at_least(count, self)\n\n    def at_most(self, count: int) -&gt; \"QuantifyMaximum\":\n        return at_most(count, self)\n\n    def between(self, min_count: int, max_count: int) -&gt; \"QuantifyBetween\":\n        return between(min_count, max_count, self)\n\n    def one_or_more(self) -&gt; \"KleenePlus\":\n        return one_or_more(self)\n\n    def zero_or_more(self) -&gt; \"KleeneStar\":\n        return zero_or_more(self)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.Term.display_ascii_tree","title":"<code>display_ascii_tree(indent='', is_last=True)</code>","text":"<p>Display the regex tree in ASCII format.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n    \"\"\"Display the regex tree in ASCII format.\"\"\"\n    branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n    result = indent + branch + self._display_node() + \"\\n\"\n\n    # Calculate the new indent for children\n    new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n    # Let each subclass handle its children\n    result += self._display_children(new_indent)\n    return result\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.Term.matches","title":"<code>matches(value)</code>","text":"<p>Check that a given value is in the language defined by the Term.</p> <p>We make the assumption that the language defined by the term can be defined with a regular expression.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def matches(self, value: str) -&gt; bool:\n    \"\"\"Check that a given value is in the language defined by the Term.\n\n    We make the assumption that the language defined by the term can\n    be defined with a regular expression.\n\n    \"\"\"\n    pattern = to_regex(self)\n    compiled = re.compile(pattern)\n    if compiled.fullmatch(str(value)):\n        return True\n    return False\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.at_least","title":"<code>at_least(count, term)</code>","text":"<p>Repeat the term at least <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_least(count: int, term: Union[Term, str]) -&gt; QuantifyMinimum:\n    \"\"\"Repeat the term at least `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMinimum(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.at_most","title":"<code>at_most(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_most(count: int, term: Union[Term, str]) -&gt; QuantifyMaximum:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMaximum(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.either","title":"<code>either(*terms)</code>","text":"<p>Represents an alternative between different terms or strings.</p> <p>This factory function automatically translates string arguments into <code>String</code> objects.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def either(*terms: Union[str, Term]):\n    \"\"\"Represents an alternative between different terms or strings.\n\n    This factory function automatically translates string arguments\n    into `String` objects.\n\n    \"\"\"\n    terms = [String(arg) if isinstance(arg, str) else arg for arg in terms]\n    return Alternatives(terms)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.exactly","title":"<code>exactly(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def exactly(count: int, term: Union[Term, str]) -&gt; QuantifyExact:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyExact(term, count)\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.python_types_to_terms","title":"<code>python_types_to_terms(ptype, recursion_depth=0)</code>","text":"<p>Convert Python types to Outlines DSL terms that constrain LLM output.</p> <p>Parameters:</p> Name Type Description Default <code>ptype</code> <code>Any</code> <p>The Python type to convert</p> required <code>recursion_depth</code> <code>int</code> <p>Current recursion depth to prevent infinite recursion</p> <code>0</code> <p>Returns:</p> Type Description <code>Term</code> <p>The corresponding DSL <code>Term</code> instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def python_types_to_terms(ptype: Any, recursion_depth: int = 0) -&gt; Term:\n    \"\"\"Convert Python types to Outlines DSL terms that constrain LLM output.\n\n    Parameters\n    ----------\n    ptype\n        The Python type to convert\n    recursion_depth\n        Current recursion depth to prevent infinite recursion\n\n    Returns\n    -------\n    Term\n        The corresponding DSL `Term` instance.\n\n    \"\"\"\n    if recursion_depth &gt; 10:\n        raise RecursionError(\n            f\"Maximum recursion depth exceeded when converting {ptype}. \"\n            \"This might be due to a recursive type definition.\"\n        )\n\n    # First handle Term instances\n    if isinstance(ptype, Term):\n        return ptype\n\n    # Basic types\n    if is_int(ptype):\n        return types.integer\n    elif is_float(ptype):\n        return types.number\n    elif is_bool(ptype):\n        return types.boolean\n    elif is_str(ptype):\n        return types.string\n    elif is_native_dict(ptype):\n        return CFG(grammars.json)\n    elif is_time(ptype):\n        return types.time\n    elif is_date(ptype):\n        return types.date\n    elif is_datetime(ptype):\n        return types.datetime\n\n    # Basic type instances\n    if is_str_instance(ptype):\n        return String(ptype)\n    elif is_int_instance(ptype) or is_float_instance(ptype):\n        return Regex(str(ptype))\n\n    # Structured types\n    structured_type_checks = [\n        lambda x: is_dataclass(x),\n        lambda x: is_typed_dict(x),\n        lambda x: is_pydantic_model(x),\n    ]\n    if any(check(ptype) for check in structured_type_checks):\n        schema = TypeAdapter(ptype).json_schema()\n        return JsonSchema(schema)\n\n    elif is_genson_schema_builder(ptype):\n        schema = ptype.to_json()\n        return JsonSchema(schema)\n\n    if is_enum(ptype):\n        return Alternatives(\n            [\n                python_types_to_terms(member, recursion_depth + 1)\n                for member in _get_enum_members(ptype)\n            ]\n        )\n\n    args = get_args(ptype)\n    if is_literal(ptype):\n        return _handle_literal(args)\n    elif is_union(ptype):\n        return _handle_union(args, recursion_depth)\n    elif is_typing_list(ptype):\n        return _handle_list(args, recursion_depth)\n    elif is_typing_tuple(ptype):\n        return _handle_tuple(args, recursion_depth)\n    elif is_typing_dict(ptype):\n        return _handle_dict(args, recursion_depth)\n\n    if is_callable(ptype):\n        return JsonSchema(get_schema_from_signature(ptype))\n\n    type_name = getattr(ptype, \"__name__\", ptype)\n    raise TypeError(\n        f\"Type {type_name} is currently not supported. Please open an issue: \"\n        \"https://github.com/dottxt-ai/outlines/issues\"\n    )\n</code></pre>"},{"location":"api_reference/#outlines.types.dsl.to_regex","title":"<code>to_regex(term)</code>","text":"<p>Convert a term to a regular expression.</p> <p>We only consider self-contained terms that do not refer to another rule.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>Term</code> <p>The term to convert to a regular expression.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The regular expression as a string.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def to_regex(term: Term) -&gt; str:\n    \"\"\"Convert a term to a regular expression.\n\n    We only consider self-contained terms that do not refer to another rule.\n\n    Parameters\n    ----------\n    term\n        The term to convert to a regular expression.\n\n    Returns\n    -------\n    str\n        The regular expression as a string.\n\n    \"\"\"\n    if isinstance(term, String):\n        return re.escape(term.value)\n    elif isinstance(term, Regex):\n        return f\"({term.pattern})\"\n    elif isinstance(term, JsonSchema):\n        regex_str = outlines_core.json_schema.build_regex_from_schema(term.schema, term.whitespace_pattern)\n        return f\"({regex_str})\"\n    elif isinstance(term, Choice):\n        regexes = [to_regex(python_types_to_terms(item)) for item in term.items]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, KleeneStar):\n        return f\"({to_regex(term.term)})*\"\n    elif isinstance(term, KleenePlus):\n        return f\"({to_regex(term.term)})+\"\n    elif isinstance(term, Optional):\n        return f\"({to_regex(term.term)})?\"\n    elif isinstance(term, Alternatives):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, Sequence):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"{''.join(regexes)}\"\n    elif isinstance(term, QuantifyExact):\n        return f\"({to_regex(term.term)}){{{term.count}}}\"\n    elif isinstance(term, QuantifyMinimum):\n        return f\"({to_regex(term.term)}){{{term.min_count},}}\"\n    elif isinstance(term, QuantifyMaximum):\n        return f\"({to_regex(term.term)}){{,{term.max_count}}}\"\n    elif isinstance(term, QuantifyBetween):\n        return f\"({to_regex(term.term)}){{{term.min_count},{term.max_count}}}\"\n    else:\n        raise TypeError(\n            f\"Cannot convert object {repr(term)} to a regular expression.\"\n        )\n</code></pre>"},{"location":"api_reference/#outlines.types.json_schema_utils","title":"<code>json_schema_utils</code>","text":"<p>Convert JSON Schema dicts to Python types.</p>"},{"location":"api_reference/#outlines.types.json_schema_utils.json_schema_dict_to_dataclass","title":"<code>json_schema_dict_to_dataclass(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a dataclass.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a dataclass</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the dataclass</p> <code>None</code> <p>Returns:</p> Type Description <code>type</code> <p>The dataclass</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_dataclass(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type:\n    \"\"\"Convert a JSON Schema dict into a dataclass.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a dataclass\n    name: Optional[str]\n        The name of the dataclass\n\n    Returns\n    -------\n    type\n        The dataclass\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n    defaults: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"dataclass\")\n        annotations[property] = typ\n\n        if property not in required:\n            defaults[property] = None\n\n    class_dict = {\n        '__annotations__': annotations,\n        '__module__': __name__,\n    }\n\n    for property, default_val in defaults.items():\n        class_dict[property] = field(default=default_val)\n\n    cls = type(name or \"AnonymousDataclass\", (), class_dict)\n    return dataclass(cls)\n</code></pre>"},{"location":"api_reference/#outlines.types.json_schema_utils.json_schema_dict_to_pydantic","title":"<code>json_schema_dict_to_pydantic(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a Pydantic BaseModel class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Pydantic BaseModel</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the Pydantic BaseModel</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic BaseModel class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_pydantic(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type[BaseModel]:\n    \"\"\"Convert a JSON Schema dict into a Pydantic BaseModel class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Pydantic BaseModel\n    name: Optional[str]\n        The name of the Pydantic BaseModel\n\n    Returns\n    -------\n    type[BaseModel]\n        The Pydantic BaseModel class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    field_definitions: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"pydantic\")\n        if property not in required:\n            field_definitions[property] = (Optional[typ], None)\n        else:\n            field_definitions[property] = (typ, ...)\n\n    return create_model(name or \"AnonymousPydanticModel\", **field_definitions)\n</code></pre>"},{"location":"api_reference/#outlines.types.json_schema_utils.json_schema_dict_to_typeddict","title":"<code>json_schema_dict_to_typeddict(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a TypedDict class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a TypedDict</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the TypedDict</p> <code>None</code> <p>Returns:</p> Type Description <code>_TypedDictMeta</code> <p>The TypedDict class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_typeddict(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; _TypedDictMeta:\n    \"\"\"Convert a JSON Schema dict into a TypedDict class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a TypedDict\n    name: Optional[str]\n        The name of the TypedDict\n\n    Returns\n    -------\n    _TypedDictMeta\n        The TypedDict class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"typeddict\")\n        if property not in required:\n            typ = Optional[typ]\n        annotations[property] = typ\n\n    return TypedDict(name or \"AnonymousTypedDict\", annotations)  # type: ignore\n</code></pre>"},{"location":"api_reference/#outlines.types.json_schema_utils.schema_type_to_python","title":"<code>schema_type_to_python(schema, caller_target_type)</code>","text":"<p>Get a Python type from a JSON Schema dict.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Python type</p> required <code>caller_target_type</code> <code>Literal['pydantic', 'typeddict', 'dataclass']</code> <p>The type of the caller</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The Python type</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def schema_type_to_python(\n    schema: dict,\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n) -&gt; Any:\n    \"\"\"Get a Python type from a JSON Schema dict.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Python type\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n        The type of the caller\n\n    Returns\n    -------\n    Any\n        The Python type\n\n    \"\"\"\n    if \"enum\" in schema:\n        values = schema[\"enum\"]\n        return Literal[tuple(values)]\n\n    t = schema.get(\"type\")\n\n    if t == \"string\":\n        return str\n    elif t == \"integer\":\n        return int\n    elif t == \"number\":\n        return float\n    elif t == \"boolean\":\n        return bool\n    elif t == \"array\":\n        items = schema.get(\"items\", {})\n        if items:\n            item_type = schema_type_to_python(items, caller_target_type)\n        else:\n            item_type = Any\n        return List[item_type]  # type: ignore\n    elif t == \"object\":\n        name = schema.get(\"title\")\n        if caller_target_type == \"pydantic\":\n            return json_schema_dict_to_pydantic(schema, name)\n        elif caller_target_type == \"typeddict\":\n            return json_schema_dict_to_typeddict(schema, name)\n        elif caller_target_type == \"dataclass\":\n            return json_schema_dict_to_dataclass(schema, name)\n\n    return Any\n</code></pre>"},{"location":"api_reference/#outlines.types.locale","title":"<code>locale</code>","text":"<p>Locale-specific regex patterns.</p>"},{"location":"api_reference/#outlines.types.locale.us","title":"<code>us</code>","text":"<p>Locale-specific regex patterns for the United States.</p>"},{"location":"api_reference/#outlines.types.utils","title":"<code>utils</code>","text":"<p>Utility functions for the types module.</p>"},{"location":"api_reference/#outlines.types.utils.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/types/utils.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; dict:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    try:\n        fn_name = fn.__name__\n    except Exception as e:\n        fn_name = \"Arguments\"\n        warnings.warn(\n            f\"The function name could not be determined. Using default name 'Arguments' instead. For debugging, here is exact error:\\n{e}\",\n            category=UserWarning,\n        )\n    model = create_model(fn_name, **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>outlines</li> <li>applications</li> <li>backends<ul> <li>base</li> <li>llguidance</li> <li>outlines_core</li> <li>xgrammar</li> </ul> </li> <li>caching</li> <li>generator</li> <li>grammars</li> <li>inputs</li> <li>models<ul> <li>anthropic</li> <li>base</li> <li>dottxt</li> <li>gemini</li> <li>llamacpp</li> <li>mistral</li> <li>mlxlm</li> <li>ollama</li> <li>openai</li> <li>sglang</li> <li>tgi</li> <li>tokenizer</li> <li>transformers</li> <li>utils</li> <li>vllm</li> <li>vllm_offline</li> </ul> </li> <li>processors<ul> <li>base_logits_processor</li> <li>tensor_adapters<ul> <li>base</li> <li>mlx</li> <li>numpy</li> <li>torch</li> </ul> </li> </ul> </li> <li>templates</li> <li>types<ul> <li>airports</li> <li>countries</li> <li>dsl</li> <li>json_schema_utils</li> <li>locale<ul> <li>us</li> </ul> </li> <li>utils</li> </ul> </li> </ul>"},{"location":"api_reference/applications/","title":"applications","text":"<p>Encapsulate a prompt template and an output type into a reusable object.</p>"},{"location":"api_reference/applications/#outlines.applications.Application","title":"<code>Application</code>","text":"<p>Application is a class that encapsulates a prompt template and an output type. It can be called to generate a response by providing a model, the values to be substituted in the template in a dictionary and optional inference parameters.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[Template, Callable]</code> <p>A callable that takes arguments and returns a prompt string.</p> required <code>output_type</code> <code>Any</code> <p>The expected output type of the generated response.</p> <code>None</code> <p>Examples:</p> <pre><code>from pydantic import BaseModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines import models, Application\nfrom outlines.types import JsonType\nfrom outlines.templates import Template\n\nclass OutputModel(BaseModel):\n    result: int\n\nmodel = models.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\ntemplate_string = \"What is 2 times {{ num }}?\"\ntemplate = Template.from_string(template_string)\n\napplication = Application(template, JsonType(OutputModel))\n\nresult = application(model, {\"num\": 3}, max_new_tokens=20)\nprint(result)  # Expected output: { \"result\" : 6 }\n</code></pre> Source code in <code>outlines/applications.py</code> <pre><code>class Application:\n    \"\"\"\n    Application is a class that encapsulates a prompt template and an\n    output type. It can be called to generate a response by providing a\n    model, the values to be substituted in the template in a dictionary\n    and optional inference parameters.\n\n    Parameters\n    ----------\n    template : Union[Template, Callable]\n        A callable that takes arguments and returns a prompt string.\n    output_type : Any\n        The expected output type of the generated response.\n\n    Examples\n    --------\n    ```python\n    from pydantic import BaseModel\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    from outlines import models, Application\n    from outlines.types import JsonType\n    from outlines.templates import Template\n\n    class OutputModel(BaseModel):\n        result: int\n\n    model = models.from_transformers(\n        AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n        AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n    )\n\n    template_string = \"What is 2 times {{ num }}?\"\n    template = Template.from_string(template_string)\n\n    application = Application(template, JsonType(OutputModel))\n\n    result = application(model, {\"num\": 3}, max_new_tokens=20)\n    print(result)  # Expected output: { \"result\" : 6 }\n    ```\n\n    \"\"\"\n    def __init__(\n        self,\n        template: Union[Template, Callable],\n        output_type: Optional[Any] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        template\n            The template to use to build the prompt.\n        output_type\n            The output type provided to the generator.\n\n        \"\"\"\n        self.template = template\n        self.output_type = output_type\n        self.generator: Optional[Union[\n            BlackBoxGenerator, SteerableGenerator\n        ]] = None\n        self.model: Optional[Model] = None\n\n    def __call__(\n        self,\n        model: Model,\n        template_vars: Dict[str, Any],\n        **inference_kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The model to use to generate the response.\n        template_vars\n            The variables to be substituted in the template.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n        Returns\n        -------\n        Any\n            The generated response.\n        \"\"\"\n        if model is None:\n            raise ValueError(\"you must provide a model\")\n        # We save the generator to avoid creating a new one for each call.\n        # If the model has changed since the last call, we create a new\n        # generator.\n        if model != self.model:\n            self.model = model\n            self.generator = Generator(model, self.output_type)  # type: ignore\n\n        prompt = self.template(**template_vars)\n        assert self.generator is not None\n        return self.generator(prompt, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/applications/#outlines.applications.Application.__call__","title":"<code>__call__(model, template_vars, **inference_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use to generate the response.</p> required <code>template_vars</code> <code>Dict[str, Any]</code> <p>The variables to be substituted in the template.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The generated response.</p> Source code in <code>outlines/applications.py</code> <pre><code>def __call__(\n    self,\n    model: Model,\n    template_vars: Dict[str, Any],\n    **inference_kwargs\n) -&gt; Any:\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The model to use to generate the response.\n    template_vars\n        The variables to be substituted in the template.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n    Returns\n    -------\n    Any\n        The generated response.\n    \"\"\"\n    if model is None:\n        raise ValueError(\"you must provide a model\")\n    # We save the generator to avoid creating a new one for each call.\n    # If the model has changed since the last call, we create a new\n    # generator.\n    if model != self.model:\n        self.model = model\n        self.generator = Generator(model, self.output_type)  # type: ignore\n\n    prompt = self.template(**template_vars)\n    assert self.generator is not None\n    return self.generator(prompt, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/applications/#outlines.applications.Application.__init__","title":"<code>__init__(template, output_type=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[Template, Callable]</code> <p>The template to use to build the prompt.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided to the generator.</p> <code>None</code> Source code in <code>outlines/applications.py</code> <pre><code>def __init__(\n    self,\n    template: Union[Template, Callable],\n    output_type: Optional[Any] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    template\n        The template to use to build the prompt.\n    output_type\n        The output type provided to the generator.\n\n    \"\"\"\n    self.template = template\n    self.output_type = output_type\n    self.generator: Optional[Union[\n        BlackBoxGenerator, SteerableGenerator\n    ]] = None\n    self.model: Optional[Model] = None\n</code></pre>"},{"location":"api_reference/caching/","title":"caching","text":"<p>Caching and memoization of function calls.</p>"},{"location":"api_reference/caching/#outlines.caching.cache","title":"<code>cache(expire=None, typed=False, ignore=())</code>","text":"<p>Caching decorator for memoizing function calls.</p> <p>The cache key is created based on the values returned by the key_function callable if provided or based on the arguments of the decorated function directly otherwise</p> <p>This is based on <code>diskcache</code>'s <code>memoize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>expire</code> <code>Optional[float]</code> <p>Seconds until arguments expire.</p> <code>None</code> <code>typed</code> <p>Cache different types separately.</p> <code>False</code> <code>ignore</code> <p>Positional or keyword arguments to ignore.</p> <code>()</code> <p>Returns:</p> Type Description <code>    A decorator function that can be applied to other functions.</code> Source code in <code>outlines/caching.py</code> <pre><code>def cache(expire: Optional[float] = None, typed=False, ignore=()):\n    \"\"\"Caching decorator for memoizing function calls.\n\n    The cache key is created based on the values returned by the key_function callable\n    if provided or based on the arguments of the decorated function directly otherwise\n\n    This is based on `diskcache`'s `memoize`.\n\n    Parameters\n    ----------\n    expire\n        Seconds until arguments expire.\n    typed\n        Cache different types separately.\n    ignore\n        Positional or keyword arguments to ignore.\n\n    Returns\n    -------\n        A decorator function that can be applied to other functions.\n    \"\"\"\n\n    def decorator(cached_function: Callable):\n        memory = get_cache()\n\n        base = (full_name(cached_function),)\n\n        if asyncio.iscoroutinefunction(cached_function):  # pragma: no cover\n\n            async def wrapper(*args, **kwargs):\n                if not _caching_enabled:\n                    return await cached_function(*args, **kwargs)\n\n                cache_key = wrapper.__cache_key__(*args, **kwargs)\n                result = wrapper.__memory__.get(cache_key, default=ENOVAL, retry=True)\n\n                if result is ENOVAL:\n                    result = await cached_function(*args, **kwargs)\n                    wrapper.__memory__.set(cache_key, result, expire, retry=True)\n\n                return result\n\n        else:\n\n            def wrapper(*args, **kwargs):\n                if not _caching_enabled:\n                    return cached_function(*args, **kwargs)\n\n                cache_key = wrapper.__cache_key__(*args, **kwargs)\n                result = wrapper.__memory__.get(cache_key, default=ENOVAL, retry=True)\n\n                if result is ENOVAL:\n                    result = cached_function(*args, **kwargs)\n                    wrapper.__memory__.set(cache_key, result, expire, retry=True)\n\n                return result\n\n        def __cache_key__(*args, **kwargs):\n            \"\"\"Make key for cache given function arguments.\"\"\"\n            return args_to_key(base, args, kwargs, typed, ignore)\n\n        wrapper.__cache_key__ = __cache_key__  # type: ignore\n        wrapper.__memory__ = memory  # type: ignore\n        wrapper.__wrapped__ = cached_function  # type: ignore\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api_reference/caching/#outlines.caching.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Erase the cache completely.</p> Source code in <code>outlines/caching.py</code> <pre><code>def clear_cache():\n    \"\"\"Erase the cache completely.\"\"\"\n    memory = get_cache()\n    memory.clear()\n</code></pre>"},{"location":"api_reference/caching/#outlines.caching.disable_cache","title":"<code>disable_cache()</code>","text":"<p>Disable the cache for this session.</p> <p>Generative models output different results each time they are called when sampling. This can be a desirable property for some workflows, in which case one can call <code>outlines.call.disable</code> to disable the cache for the session.</p> <p>This function does not delete the cache, call <code>outlines.cache.clear</code> instead. It also does not overwrite the cache with the values returned during the session.</p> Example <p><code>outlines.cache.disable</code> should be called right after importing outlines:</p> <p>import outlines.caching as cache cache.disable_cache()</p> Source code in <code>outlines/caching.py</code> <pre><code>def disable_cache():\n    \"\"\"Disable the cache for this session.\n\n    Generative models output different results each time they are called when\n    sampling. This can be a desirable property for some workflows, in which case\n    one can call `outlines.call.disable` to disable the cache for the session.\n\n    This function does not delete the cache, call `outlines.cache.clear`\n    instead. It also does not overwrite the cache with the values returned\n    during the session.\n\n    Example\n    -------\n\n    `outlines.cache.disable` should be called right after importing outlines:\n\n    &gt;&gt;&gt; import outlines.caching as cache\n    &gt;&gt;&gt; cache.disable_cache()\n\n    \"\"\"\n    global _caching_enabled\n    _caching_enabled = False\n</code></pre>"},{"location":"api_reference/caching/#outlines.caching.get_cache","title":"<code>get_cache()</code>  <code>cached</code>","text":"<p>Get the context object that contains previously-computed return values.</p> <p>The cache is used to avoid unnecessary computations and API calls, which can be long and expensive for large models.</p> <p>The cache directory defaults to <code>HOMEDIR/.cache/outlines</code>, but this choice can be overridden by the user by setting the value of the <code>OUTLINES_CACHE_DIR</code> environment variable.</p> Source code in <code>outlines/caching.py</code> <pre><code>@functools.lru_cache(1)\ndef get_cache():\n    \"\"\"Get the context object that contains previously-computed return values.\n\n    The cache is used to avoid unnecessary computations and API calls, which can\n    be long and expensive for large models.\n\n    The cache directory defaults to `HOMEDIR/.cache/outlines`, but this choice\n    can be overridden by the user by setting the value of the `OUTLINES_CACHE_DIR`\n    environment variable.\n\n    \"\"\"\n    from outlines._version import __version__ as outlines_version  # type: ignore\n\n    outlines_cache_dir = os.environ.get(\"OUTLINES_CACHE_DIR\")\n    xdg_cache_home = os.environ.get(\"XDG_CACHE_HOME\")\n    home_dir = os.path.normpath(os.path.expanduser(\"~\"))\n    if outlines_cache_dir:\n        # OUTLINES_CACHE_DIR takes precedence\n        cache_dir = outlines_cache_dir\n    elif xdg_cache_home:  # pragma: no cover\n        cache_dir = os.path.join(xdg_cache_home, \".cache\", \"outlines\")\n    elif home_dir != \"/\": # pragma: no cover\n        cache_dir = os.path.join(home_dir, \".cache\", \"outlines\")\n    else:  # pragma: no cover\n        # home_dir may be / inside a docker container without existing user\n        tempdir = tempfile.gettempdir()\n        cache_dir = os.path.join(tempdir, \".cache\", \"outlines\")\n\n    memory = Cache(\n        cache_dir,\n        eviction_policy=\"none\",\n        cull_limit=0,\n        disk=CloudpickleDisk,\n    )\n\n    # ensure if version upgrade occurs, old cache is pruned\n    if outlines_version != memory.get(\"__version__\"):\n        memory.clear()\n    memory[\"__version__\"] = outlines_version\n\n    return memory\n</code></pre>"},{"location":"api_reference/generator/","title":"generator","text":"<p>Encapsulate a model and an output type into a reusable object.</p>"},{"location":"api_reference/generator/#outlines.generator.AsyncBlackBoxGenerator","title":"<code>AsyncBlackBoxGenerator</code>","text":"<p>Asynchronous generator for which we don't control constrained generation.</p> <p>The output type provided is not compiled into a logits processor, but is instead directly passed on to the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>class AsyncBlackBoxGenerator:\n    \"\"\"Asynchronous generator for which we don't control constrained\n    generation.\n\n    The output type provided is not compiled into a logits processor, but is\n    instead directly passed on to the model.\n\n    \"\"\"\n    output_type: Optional[Any]\n\n    def __init__(self, model: AsyncBlackBoxModel, output_type: Optional[Any]):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type that will be used to constrain the generation.\n\n        \"\"\"\n        self.model = model\n        self.output_type = output_type\n\n    async def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return await self.model.generate(\n            prompt, self.output_type, **inference_kwargs\n        )\n\n    async def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        return await self.model.generate_batch(\n            prompts, self.output_type, **inference_kwargs\n        )\n\n    async def stream(self, prompt: Any, **inference_kwargs) -&gt; AsyncIterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        async for chunk in self.model.generate_stream(  # pragma: no cover\n            prompt, self.output_type, **inference_kwargs\n        ):\n            yield chunk\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.AsyncBlackBoxGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return await self.model.generate(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.AsyncBlackBoxGenerator.__init__","title":"<code>__init__(model, output_type)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncBlackBoxModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type that will be used to constrain the generation.</p> required Source code in <code>outlines/generator.py</code> <pre><code>def __init__(self, model: AsyncBlackBoxModel, output_type: Optional[Any]):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type that will be used to constrain the generation.\n\n    \"\"\"\n    self.model = model\n    self.output_type = output_type\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.AsyncBlackBoxGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    return await self.model.generate_batch(\n        prompts, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.AsyncBlackBoxGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>async def stream(self, prompt: Any, **inference_kwargs) -&gt; AsyncIterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    async for chunk in self.model.generate_stream(  # pragma: no cover\n        prompt, self.output_type, **inference_kwargs\n    ):\n        yield chunk\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.BlackBoxGenerator","title":"<code>BlackBoxGenerator</code>","text":"<p>Synchronous generator for which we don't control constrained generation.</p> <p>The output type provided is not compiled into a logits processor, but is instead directly passed on to the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>class BlackBoxGenerator:\n    \"\"\"Synchronous generator for which we don't control constrained\n    generation.\n\n    The output type provided is not compiled into a logits processor, but is\n    instead directly passed on to the model.\n\n    \"\"\"\n    output_type: Optional[Any]\n\n    def __init__(self, model: BlackBoxModel, output_type: Optional[Any]):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type that will be used to constrain the generation.\n\n        \"\"\"\n        self.model = model\n        self.output_type = output_type\n\n    def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return self.model.generate(\n            prompt, self.output_type, **inference_kwargs\n        )\n\n    def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        return self.model.generate_batch(\n            prompts, self.output_type, **inference_kwargs\n        )\n\n    def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        return self.model.generate_stream(\n            prompt, self.output_type, **inference_kwargs\n        )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.BlackBoxGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return self.model.generate(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.BlackBoxGenerator.__init__","title":"<code>__init__(model, output_type)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>BlackBoxModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type that will be used to constrain the generation.</p> required Source code in <code>outlines/generator.py</code> <pre><code>def __init__(self, model: BlackBoxModel, output_type: Optional[Any]):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type that will be used to constrain the generation.\n\n    \"\"\"\n    self.model = model\n    self.output_type = output_type\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.BlackBoxGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    return self.model.generate_batch(\n        prompts, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.BlackBoxGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    return self.model.generate_stream(\n        prompt, self.output_type, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator","title":"<code>SteerableGenerator</code>","text":"<p>Represents a generator for which we control constrained generation.</p> <p>The generator is responsible for building and storing the logits processor (which can be quite expensive to build), and then passing it to the model when the generator is called.</p> <p>The argument defining constrained generation can be of 2 types associated to different methods to create an instance of the generator: - <code>output_type</code> (through <code>__init__</code>): an output type as defined in the   <code>outlines.types</code> module - <code>processor</code> (through <code>from_processor</code>): an already built logits processor    as defined in the <code>outlines.processors</code> module</p> <p>The 2 parameters are mutually exclusive.</p> Source code in <code>outlines/generator.py</code> <pre><code>class SteerableGenerator:\n    \"\"\"Represents a generator for which we control constrained generation.\n\n    The generator is responsible for building and storing the logits processor\n    (which can be quite expensive to build), and then passing it to the model\n    when the generator is called.\n\n    The argument defining constrained generation can be of 2 types associated\n    to different methods to create an instance of the generator:\n    - `output_type` (through `__init__`): an output type as defined in the\n      `outlines.types` module\n    - `processor` (through `from_processor`): an already built logits processor\n       as defined in the `outlines.processors` module\n\n    The 2 parameters are mutually exclusive.\n\n    \"\"\"\n    logits_processor: Optional[LogitsProcessorType]\n\n    def __init__(\n        self,\n        model: SteerableModel,\n        output_type: Optional[Any],\n        backend_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        output_type\n            The output type expressed as a Python type\n        backend_name\n            The name of the backend to use to create the logits processor.\n\n        \"\"\"\n        self.model = model\n        if output_type is None:\n            self.logits_processor = None\n        else:\n            term = python_types_to_terms(output_type)\n            if isinstance(term, CFG):\n                cfg_string = term.definition\n                self.logits_processor = get_cfg_logits_processor(\n                    backend_name,\n                    model,\n                    cfg_string,\n                )\n            elif isinstance(term, JsonSchema):\n                self.logits_processor = get_json_schema_logits_processor(\n                    backend_name,\n                    model,\n                    term.schema,\n                )\n            else:\n                regex_string = to_regex(term)\n                self.logits_processor = get_regex_logits_processor(\n                    backend_name,\n                    model,\n                    regex_string,\n                )\n\n    @classmethod\n    def from_processor(\n        cls, model: SteerableModel, processor: LogitsProcessorType\n    ):\n        \"\"\"Create a generator from a logits processor.\n\n        Parameters\n        ----------\n        model\n            An instance of an Outlines model.\n        processor\n            An instance of a logits processor.\n\n        \"\"\"\n        instance = cls.__new__(cls)\n        instance.model = model\n        instance.logits_processor = processor\n\n        return instance\n\n    def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate(\n            prompt, self.logits_processor, **inference_kwargs\n        )\n\n    def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        Parameters\n        ----------\n        prompts\n            The list of prompts to use to generate a batch of responses.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate_batch(\n            prompts, self.logits_processor, **inference_kwargs\n        )\n\n    def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        prompt\n            The prompt to use to generate a response.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        if self.logits_processor is not None:\n            self.logits_processor.reset()\n        return self.model.generate_stream(\n            prompt, self.logits_processor, **inference_kwargs\n        )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator.__call__","title":"<code>__call__(prompt, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def __call__(self, prompt: Any, **inference_kwargs) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate(\n        prompt, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator.__init__","title":"<code>__init__(model, output_type, backend_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type expressed as a Python type</p> required <code>backend_name</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor.</p> <code>None</code> Source code in <code>outlines/generator.py</code> <pre><code>def __init__(\n    self,\n    model: SteerableModel,\n    output_type: Optional[Any],\n    backend_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type expressed as a Python type\n    backend_name\n        The name of the backend to use to create the logits processor.\n\n    \"\"\"\n    self.model = model\n    if output_type is None:\n        self.logits_processor = None\n    else:\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            cfg_string = term.definition\n            self.logits_processor = get_cfg_logits_processor(\n                backend_name,\n                model,\n                cfg_string,\n            )\n        elif isinstance(term, JsonSchema):\n            self.logits_processor = get_json_schema_logits_processor(\n                backend_name,\n                model,\n                term.schema,\n            )\n        else:\n            regex_string = to_regex(term)\n            self.logits_processor = get_regex_logits_processor(\n                backend_name,\n                model,\n                regex_string,\n            )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator.batch","title":"<code>batch(prompts, **inference_kwargs)</code>","text":"<p>Generate a batch of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[Any]</code> <p>The list of prompts to use to generate a batch of responses.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def batch(self, prompts: List[Any], **inference_kwargs) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    Parameters\n    ----------\n    prompts\n        The list of prompts to use to generate a batch of responses.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate_batch(\n        prompts, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator.from_processor","title":"<code>from_processor(model, processor)</code>  <code>classmethod</code>","text":"<p>Create a generator from a logits processor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>An instance of an Outlines model.</p> required <code>processor</code> <code>LogitsProcessorType</code> <p>An instance of a logits processor.</p> required Source code in <code>outlines/generator.py</code> <pre><code>@classmethod\ndef from_processor(\n    cls, model: SteerableModel, processor: LogitsProcessorType\n):\n    \"\"\"Create a generator from a logits processor.\n\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    processor\n        An instance of a logits processor.\n\n    \"\"\"\n    instance = cls.__new__(cls)\n    instance.model = model\n    instance.logits_processor = processor\n\n    return instance\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.SteerableGenerator.stream","title":"<code>stream(prompt, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Any</code> <p>The prompt to use to generate a response.</p> required <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/generator.py</code> <pre><code>def stream(self, prompt: Any, **inference_kwargs) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate a response.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    if self.logits_processor is not None:\n        self.logits_processor.reset()\n    return self.model.generate_stream(\n        prompt, self.logits_processor, **inference_kwargs\n    )\n</code></pre>"},{"location":"api_reference/generator/#outlines.generator.Generator","title":"<code>Generator(model, output_type=None, backend=None, *, processor=None)</code>","text":"<p>Create a generator for the given model and output parameters.</p> <p>The 2 parameters output_type and processor are mutually exclusive. The parameters processor is only supported for SteerableModel instances (typically local models) and is intended to be only used by advanced users.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Model, AsyncModel]</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type expressed as a Python type or a type defined in the outlines.types.dsl module.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor. Only used for steerable models if there is an output type and <code>processor</code> is not provided.</p> <code>None</code> <code>processor</code> <code>Optional[LogitsProcessorType]</code> <p>An instance of a logits processor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]</code> <p>A generator instance.</p> Source code in <code>outlines/generator.py</code> <pre><code>def Generator(\n    model: Union[Model, AsyncModel],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    *,\n    processor: Optional[LogitsProcessorType] = None,\n) -&gt; Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]:\n    \"\"\"Create a generator for the given model and output parameters.\n\n    The 2 parameters output_type and processor are mutually exclusive. The\n    parameters processor is only supported for SteerableModel instances\n    (typically local models) and is intended to be only used by advanced users.\n\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type expressed as a Python type or a type defined in the\n        outlines.types.dsl module.\n    backend\n        The name of the backend to use to create the logits processor. Only\n        used for steerable models if there is an output type and `processor` is\n        not provided.\n    processor\n        An instance of a logits processor.\n\n    Returns\n    -------\n    Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]\n        A generator instance.\n\n    \"\"\"\n    provided_output_params = sum(\n        param is not None\n        for param in [output_type, processor]\n    )\n    if provided_output_params &gt; 1:\n        raise ValueError(\n            \"At most one of output_type or processor can be provided\"\n        )\n\n    if isinstance(model, SteerableModel): # type: ignore\n        if processor is not None:\n            return SteerableGenerator.from_processor(model, processor) # type: ignore\n        else:\n            return SteerableGenerator(model, output_type, backend) # type: ignore\n    else:\n        if processor is not None:\n            raise NotImplementedError(\n                \"This model does not support logits processors\"\n            )\n        if isinstance(model, AsyncBlackBoxModel): # type: ignore\n            return AsyncBlackBoxGenerator(model, output_type) # type: ignore\n        elif isinstance(model, BlackBoxModel): # type: ignore\n            return BlackBoxGenerator(model, output_type) # type: ignore\n        else:\n            raise ValueError(\n                \"The model argument must be an instance of \"\n                \"SteerableModel, BlackBoxModel or AsyncBlackBoxModel\"\n            )\n</code></pre>"},{"location":"api_reference/grammars/","title":"grammars","text":"<p>A few common Lark grammars.</p>"},{"location":"api_reference/grammars/#outlines.grammars.read_grammar","title":"<code>read_grammar(grammar_file_name, base_grammar_path=GRAMMAR_PATH)</code>","text":"<p>Read grammar file from default grammar path.</p> <p>Parameters:</p> Name Type Description Default <code>grammar_file_name</code> <code>str</code> <p>The name of the grammar file to read.</p> required <code>base_grammar_path</code> <code>Path</code> <p>The path to the directory containing the grammar file.</p> <code>GRAMMAR_PATH</code> <p>Returns:</p> Type Description <code>str</code> <p>The contents of the grammar file.</p> Source code in <code>outlines/grammars.py</code> <pre><code>def read_grammar(\n    grammar_file_name: str,\n    base_grammar_path: Path = GRAMMAR_PATH,\n) -&gt; str:\n    \"\"\"Read grammar file from default grammar path.\n\n    Parameters\n    ----------\n    grammar_file_name\n        The name of the grammar file to read.\n    base_grammar_path\n        The path to the directory containing the grammar file.\n\n    Returns\n    -------\n    str\n        The contents of the grammar file.\n\n    \"\"\"\n    full_path = base_grammar_path / grammar_file_name\n    with open(full_path) as file:\n        return file.read()\n</code></pre>"},{"location":"api_reference/inputs/","title":"inputs","text":"<p>Contain classes used to define the inputs of a model.</p>"},{"location":"api_reference/inputs/#outlines.inputs.Audio","title":"<code>Audio</code>  <code>dataclass</code>","text":"<p>Contains an audio that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports audio processing.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Any</code> <p>The audio to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Audio:\n    \"\"\"Contains an audio that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports audio\n    processing.\n\n    Parameters\n    ----------\n    audio\n        The audio to use in the text generation.\n\n    \"\"\"\n    audio: Any\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat","title":"<code>Chat</code>  <code>dataclass</code>","text":"<p>Contains the input for a chat model.</p> <p>Provide an instance of this class as the <code>model_input</code> argument to a model that supports chat.</p> <p>Each message contained in the messages list must be a dict with 'role' and 'content' keys. The role can be 'user', 'assistant', or 'system'. The content supports either: - a text string, - a list containing text and assets (e.g., [\"Describe...\", Image(...)]), - only for HuggingFace transformers models, a list of dict items with explicit types (e.g.,   [{\"type\": \"text\", \"text\": \"Describe...\"}, {\"type\": \"image\", \"image\": Image(...)}])</p> <p>Examples:</p> <pre><code># Initialize the chat with a system message.\nchat_prompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n])\n\n# Add a user message with an image and call the model (not shown here).\nchat_prompt.add_user_message([\"Describe the image below\", Image(image)])\n\n# Add as an assistant message the response from the model.\nchat_prompt.add_assistant_message(\"There is a black cat sitting on a couch.\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>The list of messages that will be provided to the model.</p> <code>None</code> Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Chat:\n    \"\"\"Contains the input for a chat model.\n\n    Provide an instance of this class as the `model_input` argument to a model\n    that supports chat.\n\n    Each message contained in the messages list must be a dict with 'role' and\n    'content' keys. The role can be 'user', 'assistant', or 'system'. The content\n    supports either:\n    - a text string,\n    - a list containing text and assets (e.g., [\"Describe...\", Image(...)]),\n    - only for HuggingFace transformers models, a list of dict items with explicit types (e.g.,\n      [{\"type\": \"text\", \"text\": \"Describe...\"}, {\"type\": \"image\", \"image\": Image(...)}])\n\n    Examples\n    --------\n    ```python\n    # Initialize the chat with a system message.\n    chat_prompt = Chat([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    ])\n\n    # Add a user message with an image and call the model (not shown here).\n    chat_prompt.add_user_message([\"Describe the image below\", Image(image)])\n\n    # Add as an assistant message the response from the model.\n    chat_prompt.add_assistant_message(\"There is a black cat sitting on a couch.\")\n    ```\n\n    Parameters\n    ----------\n    messages\n        The list of messages that will be provided to the model.\n\n    \"\"\"\n    messages: List[Dict[str, Any]] = None # type: ignore\n\n    def __post_init__(self):\n        if self.messages is None:\n            self.messages = []\n\n    def append(self, message: Dict[str, Any]):\n        \"\"\"Add a message to the chat.\n\n        Parameters\n        ----------\n        message\n            The message to add to the chat.\n\n        \"\"\"\n        self.messages.append(message)\n\n    def extend(self, messages: List[Dict[str, Any]]):\n        \"\"\"Add a list of messages to the chat.\n\n        Parameters\n        ----------\n        messages\n            The list of messages to add to the chat.\n\n        \"\"\"\n        self.messages.extend(messages)\n\n    def pop(self) -&gt; Dict[str, Any]:\n        \"\"\"Remove the last message from the chat.\n\n        Returns\n        -------\n        message\n            The removed message.\n\n        \"\"\"\n        return self.messages.pop()\n\n    def add_system_message(self, content: str | List[Any]):\n        \"\"\"Add a system message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the system message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"system\", \"content\": content})\n\n    def add_user_message(self, content: str | List[Any]):\n        \"\"\"Add a user message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the user message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"user\", \"content\": content})\n\n    def add_assistant_message(self, content: str | List[Any]):\n        \"\"\"Add an assistant message to the chat.\n\n        Parameters\n        ----------\n        content\n            The content of the assistant message.\n\n        \"\"\"\n        self.messages.append({\"role\": \"assistant\", \"content\": content})\n\n    def __str__(self):\n        return \"\\n\".join(str(message) for message in self.messages)\n\n    def __repr__(self):\n        return f\"Chat(messages={self.messages})\"\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.add_assistant_message","title":"<code>add_assistant_message(content)</code>","text":"<p>Add an assistant message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the assistant message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_assistant_message(self, content: str | List[Any]):\n    \"\"\"Add an assistant message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the assistant message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"assistant\", \"content\": content})\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.add_system_message","title":"<code>add_system_message(content)</code>","text":"<p>Add a system message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the system message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_system_message(self, content: str | List[Any]):\n    \"\"\"Add a system message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the system message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"system\", \"content\": content})\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.add_user_message","title":"<code>add_user_message(content)</code>","text":"<p>Add a user message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | List[Any]</code> <p>The content of the user message.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def add_user_message(self, content: str | List[Any]):\n    \"\"\"Add a user message to the chat.\n\n    Parameters\n    ----------\n    content\n        The content of the user message.\n\n    \"\"\"\n    self.messages.append({\"role\": \"user\", \"content\": content})\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.append","title":"<code>append(message)</code>","text":"<p>Add a message to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Dict[str, Any]</code> <p>The message to add to the chat.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def append(self, message: Dict[str, Any]):\n    \"\"\"Add a message to the chat.\n\n    Parameters\n    ----------\n    message\n        The message to add to the chat.\n\n    \"\"\"\n    self.messages.append(message)\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.extend","title":"<code>extend(messages)</code>","text":"<p>Add a list of messages to the chat.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>The list of messages to add to the chat.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>def extend(self, messages: List[Dict[str, Any]]):\n    \"\"\"Add a list of messages to the chat.\n\n    Parameters\n    ----------\n    messages\n        The list of messages to add to the chat.\n\n    \"\"\"\n    self.messages.extend(messages)\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Chat.pop","title":"<code>pop()</code>","text":"<p>Remove the last message from the chat.</p> <p>Returns:</p> Type Description <code>message</code> <p>The removed message.</p> Source code in <code>outlines/inputs.py</code> <pre><code>def pop(self) -&gt; Dict[str, Any]:\n    \"\"\"Remove the last message from the chat.\n\n    Returns\n    -------\n    message\n        The removed message.\n\n    \"\"\"\n    return self.messages.pop()\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Image","title":"<code>Image</code>  <code>dataclass</code>","text":"<p>Contains an image that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports vision.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Image:\n    \"\"\"Contains an image that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports vision.\n\n    Parameters\n    ----------\n    image\n        The image to use in the text generation.\n\n    \"\"\"\n    image: PILImage.Image\n\n    def __post_init__(self):\n        image = self.image\n\n        if not image.format:\n            raise TypeError(\n                \"Could not read the format of the image passed to the model.\"\n            )\n\n        buffer = BytesIO()\n        image.save(buffer, format=image.format)\n        self.image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        self.image_format = f\"image/{image.format.lower()}\"\n</code></pre>"},{"location":"api_reference/inputs/#outlines.inputs.Video","title":"<code>Video</code>  <code>dataclass</code>","text":"<p>Contains a video that can be passed to a multimodal model.</p> <p>Provide one or several instances of this class along with a text prompt in a list as the <code>model_input</code> argument to a model that supports video processing.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Any</code> <p>The video to use in the text generation.</p> required Source code in <code>outlines/inputs.py</code> <pre><code>@dataclass\nclass Video:\n    \"\"\"Contains a video that can be passed to a multimodal model.\n\n    Provide one or several instances of this class along with a text prompt\n    in a list as the `model_input` argument to a model that supports video\n    processing.\n\n    Parameters\n    ----------\n    video\n        The video to use in the text generation.\n\n    \"\"\"\n    video: Any\n</code></pre>"},{"location":"api_reference/templates/","title":"templates","text":"<p>Create templates to easily build prompts.</p>"},{"location":"api_reference/templates/#outlines.templates.Template","title":"<code>Template</code>  <code>dataclass</code>","text":"<p>Represents a prompt template.</p> <p>We return a <code>Template</code> class instead of a simple function so the template can be accessed by callers.</p> Source code in <code>outlines/templates.py</code> <pre><code>@dataclass\nclass Template:\n    \"\"\"Represents a prompt template.\n\n    We return a `Template` class instead of a simple function so the\n    template can be accessed by callers.\n\n    \"\"\"\n    template: jinja2.Template\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        str\n            The rendered template as a Python string.\n\n        \"\"\"\n        return self.template.render(**kwargs)\n\n    @classmethod\n    def from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a string containing a Jinja\n        template.\n\n        Parameters\n        ----------\n        content : str\n            The string content to be converted into a template.\n\n        Returns\n        -------\n        Template\n            An instance of the class with the provided content as a template.\n\n        \"\"\"\n        return cls(build_template_from_string(content, filters))\n\n    @classmethod\n    def from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a file containing a Jinja\n        template.\n\n        Note: This method does not allow to include and inheritance to\n        reference files that are outside the folder or subfolders of the file\n        given to `from_file`.\n\n        Parameters\n        ----------\n        path : Path\n            The path to the file containing the Jinja template.\n\n        Returns\n        -------\n        Template\n            An instance of the Template class with the template loaded from the\n            file.\n\n        \"\"\"\n        # We don't use a `Signature` here because it seems not feasible to\n        # infer one from a Jinja2 environment that is\n        # split across multiple files (since e.g. we support features like\n        # Jinja2 includes and template inheritance)\n        return cls(build_template_from_file(path, filters))\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.Template.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a Python string.</p> Source code in <code>outlines/templates.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    str\n        The rendered template as a Python string.\n\n    \"\"\"\n    return self.template.render(**kwargs)\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.Template.from_file","title":"<code>from_file(path, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a file containing a Jinja template.</p> <p>Note: This method does not allow to include and inheritance to reference files that are outside the folder or subfolders of the file given to <code>from_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file containing the Jinja template.</p> required <p>Returns:</p> Type Description <code>Template</code> <p>An instance of the Template class with the template loaded from the file.</p> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a file containing a Jinja\n    template.\n\n    Note: This method does not allow to include and inheritance to\n    reference files that are outside the folder or subfolders of the file\n    given to `from_file`.\n\n    Parameters\n    ----------\n    path : Path\n        The path to the file containing the Jinja template.\n\n    Returns\n    -------\n    Template\n        An instance of the Template class with the template loaded from the\n        file.\n\n    \"\"\"\n    # We don't use a `Signature` here because it seems not feasible to\n    # infer one from a Jinja2 environment that is\n    # split across multiple files (since e.g. we support features like\n    # Jinja2 includes and template inheritance)\n    return cls(build_template_from_file(path, filters))\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.Template.from_string","title":"<code>from_string(content, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a string containing a Jinja template.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The string content to be converted into a template.</p> required <p>Returns:</p> Type Description <code>Template</code> <p>An instance of the class with the provided content as a template.</p> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a string containing a Jinja\n    template.\n\n    Parameters\n    ----------\n    content : str\n        The string content to be converted into a template.\n\n    Returns\n    -------\n    Template\n        An instance of the class with the provided content as a template.\n\n    \"\"\"\n    return cls(build_template_from_string(content, filters))\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.Vision","title":"<code>Vision(prompt, image)</code>","text":"<p>This factory function replaces the deprecated <code>Vision</code> class until it is fully removed in outlines v1.2.0.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to use to generate the response.</p> required <code>image</code> <code>Image</code> <p>The image to use to generate the response.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list containing the prompt and Image instance.</p> Source code in <code>outlines/templates.py</code> <pre><code>def Vision(prompt: str, image: PILImage.Image) -&gt; list:\n    \"\"\"This factory function replaces the deprecated `Vision` class until it is\n    fully removed in outlines v1.2.0.\n\n    Parameters\n    ----------\n    prompt\n        The prompt to use to generate the response.\n    image\n        The image to use to generate the response.\n\n    Returns\n    -------\n    list\n        A list containing the prompt and Image instance.\n    \"\"\"\n    warnings.warn(\"\"\"\n        The Vision function is deprecated and will be removed in outlines 1.2.0.\n        Instead of using Vision, please use a prompt along with an\n        outlines.inputs.Image instance.\n        For instance:\n        ```python\n        import openai\n        from outlines import Image, from_openai\n        model = from_openai(\"gpt-4o\")\n        response = model(\n            [\"A beautiful image of a cat\", Image(my_image)],\n            max_tokens=100\n        )\n        ```\n        \"\"\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return [prompt, Image(image)]\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.create_jinja_env","title":"<code>create_jinja_env(loader, filters)</code>","text":"<p>Create a new Jinja environment.</p> <p>The Jinja environment is loaded with a set of pre-defined filters: - <code>name</code>: get the name of a function - <code>description</code>: get a function's docstring - <code>source</code>: get a function's source code - <code>signature</code>: get a function's signature - <code>args</code>: get a function's arguments - <code>schema</code>: display a JSON Schema</p> <p>Users may pass additional filters, and/or override existing ones.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>Optional[BaseLoader]</code> <p>An optional <code>BaseLoader</code> instance</p> required <code>filters</code> <code>Dict[str, Callable]</code> <p>A dictionary of filters, map between the filter's name and the corresponding function.</p> required Source code in <code>outlines/templates.py</code> <pre><code>def create_jinja_env(\n    loader: Optional[jinja2.BaseLoader], filters: Dict[str, Callable]\n) -&gt; jinja2.Environment:\n    \"\"\"Create a new Jinja environment.\n\n    The Jinja environment is loaded with a set of pre-defined filters:\n    - `name`: get the name of a function\n    - `description`: get a function's docstring\n    - `source`: get a function's source code\n    - `signature`: get a function's signature\n    - `args`: get a function's arguments\n    - `schema`: display a JSON Schema\n\n    Users may pass additional filters, and/or override existing ones.\n\n    Parameters\n    ----------\n    loader\n       An optional `BaseLoader` instance\n    filters\n       A dictionary of filters, map between the filter's name and the\n       corresponding function.\n\n    \"\"\"\n    env = jinja2.Environment(\n        loader=loader,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=jinja2.StrictUndefined,\n    )\n\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n    env.filters[\"args\"] = get_fn_args\n\n    # The filters passed by the user may override the\n    # pre-defined filters.\n    for name, filter_fn in filters.items():\n        env.filters[name] = filter_fn\n\n    return env\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_fn_args","title":"<code>get_fn_args(fn)</code>","text":"<p>Returns the arguments of a function with annotations and default values if provided.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_args(fn: Callable):\n    \"\"\"Returns the arguments of a function with annotations and default values if provided.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `args` filter only applies to callables.\")\n\n    arg_str_list = []\n    signature = inspect.signature(fn)\n    arg_str_list = [str(param) for param in signature.parameters.values()]\n    arg_str = \", \".join(arg_str_list)\n    return arg_str\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:  # pragma: no cover\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:  # pragma: no cover\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api_reference/templates/#outlines.templates.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/templates.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value: # pragma: no cover\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api_reference/backends/","title":"backends","text":"<p>Module to define the backends in charge of creating logits processors.</p>"},{"location":"api_reference/backends/#outlines.backends.BaseBackend","title":"<code>BaseBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all backends.</p> <p>The subclasses must implement methods that create a logits processor from a JSON schema, regex or CFG.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>class BaseBackend(ABC):\n    \"\"\"Base class for all backends.\n\n    The subclasses must implement methods that create a logits processor\n    from a JSON schema, regex or CFG.\n\n    \"\"\"\n\n    @abstractmethod\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.BaseBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.BaseBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.BaseBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.LLGuidanceBackend","title":"<code>LLGuidanceBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for LLGuidance.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceBackend(BaseBackend):\n    \"\"\"Backend for LLGuidance.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import llguidance as llg\n\n        self.llg = llg\n        self.tensor_library_name = model.tensor_library_name\n        self.llg_tokenizer = self._create_llg_tokenizer(model)\n\n    def _create_llg_tokenizer(self, model: SteerableModel) -&gt; \"LLGTokenizer\":\n        \"\"\"Create an llg tokenizer from the Outlines model's tokenizer.\n\n        Parameters\n        ----------\n        model: Model\n            The Outlines model.\n\n        Returns\n        -------\n        LLGTokenizer\n            The llg tokenizer.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(model.hf_tokenizer)\n\n        elif isinstance(model, LlamaCpp):\n            import llama_cpp\n            import llguidance.llamacpp\n\n            vocab = llama_cpp.llama_model_get_vocab(model.model.model)\n            return llguidance.llamacpp.lltokenizer_from_vocab(vocab)\n\n        elif isinstance(model, MLXLM): # pragma: no cover\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(\n                model.mlx_tokenizer._tokenizer\n            )\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported model type: {type(model)}. \"\n                \"Llguidance only supports LlamaCpp, MLXLM \"\n                \"and Transformers models.\"\n            )\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"regex\", regex)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        # We try both lark and ebnf\n        try:\n            grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n        except ValueError:\n            grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.LLGuidanceBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import llguidance as llg\n\n    self.llg = llg\n    self.tensor_library_name = model.tensor_library_name\n    self.llg_tokenizer = self._create_llg_tokenizer(model)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.LLGuidanceBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    # We try both lark and ebnf\n    try:\n        grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n    except ValueError:\n        grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.LLGuidanceBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.LLGuidanceBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"regex\", regex)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.OutlinesCoreBackend","title":"<code>OutlinesCoreBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreBackend(BaseBackend):\n    \"\"\"Backend for Outlines Core.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            tokenizer = model.tokenizer\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, LlamaCpp):\n            tokenizer = model.tokenizer # type: ignore\n            vocabulary = tokenizer.vocabulary\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer # type: ignore\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        self.eos_token_id = eos_token_id\n        self.vocabulary = self.create_outlines_core_vocabulary(\n            vocabulary, eos_token_id, eos_token, token_to_str\n        )\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ):\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n        return self.get_regex_logits_processor(regex)\n\n    def get_regex_logits_processor(self, regex: str):\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        index = Index(regex, self.vocabulary)\n        return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n\n    def get_cfg_logits_processor(self, grammar):\n        raise NotImplementedError(\n            \"Outlines Core does not support context-free grammar.\"\n        )\n\n    @staticmethod\n    def create_outlines_core_vocabulary(\n        vocab: Dict[str, int],\n        eos_token_id: int,\n        eos_token: str,\n        token_to_str: Callable[[str], str]\n    ) -&gt; Vocabulary:\n        \"\"\"Create an Outlines Core Vocabulary instance.\n\n        Parameters\n        ----------\n        vocab: Dict[str, int]\n            The vocabulary to create an Outlines Core vocabulary from.\n        eos_token_id: int\n            The EOS token ID.\n        eos_token: str\n            The EOS token.\n        token_to_str: Callable[[str], str]\n            The function to convert a token to a string.\n\n        Returns\n        -------\n        Vocabulary\n            The Outlines Core Vocabulary instance.\n\n        \"\"\"\n        formatted_vocab = {}\n        for token, token_id in vocab.items():\n            # This step is necessary to transform special tokens into their\n            # string representation, in particular for spacing. We need those\n            # string representations as outlines core first builds an FSM from\n            # the regex provided that only contains regular strings.\n            token_as_str = token_to_str(token)\n            formatted_vocab[token_as_str] = [token_id]\n        formatted_vocab.pop(eos_token)\n        return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.OutlinesCoreBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    if isinstance(model, Transformers):\n        tokenizer = model.tokenizer\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, LlamaCpp):\n        tokenizer = model.tokenizer # type: ignore\n        vocabulary = tokenizer.vocabulary\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer # type: ignore\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n    else: # pragma: no cover\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    self.eos_token_id = eos_token_id\n    self.vocabulary = self.create_outlines_core_vocabulary(\n        vocabulary, eos_token_id, eos_token, token_to_str\n    )\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.OutlinesCoreBackend.create_outlines_core_vocabulary","title":"<code>create_outlines_core_vocabulary(vocab, eos_token_id, eos_token, token_to_str)</code>  <code>staticmethod</code>","text":"<p>Create an Outlines Core Vocabulary instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Dict[str, int]</code> <p>The vocabulary to create an Outlines Core vocabulary from.</p> required <code>eos_token_id</code> <code>int</code> <p>The EOS token ID.</p> required <code>eos_token</code> <code>str</code> <p>The EOS token.</p> required <code>token_to_str</code> <code>Callable[[str], str]</code> <p>The function to convert a token to a string.</p> required <p>Returns:</p> Type Description <code>Vocabulary</code> <p>The Outlines Core Vocabulary instance.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>@staticmethod\ndef create_outlines_core_vocabulary(\n    vocab: Dict[str, int],\n    eos_token_id: int,\n    eos_token: str,\n    token_to_str: Callable[[str], str]\n) -&gt; Vocabulary:\n    \"\"\"Create an Outlines Core Vocabulary instance.\n\n    Parameters\n    ----------\n    vocab: Dict[str, int]\n        The vocabulary to create an Outlines Core vocabulary from.\n    eos_token_id: int\n        The EOS token ID.\n    eos_token: str\n        The EOS token.\n    token_to_str: Callable[[str], str]\n        The function to convert a token to a string.\n\n    Returns\n    -------\n    Vocabulary\n        The Outlines Core Vocabulary instance.\n\n    \"\"\"\n    formatted_vocab = {}\n    for token, token_id in vocab.items():\n        # This step is necessary to transform special tokens into their\n        # string representation, in particular for spacing. We need those\n        # string representations as outlines core first builds an FSM from\n        # the regex provided that only contains regular strings.\n        token_as_str = token_to_str(token)\n        formatted_vocab[token_as_str] = [token_id]\n    formatted_vocab.pop(eos_token)\n    return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.OutlinesCoreBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n):\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n    return self.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.OutlinesCoreBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_regex_logits_processor(self, regex: str):\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    index = Index(regex, self.vocabulary)\n    return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.XGrammarBackend","title":"<code>XGrammarBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarBackend(BaseBackend):\n    \"\"\"Backend for XGrammar.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import xgrammar as xgr\n\n        if isinstance(model, Transformers):\n            tokenizer = model.hf_tokenizer\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer._tokenizer\n        else: # pragma: no cover\n            raise ValueError(\n                \"The xgrammar backend only supports Transformers and \"\n                + \"MLXLM models\"\n            )\n\n        tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n            tokenizer,\n            vocab_size=len(tokenizer.get_vocab())\n        )\n        self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_json_schema(\n            json_schema\n        )\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_regex(regex)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.XGrammarBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import xgrammar as xgr\n\n    if isinstance(model, Transformers):\n        tokenizer = model.hf_tokenizer\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer._tokenizer\n    else: # pragma: no cover\n        raise ValueError(\n            \"The xgrammar backend only supports Transformers and \"\n            + \"MLXLM models\"\n        )\n\n    tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n        tokenizer,\n        vocab_size=len(tokenizer.get_vocab())\n    )\n    self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.XGrammarBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.XGrammarBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_json_schema(\n        json_schema\n    )\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.XGrammarBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_regex(regex)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(backend_name, model, grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_cfg_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    grammar: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or CFG_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_cfg_logits_processor(grammar)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(backend_name, model, json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_json_schema_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    json_schema: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or JSON_SCHEMA_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_json_schema_logits_processor(json_schema)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.get_regex_logits_processor","title":"<code>get_regex_logits_processor(backend_name, model, regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>backend_name</code> <code>str | None</code> <p>The name of the backend to use.</p> required <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/__init__.py</code> <pre><code>def get_regex_logits_processor(\n    backend_name: str | None,\n    model: SteerableModel,\n    regex: str,\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    backend_name: str | None\n        The name of the backend to use.\n    model: Model\n        The Outlines model of the user.\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    backend = _get_backend(\n        backend_name or REGEX_DEFAULT_BACKEND,\n        model,\n    )\n    return backend.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.base","title":"<code>base</code>","text":"<p>Base class for all backends.</p>"},{"location":"api_reference/backends/#outlines.backends.base.BaseBackend","title":"<code>BaseBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all backends.</p> <p>The subclasses must implement methods that create a logits processor from a JSON schema, regex or CFG.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>class BaseBackend(ABC):\n    \"\"\"Base class for all backends.\n\n    The subclasses must implement methods that create a logits processor\n    from a JSON schema, regex or CFG.\n\n    \"\"\"\n\n    @abstractmethod\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.base.BaseBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.base.BaseBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.base.BaseBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance","title":"<code>llguidance</code>","text":"<p>Backend class for LLGuidance.</p>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceBackend","title":"<code>LLGuidanceBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for LLGuidance.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceBackend(BaseBackend):\n    \"\"\"Backend for LLGuidance.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import llguidance as llg\n\n        self.llg = llg\n        self.tensor_library_name = model.tensor_library_name\n        self.llg_tokenizer = self._create_llg_tokenizer(model)\n\n    def _create_llg_tokenizer(self, model: SteerableModel) -&gt; \"LLGTokenizer\":\n        \"\"\"Create an llg tokenizer from the Outlines model's tokenizer.\n\n        Parameters\n        ----------\n        model: Model\n            The Outlines model.\n\n        Returns\n        -------\n        LLGTokenizer\n            The llg tokenizer.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(model.hf_tokenizer)\n\n        elif isinstance(model, LlamaCpp):\n            import llama_cpp\n            import llguidance.llamacpp\n\n            vocab = llama_cpp.llama_model_get_vocab(model.model.model)\n            return llguidance.llamacpp.lltokenizer_from_vocab(vocab)\n\n        elif isinstance(model, MLXLM): # pragma: no cover\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(\n                model.mlx_tokenizer._tokenizer\n            )\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported model type: {type(model)}. \"\n                \"Llguidance only supports LlamaCpp, MLXLM \"\n                \"and Transformers models.\"\n            )\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"regex\", regex)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        # We try both lark and ebnf\n        try:\n            grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n        except ValueError:\n            grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import llguidance as llg\n\n    self.llg = llg\n    self.tensor_library_name = model.tensor_library_name\n    self.llg_tokenizer = self._create_llg_tokenizer(model)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    # We try both lark and ebnf\n    try:\n        grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n    except ValueError:\n        grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"regex\", regex)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceLogitsProcessor","title":"<code>LLGuidanceLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits Processor for the LLGuidance backend.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits Processor for the LLGuidance backend.\"\"\"\n\n    def __init__(\n        self,\n        grammar: str,\n        llg_tokenizer,\n        tensor_library_name: str,\n    ) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        grammar: str\n            The grammar spec to use to create the LLMatcher\n        llg_tokenizer: LLTokenizer\n            The LLGuidance tokenizer\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        self.is_first_token = True\n        self.grammar = grammar\n        self.llg_tokenizer = llg_tokenizer\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int) -&gt; None:\n        \"\"\"Setup the LLMatchers, the bitmask and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size of the input\n\n        \"\"\"\n        from llguidance import LLMatcher\n\n        self.ll_matchers = [\n            LLMatcher(self.llg_tokenizer, self.grammar)\n            for _ in range(batch_size)\n        ]\n\n        # we must adapt the bitmask creation and the bias function to the\n        # tensor library used by the model\n        if self.tensor_library_name == \"torch\":\n            import llguidance.torch\n\n            self.bitmask = llguidance.torch.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"numpy\":\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_numpy\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported tensor library: {self.tensor_library_name}\")\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the MLX backend.\"\"\"\n        import llguidance.mlx\n        import llguidance.numpy\n\n        biased_logits_array = []\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            biased_logits = llguidance.mlx.apply_token_bitmask(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Torch backend.\"\"\"\n        import llguidance.torch\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.torch.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                self.tensor_adapter.get_device(logits)\n            )\n            llguidance.torch.apply_token_bitmask_inplace(\n                logits[i], # type: ignore\n                self.bitmask[i]\n            )\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Numpy backend.\"\"\"\n        import llguidance.numpy\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            llguidance.numpy.apply_token_bitmask_inplace(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the instances of LLMatcher to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        if self.is_first_token:\n            self._setup(self.tensor_adapter.shape(input_ids)[0])\n            self.is_first_token = False\n\n        # we do not make the matchers consume the last token during the first\n        # generation step because no tokens have been generated yet\n        else:\n            for i in range(self.tensor_adapter.shape(input_ids)[0]):\n                sequence = input_ids[i] # type: ignore\n                last_token = sequence[-1].item()\n                self.ll_matchers[i].consume_token(last_token)\n                error = self.ll_matchers[i].get_error()\n                if error:\n                    warnings.warn(f\"Error in LLMatcher: {error}\")\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.__init__","title":"<code>__init__(grammar, llg_tokenizer, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The grammar spec to use to create the LLMatcher</p> required <code>llg_tokenizer</code> <p>The LLGuidance tokenizer</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(\n    self,\n    grammar: str,\n    llg_tokenizer,\n    tensor_library_name: str,\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    grammar: str\n        The grammar spec to use to create the LLMatcher\n    llg_tokenizer: LLTokenizer\n        The LLGuidance tokenizer\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    self.is_first_token = True\n    self.grammar = grammar\n    self.llg_tokenizer = llg_tokenizer\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the instances of LLMatcher to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the instances of LLMatcher to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    if self.is_first_token:\n        self._setup(self.tensor_adapter.shape(input_ids)[0])\n        self.is_first_token = False\n\n    # we do not make the matchers consume the last token during the first\n    # generation step because no tokens have been generated yet\n    else:\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            sequence = input_ids[i] # type: ignore\n            last_token = sequence[-1].item()\n            self.ll_matchers[i].consume_token(last_token)\n            error = self.ll_matchers[i].get_error()\n            if error:\n                warnings.warn(f\"Error in LLMatcher: {error}\")\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core","title":"<code>outlines_core</code>","text":"<p>Backend class for Outlines Core.</p>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreBackend","title":"<code>OutlinesCoreBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreBackend(BaseBackend):\n    \"\"\"Backend for Outlines Core.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            tokenizer = model.tokenizer\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, LlamaCpp):\n            tokenizer = model.tokenizer # type: ignore\n            vocabulary = tokenizer.vocabulary\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer # type: ignore\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        self.eos_token_id = eos_token_id\n        self.vocabulary = self.create_outlines_core_vocabulary(\n            vocabulary, eos_token_id, eos_token, token_to_str\n        )\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ):\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n        return self.get_regex_logits_processor(regex)\n\n    def get_regex_logits_processor(self, regex: str):\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        index = Index(regex, self.vocabulary)\n        return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n\n    def get_cfg_logits_processor(self, grammar):\n        raise NotImplementedError(\n            \"Outlines Core does not support context-free grammar.\"\n        )\n\n    @staticmethod\n    def create_outlines_core_vocabulary(\n        vocab: Dict[str, int],\n        eos_token_id: int,\n        eos_token: str,\n        token_to_str: Callable[[str], str]\n    ) -&gt; Vocabulary:\n        \"\"\"Create an Outlines Core Vocabulary instance.\n\n        Parameters\n        ----------\n        vocab: Dict[str, int]\n            The vocabulary to create an Outlines Core vocabulary from.\n        eos_token_id: int\n            The EOS token ID.\n        eos_token: str\n            The EOS token.\n        token_to_str: Callable[[str], str]\n            The function to convert a token to a string.\n\n        Returns\n        -------\n        Vocabulary\n            The Outlines Core Vocabulary instance.\n\n        \"\"\"\n        formatted_vocab = {}\n        for token, token_id in vocab.items():\n            # This step is necessary to transform special tokens into their\n            # string representation, in particular for spacing. We need those\n            # string representations as outlines core first builds an FSM from\n            # the regex provided that only contains regular strings.\n            token_as_str = token_to_str(token)\n            formatted_vocab[token_as_str] = [token_id]\n        formatted_vocab.pop(eos_token)\n        return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    if isinstance(model, Transformers):\n        tokenizer = model.tokenizer\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, LlamaCpp):\n        tokenizer = model.tokenizer # type: ignore\n        vocabulary = tokenizer.vocabulary\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer # type: ignore\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n    else: # pragma: no cover\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    self.eos_token_id = eos_token_id\n    self.vocabulary = self.create_outlines_core_vocabulary(\n        vocabulary, eos_token_id, eos_token, token_to_str\n    )\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreBackend.create_outlines_core_vocabulary","title":"<code>create_outlines_core_vocabulary(vocab, eos_token_id, eos_token, token_to_str)</code>  <code>staticmethod</code>","text":"<p>Create an Outlines Core Vocabulary instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Dict[str, int]</code> <p>The vocabulary to create an Outlines Core vocabulary from.</p> required <code>eos_token_id</code> <code>int</code> <p>The EOS token ID.</p> required <code>eos_token</code> <code>str</code> <p>The EOS token.</p> required <code>token_to_str</code> <code>Callable[[str], str]</code> <p>The function to convert a token to a string.</p> required <p>Returns:</p> Type Description <code>Vocabulary</code> <p>The Outlines Core Vocabulary instance.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>@staticmethod\ndef create_outlines_core_vocabulary(\n    vocab: Dict[str, int],\n    eos_token_id: int,\n    eos_token: str,\n    token_to_str: Callable[[str], str]\n) -&gt; Vocabulary:\n    \"\"\"Create an Outlines Core Vocabulary instance.\n\n    Parameters\n    ----------\n    vocab: Dict[str, int]\n        The vocabulary to create an Outlines Core vocabulary from.\n    eos_token_id: int\n        The EOS token ID.\n    eos_token: str\n        The EOS token.\n    token_to_str: Callable[[str], str]\n        The function to convert a token to a string.\n\n    Returns\n    -------\n    Vocabulary\n        The Outlines Core Vocabulary instance.\n\n    \"\"\"\n    formatted_vocab = {}\n    for token, token_id in vocab.items():\n        # This step is necessary to transform special tokens into their\n        # string representation, in particular for spacing. We need those\n        # string representations as outlines core first builds an FSM from\n        # the regex provided that only contains regular strings.\n        token_as_str = token_to_str(token)\n        formatted_vocab[token_as_str] = [token_id]\n    formatted_vocab.pop(eos_token)\n    return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n):\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n    return self.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_regex_logits_processor(self, regex: str):\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    index = Index(regex, self.vocabulary)\n    return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor","title":"<code>OutlinesCoreLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for Outlines Core.\"\"\"\n\n    def __init__(\n        self, index: Index, tensor_library_name: str\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        index: Index\n            The Outlines Core `Index` instance to use to create the Outlines\n            Core `Guide` instances that will be used to bias the logits\n        tensor_library_name: str\n            The tensor library name to use for the logits processor.\n\n        \"\"\"\n        self.index = index\n        self.tensor_library_name = tensor_library_name\n        self.is_first_token = True\n        super().__init__(tensor_library_name)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the logits processor.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Set the guides, bitmasks and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size and the device\n        of the logits.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size.\n        vocab_size: int\n            The vocabulary size.\n\n        \"\"\"\n        if self.tensor_library_name == \"torch\":\n            from outlines_core.kernels.torch import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_torch\n\n        elif self.tensor_library_name == \"numpy\":\n            from outlines_core.kernels.numpy import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_numpy\n\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            from outlines_core.kernels.mlx import (\n                allocate_token_bitmask\n            )\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_mlx\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._guides = [Guide(self.index) for _ in range(batch_size)]\n        self._bitmasks = [\n            self.allocate_token_bitmask(vocab_size)\n            for _ in range(batch_size)\n        ]\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        from outlines_core.kernels.mlx import (\n            apply_token_bitmask,\n            fill_next_token_bitmask\n        )\n\n        biased_logits_array = []\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            biased_logits = apply_token_bitmask(\n                self.tensor_adapter.unsqueeze(logits[i]), self._bitmasks[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        from outlines_core.kernels.torch import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                self.tensor_adapter.get_device(logits)\n            )\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Numpy tensors.\"\"\"\n        from outlines_core.kernels.numpy import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the guides to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n                # This circumvents issue #227 in outlines_core\n                # Ideally, we would be able to advance all the times as the final\n                # state would accept the eos token leading to itself\n                if (\n                    not self._guides[i].is_finished()\n                    or self._guides[i].accepts_tokens([last_token_id])\n                ):\n                    self._guides[i].advance(\n                        token_id=last_token_id,\n                        return_tokens=False\n                    )\n\n        return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.__init__","title":"<code>__init__(index, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The Outlines Core <code>Index</code> instance to use to create the Outlines Core <code>Guide</code> instances that will be used to bias the logits</p> required <code>tensor_library_name</code> <code>str</code> <p>The tensor library name to use for the logits processor.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(\n    self, index: Index, tensor_library_name: str\n):\n    \"\"\"\n    Parameters\n    ----------\n    index: Index\n        The Outlines Core `Index` instance to use to create the Outlines\n        Core `Guide` instances that will be used to bias the logits\n    tensor_library_name: str\n        The tensor library name to use for the logits processor.\n\n    \"\"\"\n    self.index = index\n    self.tensor_library_name = tensor_library_name\n    self.is_first_token = True\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the guides to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the guides to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n            # This circumvents issue #227 in outlines_core\n            # Ideally, we would be able to advance all the times as the final\n            # state would accept the eos token leading to itself\n            if (\n                not self._guides[i].is_finished()\n                or self._guides[i].accepts_tokens([last_token_id])\n            ):\n                self._guides[i].advance(\n                    token_id=last_token_id,\n                    return_tokens=False\n                )\n\n    return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the logits processor.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar","title":"<code>xgrammar</code>","text":"<p>Backend class for XGrammar.</p>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarBackend","title":"<code>XGrammarBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarBackend(BaseBackend):\n    \"\"\"Backend for XGrammar.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import xgrammar as xgr\n\n        if isinstance(model, Transformers):\n            tokenizer = model.hf_tokenizer\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer._tokenizer\n        else: # pragma: no cover\n            raise ValueError(\n                \"The xgrammar backend only supports Transformers and \"\n                + \"MLXLM models\"\n            )\n\n        tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n            tokenizer,\n            vocab_size=len(tokenizer.get_vocab())\n        )\n        self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_json_schema(\n            json_schema\n        )\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_regex(regex)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import xgrammar as xgr\n\n    if isinstance(model, Transformers):\n        tokenizer = model.hf_tokenizer\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer._tokenizer\n    else: # pragma: no cover\n        raise ValueError(\n            \"The xgrammar backend only supports Transformers and \"\n            + \"MLXLM models\"\n        )\n\n    tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n        tokenizer,\n        vocab_size=len(tokenizer.get_vocab())\n    )\n    self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_json_schema(\n        json_schema\n    )\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_regex(regex)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarLogitsProcessor","title":"<code>XGrammarLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for XGrammar.\"\"\"\n\n    def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n        \"\"\"\n        Parameters\n        ----------\n        compiled_grammar: str\n            The compiled grammar to use to create the logits processor.\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        import xgrammar as xgr\n\n        self.xgr = xgr\n        self.is_first_token = True\n        self.compiled_grammar = compiled_grammar\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Setup the logits processor for a new generation.\"\"\"\n        if self.tensor_library_name == \"torch\":\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._matchers = [\n            self.xgr.GrammarMatcher(self.compiled_grammar)\n            for _ in range(batch_size)\n        ]\n        self._bitmask = self.xgr.allocate_token_bitmask(batch_size, vocab_size)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            self.tensor_adapter.get_device(logits)\n        )\n        self.xgr.apply_token_bitmask_inplace(logits, self._bitmask)\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            \"cpu\"\n        )\n\n        return logits\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        import mlx.core as mx\n        from xgrammar.kernels.apply_token_bitmask_mlx import apply_token_bitmask_mlx\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        biased_logits = apply_token_bitmask_mlx(\n            mx.array(self._bitmask.numpy()), logits, self.tensor_adapter.shape(logits)[1]\n        )\n\n        return biased_logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                if not self._matchers[i].is_terminated(): # pragma: no cover\n                    last_token_id = self.tensor_adapter.to_scalar(\n                        input_ids[i][-1] # type: ignore\n                    )\n                    assert self._matchers[i].accept_token(last_token_id)\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarLogitsProcessor.__init__","title":"<code>__init__(compiled_grammar, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>compiled_grammar</code> <code>str</code> <p>The compiled grammar to use to create the logits processor.</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n    \"\"\"\n    Parameters\n    ----------\n    compiled_grammar: str\n        The compiled grammar to use to create the logits processor.\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    import xgrammar as xgr\n\n    self.xgr = xgr\n    self.is_first_token = True\n    self.compiled_grammar = compiled_grammar\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the XGrammar matchers to bias the logits.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            if not self._matchers[i].is_terminated(): # pragma: no cover\n                last_token_id = self.tensor_adapter.to_scalar(\n                    input_ids[i][-1] # type: ignore\n                )\n                assert self._matchers[i].accept_token(last_token_id)\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/#outlines.backends.xgrammar.XGrammarLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/backends/base/","title":"base","text":"<p>Base class for all backends.</p>"},{"location":"api_reference/backends/base/#outlines.backends.base.BaseBackend","title":"<code>BaseBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all backends.</p> <p>The subclasses must implement methods that create a logits processor from a JSON schema, regex or CFG.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>class BaseBackend(ABC):\n    \"\"\"Base class for all backends.\n\n    The subclasses must implement methods that create a logits processor\n    from a JSON schema, regex or CFG.\n\n    \"\"\"\n\n    @abstractmethod\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessorType\n            The logits processor.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/backends/base/#outlines.backends.base.BaseBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_cfg_logits_processor(self, grammar: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/base/#outlines.backends.base.BaseBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/base/#outlines.backends.base.BaseBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>  <code>abstractmethod</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessorType</code> <p>The logits processor.</p> Source code in <code>outlines/backends/base.py</code> <pre><code>@abstractmethod\ndef get_regex_logits_processor(self, regex: str) -&gt; LogitsProcessorType:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessorType\n        The logits processor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/backends/llguidance/","title":"llguidance","text":"<p>Backend class for LLGuidance.</p>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceBackend","title":"<code>LLGuidanceBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for LLGuidance.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceBackend(BaseBackend):\n    \"\"\"Backend for LLGuidance.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import llguidance as llg\n\n        self.llg = llg\n        self.tensor_library_name = model.tensor_library_name\n        self.llg_tokenizer = self._create_llg_tokenizer(model)\n\n    def _create_llg_tokenizer(self, model: SteerableModel) -&gt; \"LLGTokenizer\":\n        \"\"\"Create an llg tokenizer from the Outlines model's tokenizer.\n\n        Parameters\n        ----------\n        model: Model\n            The Outlines model.\n\n        Returns\n        -------\n        LLGTokenizer\n            The llg tokenizer.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(model.hf_tokenizer)\n\n        elif isinstance(model, LlamaCpp):\n            import llama_cpp\n            import llguidance.llamacpp\n\n            vocab = llama_cpp.llama_model_get_vocab(model.model.model)\n            return llguidance.llamacpp.lltokenizer_from_vocab(vocab)\n\n        elif isinstance(model, MLXLM): # pragma: no cover\n            import llguidance.hf\n\n            return llguidance.hf.from_tokenizer(\n                model.mlx_tokenizer._tokenizer\n            )\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported model type: {type(model)}. \"\n                \"Llguidance only supports LlamaCpp, MLXLM \"\n                \"and Transformers models.\"\n            )\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        grammar_spec = self.llg.grammar_from(\"regex\", regex)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; LLGuidanceLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        # We try both lark and ebnf\n        try:\n            grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n        except ValueError:\n            grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n        return LLGuidanceLogitsProcessor(\n            grammar_spec, self.llg_tokenizer, self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import llguidance as llg\n\n    self.llg = llg\n    self.tensor_library_name = model.tensor_library_name\n    self.llg_tokenizer = self._create_llg_tokenizer(model)\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    # We try both lark and ebnf\n    try:\n        grammar_spec = self.llg.grammar_from(\"grammar\", grammar)\n    except ValueError:\n        grammar_spec = self.llg.grammar_from(\"lark\", grammar)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"json_schema\", json_schema)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; LLGuidanceLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    grammar_spec = self.llg.grammar_from(\"regex\", regex)\n    return LLGuidanceLogitsProcessor(\n        grammar_spec, self.llg_tokenizer, self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceLogitsProcessor","title":"<code>LLGuidanceLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits Processor for the LLGuidance backend.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>class LLGuidanceLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits Processor for the LLGuidance backend.\"\"\"\n\n    def __init__(\n        self,\n        grammar: str,\n        llg_tokenizer,\n        tensor_library_name: str,\n    ) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        grammar: str\n            The grammar spec to use to create the LLMatcher\n        llg_tokenizer: LLTokenizer\n            The LLGuidance tokenizer\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        self.is_first_token = True\n        self.grammar = grammar\n        self.llg_tokenizer = llg_tokenizer\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int) -&gt; None:\n        \"\"\"Setup the LLMatchers, the bitmask and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size of the input\n\n        \"\"\"\n        from llguidance import LLMatcher\n\n        self.ll_matchers = [\n            LLMatcher(self.llg_tokenizer, self.grammar)\n            for _ in range(batch_size)\n        ]\n\n        # we must adapt the bitmask creation and the bias function to the\n        # tensor library used by the model\n        if self.tensor_library_name == \"torch\":\n            import llguidance.torch\n\n            self.bitmask = llguidance.torch.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"numpy\":\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_numpy\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            import llguidance.numpy\n\n            self.bitmask = llguidance.numpy.allocate_token_bitmask(batch_size, self.llg_tokenizer.vocab_size)\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported tensor library: {self.tensor_library_name}\")\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the MLX backend.\"\"\"\n        import llguidance.mlx\n        import llguidance.numpy\n\n        biased_logits_array = []\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            biased_logits = llguidance.mlx.apply_token_bitmask(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Torch backend.\"\"\"\n        import llguidance.torch\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.torch.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                self.tensor_adapter.get_device(logits)\n            )\n            llguidance.torch.apply_token_bitmask_inplace(\n                logits[i], # type: ignore\n                self.bitmask[i]\n            )\n            self.bitmask = self.tensor_adapter.to_device(\n                self.bitmask,\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for the Numpy backend.\"\"\"\n        import llguidance.numpy\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            llguidance.numpy.fill_next_token_bitmask(self.ll_matchers[i], self.bitmask, i)\n            llguidance.numpy.apply_token_bitmask_inplace(\n                logits[i], self.bitmask[i] # type: ignore\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the instances of LLMatcher to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        if self.is_first_token:\n            self._setup(self.tensor_adapter.shape(input_ids)[0])\n            self.is_first_token = False\n\n        # we do not make the matchers consume the last token during the first\n        # generation step because no tokens have been generated yet\n        else:\n            for i in range(self.tensor_adapter.shape(input_ids)[0]):\n                sequence = input_ids[i] # type: ignore\n                last_token = sequence[-1].item()\n                self.ll_matchers[i].consume_token(last_token)\n                error = self.ll_matchers[i].get_error()\n                if error:\n                    warnings.warn(f\"Error in LLMatcher: {error}\")\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.__init__","title":"<code>__init__(grammar, llg_tokenizer, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The grammar spec to use to create the LLMatcher</p> required <code>llg_tokenizer</code> <p>The LLGuidance tokenizer</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def __init__(\n    self,\n    grammar: str,\n    llg_tokenizer,\n    tensor_library_name: str,\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    grammar: str\n        The grammar spec to use to create the LLMatcher\n    llg_tokenizer: LLTokenizer\n        The LLGuidance tokenizer\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    self.is_first_token = True\n    self.grammar = grammar\n    self.llg_tokenizer = llg_tokenizer\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the instances of LLMatcher to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the instances of LLMatcher to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    if self.is_first_token:\n        self._setup(self.tensor_adapter.shape(input_ids)[0])\n        self.is_first_token = False\n\n    # we do not make the matchers consume the last token during the first\n    # generation step because no tokens have been generated yet\n    else:\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            sequence = input_ids[i] # type: ignore\n            last_token = sequence[-1].item()\n            self.ll_matchers[i].consume_token(last_token)\n            error = self.ll_matchers[i].get_error()\n            if error:\n                warnings.warn(f\"Error in LLMatcher: {error}\")\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/llguidance/#outlines.backends.llguidance.LLGuidanceLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/llguidance.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/backends/outlines_core/","title":"outlines_core","text":"<p>Backend class for Outlines Core.</p>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreBackend","title":"<code>OutlinesCoreBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreBackend(BaseBackend):\n    \"\"\"Backend for Outlines Core.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        if isinstance(model, Transformers):\n            tokenizer = model.tokenizer\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, LlamaCpp):\n            tokenizer = model.tokenizer # type: ignore\n            vocabulary = tokenizer.vocabulary\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = tokenizer.convert_token_to_string\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer # type: ignore\n            vocabulary = tokenizer.get_vocab()\n            eos_token_id = tokenizer.eos_token_id\n            eos_token = tokenizer.eos_token\n            token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n        else: # pragma: no cover\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n        self.eos_token_id = eos_token_id\n        self.vocabulary = self.create_outlines_core_vocabulary(\n            vocabulary, eos_token_id, eos_token, token_to_str\n        )\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ):\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n        return self.get_regex_logits_processor(regex)\n\n    def get_regex_logits_processor(self, regex: str):\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        index = Index(regex, self.vocabulary)\n        return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n\n    def get_cfg_logits_processor(self, grammar):\n        raise NotImplementedError(\n            \"Outlines Core does not support context-free grammar.\"\n        )\n\n    @staticmethod\n    def create_outlines_core_vocabulary(\n        vocab: Dict[str, int],\n        eos_token_id: int,\n        eos_token: str,\n        token_to_str: Callable[[str], str]\n    ) -&gt; Vocabulary:\n        \"\"\"Create an Outlines Core Vocabulary instance.\n\n        Parameters\n        ----------\n        vocab: Dict[str, int]\n            The vocabulary to create an Outlines Core vocabulary from.\n        eos_token_id: int\n            The EOS token ID.\n        eos_token: str\n            The EOS token.\n        token_to_str: Callable[[str], str]\n            The function to convert a token to a string.\n\n        Returns\n        -------\n        Vocabulary\n            The Outlines Core Vocabulary instance.\n\n        \"\"\"\n        formatted_vocab = {}\n        for token, token_id in vocab.items():\n            # This step is necessary to transform special tokens into their\n            # string representation, in particular for spacing. We need those\n            # string representations as outlines core first builds an FSM from\n            # the regex provided that only contains regular strings.\n            token_as_str = token_to_str(token)\n            formatted_vocab[token_as_str] = [token_id]\n        formatted_vocab.pop(eos_token)\n        return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    if isinstance(model, Transformers):\n        tokenizer = model.tokenizer\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, LlamaCpp):\n        tokenizer = model.tokenizer # type: ignore\n        vocabulary = tokenizer.vocabulary\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = tokenizer.convert_token_to_string\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer # type: ignore\n        vocabulary = tokenizer.get_vocab()\n        eos_token_id = tokenizer.eos_token_id\n        eos_token = tokenizer.eos_token\n        token_to_str = lambda token: tokenizer.convert_tokens_to_string([token]) # type: ignore\n    else: # pragma: no cover\n        raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    self.eos_token_id = eos_token_id\n    self.vocabulary = self.create_outlines_core_vocabulary(\n        vocabulary, eos_token_id, eos_token, token_to_str\n    )\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreBackend.create_outlines_core_vocabulary","title":"<code>create_outlines_core_vocabulary(vocab, eos_token_id, eos_token, token_to_str)</code>  <code>staticmethod</code>","text":"<p>Create an Outlines Core Vocabulary instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Dict[str, int]</code> <p>The vocabulary to create an Outlines Core vocabulary from.</p> required <code>eos_token_id</code> <code>int</code> <p>The EOS token ID.</p> required <code>eos_token</code> <code>str</code> <p>The EOS token.</p> required <code>token_to_str</code> <code>Callable[[str], str]</code> <p>The function to convert a token to a string.</p> required <p>Returns:</p> Type Description <code>Vocabulary</code> <p>The Outlines Core Vocabulary instance.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>@staticmethod\ndef create_outlines_core_vocabulary(\n    vocab: Dict[str, int],\n    eos_token_id: int,\n    eos_token: str,\n    token_to_str: Callable[[str], str]\n) -&gt; Vocabulary:\n    \"\"\"Create an Outlines Core Vocabulary instance.\n\n    Parameters\n    ----------\n    vocab: Dict[str, int]\n        The vocabulary to create an Outlines Core vocabulary from.\n    eos_token_id: int\n        The EOS token ID.\n    eos_token: str\n        The EOS token.\n    token_to_str: Callable[[str], str]\n        The function to convert a token to a string.\n\n    Returns\n    -------\n    Vocabulary\n        The Outlines Core Vocabulary instance.\n\n    \"\"\"\n    formatted_vocab = {}\n    for token, token_id in vocab.items():\n        # This step is necessary to transform special tokens into their\n        # string representation, in particular for spacing. We need those\n        # string representations as outlines core first builds an FSM from\n        # the regex provided that only contains regular strings.\n        token_as_str = token_to_str(token)\n        formatted_vocab[token_as_str] = [token_id]\n    formatted_vocab.pop(eos_token)\n    return Vocabulary(eos_token_id, formatted_vocab)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n):\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    regex = outlines_core.json_schema.build_regex_from_schema(json_schema)\n    return self.get_regex_logits_processor(regex)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def get_regex_logits_processor(self, regex: str):\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    index = Index(regex, self.vocabulary)\n    return OutlinesCoreLogitsProcessor(index, self.tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor","title":"<code>OutlinesCoreLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for Outlines Core.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>class OutlinesCoreLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for Outlines Core.\"\"\"\n\n    def __init__(\n        self, index: Index, tensor_library_name: str\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        index: Index\n            The Outlines Core `Index` instance to use to create the Outlines\n            Core `Guide` instances that will be used to bias the logits\n        tensor_library_name: str\n            The tensor library name to use for the logits processor.\n\n        \"\"\"\n        self.index = index\n        self.tensor_library_name = tensor_library_name\n        self.is_first_token = True\n        super().__init__(tensor_library_name)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the logits processor.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Set the guides, bitmasks and some functions used in the\n        `process_logits` method.\n\n        This method is called when the first token is generated instead of\n        at initialization because we need to know the batch size and the device\n        of the logits.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size.\n        vocab_size: int\n            The vocabulary size.\n\n        \"\"\"\n        if self.tensor_library_name == \"torch\":\n            from outlines_core.kernels.torch import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_torch\n\n        elif self.tensor_library_name == \"numpy\":\n            from outlines_core.kernels.numpy import allocate_token_bitmask\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_numpy\n\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            from outlines_core.kernels.mlx import (\n                allocate_token_bitmask\n            )\n\n            self.allocate_token_bitmask = allocate_token_bitmask\n            self.bias_logits = self._bias_logits_mlx\n\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._guides = [Guide(self.index) for _ in range(batch_size)]\n        self._bitmasks = [\n            self.allocate_token_bitmask(vocab_size)\n            for _ in range(batch_size)\n        ]\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        from outlines_core.kernels.mlx import (\n            apply_token_bitmask,\n            fill_next_token_bitmask\n        )\n\n        biased_logits_array = []\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            biased_logits = apply_token_bitmask(\n                self.tensor_adapter.unsqueeze(logits[i]), self._bitmasks[i] # type: ignore\n            )\n            biased_logits_array.append(biased_logits)\n\n        return self.tensor_adapter.concatenate(biased_logits_array)\n\n    def _bias_logits_torch(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        from outlines_core.kernels.torch import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                self.tensor_adapter.get_device(logits)\n            )\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n            self._bitmasks[i] = self.tensor_adapter.to_device(\n                self._bitmasks[i],\n                \"cpu\"\n            )\n\n        return logits\n\n    def _bias_logits_numpy(\n        self, batch_size: int, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Numpy tensors.\"\"\"\n        from outlines_core.kernels.numpy import (\n            apply_token_bitmask_inplace,\n            fill_next_token_bitmask\n        )\n\n        for i in range(batch_size):\n            fill_next_token_bitmask(self._guides[i], self._bitmasks[i])\n            apply_token_bitmask_inplace(\n                self.tensor_adapter.unsqueeze(logits[i]), # type: ignore\n                self._bitmasks[i]\n            )\n\n        return logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the guides to bias the logits.\n\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences.\n        logits\n            The logits for the current generation step.\n\n        Returns\n        -------\n        TensorType\n            The biased logits.\n\n        \"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n                # This circumvents issue #227 in outlines_core\n                # Ideally, we would be able to advance all the times as the final\n                # state would accept the eos token leading to itself\n                if (\n                    not self._guides[i].is_finished()\n                    or self._guides[i].accepts_tokens([last_token_id])\n                ):\n                    self._guides[i].advance(\n                        token_id=last_token_id,\n                        return_tokens=False\n                    )\n\n        return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.__init__","title":"<code>__init__(index, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The Outlines Core <code>Index</code> instance to use to create the Outlines Core <code>Guide</code> instances that will be used to bias the logits</p> required <code>tensor_library_name</code> <code>str</code> <p>The tensor library name to use for the logits processor.</p> required Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def __init__(\n    self, index: Index, tensor_library_name: str\n):\n    \"\"\"\n    Parameters\n    ----------\n    index: Index\n        The Outlines Core `Index` instance to use to create the Outlines\n        Core `Guide` instances that will be used to bias the logits\n    tensor_library_name: str\n        The tensor library name to use for the logits processor.\n\n    \"\"\"\n    self.index = index\n    self.tensor_library_name = tensor_library_name\n    self.is_first_token = True\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the guides to bias the logits.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The biased logits.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the guides to bias the logits.\n\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences.\n    logits\n        The logits for the current generation step.\n\n    Returns\n    -------\n    TensorType\n        The biased logits.\n\n    \"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            last_token_id = self.tensor_adapter.to_scalar(input_ids[i][-1]) # type: ignore\n            # This circumvents issue #227 in outlines_core\n            # Ideally, we would be able to advance all the times as the final\n            # state would accept the eos token leading to itself\n            if (\n                not self._guides[i].is_finished()\n                or self._guides[i].accepts_tokens([last_token_id])\n            ):\n                self._guides[i].advance(\n                    token_id=last_token_id,\n                    return_tokens=False\n                )\n\n    return self.bias_logits(batch_size, logits)\n</code></pre>"},{"location":"api_reference/backends/outlines_core/#outlines.backends.outlines_core.OutlinesCoreLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor.</p> Source code in <code>outlines/backends/outlines_core.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the logits processor.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/backends/xgrammar/","title":"xgrammar","text":"<p>Backend class for XGrammar.</p>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarBackend","title":"<code>XGrammarBackend</code>","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Backend for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarBackend(BaseBackend):\n    \"\"\"Backend for XGrammar.\"\"\"\n\n    def __init__(self, model: SteerableModel):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            The Outlines model of the user.\n\n        \"\"\"\n        import xgrammar as xgr\n\n        if isinstance(model, Transformers):\n            tokenizer = model.hf_tokenizer\n        elif isinstance(model, MLXLM): # pragma: no cover\n            tokenizer = model.mlx_tokenizer._tokenizer\n        else: # pragma: no cover\n            raise ValueError(\n                \"The xgrammar backend only supports Transformers and \"\n                + \"MLXLM models\"\n            )\n\n        tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n            tokenizer,\n            vocab_size=len(tokenizer.get_vocab())\n        )\n        self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n        self.tensor_library_name = model.tensor_library_name\n\n    def get_json_schema_logits_processor(\n        self, json_schema: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a JSON schema.\n\n        Parameters\n        ----------\n        json_schema: str\n            The JSON schema to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_json_schema(\n            json_schema\n        )\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_regex_logits_processor(\n        self, regex: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a regex.\n\n        Parameters\n        ----------\n        regex: str\n            The regex to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_regex(regex)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n\n    def get_cfg_logits_processor(\n        self, grammar: str\n    ) -&gt; XGrammarLogitsProcessor:\n        \"\"\"Create a logits processor from a context-free grammar.\n\n        Parameters\n        ----------\n        grammar: str\n            The context-free grammar to create a logits processor from.\n\n        Returns\n        -------\n        LogitsProcessor\n            The logits processor to use to constrain the generation.\n\n        \"\"\"\n        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n        return XGrammarLogitsProcessor(\n            compiled_grammar,\n            self.tensor_library_name\n        )\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarBackend.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>SteerableModel</code> <p>The Outlines model of the user.</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, model: SteerableModel):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        The Outlines model of the user.\n\n    \"\"\"\n    import xgrammar as xgr\n\n    if isinstance(model, Transformers):\n        tokenizer = model.hf_tokenizer\n    elif isinstance(model, MLXLM): # pragma: no cover\n        tokenizer = model.mlx_tokenizer._tokenizer\n    else: # pragma: no cover\n        raise ValueError(\n            \"The xgrammar backend only supports Transformers and \"\n            + \"MLXLM models\"\n        )\n\n    tokenizer_info = xgr.TokenizerInfo.from_huggingface(\n        tokenizer,\n        vocab_size=len(tokenizer.get_vocab())\n    )\n    self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)\n    self.tensor_library_name = model.tensor_library_name\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarBackend.get_cfg_logits_processor","title":"<code>get_cfg_logits_processor(grammar)</code>","text":"<p>Create a logits processor from a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The context-free grammar to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_cfg_logits_processor(\n    self, grammar: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a context-free grammar.\n\n    Parameters\n    ----------\n    grammar: str\n        The context-free grammar to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_grammar(grammar)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarBackend.get_json_schema_logits_processor","title":"<code>get_json_schema_logits_processor(json_schema)</code>","text":"<p>Create a logits processor from a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>str</code> <p>The JSON schema to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_json_schema_logits_processor(\n    self, json_schema: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a JSON schema.\n\n    Parameters\n    ----------\n    json_schema: str\n        The JSON schema to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_json_schema(\n        json_schema\n    )\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarBackend.get_regex_logits_processor","title":"<code>get_regex_logits_processor(regex)</code>","text":"<p>Create a logits processor from a regex.</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>The regex to create a logits processor from.</p> required <p>Returns:</p> Type Description <code>LogitsProcessor</code> <p>The logits processor to use to constrain the generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def get_regex_logits_processor(\n    self, regex: str\n) -&gt; XGrammarLogitsProcessor:\n    \"\"\"Create a logits processor from a regex.\n\n    Parameters\n    ----------\n    regex: str\n        The regex to create a logits processor from.\n\n    Returns\n    -------\n    LogitsProcessor\n        The logits processor to use to constrain the generation.\n\n    \"\"\"\n    compiled_grammar = self.grammar_compiler.compile_regex(regex)\n    return XGrammarLogitsProcessor(\n        compiled_grammar,\n        self.tensor_library_name\n    )\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarLogitsProcessor","title":"<code>XGrammarLogitsProcessor</code>","text":"<p>               Bases: <code>OutlinesLogitsProcessor</code></p> <p>Logits processor for XGrammar.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>class XGrammarLogitsProcessor(OutlinesLogitsProcessor):\n    \"\"\"Logits processor for XGrammar.\"\"\"\n\n    def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n        \"\"\"\n        Parameters\n        ----------\n        compiled_grammar: str\n            The compiled grammar to use to create the logits processor.\n        tensor_library_name: str\n            The name of the tensor library used by the model\n\n        \"\"\"\n        import xgrammar as xgr\n\n        self.xgr = xgr\n        self.is_first_token = True\n        self.compiled_grammar = compiled_grammar\n        self.tensor_library_name = tensor_library_name\n        super().__init__(tensor_library_name)\n\n    def reset(self):\n        \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n        self.is_first_token = True\n\n    def _setup(self, batch_size: int, vocab_size: int) -&gt; None:\n        \"\"\"Setup the logits processor for a new generation.\"\"\"\n        if self.tensor_library_name == \"torch\":\n            self._bias_logits = self._bias_logits_torch\n        elif self.tensor_library_name == \"mlx\": # pragma: no cover\n            self._bias_logits = self._bias_logits_mlx\n        else: # pragma: no cover\n            raise ValueError(\n                f\"Unsupported tensor library: {self.tensor_library_name}\"\n            )\n\n        self._matchers = [\n            self.xgr.GrammarMatcher(self.compiled_grammar)\n            for _ in range(batch_size)\n        ]\n        self._bitmask = self.xgr.allocate_token_bitmask(batch_size, vocab_size)\n\n    def _bias_logits_torch(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for Torch tensors.\"\"\"\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            self.tensor_adapter.get_device(logits)\n        )\n        self.xgr.apply_token_bitmask_inplace(logits, self._bitmask)\n        self._bitmask = self.tensor_adapter.to_device(\n            self._bitmask,\n            \"cpu\"\n        )\n\n        return logits\n\n    def _bias_logits_mlx( # pragma: no cover\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Bias the logits for MLX tensors.\"\"\"\n        import mlx.core as mx\n        from xgrammar.kernels.apply_token_bitmask_mlx import apply_token_bitmask_mlx\n\n        for i in range(self.tensor_adapter.shape(input_ids)[0]):\n            if not self._matchers[i].is_terminated():\n                self._matchers[i].fill_next_token_bitmask(self._bitmask, i)\n\n        biased_logits = apply_token_bitmask_mlx(\n            mx.array(self._bitmask.numpy()), logits, self.tensor_adapter.shape(logits)[1]\n        )\n\n        return biased_logits\n\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n        batch_size = self.tensor_adapter.shape(input_ids)[0]\n        vocab_size = self.tensor_adapter.shape(logits)[1]\n\n        if self.is_first_token:\n            self._setup(batch_size, vocab_size)\n            self.is_first_token = False\n        else:\n            for i in range(batch_size):\n                if not self._matchers[i].is_terminated(): # pragma: no cover\n                    last_token_id = self.tensor_adapter.to_scalar(\n                        input_ids[i][-1] # type: ignore\n                    )\n                    assert self._matchers[i].accept_token(last_token_id)\n\n        return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarLogitsProcessor.__init__","title":"<code>__init__(compiled_grammar, tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>compiled_grammar</code> <code>str</code> <p>The compiled grammar to use to create the logits processor.</p> required <code>tensor_library_name</code> <code>str</code> <p>The name of the tensor library used by the model</p> required Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def __init__(self, compiled_grammar: str, tensor_library_name: str,):\n    \"\"\"\n    Parameters\n    ----------\n    compiled_grammar: str\n        The compiled grammar to use to create the logits processor.\n    tensor_library_name: str\n        The name of the tensor library used by the model\n\n    \"\"\"\n    import xgrammar as xgr\n\n    self.xgr = xgr\n    self.is_first_token = True\n    self.compiled_grammar = compiled_grammar\n    self.tensor_library_name = tensor_library_name\n    super().__init__(tensor_library_name)\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>","text":"<p>Use the XGrammar matchers to bias the logits.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Use the XGrammar matchers to bias the logits.\"\"\"\n    batch_size = self.tensor_adapter.shape(input_ids)[0]\n    vocab_size = self.tensor_adapter.shape(logits)[1]\n\n    if self.is_first_token:\n        self._setup(batch_size, vocab_size)\n        self.is_first_token = False\n    else:\n        for i in range(batch_size):\n            if not self._matchers[i].is_terminated(): # pragma: no cover\n                last_token_id = self.tensor_adapter.to_scalar(\n                    input_ids[i][-1] # type: ignore\n                )\n                assert self._matchers[i].accept_token(last_token_id)\n\n    return self._bias_logits(input_ids, logits)\n</code></pre>"},{"location":"api_reference/backends/xgrammar/#outlines.backends.xgrammar.XGrammarLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Ensure self._setup is called again for the next generation.</p> Source code in <code>outlines/backends/xgrammar.py</code> <pre><code>def reset(self):\n    \"\"\"Ensure self._setup is called again for the next generation.\"\"\"\n    self.is_first_token = True\n</code></pre>"},{"location":"api_reference/models/","title":"models","text":"<p>Module that contains all the models integrated in outlines.</p> <p>We group the models in submodules by provider instead of theme (completion, chat completion, diffusers, etc.) and use routing functions everywhere else in the codebase.</p>"},{"location":"api_reference/models/#outlines.models.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/#outlines.models.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/models/#outlines.models.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/#outlines.models.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/models/#outlines.models.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/models/#outlines.models.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic","title":"<code>anthropic</code>","text":"<p>Integration with Anthropic's API.</p>"},{"location":"api_reference/models/#outlines.models.anthropic.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.AnthropicTypeAdapter","title":"<code>AnthropicTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Anthropic</code> model.</p> <p><code>AnthropicTypeAdapter</code> is responsible for preparing the arguments to Anthropic's <code>messages.create</code> method: the input (prompt and possibly image). Anthropic does not support defining the output type, so <code>format_output_type</code> is not implemented.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class AnthropicTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Anthropic` model.\n\n    `AnthropicTypeAdapter` is responsible for preparing the arguments to\n    Anthropic's `messages.create` method: the input (prompt and possibly\n    image).\n    Anthropic does not support defining the output type, so\n    `format_output_type` is not implemented.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\n            \"messages\": [self._create_message(\"user\", model_input)]\n        }\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"messages\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `messages` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"messages\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_content_messages = [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": image.image_format,\n                        \"data\": image.image_str,\n                    },\n                }\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    *image_content_messages,\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(self, output_type):\n        \"\"\"Not implemented for Anthropic.\"\"\"\n        if output_type is None:\n            return {}\n        else:\n            raise NotImplementedError(\n                f\"The output type {output_type} is not available with \"\n                \"Anthropic.\"\n            )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.AnthropicTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `messages` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"messages\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.AnthropicTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.AnthropicTypeAdapter.format_output_type","title":"<code>format_output_type(output_type)</code>","text":"<p>Not implemented for Anthropic.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def format_output_type(self, output_type):\n    \"\"\"Not implemented for Anthropic.\"\"\"\n    if output_type is None:\n        return {}\n    else:\n        raise NotImplementedError(\n            f\"The output type {output_type} is not available with \"\n            \"Anthropic.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.anthropic.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base","title":"<code>base</code>","text":"<p>Base classes for all models and model type adapters.</p>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel","title":"<code>AsyncModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all asynchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class AsyncModel(ABC):\n    \"\"\"Base class for all asynchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    async def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator(\"prompt\")\n        ```\n        and\n        ```python\n        await model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator(model_input, **inference_kwargs)\n\n    async def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        await model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    async def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        async for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        async for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n\n        async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n            yield chunk\n\n    @abstractmethod\n    async def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A coroutine that will produce an async iterator of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator(\"prompt\")\n</code></pre> and <pre><code>await model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator(\"prompt\")\n    ```\n    and\n    ```python\n    await model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>await model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    await model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A coroutine that will produce an async iterator of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A coroutine that will produce an async iterator of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.AsyncModel.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nasync for chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>async for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    async for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    async for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n\n    async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.base.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt","title":"<code>dottxt</code>","text":"<p>Integration with Dottxt's API.</p>"},{"location":"api_reference/models/#outlines.models.dottxt.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.DottxtTypeAdapter","title":"<code>DottxtTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Dottxt</code> model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class DottxtTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Dottxt` model.\"\"\"\n\n    def format_input(self, model_input: str) -&gt; str:\n        \"\"\"Format the prompt to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The input to pass to the client.\n\n        \"\"\"\n        if isinstance(model_input, str):\n            return model_input\n        raise TypeError(\n            f\"The input type {model_input} is not available with Dottxt. \"\n            \"The only available type is `str`.\"\n        )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        str\n            The output type to pass to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if output_type is None:\n            raise TypeError(\n                \"You must provide an output type. Dottxt only supports \"\n                \"constrained generation.\"\n            )\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Dottxt. \"\n                \"Consider using a local mode instead.\"\n            )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.DottxtTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Format the prompt to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_input(self, model_input: str) -&gt; str:\n    \"\"\"Format the prompt to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The input to pass to the client.\n\n    \"\"\"\n    if isinstance(model_input, str):\n        return model_input\n    raise TypeError(\n        f\"The input type {model_input} is not available with Dottxt. \"\n        \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.DottxtTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The output type to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    str\n        The output type to pass to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if output_type is None:\n        raise TypeError(\n            \"You must provide an output type. Dottxt only supports \"\n            \"constrained generation.\"\n        )\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Dottxt. \"\n            \"Consider using a local mode instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.dottxt.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini","title":"<code>gemini</code>","text":"<p>Integration with Gemini's API.</p>"},{"location":"api_reference/models/#outlines.models.gemini.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.GeminiTypeAdapter","title":"<code>GeminiTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Gemini</code> model.</p> <p><code>GeminiTypeAdapter</code> is responsible for preparing the arguments to Gemini's client <code>models.generate_content</code> method: the input (prompt and possibly image), as well as the output type (either JSON or multiple choice).</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class GeminiTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Gemini` model.\n\n    `GeminiTypeAdapter` is responsible for preparing the arguments to Gemini's\n    client `models.generate_content` method: the input (prompt and possibly\n    image), as well as the output type (either JSON or multiple choice).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `contents` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `contents` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Gemini. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\"contents\": [self._create_text_part(model_input)]}\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"contents\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `contents` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"contents\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        # Gemini uses \"model\" instead of \"assistant\"\n        if role == \"assistant\":\n            role = \"model\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"parts\": [self._create_text_part(content)],\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_part(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"parts\": [\n                    self._create_text_part(prompt),\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n        return {\"contents\": [prompt, *image_parts]}\n\n\n    def _create_text_part(self, text: str) -&gt; dict:\n        \"\"\"Create a text input part for a message.\"\"\"\n        return {\n            \"text\": text,\n        }\n\n    def _create_img_part(self, image: Image) -&gt; dict:\n        \"\"\"Create an image input part for a message.\"\"\"\n        return {\n            \"inline_data\": {\n                \"mime_type\": image.image_format,\n                \"data\": image.image_str,\n            }\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `generation_config` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `generation_config` argument to pass to the client.\n\n        \"\"\"\n\n        # Unsupported output pytes\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` \"\n                \"keyword in Json Schema are available with Gemini. Use an \"\n                \"open source model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Gemini. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                JsonSchema.convert_to(\n                    output_type,\n                    [\"dataclass\", \"typeddict\", \"pydantic\"]\n                )\n            )\n\n        # List of structured types\n        elif is_typing_list(output_type):\n            return self.format_list_output_type(output_type)\n\n        # Multiple choice types\n        elif is_enum(output_type):\n            return self.format_enum_output_type(output_type)\n        elif is_literal(output_type):\n            enum = get_enum_from_literal(output_type)\n            return self.format_enum_output_type(enum)\n        elif isinstance(output_type, Choice):\n            enum = get_enum_from_choice(output_type)\n            return self.format_enum_output_type(enum)\n\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Gemini. \"\n                \"Consider using a local model or dottxt instead.\"\n            )\n\n    def format_enum_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"text/x.enum\",\n            \"response_schema\": output_type,\n        }\n\n    def format_json_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"application/json\",\n            \"response_schema\": output_type,\n        }\n\n    def format_list_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        args = get_args(output_type)\n\n        if len(args) == 1:\n            item_type = args[0]\n\n            if JsonSchema.is_json_schema(item_type):\n                return {\n                    \"response_mime_type\": \"application/json\",\n                    \"response_schema\": list[  # type: ignore\n                        JsonSchema.convert_to(\n                            item_type,\n                            [\"dataclass\", \"typeddict\", \"pydantic\"]\n                        )\n                    ],\n                }\n            else:\n                raise TypeError(\n                    \"The list items output type must contain a JSON schema \"\n                    \"type.\"\n                )\n\n        raise TypeError(\n            f\"Gemini only supports homogeneous lists: \"\n            \"list[BaseModel], list[TypedDict] or list[dataclass]. \"\n            f\"Got {output_type} instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.GeminiTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `contents` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"contents\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.GeminiTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>contents</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `contents` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `contents` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Gemini. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.GeminiTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>generation_config</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>generation_config</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `generation_config` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `generation_config` argument to pass to the client.\n\n    \"\"\"\n\n    # Unsupported output pytes\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` \"\n            \"keyword in Json Schema are available with Gemini. Use an \"\n            \"open source model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Gemini. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            JsonSchema.convert_to(\n                output_type,\n                [\"dataclass\", \"typeddict\", \"pydantic\"]\n            )\n        )\n\n    # List of structured types\n    elif is_typing_list(output_type):\n        return self.format_list_output_type(output_type)\n\n    # Multiple choice types\n    elif is_enum(output_type):\n        return self.format_enum_output_type(output_type)\n    elif is_literal(output_type):\n        enum = get_enum_from_literal(output_type)\n        return self.format_enum_output_type(enum)\n    elif isinstance(output_type, Choice):\n        enum = get_enum_from_choice(output_type)\n        return self.format_enum_output_type(enum)\n\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Gemini. \"\n            \"Consider using a local model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.gemini.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp","title":"<code>llamacpp</code>","text":"<p>Integration with the <code>llama-cpp-python</code> library.</p>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCppTokenizer","title":"<code>LlamaCppTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTokenizer(Tokenizer):\n    def __init__(self, model: \"Llama\"):\n        self.tokenizer = model.tokenizer()\n        self.special_tokens: Set[str] = set()\n        self.vocabulary: Dict[str, int] = dict()\n\n        # TODO: Remove when https://github.com/ggerganov/llama.cpp/pull/5613\n        # is resolved\n        self._hf_tokenizer = None\n        if (\n            hasattr(model, \"tokenizer_\")\n            and hasattr(model.tokenizer_, \"hf_tokenizer\")\n        ):\n            self._hf_tokenizer = model.tokenizer_.hf_tokenizer\n            self.eos_token_id = self._hf_tokenizer.eos_token_id\n            self.eos_token = self._hf_tokenizer.eos_token\n            self.vocabulary = self._hf_tokenizer.get_vocab()\n        else:\n            from llama_cpp import (\n                llama_model_get_vocab,\n                llama_token_to_piece,\n            )\n\n            self.eos_token_id = model.token_eos()\n            size = 32\n            buffer = (ctypes.c_char * size)()\n            for i in range(model.n_vocab()):\n                n = llama_token_to_piece(\n                    llama_model_get_vocab(model.model),\n                    i,\n                    buffer,\n                    size,\n                    0,\n                    True\n                )\n                token_piece = buffer[:n].decode(\"utf-8\", errors=\"replace\") # type: ignore\n                self.vocabulary[token_piece] = i\n                if i == self.eos_token_id:\n                    self.eos_token = token_piece\n\n        self.pad_token_id = self.eos_token_id\n        # ensure stable ordering of vocabulary\n        self.vocabulary = {\n            tok: tok_id\n            for tok, tok_id\n            in sorted(self.vocabulary.items(), key=lambda x: x[1])\n        }\n        self._hash = None\n\n    def decode(self, token_ids: List[int]) -&gt; List[str]:\n        decoded_bytes = self.tokenizer.detokenize(token_ids)\n        return [decoded_bytes.decode(\"utf-8\", errors=\"ignore\")]\n\n    def encode(\n        self,\n        prompt: Union[str, List[str]],\n        add_bos: bool = True,\n        special: bool = True,\n    ) -&gt; Tuple[List[int], List[int]]:\n        if isinstance(prompt, list):\n            raise NotImplementedError(\n                \"llama-cpp-python tokenizer doesn't support batch tokenization\"\n            )\n        token_ids = self.tokenizer.tokenize(\n            prompt.encode(\"utf-8\", errors=\"ignore\"),\n            add_bos=add_bos,\n            special=special,\n        )\n        # generate attention mask, missing from llama-cpp-python\n        attention_mask = [\n            1 if token_id != self.pad_token_id else 0 for token_id in token_ids\n        ]\n        return token_ids, attention_mask\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        if self._hf_tokenizer is not None:\n            from transformers.file_utils import SPIECE_UNDERLINE\n\n            token_str = self._hf_tokenizer.convert_tokens_to_string([token])\n            if (\n                token.startswith(SPIECE_UNDERLINE)\n                or token == \"&lt;0x20&gt;\"\n            ):  # pragma: no cover\n                token_str = \" \" + token_str\n            return token_str\n        else:\n            return token\n\n    def __eq__(self, other):\n        if not isinstance(other, LlamaCppTokenizer):\n            return False\n        return self.__getstate__() == other.__getstate__()\n\n    def __hash__(self):\n        # We create a custom hash as pickle.dumps(self) is not stable\n        if self._hash is None:\n            self._hash = hash((\n                tuple(sorted(self.vocabulary.items())),\n                self.eos_token_id,\n                self.eos_token,\n                self.pad_token_id,\n                tuple(sorted(self.special_tokens)),\n            ))\n        return self._hash\n\n    def __getstate__(self):\n        \"\"\"Create a stable representation for outlines.caching\"\"\"\n        return (\n            self.vocabulary,\n            self.eos_token_id,\n            self.eos_token,\n            self.pad_token_id,\n            sorted(self.special_tokens),\n        )\n\n    def __setstate__(self, state):\n        raise NotImplementedError(\"Cannot load a pickled llamacpp tokenizer\")\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCppTokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a stable representation for outlines.caching</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Create a stable representation for outlines.caching\"\"\"\n    return (\n        self.vocabulary,\n        self.eos_token_id,\n        self.eos_token,\n        self.pad_token_id,\n        sorted(self.special_tokens),\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCppTypeAdapter","title":"<code>LlamaCppTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>LlamaCpp</code> model.</p> <p><code>LlamaCppTypeAdapter</code> is responsible for preparing the arguments to the <code>Llama</code> object text generation methods.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `LlamaCpp` model.\n\n    `LlamaCppTypeAdapter` is responsible for preparing the arguments to\n    the `Llama` object text generation methods.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"LlamaCpp. The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; list:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"LlamaCpp does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return  [\n            {\n                \"role\": message[\"role\"],\n                \"content\": message[\"content\"],\n            }\n            for message in model_input.messages\n        ]\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        LogitsProcessorList\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from llama_cpp import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"LlamaCpp. The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>LogitsProcessorList</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    LogitsProcessorList\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from llama_cpp import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.llamacpp.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral","title":"<code>mistral</code>","text":"<p>Integration with Mistral AI API.</p>"},{"location":"api_reference/models/#outlines.models.mistral.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter","title":"<code>MistralTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Mistral</code> model.</p> <p>Prepares arguments for Mistral's client <code>chat.complete</code>, <code>chat.complete_async</code>, or <code>chat.stream</code> methods. Handles input (prompt or chat messages) and output type (JSON schema types).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class MistralTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Mistral` model.\n\n    Prepares arguments for Mistral's client `chat.complete`,\n    `chat.complete_async`, or `chat.stream` methods. Handles input (prompt or\n    chat messages) and output type (JSON schema types).\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Format a string input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : str\n            The input string prompt.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=model_input)]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Format a list input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : list\n            The input list, containing a string prompt and optionally Image\n            objects (vision models only).\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=self._create_message_content(model_input))]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a Chat input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : Chat\n            The Chat object containing a list of message dictionaries.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n        messages = []\n\n        for message in model_input.messages:\n            role = message[\"role\"]\n            content = message[\"content\"]\n            if role == \"user\":\n                messages.append(\n                    UserMessage(content=self._create_message_content(content))\n                )\n            elif role == \"assistant\":\n                messages.append(AssistantMessage(content=content))\n            elif role == \"system\":\n                messages.append(SystemMessage(content=content))\n            else:\n                raise ValueError(f\"Unsupported role: {role}\")\n\n        return messages\n\n    def _create_message_content(\n        self, content: Union[str, list]\n    ) -&gt; Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]:\n        \"\"\"Create message content from an input.\n\n        Parameters\n        ----------\n        content : Union[str, list]\n            The content to format, either a string or a list containing a\n            string and optionally Image objects.\n\n        Returns\n        -------\n        Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]\n            The formatted content, either a string or a list of content parts\n            (text and image URLs).\n\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list):\n            if not content:\n                raise ValueError(\"Content list cannot be empty.\")\n            if not isinstance(content[0], str):\n                raise ValueError(\n                    \"The first item in the list should be a string.\"\n                )\n            if len(content) == 1:\n                return content[0]\n            content_parts: List[Dict[str, Union[str, Dict[str, str]]]] = [\n                {\"type\": \"text\", \"text\": content[0]}\n            ]\n            for item in content[1:]:\n                if isinstance(item, Image):\n                    data_url = f\"data:{item.image_format};base64,{item.image_str}\"\n                    content_parts.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": data_url}\n                    })\n                else:\n                    raise ValueError(\n                        f\"Invalid item type in content list: {type(item)}. \"\n                        + \"Expected Image objects after the first string.\"\n                    )\n            return content_parts\n        else:\n            raise TypeError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"Content must be a string or a list starting with a string \"\n                + \"followed by optional Image objects.\"\n            )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type : Optional[Any]\n            The desired output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `response_format` dict to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif is_pydantic_model(output_type):\n            schema = output_type.model_json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_dataclass(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_typed_dict(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_genson_schema_builder(output_type):\n            schema = json.loads(output_type.to_json())\n            return self.format_json_schema_type(schema)\n        elif isinstance(output_type, JsonSchema):\n            return self.format_json_schema_type(json.loads(output_type.schema))\n\n        # Json mode\n        elif is_native_dict(output_type):\n            return {\"type\": \"json_object\"}\n\n        # Unsupported types\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not available with \"\n                \"Mistral.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Mistral.\"\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", str(output_type))\n            raise TypeError(\n                f\"The type {type_name} is not available with Mistral.\"\n            )\n\n    def format_json_schema_type(\n        self, schema: dict, schema_name: str = \"default\"\n    ) -&gt; dict:\n        \"\"\"Create the `response_format` argument to pass to the client from a\n        JSON schema dictionary.\n\n        Parameters\n        ----------\n        schema : dict\n            The JSON schema to format.\n        schema_name : str\n            The name of the schema.\n\n        Returns\n        -------\n        dict\n            The value of the `response_format` argument to pass to the client.\n\n        \"\"\"\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"schema\": schema,\n                \"name\": schema_name.lower(),\n                \"strict\": True\n            }\n        }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Format a Chat input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat</code> <p>The Chat object containing a list of message dictionaries.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a Chat input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : Chat\n        The Chat object containing a list of message dictionaries.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n    messages = []\n\n    for message in model_input.messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        if role == \"user\":\n            messages.append(\n                UserMessage(content=self._create_message_content(content))\n            )\n        elif role == \"assistant\":\n            messages.append(AssistantMessage(content=content))\n        elif role == \"system\":\n            messages.append(SystemMessage(content=content))\n        else:\n            raise ValueError(f\"Unsupported role: {role}\")\n\n    return messages\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_json_schema_type","title":"<code>format_json_schema_type(schema, schema_name='default')</code>","text":"<p>Create the <code>response_format</code> argument to pass to the client from a JSON schema dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to format.</p> required <code>schema_name</code> <code>str</code> <p>The name of the schema.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The value of the <code>response_format</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_json_schema_type(\n    self, schema: dict, schema_name: str = \"default\"\n) -&gt; dict:\n    \"\"\"Create the `response_format` argument to pass to the client from a\n    JSON schema dictionary.\n\n    Parameters\n    ----------\n    schema : dict\n        The JSON schema to format.\n    schema_name : str\n        The name of the schema.\n\n    Returns\n    -------\n    dict\n        The value of the `response_format` argument to pass to the client.\n\n    \"\"\"\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"schema\": schema,\n            \"name\": schema_name.lower(),\n            \"strict\": True\n        }\n    }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Format a list input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list</code> <p>The input list, containing a string prompt and optionally Image objects (vision models only).</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Format a list input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : list\n        The input list, containing a string prompt and optionally Image\n        objects (vision models only).\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=self._create_message_content(model_input))]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The desired output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>response_format</code> dict to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type : Optional[Any]\n        The desired output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `response_format` dict to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif is_pydantic_model(output_type):\n        schema = output_type.model_json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_dataclass(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_typed_dict(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_genson_schema_builder(output_type):\n        schema = json.loads(output_type.to_json())\n        return self.format_json_schema_type(schema)\n    elif isinstance(output_type, JsonSchema):\n        return self.format_json_schema_type(json.loads(output_type.schema))\n\n    # Json mode\n    elif is_native_dict(output_type):\n        return {\"type\": \"json_object\"}\n\n    # Unsupported types\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not available with \"\n            \"Mistral.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Mistral.\"\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", str(output_type))\n        raise TypeError(\n            f\"The type {type_name} is not available with Mistral.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.MistralTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Format a string input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input string prompt.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Format a string input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : str\n        The input string prompt.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=model_input)]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mistral.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm","title":"<code>mlxlm</code>","text":"<p>Integration with the <code>mlx_lm</code> library.</p>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLMTypeAdapter","title":"<code>MLXLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>MLXLM</code> model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `MLXLM` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"mlx-lm. The available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str):\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"mlx-lm does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[list[OutlinesLogitsProcessor]]\n            The logits processor argument to be passed to the model.\n\n        \"\"\"\n        if not output_type:\n            return None\n        return [output_type]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"mlx-lm. The available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.MLXLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[list[OutlinesLogitsProcessor]]</code> <p>The logits processor argument to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[list[OutlinesLogitsProcessor]]\n        The logits processor argument to be passed to the model.\n\n    \"\"\"\n    if not output_type:\n        return None\n    return [output_type]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.mlxlm.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama","title":"<code>ollama</code>","text":"<p>Integration with the <code>ollama</code> library.</p>"},{"location":"api_reference/models/#outlines.models.ollama.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter","title":"<code>OllamaTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Ollama</code> model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class OllamaTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Ollama` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The formatted value of the `messages` argument to be passed to\n            the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            return {\n                \"role\": role,\n                \"content\": prompt,\n                \"image\": [image.image_str for image in images],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(\n        self, output_type: Optional[Any] = None\n    ) -&gt; Optional[dict]:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Optional[str]\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        if output_type is None:\n            return None\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Ollama. \"\n                \"Consider using a local model instead.\"\n            )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted value of the <code>messages</code> argument to be passed to the client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The formatted value of the `messages` argument to be passed to\n        the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[Any] = None\n) -&gt; Optional[dict]:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Optional[str]\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    if output_type is None:\n        return None\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Ollama. \"\n            \"Consider using a local model instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.OllamaTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.ollama.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai","title":"<code>openai</code>","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api_reference/models/#outlines.models.openai.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter","title":"<code>OpenAITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>OpenAI</code> model.</p> <p><code>OpenAITypeAdapter</code> is responsible for preparing the arguments to OpenAI's <code>completions.create</code> methods: the input (prompt and possibly image), as well as the output type (only JSON).</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `OpenAI` model.\n\n    `OpenAITypeAdapter` is responsible for preparing the arguments to OpenAI's\n    `completions.create` methods: the input (prompt and possibly image), as\n    well as the output type (only JSON).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_content(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def _create_img_content(self, image: Image) -&gt; dict:\n        \"\"\"Create the content for an image input.\"\"\"\n        return {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:{image.image_format};base64,{image.image_str}\"  # noqa: E702\n            },\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client based on the\n        output type specified by the user.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` keyword \"\n                \"in Json Schema are available with OpenAI. Use an open source \"\n                \"model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n        elif is_native_dict(output_type):\n            return self.format_json_mode_type()\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n    def format_json_output_type(self, schema: dict) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified a `Json` output type.\n\n        \"\"\"\n        # OpenAI requires `additionalProperties` to be set to False\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"response_format\": {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"name\": \"default\",\n                    \"strict\": True,\n                    \"schema\": schema,\n                },\n            }\n        }\n\n    def format_json_mode_type(self) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified the output type should be a JSON but without specifying the\n        schema (also called \"JSON mode\").\n\n        \"\"\"\n        return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_json_mode_type","title":"<code>format_json_mode_type()</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified the output type should be a JSON but without specifying the schema (also called \"JSON mode\").</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_mode_type(self) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified the output type should be a JSON but without specifying the\n    schema (also called \"JSON mode\").\n\n    \"\"\"\n    return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_json_output_type","title":"<code>format_json_output_type(schema)</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified a <code>Json</code> output type.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_output_type(self, schema: dict) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified a `Json` output type.\n\n    \"\"\"\n    # OpenAI requires `additionalProperties` to be set to False\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"default\",\n                \"strict\": True,\n                \"schema\": schema,\n            },\n        }\n    }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to the client based on the output type specified by the user.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client based on the\n    output type specified by the user.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` keyword \"\n            \"in Json Schema are available with OpenAI. Use an open source \"\n            \"model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n    elif is_native_dict(output_type):\n        return self.format_json_mode_type()\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.OpenAITypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.openai.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang","title":"<code>sglang</code>","text":"<p>Integration with an SGLang server.</p>"},{"location":"api_reference/models/#outlines.models.sglang.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLangTypeAdapter","title":"<code>SGLangTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>SGLang</code> and <code>AsyncSGLang</code> models.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLangTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `SGLang` and `AsyncSGLang` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the sglang\n        server expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            warnings.warn(\n                \"SGLang grammar-based structured outputs expects an EBNF \"\n                \"grammar instead of a Lark grammar as is generally used in \"\n                \"Outlines. The grammar cannot be used as a structured output \"\n                \"type with an outlines backend, it is only compatible with \"\n                \"the sglang and llguidance backends.\"\n            )\n            return {\"extra_body\": {\"ebnf\": term.definition}}\n        elif isinstance(term, JsonSchema):\n            return OpenAITypeAdapter().format_json_output_type(\n                json.loads(term.schema)\n            )\n        else:\n            return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLangTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the sglang server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the sglang\n    server expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.SGLangTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        warnings.warn(\n            \"SGLang grammar-based structured outputs expects an EBNF \"\n            \"grammar instead of a Lark grammar as is generally used in \"\n            \"Outlines. The grammar cannot be used as a structured output \"\n            \"type with an outlines backend, it is only compatible with \"\n            \"the sglang and llguidance backends.\"\n        )\n        return {\"extra_body\": {\"ebnf\": term.definition}}\n    elif isinstance(term, JsonSchema):\n        return OpenAITypeAdapter().format_json_output_type(\n            json.loads(term.schema)\n        )\n    else:\n        return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.sglang.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi","title":"<code>tgi</code>","text":"<p>Integration with a TGI server.</p>"},{"location":"api_reference/models/#outlines.models.tgi.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGITypeAdapter","title":"<code>TGITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>TGI</code> and <code>AsyncTGI</code> models.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `TGI` and `AsyncTGI` models.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the client.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {input} is not available with TGI. \"\n            + \"The only available type is `str`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Argument\n        --------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            raise NotImplementedError(\n                \"TGI does not support CFG-based structured outputs.\"\n            )\n        elif isinstance(term, JsonSchema):\n            return {\n                \"grammar\": {\n                    \"type\": \"json\",\n                    \"value\": json.loads(term.schema),\n                }\n            }\n        else:\n            return {\n                \"grammar\": {\n                    \"type\": \"regex\",\n                    \"value\": to_regex(term),\n                }\n            }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the client.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the client.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {input} is not available with TGI. \"\n        + \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.TGITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> Argument <p>output_type     The structured output type provided.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Argument\n    --------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        raise NotImplementedError(\n            \"TGI does not support CFG-based structured outputs.\"\n        )\n    elif isinstance(term, JsonSchema):\n        return {\n            \"grammar\": {\n                \"type\": \"json\",\n                \"value\": json.loads(term.schema),\n            }\n        }\n    else:\n        return {\n            \"grammar\": {\n                \"type\": \"regex\",\n                \"value\": to_regex(term),\n            }\n        }\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tgi.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tokenizer","title":"<code>tokenizer</code>","text":""},{"location":"api_reference/models/#outlines.models.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Hashable</code>, <code>Protocol</code></p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>class Tokenizer(Hashable, Protocol):\n    eos_token: str\n    eos_token_id: int\n    pad_token_id: int\n    vocabulary: Dict[str, int]\n    special_tokens: Set[str]\n\n    def encode(\n        self, prompt: Union[str, List[str]]\n    ) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n        \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n        ...\n\n    def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n        \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n        ...\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        \"\"\"Convert a token to its equivalent string.\n\n        This is for instance useful for BPE tokenizers where whitespaces are\n        represented by the special characted `\u0120`. This prevents matching a raw\n        token that includes `\u0120` with a string.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tokenizer.Tokenizer.convert_token_to_string","title":"<code>convert_token_to_string(token)</code>","text":"<p>Convert a token to its equivalent string.</p> <p>This is for instance useful for BPE tokenizers where whitespaces are represented by the special characted <code>\u0120</code>. This prevents matching a raw token that includes <code>\u0120</code> with a string.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def convert_token_to_string(self, token: str) -&gt; str:\n    \"\"\"Convert a token to its equivalent string.\n\n    This is for instance useful for BPE tokenizers where whitespaces are\n    represented by the special characted `\u0120`. This prevents matching a raw\n    token that includes `\u0120` with a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids)</code>","text":"<p>Translate an array of token ids to a string or list of strings.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n    \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.tokenizer.Tokenizer.encode","title":"<code>encode(prompt)</code>","text":"<p>Translate the input prompts into arrays of token ids and attention mask.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def encode(\n    self, prompt: Union[str, List[str]]\n) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n    \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers","title":"<code>transformers</code>","text":"<p>Integration with the <code>transformers</code> library.</p>"},{"location":"api_reference/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersMultiModalTypeAdapter","title":"<code>TransformersMultiModalTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for <code>TransformersMultiModal</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModalTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for `TransformersMultiModal` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Fomat the prompt arguments to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available. Please \"\n            + \"provide a list containing a text prompt and assets \"\n            + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n            + \"model or a `Chat` instance.\"\n        )\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; dict:\n        conversation = []\n        assets = []\n\n        # process each message, convert if needed to standardized multimodal chat template format\n        # and collect assets for HF processor\n        for message in model_input.messages:\n            processed_message, message_assets = self._prepare_message(\n                message[\"role\"], message[\"content\"]\n            )\n            conversation.append(processed_message)\n            assets.extend(message_assets)\n\n        formatted_prompt = self.tokenizer.apply_chat_template(\n            conversation,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        # use the formatted prompt and the assets to format the input\n        return self.format_list_input([formatted_prompt, *assets])\n\n    def _prepare_message(self, role: str, content: str | list) -&gt; tuple[dict, list]:\n        \"\"\"Create a message.\"\"\"\n        if isinstance(content, str):\n            return {\"role\": role, \"content\": content}, []\n\n        elif isinstance(content, list):\n            if all(isinstance(item, dict) for item in content): # HF multimodal chat template\n                return {\"role\": role, \"content\": content}, self._extract_assets_from_content(content)\n            else: # list of string + assets\n                prompt = content[0]\n                assets = content[1:]\n                assets_dict = [self._format_asset_for_template(asset) for asset in assets]\n\n                return {\"role\": role, \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *assets_dict\n                ]}, assets\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"The content must be a string or a list containing text and assets \"\n                + \"or a list of dict items with explicit types.\"\n            )\n\n    def _extract_assets_from_content(self, content: list) -&gt; list:\n        \"\"\"Process a list of dict items.\"\"\"\n        assets = []\n\n        for item in content:\n            if len(item) &gt; 2:\n                raise ValueError(\n                    f\"Found item with multiple keys: {item}. \"\n                    + \"Each item in the content list must be a dictionary with a 'type' key and a single asset key. \"\n                    + \"To include multiple assets, use separate dictionary items. \"\n                    + \"For example: [{{'type': 'image', 'image': image1}}, {{'type': 'image', 'image': image2}}]. \"\n                )\n\n            if \"type\" not in item:\n                raise ValueError(\n                    \"Each item in the content list must be a dictionary with a 'type' key. \"\n                    + \"Valid types are 'text', 'image', 'video', or 'audio'. \"\n                    + \"For instance {{'type': 'text', 'text': 'your message'}}. \"\n                    + f\"Found item without 'type' key: {item}\"\n                )\n            if item[\"type\"] == \"text\":\n                continue\n            elif item[\"type\"] in [\"image\", \"video\", \"audio\"]:\n                asset_key = item[\"type\"]\n                if asset_key not in item:\n                    raise ValueError(\n                        f\"Item with type '{asset_key}' must contain a '{asset_key}' key. \"\n                        + f\"Found item: {item}\"\n                    )\n                if isinstance(item[asset_key], (Image, Video, Audio)):\n                    assets.append(item[asset_key])\n                else:\n                    raise ValueError(\n                        \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                        + f\"Unsupported asset type: {type(item[asset_key])}\"\n                    )\n            else:\n                raise ValueError(\n                    \"Content must be 'text', 'image', 'video' or 'audio'. \"\n                    + f\"Unsupported content type: {item['type']}\")\n        return assets\n\n    def _format_asset_for_template(self, asset: Image | Video | Audio) -&gt; dict:\n        \"\"\"Process an asset.\"\"\"\n        if isinstance(asset, Image):\n            return {\"type\": \"image\", \"image\": asset}\n        elif isinstance(asset, Video):\n            return {\"type\": \"video\", \"video\": asset}\n        elif isinstance(asset, Audio):\n            return {\"type\": \"audio\", \"audio\": asset}\n        else:\n            raise ValueError(\n                \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                + f\"Unsupported asset type: {type(asset)}\"\n            )\n\n    @format_input.register(list)\n    def format_list_input(self, model_input: list) -&gt; dict:\n        prompt = model_input[0]\n        assets = model_input[1:]\n\n        if not assets:  # handle empty assets case\n            return {\"text\": prompt}\n\n        asset_types = set(type(asset) for asset in assets)\n        if len(asset_types) &gt; 1:\n            raise ValueError(\n                \"All assets must be of the same type. \"\n                + f\"Found types: {asset_types}\"\n            )\n        asset_type = asset_types.pop()\n\n        if asset_type == Image:\n            return {\n                \"text\": prompt,\n                \"images\": [asset.image for asset in assets]\n            }\n        elif asset_type == Audio: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"audio\": [asset.audio for asset in assets]\n            }\n        elif asset_type == Video: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"videos\": [asset.video for asset in assets]\n            }\n        else:\n            raise ValueError(f\"Unsupported asset type: {asset_type}\")\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Argument\n        --------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Fomat the prompt arguments to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Fomat the prompt arguments to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available. Please \"\n        + \"provide a list containing a text prompt and assets \"\n        + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n        + \"model or a `Chat` instance.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> Argument <p>output_type     The logits processor provided.</p> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Argument\n    --------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersTypeAdapter","title":"<code>TransformersTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Transformers` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available.\"\n            \"The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available.\"\n        \"The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.TransformersTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/models/#outlines.models.utils.set_additional_properties_false_json_schema","title":"<code>set_additional_properties_false_json_schema(schema)</code>","text":"<p>Set additionalProperties to False to all objects in the schema using jsonpath.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to modify</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The modified schema with additionalProperties set to False</p> Source code in <code>outlines/models/utils.py</code> <pre><code>def set_additional_properties_false_json_schema(schema: dict) -&gt; dict:\n    \"\"\"Set additionalProperties to False to all objects in the schema using jsonpath.\n\n    Parameters\n    ----------\n    schema\n        The JSON schema to modify\n\n    Returns\n    -------\n    dict\n        The modified schema with additionalProperties set to False\n    \"\"\"\n    # Get all nodes\n    jsonpath_expr = jsonpath_ng.parse('$..*')\n    matches = jsonpath_expr.find(schema)\n\n    # Go over all nodes and set additionalProperties to False if it's an object\n    for match in matches:\n        if match.value == 'object':\n            if 'additionalProperties' not in match.context.value:\n                match.context.value['additionalProperties'] = False\n\n    return schema\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm","title":"<code>vllm</code>","text":"<p>Integration with a vLLM server.</p>"},{"location":"api_reference/models/#outlines.models.vllm.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLMTypeAdapter","title":"<code>VLLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLM</code> and <code>AsyncVLLM</code> models.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLM` and `AsyncVLLM` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the vLLM server\n        expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the model.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"guided_grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            extra_body = {\"guided_json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n            return extra_body\n        else:\n            return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the vLLM server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the vLLM server\n    expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.VLLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the model.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"guided_grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        extra_body = {\"guided_json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n        return extra_body\n    else:\n        return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline","title":"<code>vllm_offline</code>","text":"<p>Integration with the <code>vllm</code> library (offline mode).</p>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter","title":"<code>VLLMOfflineTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLMOffline</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOfflineTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLMOffline` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"VLLM offline. The only available types are `str` and \"\n            \"`Chat` (containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_input_str(self, model_input: str) -&gt; str:\n        \"\"\"Format a `str` input.\n\n        \"\"\"\n        return model_input\n\n    @format_input.register(Chat)\n    def format_input_chat(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a `Chat` input.\n\n        \"\"\"\n        for message in model_input.messages:\n            content = message[\"content\"]\n            if isinstance(content, list):\n                raise ValueError(\n                    \"Assets are not supported for vLLM offline.\"\n                    \"Please only use text content in the `Chat` input.\"\n                )\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the model.\n\n        For vLLM, the structured output definition is set in the\n        `GuidedDecodingParams` constructor that is provided as a value to the\n        `guided_decoding` parameter of the `SamplingParams` constructor, itself\n        provided as a value to the `sampling_params` parameter of the `generate`\n        method.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The arguments to provide to the `GuidedDecodingParams` constructor.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            guided_decoding_params = {\"json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n            return guided_decoding_params\n        else:\n            return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"VLLM offline. The only available types are `str` and \"\n        \"`Chat` (containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_chat","title":"<code>format_input_chat(model_input)</code>","text":"<p>Format a <code>Chat</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(Chat)\ndef format_input_chat(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a `Chat` input.\n\n    \"\"\"\n    for message in model_input.messages:\n        content = message[\"content\"]\n        if isinstance(content, list):\n            raise ValueError(\n                \"Assets are not supported for vLLM offline.\"\n                \"Please only use text content in the `Chat` input.\"\n            )\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_str","title":"<code>format_input_str(model_input)</code>","text":"<p>Format a <code>str</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(str)\ndef format_input_str(self, model_input: str) -&gt; str:\n    \"\"\"Format a `str` input.\n\n    \"\"\"\n    return model_input\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the model.</p> <p>For vLLM, the structured output definition is set in the <code>GuidedDecodingParams</code> constructor that is provided as a value to the <code>guided_decoding</code> parameter of the <code>SamplingParams</code> constructor, itself provided as a value to the <code>sampling_params</code> parameter of the <code>generate</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments to provide to the <code>GuidedDecodingParams</code> constructor.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the model.\n\n    For vLLM, the structured output definition is set in the\n    `GuidedDecodingParams` constructor that is provided as a value to the\n    `guided_decoding` parameter of the `SamplingParams` constructor, itself\n    provided as a value to the `sampling_params` parameter of the `generate`\n    method.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The arguments to provide to the `GuidedDecodingParams` constructor.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        guided_decoding_params = {\"json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n        return guided_decoding_params\n    else:\n        return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/#outlines.models.vllm_offline.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/models/anthropic/","title":"anthropic","text":"<p>Integration with Anthropic's API.</p>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.Anthropic","title":"<code>Anthropic</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>anthropic.Anthropic</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>anthropic.Anthropic</code> client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class Anthropic(Model):\n    \"\"\"Thin wrapper around the `anthropic.Anthropic` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `anthropic.Anthropic` client.\n\n    \"\"\"\n    def __init__(\n        self, client: \"AnthropicClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `anthropic.Anthropic` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = AnthropicTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        completion = self.client.messages.create(\n            **messages,\n            **inference_kwargs,\n        )\n        return completion.content[0].text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Anthropic does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Anthropic.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            As structured generation is not supported by Anthropic, the value\n            of this argument must be `None`. Otherwise, an error will be\n            raised at runtime.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n\n        if output_type is not None:\n            raise NotImplementedError(\n                f\"The type {output_type} is not available with Anthropic.\"\n            )\n\n        if (\n            \"model\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model\"] = self.model_name\n\n        stream = self.client.messages.create(\n            **messages,\n            stream=True,\n            **inference_kwargs,\n        )\n\n        for chunk in stream:\n            if (\n                chunk.type == \"content_block_delta\"\n                and chunk.delta.type == \"text_delta\"\n            ):\n                yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.Anthropic.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def __init__(\n    self, client: \"AnthropicClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = AnthropicTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.Anthropic.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    completion = self.client.messages.create(\n        **messages,\n        **inference_kwargs,\n    )\n    return completion.content[0].text\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.Anthropic.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using Anthropic.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>As structured generation is not supported by Anthropic, the value of this argument must be <code>None</code>. Otherwise, an error will be raised at runtime.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Anthropic.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        As structured generation is not supported by Anthropic, the value\n        of this argument must be `None`. Otherwise, an error will be\n        raised at runtime.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n\n    if output_type is not None:\n        raise NotImplementedError(\n            f\"The type {output_type} is not available with Anthropic.\"\n        )\n\n    if (\n        \"model\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model\"] = self.model_name\n\n    stream = self.client.messages.create(\n        **messages,\n        stream=True,\n        **inference_kwargs,\n    )\n\n    for chunk in stream:\n        if (\n            chunk.type == \"content_block_delta\"\n            and chunk.delta.type == \"text_delta\"\n        ):\n            yield chunk.delta.text\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.AnthropicTypeAdapter","title":"<code>AnthropicTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Anthropic</code> model.</p> <p><code>AnthropicTypeAdapter</code> is responsible for preparing the arguments to Anthropic's <code>messages.create</code> method: the input (prompt and possibly image). Anthropic does not support defining the output type, so <code>format_output_type</code> is not implemented.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>class AnthropicTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Anthropic` model.\n\n    `AnthropicTypeAdapter` is responsible for preparing the arguments to\n    Anthropic's `messages.create` method: the input (prompt and possibly\n    image).\n    Anthropic does not support defining the output type, so\n    `format_output_type` is not implemented.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\n            \"messages\": [self._create_message(\"user\", model_input)]\n        }\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"messages\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `messages` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"messages\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_content_messages = [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": image.image_format,\n                        \"data\": image.image_str,\n                    },\n                }\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    *image_content_messages,\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(self, output_type):\n        \"\"\"Not implemented for Anthropic.\"\"\"\n        if output_type is None:\n            return {}\n        else:\n            raise NotImplementedError(\n                f\"The output type {output_type} is not available with \"\n                \"Anthropic.\"\n            )\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.AnthropicTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `messages` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"messages\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.AnthropicTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Anthropic. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.AnthropicTypeAdapter.format_output_type","title":"<code>format_output_type(output_type)</code>","text":"<p>Not implemented for Anthropic.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def format_output_type(self, output_type):\n    \"\"\"Not implemented for Anthropic.\"\"\"\n    if output_type is None:\n        return {}\n    else:\n        raise NotImplementedError(\n            f\"The output type {output_type} is not available with \"\n            \"Anthropic.\"\n        )\n</code></pre>"},{"location":"api_reference/models/anthropic/#outlines.models.anthropic.from_anthropic","title":"<code>from_anthropic(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Anthropic</code> model instance from an <code>anthropic.Anthropic</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Anthropic</code> <p>An <code>anthropic.Anthropic</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Anthropic</code> <p>An Outlines <code>Anthropic</code> model instance.</p> Source code in <code>outlines/models/anthropic.py</code> <pre><code>def from_anthropic(\n    client: \"AnthropicClient\", model_name: Optional[str] = None\n) -&gt; Anthropic:\n    \"\"\"Create an Outlines `Anthropic` model instance from an\n    `anthropic.Anthropic` client instance.\n\n    Parameters\n    ----------\n    client\n        An `anthropic.Anthropic` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Anthropic\n        An Outlines `Anthropic` model instance.\n\n    \"\"\"\n    return Anthropic(client, model_name)\n</code></pre>"},{"location":"api_reference/models/base/","title":"base","text":"<p>Base classes for all models and model type adapters.</p>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel","title":"<code>AsyncModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all asynchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class AsyncModel(ABC):\n    \"\"\"Base class for all asynchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    async def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator(\"prompt\")\n        ```\n        and\n        ```python\n        await model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator(model_input, **inference_kwargs)\n\n    async def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        await generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        await model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return await generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    async def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        async for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        async for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n\n        async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n            yield chunk\n\n    @abstractmethod\n    async def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; AsyncIterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        AsyncIterator[Any]\n            A coroutine that will produce an async iterator of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator(\"prompt\")\n</code></pre> and <pre><code>await model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator(\"prompt\")\n    ```\n    and\n    ```python\n    await model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nawait generator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>await model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    await generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    await model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return await generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A coroutine that will produce an async iterator of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\nasync def generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A coroutine that will produce an async iterator of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.AsyncModel.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nasync for chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>async for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>async def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; AsyncIterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    async for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    async for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    AsyncIterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n\n    async for chunk in generator.stream(model_input, **inference_kwargs):  # type: ignore\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all synchronous models.</p> <p>This class defines shared <code>__call__</code>, <code>batch</code> and <code>stream</code> methods that can be used to call the model directly. The <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods must be implemented by the subclasses. All models inheriting from this class must define a <code>type_adapter</code> attribute of type <code>ModelTypeAdapter</code>. The methods of the <code>type_adapter</code> attribute are used in the <code>generate</code>, <code>generate_batch</code>, and <code>generate_stream</code> methods to format the input and output types received by the model. Additionally, steerable models must define a <code>tensor_library_name</code> attribute.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"Base class for all synchronous models.\n\n    This class defines shared `__call__`, `batch` and `stream` methods that can\n    be used to call the model directly. The `generate`, `generate_batch`, and\n    `generate_stream` methods must be implemented by the subclasses.\n    All models inheriting from this class must define a `type_adapter`\n    attribute of type `ModelTypeAdapter`. The methods of the `type_adapter`\n    attribute are used in the `generate`, `generate_batch`, and\n    `generate_stream` methods to format the input and output types received by\n    the model.\n    Additionally, steerable models must define a `tensor_library_name`\n    attribute.\n\n    \"\"\"\n    type_adapter: ModelTypeAdapter\n    tensor_library_name: str\n\n    def __call__(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Call the model.\n\n        Users can call the model directly, in which case we will create a\n        generator instance with the output type provided and call it.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator(\"prompt\")\n        ```\n        and\n        ```python\n        model(\"prompt\", Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        from outlines.generator import Generator\n\n        return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n\n    def batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Make a batch call to the model (several inputs at once).\n\n        Users can use the `batch` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `batch` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        generator.batch([\"prompt1\", \"prompt2\"])\n        ```\n        and\n        ```python\n        model.batch([\"prompt1\", \"prompt2\"], Foo)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.batch(model_input, **inference_kwargs) # type: ignore\n\n    def stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        backend: Optional[str] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Stream a response from the model.\n\n        Users can use the `stream` method from the model directly, in which\n        case we will create a generator instance with the output type provided\n        and then invoke its `stream` method.\n        Thus, those commands are equivalent:\n        ```python\n        generator = Generator(model, Foo)\n        for chunk in generator(\"prompt\"):\n            print(chunk)\n        ```\n        and\n        ```python\n        for chunk in model.stream(\"prompt\", Foo):\n            print(chunk)\n        ```\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        backend\n            The name of the backend to use to create the logits processor that\n            will be used to generate the response. Only used for steerable\n            models if `output_type` is provided.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        from outlines import Generator\n\n        generator = Generator(self, output_type, backend)\n        return generator.stream(model_input, **inference_kwargs) # type: ignore\n\n    @abstractmethod\n    def generate(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Generate a response from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Any\n            The response generated by the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate_batch(\n        self,\n        model_input: List[Any],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; List[Any]:\n        \"\"\"Generate a batch of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The list of inputs provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        List[Any]\n            The list of responses generated by the model.\n\n        \"\"\"\n        ...\n    @abstractmethod\n    def generate_stream(\n        self,\n        model_input: Any,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any\n    ) -&gt; Iterator[Any]:\n        \"\"\"Generate a stream of responses from the model.\n\n        The output_type argument contains a logits processor for steerable\n        models while it contains a type (Json, Enum...) for black-box models.\n        This method is not intended to be used directly by end users.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n        output_type\n            The output type provided by the user.\n        **inference_kwargs\n            Additional keyword arguments to pass to the model.\n\n        Returns\n        -------\n        Iterator[Any]\n            A stream of responses from the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.__call__","title":"<code>__call__(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Call the model.</p> <p>Users can call the model directly, in which case we will create a generator instance with the output type provided and call it. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator(\"prompt\")\n</code></pre> and <pre><code>model(\"prompt\", Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def __call__(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Call the model.\n\n    Users can call the model directly, in which case we will create a\n    generator instance with the output type provided and call it.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator(\"prompt\")\n    ```\n    and\n    ```python\n    model(\"prompt\", Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    from outlines.generator import Generator\n\n    return Generator(self, output_type, backend)(model_input, **inference_kwargs)\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.batch","title":"<code>batch(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Make a batch call to the model (several inputs at once).</p> <p>Users can use the <code>batch</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>batch</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\ngenerator.batch([\"prompt1\", \"prompt2\"])\n</code></pre> and <pre><code>model.batch([\"prompt1\", \"prompt2\"], Foo)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Make a batch call to the model (several inputs at once).\n\n    Users can use the `batch` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `batch` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    generator.batch([\"prompt1\", \"prompt2\"])\n    ```\n    and\n    ```python\n    model.batch([\"prompt1\", \"prompt2\"], Foo)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.batch(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Any:\n    \"\"\"Generate a response from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Any\n        The response generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a batch of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>List[Any]</code> <p>The list of inputs provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>The list of responses generated by the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_batch(\n    self,\n    model_input: List[Any],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; List[Any]:\n    \"\"\"Generate a batch of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The list of inputs provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    List[Any]\n        The list of responses generated by the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a stream of responses from the model.</p> <p>The output_type argument contains a logits processor for steerable models while it contains a type (Json, Enum...) for black-box models. This method is not intended to be used directly by end users.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef generate_stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Generate a stream of responses from the model.\n\n    The output_type argument contains a logits processor for steerable\n    models while it contains a type (Json, Enum...) for black-box models.\n    This method is not intended to be used directly by end users.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.Model.stream","title":"<code>stream(model_input, output_type=None, backend=None, **inference_kwargs)</code>","text":"<p>Stream a response from the model.</p> <p>Users can use the <code>stream</code> method from the model directly, in which case we will create a generator instance with the output type provided and then invoke its <code>stream</code> method. Thus, those commands are equivalent: <pre><code>generator = Generator(model, Foo)\nfor chunk in generator(\"prompt\"):\n    print(chunk)\n</code></pre> and <pre><code>for chunk in model.stream(\"prompt\", Foo):\n    print(chunk)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor that will be used to generate the response. Only used for steerable models if <code>output_type</code> is provided.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>A stream of responses from the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>def stream(\n    self,\n    model_input: Any,\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    **inference_kwargs: Any\n) -&gt; Iterator[Any]:\n    \"\"\"Stream a response from the model.\n\n    Users can use the `stream` method from the model directly, in which\n    case we will create a generator instance with the output type provided\n    and then invoke its `stream` method.\n    Thus, those commands are equivalent:\n    ```python\n    generator = Generator(model, Foo)\n    for chunk in generator(\"prompt\"):\n        print(chunk)\n    ```\n    and\n    ```python\n    for chunk in model.stream(\"prompt\", Foo):\n        print(chunk)\n    ```\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n    output_type\n        The output type provided by the user.\n    backend\n        The name of the backend to use to create the logits processor that\n        will be used to generate the response. Only used for steerable\n        models if `output_type` is provided.\n    **inference_kwargs\n        Additional keyword arguments to pass to the model.\n\n    Returns\n    -------\n    Iterator[Any]\n        A stream of responses from the model.\n\n    \"\"\"\n    from outlines import Generator\n\n    generator = Generator(self, output_type, backend)\n    return generator.stream(model_input, **inference_kwargs) # type: ignore\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.ModelTypeAdapter","title":"<code>ModelTypeAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all model type adapters.</p> <p>A type adapter instance must be given as a value to the <code>type_adapter</code> attribute when instantiating a model. The type adapter is responsible for formatting the input and output types passed to the model to match the specific format expected by the associated model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>class ModelTypeAdapter(ABC):\n    \"\"\"Base class for all model type adapters.\n\n    A type adapter instance must be given as a value to the `type_adapter`\n    attribute when instantiating a model.\n    The type adapter is responsible for formatting the input and output types\n    passed to the model to match the specific format expected by the\n    associated model.\n\n    \"\"\"\n\n    @abstractmethod\n    def format_input(self, model_input: Any) -&gt; Any:\n        \"\"\"Format the user input to the expected format of the model.\n\n        For API-based models, it typically means creating the `messages`\n        argument passed to the client. For local models, it can mean casting\n        the input from str to list for instance.\n        This method is also used to validate that the input type provided by\n        the user is supported by the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n        \"\"\"Format the output type to the expected format of the model.\n\n        For black-box models, this typically means creating a `response_format`\n        argument. For steerable models, it means formatting the logits processor\n        to create the object type expected by the model.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Any\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.ModelTypeAdapter.format_input","title":"<code>format_input(model_input)</code>  <code>abstractmethod</code>","text":"<p>Format the user input to the expected format of the model.</p> <p>For API-based models, it typically means creating the <code>messages</code> argument passed to the client. For local models, it can mean casting the input from str to list for instance. This method is also used to validate that the input type provided by the user is supported by the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_input(self, model_input: Any) -&gt; Any:\n    \"\"\"Format the user input to the expected format of the model.\n\n    For API-based models, it typically means creating the `messages`\n    argument passed to the client. For local models, it can mean casting\n    the input from str to list for instance.\n    This method is also used to validate that the input type provided by\n    the user is supported by the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/base/#outlines.models.base.ModelTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>  <code>abstractmethod</code>","text":"<p>Format the output type to the expected format of the model.</p> <p>For black-box models, this typically means creating a <code>response_format</code> argument. For steerable models, it means formatting the logits processor to create the object type expected by the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/base.py</code> <pre><code>@abstractmethod\ndef format_output_type(self, output_type: Optional[Any] = None) -&gt; Any:\n    \"\"\"Format the output type to the expected format of the model.\n\n    For black-box models, this typically means creating a `response_format`\n    argument. For steerable models, it means formatting the logits processor\n    to create the object type expected by the model.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Any\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/dottxt/","title":"dottxt","text":"<p>Integration with Dottxt's API.</p>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.Dottxt","title":"<code>Dottxt</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>dottxt.client.Dottxt</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>dottxt.client.Dottxt</code> client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class Dottxt(Model):\n    \"\"\"Thin wrapper around the `dottxt.client.Dottxt` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `dottxt.client.Dottxt` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"DottxtClient\",\n        model_name: Optional[str] = None,\n        model_revision: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `dottxt.Dottxt` client.\n        model_name\n            The name of the model to use.\n        model_revision\n            The revision of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.type_adapter = DottxtTypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Dottxt.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        json_schema = self.type_adapter.format_output_type(output_type)\n\n        if (\n            \"model_name\" not in inference_kwargs\n            and self.model_name is not None\n        ):\n            inference_kwargs[\"model_name\"] = self.model_name\n\n        if (\n            \"model_revision\" not in inference_kwargs\n            and self.model_revision is not None\n        ):\n            inference_kwargs[\"model_revision\"] = self.model_revision\n\n        completion = self.client.json(\n            prompt,\n            json_schema,\n            **inference_kwargs,\n        )\n        return completion.data\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Dottxt does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Not available for Dottxt.\"\"\"\n        raise NotImplementedError(\n            \"Dottxt does not support streaming. Call the model/generator for \"\n            + \"regular generation instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.Dottxt.__init__","title":"<code>__init__(client, model_name=None, model_revision=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def __init__(\n    self,\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.model_revision = model_revision\n    self.type_adapter = DottxtTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.Dottxt.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using Dottxt.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Dottxt.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n    json_schema = self.type_adapter.format_output_type(output_type)\n\n    if (\n        \"model_name\" not in inference_kwargs\n        and self.model_name is not None\n    ):\n        inference_kwargs[\"model_name\"] = self.model_name\n\n    if (\n        \"model_revision\" not in inference_kwargs\n        and self.model_revision is not None\n    ):\n        inference_kwargs[\"model_revision\"] = self.model_revision\n\n    completion = self.client.json(\n        prompt,\n        json_schema,\n        **inference_kwargs,\n    )\n    return completion.data\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.Dottxt.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Not available for Dottxt.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Not available for Dottxt.\"\"\"\n    raise NotImplementedError(\n        \"Dottxt does not support streaming. Call the model/generator for \"\n        + \"regular generation instead.\"\n    )\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.DottxtTypeAdapter","title":"<code>DottxtTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Dottxt</code> model.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>class DottxtTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Dottxt` model.\"\"\"\n\n    def format_input(self, model_input: str) -&gt; str:\n        \"\"\"Format the prompt to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The input to pass to the client.\n\n        \"\"\"\n        if isinstance(model_input, str):\n            return model_input\n        raise TypeError(\n            f\"The input type {model_input} is not available with Dottxt. \"\n            \"The only available type is `str`.\"\n        )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        str\n            The output type to pass to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if output_type is None:\n            raise TypeError(\n                \"You must provide an output type. Dottxt only supports \"\n                \"constrained generation.\"\n            )\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs will soon be available with \"\n                \"Dottxt. Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Dottxt. \"\n                \"Consider using a local mode instead.\"\n            )\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.DottxtTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Format the prompt to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_input(self, model_input: str) -&gt; str:\n    \"\"\"Format the prompt to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The input to pass to the client.\n\n    \"\"\"\n    if isinstance(model_input, str):\n        return model_input\n    raise TypeError(\n        f\"The input type {model_input} is not available with Dottxt. \"\n        \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.DottxtTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The output type to pass to the client.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; str:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    str\n        The output type to pass to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if output_type is None:\n        raise TypeError(\n            \"You must provide an output type. Dottxt only supports \"\n            \"constrained generation.\"\n        )\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs will soon be available with \"\n            \"Dottxt. Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(str, JsonSchema.convert_to(output_type, [\"str\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Dottxt. \"\n            \"Consider using a local mode instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/dottxt/#outlines.models.dottxt.from_dottxt","title":"<code>from_dottxt(client, model_name=None, model_revision=None)</code>","text":"<p>Create an Outlines <code>Dottxt</code> model instance from a <code>dottxt.Dottxt</code> client instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Dottxt</code> <p>A <code>dottxt.Dottxt</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>model_revision</code> <code>Optional[str]</code> <p>The revision of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dottxt</code> <p>An Outlines <code>Dottxt</code> model instance.</p> Source code in <code>outlines/models/dottxt.py</code> <pre><code>def from_dottxt(\n    client: \"DottxtClient\",\n    model_name: Optional[str] = None,\n    model_revision: Optional[str] = None,\n) -&gt; Dottxt:\n    \"\"\"Create an Outlines `Dottxt` model instance from a `dottxt.Dottxt`\n    client instance.\n\n    Parameters\n    ----------\n    client\n        A `dottxt.Dottxt` client instance.\n    model_name\n        The name of the model to use.\n    model_revision\n        The revision of the model to use.\n\n    Returns\n    -------\n    Dottxt\n        An Outlines `Dottxt` model instance.\n\n    \"\"\"\n    return Dottxt(client, model_name, model_revision)\n</code></pre>"},{"location":"api_reference/models/gemini/","title":"gemini","text":"<p>Integration with Gemini's API.</p>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.Gemini","title":"<code>Gemini</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>google.genai.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>google.genai.Client</code> client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class Gemini(Model):\n    \"\"\"Thin wrapper around the `google.genai.Client` client.\n\n    This wrapper is used to convert the input and output types specified by\n    the users at a higher level to arguments to the `google.genai.Client`\n    client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A `google.genai.Client` instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = GeminiTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The response generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        completion = self.client.models.generate_content(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs}\n        )\n\n        return completion.text\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"Gemini does not support batch generation.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema, a list of such types, or a multiple choice type.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        contents = self.type_adapter.format_input(model_input)\n        generation_config = self.type_adapter.format_output_type(output_type)\n\n        stream = self.client.models.generate_content_stream(\n            **contents,\n            model=inference_kwargs.pop(\"model\", self.model_name),\n            config={**generation_config, **inference_kwargs},\n        )\n\n        for chunk in stream:\n            if hasattr(chunk, \"text\") and chunk.text:\n                yield chunk.text\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.Gemini.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/gemini.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = GeminiTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.Gemini.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The response generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; str:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The response generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    completion = self.client.models.generate_content(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs}\n    )\n\n    return completion.text\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.Gemini.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema, a list of such types, or a multiple choice type.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema, a list of such types, or a multiple choice type.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    contents = self.type_adapter.format_input(model_input)\n    generation_config = self.type_adapter.format_output_type(output_type)\n\n    stream = self.client.models.generate_content_stream(\n        **contents,\n        model=inference_kwargs.pop(\"model\", self.model_name),\n        config={**generation_config, **inference_kwargs},\n    )\n\n    for chunk in stream:\n        if hasattr(chunk, \"text\") and chunk.text:\n            yield chunk.text\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.GeminiTypeAdapter","title":"<code>GeminiTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Gemini</code> model.</p> <p><code>GeminiTypeAdapter</code> is responsible for preparing the arguments to Gemini's client <code>models.generate_content</code> method: the input (prompt and possibly image), as well as the output type (either JSON or multiple choice).</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>class GeminiTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Gemini` model.\n\n    `GeminiTypeAdapter` is responsible for preparing the arguments to Gemini's\n    client `models.generate_content` method: the input (prompt and possibly\n    image), as well as the output type (either JSON or multiple choice).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `contents` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The `contents` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Gemini. The only available types are `str`, `list` and `Chat` \"\n            \"(containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; dict:\n        return {\"contents\": [self._create_text_part(model_input)]}\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; dict:\n        return {\n            \"contents\": [\n                self._create_message(\"user\", model_input)\n            ]\n        }\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; dict:\n        \"\"\"Generate the `contents` argument to pass to the client when the user\n        passes a Chat instance.\n\n        \"\"\"\n        return {\n            \"contents\": [\n                self._create_message(message[\"role\"], message[\"content\"])\n                for message in model_input.messages\n            ]\n        }\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        # Gemini uses \"model\" instead of \"assistant\"\n        if role == \"assistant\":\n            role = \"model\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"parts\": [self._create_text_part(content)],\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_part(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"parts\": [\n                    self._create_text_part(prompt),\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n        return {\"contents\": [prompt, *image_parts]}\n\n\n    def _create_text_part(self, text: str) -&gt; dict:\n        \"\"\"Create a text input part for a message.\"\"\"\n        return {\n            \"text\": text,\n        }\n\n    def _create_img_part(self, image: Image) -&gt; dict:\n        \"\"\"Create an image input part for a message.\"\"\"\n        return {\n            \"inline_data\": {\n                \"mime_type\": image.image_format,\n                \"data\": image.image_str,\n            }\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `generation_config` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `generation_config` argument to pass to the client.\n\n        \"\"\"\n\n        # Unsupported output pytes\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` \"\n                \"keyword in Json Schema are available with Gemini. Use an \"\n                \"open source model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Gemini. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                JsonSchema.convert_to(\n                    output_type,\n                    [\"dataclass\", \"typeddict\", \"pydantic\"]\n                )\n            )\n\n        # List of structured types\n        elif is_typing_list(output_type):\n            return self.format_list_output_type(output_type)\n\n        # Multiple choice types\n        elif is_enum(output_type):\n            return self.format_enum_output_type(output_type)\n        elif is_literal(output_type):\n            enum = get_enum_from_literal(output_type)\n            return self.format_enum_output_type(enum)\n        elif isinstance(output_type, Choice):\n            enum = get_enum_from_choice(output_type)\n            return self.format_enum_output_type(enum)\n\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Gemini. \"\n                \"Consider using a local model or dottxt instead.\"\n            )\n\n    def format_enum_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"text/x.enum\",\n            \"response_schema\": output_type,\n        }\n\n    def format_json_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        return {\n            \"response_mime_type\": \"application/json\",\n            \"response_schema\": output_type,\n        }\n\n    def format_list_output_type(self, output_type: Optional[Any]) -&gt; dict:\n        args = get_args(output_type)\n\n        if len(args) == 1:\n            item_type = args[0]\n\n            if JsonSchema.is_json_schema(item_type):\n                return {\n                    \"response_mime_type\": \"application/json\",\n                    \"response_schema\": list[  # type: ignore\n                        JsonSchema.convert_to(\n                            item_type,\n                            [\"dataclass\", \"typeddict\", \"pydantic\"]\n                        )\n                    ],\n                }\n            else:\n                raise TypeError(\n                    \"The list items output type must contain a JSON schema \"\n                    \"type.\"\n                )\n\n        raise TypeError(\n            f\"Gemini only supports homogeneous lists: \"\n            \"list[BaseModel], list[TypedDict] or list[dataclass]. \"\n            f\"Got {output_type} instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.GeminiTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; dict:\n    \"\"\"Generate the `contents` argument to pass to the client when the user\n    passes a Chat instance.\n\n    \"\"\"\n    return {\n        \"contents\": [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n    }\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.GeminiTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>contents</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The <code>contents</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `contents` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The `contents` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Gemini. The only available types are `str`, `list` and `Chat` \"\n        \"(containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.GeminiTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>generation_config</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>generation_config</code> argument to pass to the client.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `generation_config` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `generation_config` argument to pass to the client.\n\n    \"\"\"\n\n    # Unsupported output pytes\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` \"\n            \"keyword in Json Schema are available with Gemini. Use an \"\n            \"open source model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Gemini. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            JsonSchema.convert_to(\n                output_type,\n                [\"dataclass\", \"typeddict\", \"pydantic\"]\n            )\n        )\n\n    # List of structured types\n    elif is_typing_list(output_type):\n        return self.format_list_output_type(output_type)\n\n    # Multiple choice types\n    elif is_enum(output_type):\n        return self.format_enum_output_type(output_type)\n    elif is_literal(output_type):\n        enum = get_enum_from_literal(output_type)\n        return self.format_enum_output_type(enum)\n    elif isinstance(output_type, Choice):\n        enum = get_enum_from_choice(output_type)\n        return self.format_enum_output_type(enum)\n\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Gemini. \"\n            \"Consider using a local model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/gemini/#outlines.models.gemini.from_gemini","title":"<code>from_gemini(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Gemini</code> model instance from a <code>google.genai.Client</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A <code>google.genai.Client</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Gemini</code> <p>An Outlines <code>Gemini</code> model instance.</p> Source code in <code>outlines/models/gemini.py</code> <pre><code>def from_gemini(client: \"Client\", model_name: Optional[str] = None) -&gt; Gemini:\n    \"\"\"Create an Outlines `Gemini` model instance from a\n    `google.genai.Client` instance.\n\n    Parameters\n    ----------\n    client\n        A `google.genai.Client` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Gemini\n        An Outlines `Gemini` model instance.\n\n    \"\"\"\n    return Gemini(client, model_name)\n</code></pre>"},{"location":"api_reference/models/llamacpp/","title":"llamacpp","text":"<p>Integration with the <code>llama-cpp-python</code> library.</p>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>llama_cpp.Llama</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>llama_cpp.Llama</code> model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp(Model):\n    \"\"\"Thin wrapper around the `llama_cpp.Llama` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `llama_cpp.Llama` model.\n    \"\"\"\n\n    tensor_library_name = \"numpy\"\n\n    def __init__(self, model: \"Llama\"):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            A `llama_cpp.Llama` model instance.\n\n        \"\"\"\n        self.model = model\n        self.tokenizer = LlamaCppTokenizer(self.model)\n        self.type_adapter = LlamaCppTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            completion = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"text\"]\n        elif isinstance(prompt, list): # pragma: no cover\n            completion = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                **inference_kwargs,\n            )\n            result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n        self.model.reset()\n\n        return result\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"LlamaCpp does not support batch generation.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        **inference_kwargs\n            Additional keyword arguments to pass to the `Llama.__call__`\n            method of the `llama-cpp-python` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n\n        if isinstance(prompt, str):\n            generator = self.model(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"text\"]\n\n        elif isinstance(prompt, list): # pragma: no cover\n            generator = self.model.create_chat_completion(\n                prompt,\n                logits_processor=self.type_adapter.format_output_type(output_type),\n                stream=True,\n                **inference_kwargs,\n            )\n            for chunk in generator:\n                yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCpp.__init__","title":"<code>__init__(model)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> model instance.</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __init__(self, model: \"Llama\"):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` model instance.\n\n    \"\"\"\n    self.model = model\n    self.tokenizer = LlamaCppTokenizer(self.model)\n    self.type_adapter = LlamaCppTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCpp.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        completion = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"text\"]\n    elif isinstance(prompt, list): # pragma: no cover\n        completion = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            **inference_kwargs,\n        )\n        result = completion[\"choices\"][0][\"message\"][\"content\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCpp.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>Llama.__call__</code> method of the <code>llama-cpp-python</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    **inference_kwargs\n        Additional keyword arguments to pass to the `Llama.__call__`\n        method of the `llama-cpp-python` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    prompt = self.type_adapter.format_input(model_input)\n\n    if isinstance(prompt, str):\n        generator = self.model(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"text\"]\n\n    elif isinstance(prompt, list): # pragma: no cover\n        generator = self.model.create_chat_completion(\n            prompt,\n            logits_processor=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **inference_kwargs,\n        )\n        for chunk in generator:\n            yield chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCppTokenizer","title":"<code>LlamaCppTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTokenizer(Tokenizer):\n    def __init__(self, model: \"Llama\"):\n        self.tokenizer = model.tokenizer()\n        self.special_tokens: Set[str] = set()\n        self.vocabulary: Dict[str, int] = dict()\n\n        # TODO: Remove when https://github.com/ggerganov/llama.cpp/pull/5613\n        # is resolved\n        self._hf_tokenizer = None\n        if (\n            hasattr(model, \"tokenizer_\")\n            and hasattr(model.tokenizer_, \"hf_tokenizer\")\n        ):\n            self._hf_tokenizer = model.tokenizer_.hf_tokenizer\n            self.eos_token_id = self._hf_tokenizer.eos_token_id\n            self.eos_token = self._hf_tokenizer.eos_token\n            self.vocabulary = self._hf_tokenizer.get_vocab()\n        else:\n            from llama_cpp import (\n                llama_model_get_vocab,\n                llama_token_to_piece,\n            )\n\n            self.eos_token_id = model.token_eos()\n            size = 32\n            buffer = (ctypes.c_char * size)()\n            for i in range(model.n_vocab()):\n                n = llama_token_to_piece(\n                    llama_model_get_vocab(model.model),\n                    i,\n                    buffer,\n                    size,\n                    0,\n                    True\n                )\n                token_piece = buffer[:n].decode(\"utf-8\", errors=\"replace\") # type: ignore\n                self.vocabulary[token_piece] = i\n                if i == self.eos_token_id:\n                    self.eos_token = token_piece\n\n        self.pad_token_id = self.eos_token_id\n        # ensure stable ordering of vocabulary\n        self.vocabulary = {\n            tok: tok_id\n            for tok, tok_id\n            in sorted(self.vocabulary.items(), key=lambda x: x[1])\n        }\n        self._hash = None\n\n    def decode(self, token_ids: List[int]) -&gt; List[str]:\n        decoded_bytes = self.tokenizer.detokenize(token_ids)\n        return [decoded_bytes.decode(\"utf-8\", errors=\"ignore\")]\n\n    def encode(\n        self,\n        prompt: Union[str, List[str]],\n        add_bos: bool = True,\n        special: bool = True,\n    ) -&gt; Tuple[List[int], List[int]]:\n        if isinstance(prompt, list):\n            raise NotImplementedError(\n                \"llama-cpp-python tokenizer doesn't support batch tokenization\"\n            )\n        token_ids = self.tokenizer.tokenize(\n            prompt.encode(\"utf-8\", errors=\"ignore\"),\n            add_bos=add_bos,\n            special=special,\n        )\n        # generate attention mask, missing from llama-cpp-python\n        attention_mask = [\n            1 if token_id != self.pad_token_id else 0 for token_id in token_ids\n        ]\n        return token_ids, attention_mask\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        if self._hf_tokenizer is not None:\n            from transformers.file_utils import SPIECE_UNDERLINE\n\n            token_str = self._hf_tokenizer.convert_tokens_to_string([token])\n            if (\n                token.startswith(SPIECE_UNDERLINE)\n                or token == \"&lt;0x20&gt;\"\n            ):  # pragma: no cover\n                token_str = \" \" + token_str\n            return token_str\n        else:\n            return token\n\n    def __eq__(self, other):\n        if not isinstance(other, LlamaCppTokenizer):\n            return False\n        return self.__getstate__() == other.__getstate__()\n\n    def __hash__(self):\n        # We create a custom hash as pickle.dumps(self) is not stable\n        if self._hash is None:\n            self._hash = hash((\n                tuple(sorted(self.vocabulary.items())),\n                self.eos_token_id,\n                self.eos_token,\n                self.pad_token_id,\n                tuple(sorted(self.special_tokens)),\n            ))\n        return self._hash\n\n    def __getstate__(self):\n        \"\"\"Create a stable representation for outlines.caching\"\"\"\n        return (\n            self.vocabulary,\n            self.eos_token_id,\n            self.eos_token,\n            self.pad_token_id,\n            sorted(self.special_tokens),\n        )\n\n    def __setstate__(self, state):\n        raise NotImplementedError(\"Cannot load a pickled llamacpp tokenizer\")\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCppTokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a stable representation for outlines.caching</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Create a stable representation for outlines.caching\"\"\"\n    return (\n        self.vocabulary,\n        self.eos_token_id,\n        self.eos_token,\n        self.pad_token_id,\n        sorted(self.special_tokens),\n    )\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCppTypeAdapter","title":"<code>LlamaCppTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>LlamaCpp</code> model.</p> <p><code>LlamaCppTypeAdapter</code> is responsible for preparing the arguments to the <code>Llama</code> object text generation methods.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `LlamaCpp` model.\n\n    `LlamaCppTypeAdapter` is responsible for preparing the arguments to\n    the `Llama` object text generation methods.\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"LlamaCpp. The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; list:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"LlamaCpp does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return  [\n            {\n                \"role\": message[\"role\"],\n                \"content\": message[\"content\"],\n            }\n            for message in model_input.messages\n        ]\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        LogitsProcessorList\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from llama_cpp import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"LlamaCpp. The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.LlamaCppTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>LogitsProcessorList</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    LogitsProcessorList\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from llama_cpp import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/llamacpp/#outlines.models.llamacpp.from_llamacpp","title":"<code>from_llamacpp(model)</code>","text":"<p>Create an Outlines <code>LlamaCpp</code> model instance from a <code>llama_cpp.Llama</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>A <code>llama_cpp.Llama</code> instance.</p> required <p>Returns:</p> Type Description <code>LlamaCpp</code> <p>An Outlines <code>LlamaCpp</code> model instance.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def from_llamacpp(model: \"Llama\"):\n    \"\"\"Create an Outlines `LlamaCpp` model instance from a\n    `llama_cpp.Llama` instance.\n\n    Parameters\n    ----------\n    model\n        A `llama_cpp.Llama` instance.\n\n    Returns\n    -------\n    LlamaCpp\n        An Outlines `LlamaCpp` model instance.\n\n    \"\"\"\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api_reference/models/mistral/","title":"mistral","text":"<p>Integration with Mistral AI API.</p>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.AsyncMistral","title":"<code>AsyncMistral</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Async thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's async methods (<code>chat.complete_async</code> or <code>chat.stream_async</code>).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class AsyncMistral(AsyncModel):\n    \"\"\"Async thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's async methods (`chat.complete_async` or `chat.stream_async`).\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model asynchronously.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.complete_async(\n                messages=messages,\n                response_format=response_format,\n                stream=False,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The mistralai library does not support batch inference.\"\n        )\n\n    async def generate_stream(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        \"\"\"Generate text from the model as an async stream of chunks.\n\n        Parameters\n        ----------\n        model_input\n            str, list, or chat input to generate from.\n        output_type\n            Optional type for structured output.\n        **inference_kwargs\n            Extra kwargs like \"model\" name.\n\n        Yields\n        ------\n        str\n            Chunks of text as they are streamed.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            response = await self.client.chat.stream_async(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        async for chunk in response:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and len(chunk.data.choices) &gt; 0\n                and hasattr(chunk.data.choices[0], \"delta\")\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.AsyncMistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.AsyncMistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate a response from the model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model asynchronously.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.complete_async(\n            messages=messages,\n            response_format=response_format,\n            stream=False,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.AsyncMistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text from the model as an async stream of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>str, list, or chat input to generate from.</p> required <code>output_type</code> <p>Optional type for structured output.</p> <code>None</code> <code>**inference_kwargs</code> <p>Extra kwargs like \"model\" name.</p> <code>{}</code> <p>Yields:</p> Type Description <code>str</code> <p>Chunks of text as they are streamed.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>async def generate_stream(\n    self,\n    model_input,\n    output_type=None,\n    **inference_kwargs,\n):\n    \"\"\"Generate text from the model as an async stream of chunks.\n\n    Parameters\n    ----------\n    model_input\n        str, list, or chat input to generate from.\n    output_type\n        Optional type for structured output.\n    **inference_kwargs\n        Extra kwargs like \"model\" name.\n\n    Yields\n    ------\n    str\n        Chunks of text as they are streamed.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        response = await self.client.chat.stream_async(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    async for chunk in response:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and len(chunk.data.choices) &gt; 0\n            and hasattr(chunk.data.choices[0], \"delta\")\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.Mistral","title":"<code>Mistral</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>mistralai.Mistral</code> client.</p> <p>Converts input and output types to arguments for the <code>mistralai.Mistral</code> client's <code>chat.complete</code> or <code>chat.stream</code> methods.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class Mistral(Model):\n    \"\"\"Thin wrapper around the `mistralai.Mistral` client.\n\n    Converts input and output types to arguments for the `mistralai.Mistral`\n    client's `chat.complete` or `chat.stream` methods.\n\n    \"\"\"\n\n    def __init__(\n        self, client: \"MistralClient\", model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client : MistralClient\n            A mistralai.Mistral client instance.\n        model_name : Optional[str]\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = MistralTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate a response from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs : Any\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The response generated by the model as text.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.complete(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs,\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        outputs = [choice.message for choice in result.choices]\n\n        if len(outputs) == 1:\n            return outputs[0].content\n        else:\n            return [m.content for m in outputs]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type=None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `mistralai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate a stream of responses from the model.\n\n        Parameters\n        ----------\n        model_input : Union[Chat, list, str]\n            The prompt or chat messages to generate a response from.\n        output_type : Optional[Any]\n            The desired format of the response (e.g., JSON schema).\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text chunks generated by the model.\n\n        \"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.stream(\n                messages=messages,\n                response_format=response_format,\n                **inference_kwargs\n            )\n        except Exception as e:\n            if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n                raise TypeError(\n                    f\"Mistral does not support your schema: {e}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n        for chunk in stream:\n            if (\n                hasattr(chunk, \"data\")\n                and chunk.data.choices\n                and chunk.data.choices[0].delta.content is not None\n            ):\n                yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.Mistral.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/mistral.py</code> <pre><code>def __init__(\n    self, client: \"MistralClient\", model_name: Optional[str] = None\n):\n    \"\"\"\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = MistralTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.Mistral.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The response generated by the model as text.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate a response from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs : Any\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The response generated by the model as text.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.complete(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs,\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    outputs = [choice.message for choice in result.choices]\n\n    if len(outputs) == 1:\n        return outputs[0].content\n    else:\n        return [m.content for m in outputs]\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.Mistral.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a stream of responses from the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt or chat messages to generate a response from.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response (e.g., JSON schema).</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text chunks generated by the model.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Generate a stream of responses from the model.\n\n    Parameters\n    ----------\n    model_input : Union[Chat, list, str]\n        The prompt or chat messages to generate a response from.\n    output_type : Optional[Any]\n        The desired format of the response (e.g., JSON schema).\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text chunks generated by the model.\n\n    \"\"\"\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.stream(\n            messages=messages,\n            response_format=response_format,\n            **inference_kwargs\n        )\n    except Exception as e:\n        if \"schema\" in str(e).lower() or \"json_schema\" in str(e).lower():\n            raise TypeError(\n                f\"Mistral does not support your schema: {e}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise RuntimeError(f\"Mistral API error: {e}\") from e\n\n    for chunk in stream:\n        if (\n            hasattr(chunk, \"data\")\n            and chunk.data.choices\n            and chunk.data.choices[0].delta.content is not None\n        ):\n            yield chunk.data.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter","title":"<code>MistralTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Mistral</code> model.</p> <p>Prepares arguments for Mistral's client <code>chat.complete</code>, <code>chat.complete_async</code>, or <code>chat.stream</code> methods. Handles input (prompt or chat messages) and output type (JSON schema types).</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>class MistralTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Mistral` model.\n\n    Prepares arguments for Mistral's client `chat.complete`,\n    `chat.complete_async`, or `chat.stream` methods. Handles input (prompt or\n    chat messages) and output type (JSON schema types).\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The `messages` argument to pass to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Format a string input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : str\n            The input string prompt.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=model_input)]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Format a list input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : list\n            The input list, containing a string prompt and optionally Image\n            objects (vision models only).\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage\n\n        return [UserMessage(content=self._create_message_content(model_input))]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a Chat input into a list of messages.\n\n        Parameters\n        ----------\n        model_input : Chat\n            The Chat object containing a list of message dictionaries.\n\n        Returns\n        -------\n        list\n            A list of Mistral message objects.\n\n        \"\"\"\n        from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n        messages = []\n\n        for message in model_input.messages:\n            role = message[\"role\"]\n            content = message[\"content\"]\n            if role == \"user\":\n                messages.append(\n                    UserMessage(content=self._create_message_content(content))\n                )\n            elif role == \"assistant\":\n                messages.append(AssistantMessage(content=content))\n            elif role == \"system\":\n                messages.append(SystemMessage(content=content))\n            else:\n                raise ValueError(f\"Unsupported role: {role}\")\n\n        return messages\n\n    def _create_message_content(\n        self, content: Union[str, list]\n    ) -&gt; Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]:\n        \"\"\"Create message content from an input.\n\n        Parameters\n        ----------\n        content : Union[str, list]\n            The content to format, either a string or a list containing a\n            string and optionally Image objects.\n\n        Returns\n        -------\n        Union[str, List[Dict[str, Union[str, Dict[str, str]]]]]\n            The formatted content, either a string or a list of content parts\n            (text and image URLs).\n\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list):\n            if not content:\n                raise ValueError(\"Content list cannot be empty.\")\n            if not isinstance(content[0], str):\n                raise ValueError(\n                    \"The first item in the list should be a string.\"\n                )\n            if len(content) == 1:\n                return content[0]\n            content_parts: List[Dict[str, Union[str, Dict[str, str]]]] = [\n                {\"type\": \"text\", \"text\": content[0]}\n            ]\n            for item in content[1:]:\n                if isinstance(item, Image):\n                    data_url = f\"data:{item.image_format};base64,{item.image_str}\"\n                    content_parts.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": data_url}\n                    })\n                else:\n                    raise ValueError(\n                        f\"Invalid item type in content list: {type(item)}. \"\n                        + \"Expected Image objects after the first string.\"\n                    )\n            return content_parts\n        else:\n            raise TypeError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"Content must be a string or a list starting with a string \"\n                + \"followed by optional Image objects.\"\n            )\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type : Optional[Any]\n            The desired output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The `response_format` dict to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        # JSON schema types\n        elif is_pydantic_model(output_type):\n            schema = output_type.model_json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_dataclass(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_typed_dict(output_type):\n            schema = TypeAdapter(output_type).json_schema()\n            return self.format_json_schema_type(schema, output_type.__name__)\n        elif is_genson_schema_builder(output_type):\n            schema = json.loads(output_type.to_json())\n            return self.format_json_schema_type(schema)\n        elif isinstance(output_type, JsonSchema):\n            return self.format_json_schema_type(json.loads(output_type.schema))\n\n        # Json mode\n        elif is_native_dict(output_type):\n            return {\"type\": \"json_object\"}\n\n        # Unsupported types\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not available with \"\n                \"Mistral.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with Mistral.\"\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", str(output_type))\n            raise TypeError(\n                f\"The type {type_name} is not available with Mistral.\"\n            )\n\n    def format_json_schema_type(\n        self, schema: dict, schema_name: str = \"default\"\n    ) -&gt; dict:\n        \"\"\"Create the `response_format` argument to pass to the client from a\n        JSON schema dictionary.\n\n        Parameters\n        ----------\n        schema : dict\n            The JSON schema to format.\n        schema_name : str\n            The name of the schema.\n\n        Returns\n        -------\n        dict\n            The value of the `response_format` argument to pass to the client.\n\n        \"\"\"\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"schema\": schema,\n                \"name\": schema_name.lower(),\n                \"strict\": True\n            }\n        }\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Format a Chat input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat</code> <p>The Chat object containing a list of message dictionaries.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a Chat input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : Chat\n        The Chat object containing a list of message dictionaries.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage, AssistantMessage, SystemMessage\n\n    messages = []\n\n    for message in model_input.messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        if role == \"user\":\n            messages.append(\n                UserMessage(content=self._create_message_content(content))\n            )\n        elif role == \"assistant\":\n            messages.append(AssistantMessage(content=content))\n        elif role == \"system\":\n            messages.append(SystemMessage(content=content))\n        else:\n            raise ValueError(f\"Unsupported role: {role}\")\n\n    return messages\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The <code>messages</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The `messages` argument to pass to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Mistral. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_json_schema_type","title":"<code>format_json_schema_type(schema, schema_name='default')</code>","text":"<p>Create the <code>response_format</code> argument to pass to the client from a JSON schema dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to format.</p> required <code>schema_name</code> <code>str</code> <p>The name of the schema.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The value of the <code>response_format</code> argument to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_json_schema_type(\n    self, schema: dict, schema_name: str = \"default\"\n) -&gt; dict:\n    \"\"\"Create the `response_format` argument to pass to the client from a\n    JSON schema dictionary.\n\n    Parameters\n    ----------\n    schema : dict\n        The JSON schema to format.\n    schema_name : str\n        The name of the schema.\n\n    Returns\n    -------\n    dict\n        The value of the `response_format` argument to pass to the client.\n\n    \"\"\"\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"schema\": schema,\n            \"name\": schema_name.lower(),\n            \"strict\": True\n        }\n    }\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Format a list input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list</code> <p>The input list, containing a string prompt and optionally Image objects (vision models only).</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Format a list input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : list\n        The input list, containing a string prompt and optionally Image\n        objects (vision models only).\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=self._create_message_content(model_input))]\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The desired output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The <code>response_format</code> dict to pass to the client.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type : Optional[Any]\n        The desired output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The `response_format` dict to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    # JSON schema types\n    elif is_pydantic_model(output_type):\n        schema = output_type.model_json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_dataclass(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_typed_dict(output_type):\n        schema = TypeAdapter(output_type).json_schema()\n        return self.format_json_schema_type(schema, output_type.__name__)\n    elif is_genson_schema_builder(output_type):\n        schema = json.loads(output_type.to_json())\n        return self.format_json_schema_type(schema)\n    elif isinstance(output_type, JsonSchema):\n        return self.format_json_schema_type(json.loads(output_type.schema))\n\n    # Json mode\n    elif is_native_dict(output_type):\n        return {\"type\": \"json_object\"}\n\n    # Unsupported types\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not available with \"\n            \"Mistral.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with Mistral.\"\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", str(output_type))\n        raise TypeError(\n            f\"The type {type_name} is not available with Mistral.\"\n        )\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.MistralTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Format a string input into a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The input string prompt.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of Mistral message objects.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Format a string input into a list of messages.\n\n    Parameters\n    ----------\n    model_input : str\n        The input string prompt.\n\n    Returns\n    -------\n    list\n        A list of Mistral message objects.\n\n    \"\"\"\n    from mistralai import UserMessage\n\n    return [UserMessage(content=model_input)]\n</code></pre>"},{"location":"api_reference/models/mistral/#outlines.models.mistral.from_mistral","title":"<code>from_mistral(client, model_name=None, async_client=False)</code>","text":"<p>Create an Outlines Mistral model instance from a mistralai.Mistral client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Mistral</code> <p>A mistralai.Mistral client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <code>async_client</code> <code>bool</code> <p>If True, return an AsyncMistral instance; otherwise, return a Mistral instance.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Mistral, AsyncMistral]</code> <p>An Outlines Mistral or AsyncMistral model instance.</p> Source code in <code>outlines/models/mistral.py</code> <pre><code>def from_mistral(\n    client: \"MistralClient\",\n    model_name: Optional[str] = None,\n    async_client: bool = False,\n) -&gt; Union[Mistral, AsyncMistral]:\n    \"\"\"Create an Outlines Mistral model instance from a mistralai.Mistral\n    client.\n\n    Parameters\n    ----------\n    client : MistralClient\n        A mistralai.Mistral client instance.\n    model_name : Optional[str]\n        The name of the model to use.\n    async_client : bool\n        If True, return an AsyncMistral instance;\n        otherwise, return a Mistral instance.\n\n    Returns\n    -------\n    Union[Mistral, AsyncMistral]\n        An Outlines Mistral or AsyncMistral model instance.\n\n    \"\"\"\n    from mistralai import Mistral as MistralClient\n\n    if not isinstance(client, MistralClient):\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"`mistralai.Mistral`.\"\n        )\n\n    if async_client:\n        return AsyncMistral(client, model_name)\n    else:\n        return Mistral(client, model_name)\n</code></pre>"},{"location":"api_reference/models/mlxlm/","title":"mlxlm","text":"<p>Integration with the <code>mlx_lm</code> library.</p>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLM","title":"<code>MLXLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around an <code>mlx_lm</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>mlx_lm</code> library.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM(Model):\n    \"\"\"Thin wrapper around an `mlx_lm` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `mlx_lm` library.\n\n    \"\"\"\n\n    tensor_library_name = \"mlx\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        model\n            An instance of an `mlx_lm` model.\n        tokenizer\n            An instance of an `mlx_lm` tokenizer or of a compatible\n            `transformers` tokenizer.\n\n        \"\"\"\n        self.model = model\n        # self.mlx_tokenizer is used by the mlx-lm in its generate function\n        self.mlx_tokenizer = tokenizer\n        # self.tokenizer is used by the logits processor\n        self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n        self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        from mlx_lm import generate\n\n        return generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n\n    def generate_batch(\n        self,\n        model_input: list[str],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate a batch of text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The list of prompts based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        list[str]\n            The list of text generated by the model.\n\n        \"\"\"\n        from mlx_lm import batch_generate\n\n        if output_type:\n            raise NotImplementedError(\n                \"mlx-lm does not support constrained generation with batching.\"\n                + \"You cannot provide an `output_type` with this method.\"\n            )\n\n        model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n        # Contrarily to the other generate methods, batch_generate requires\n        # tokenized prompts\n        add_special_tokens = [\n            (\n                self.mlx_tokenizer.bos_token is None\n                or not prompt.startswith(self.mlx_tokenizer.bos_token)\n            )\n            for prompt in model_input\n        ]\n        tokenized_model_input = [\n            self.mlx_tokenizer.encode(\n                model_input[i], add_special_tokens=add_special_tokens[i]\n            )\n            for i in range(len(model_input))\n        ]\n\n        response = batch_generate(\n            self.model,\n            self.mlx_tokenizer,\n            tokenized_model_input,\n            **kwargs,\n        )\n\n        return response.texts\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `mlx-lm`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        kwargs\n            Additional keyword arguments to pass to the `mlx-lm` library.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        from mlx_lm import stream_generate\n\n        for gen_response in stream_generate(\n            self.model,\n            self.mlx_tokenizer,\n            self.type_adapter.format_input(model_input),\n            logits_processors=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        ):\n            yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLM.__init__","title":"<code>__init__(model, tokenizer)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible <code>transformers</code> tokenizer.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def __init__(\n    self,\n    model: \"nn.Module\",\n    tokenizer: \"PreTrainedTokenizer\",\n):\n    \"\"\"\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        `transformers` tokenizer.\n\n    \"\"\"\n    self.model = model\n    # self.mlx_tokenizer is used by the mlx-lm in its generate function\n    self.mlx_tokenizer = tokenizer\n    # self.tokenizer is used by the logits processor\n    self.tokenizer = TransformerTokenizer(tokenizer._tokenizer)\n    self.type_adapter = MLXLMTypeAdapter(tokenizer=tokenizer)\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLM.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    from mlx_lm import generate\n\n    return generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLM.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate a batch of text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>list[str]</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: list[str],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate a batch of text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The list of prompts based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    list[str]\n        The list of text generated by the model.\n\n    \"\"\"\n    from mlx_lm import batch_generate\n\n    if output_type:\n        raise NotImplementedError(\n            \"mlx-lm does not support constrained generation with batching.\"\n            + \"You cannot provide an `output_type` with this method.\"\n        )\n\n    model_input = [self.type_adapter.format_input(item) for item in model_input]\n\n    # Contrarily to the other generate methods, batch_generate requires\n    # tokenized prompts\n    add_special_tokens = [\n        (\n            self.mlx_tokenizer.bos_token is None\n            or not prompt.startswith(self.mlx_tokenizer.bos_token)\n        )\n        for prompt in model_input\n    ]\n    tokenized_model_input = [\n        self.mlx_tokenizer.encode(\n            model_input[i], add_special_tokens=add_special_tokens[i]\n        )\n        for i in range(len(model_input))\n    ]\n\n    response = batch_generate(\n        self.model,\n        self.mlx_tokenizer,\n        tokenized_model_input,\n        **kwargs,\n    )\n\n    return response.texts\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using <code>mlx-lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>mlx-lm</code> library.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `mlx-lm`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    kwargs\n        Additional keyword arguments to pass to the `mlx-lm` library.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    from mlx_lm import stream_generate\n\n    for gen_response in stream_generate(\n        self.model,\n        self.mlx_tokenizer,\n        self.type_adapter.format_input(model_input),\n        logits_processors=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    ):\n        yield gen_response.text\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLMTypeAdapter","title":"<code>MLXLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>MLXLM</code> model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `MLXLM` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"mlx-lm. The available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str):\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        if not all(\n            isinstance(message[\"content\"], str)\n            for message in model_input.messages\n        ):\n            raise ValueError(\n                \"mlx-lm does not support multi-modal messages.\"\n                + \"The content of each message must be a string.\"\n            )\n\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self, output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[list[OutlinesLogitsProcessor]]\n            The logits processor argument to be passed to the model.\n\n        \"\"\"\n        if not output_type:\n            return None\n        return [output_type]\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"mlx-lm. The available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.MLXLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[list[OutlinesLogitsProcessor]]</code> <p>The logits processor argument to be passed to the model.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[List[OutlinesLogitsProcessor]]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[list[OutlinesLogitsProcessor]]\n        The logits processor argument to be passed to the model.\n\n    \"\"\"\n    if not output_type:\n        return None\n    return [output_type]\n</code></pre>"},{"location":"api_reference/models/mlxlm/#outlines.models.mlxlm.from_mlxlm","title":"<code>from_mlxlm(model, tokenizer)</code>","text":"<p>Create an Outlines <code>MLXLM</code> model instance from an <code>mlx_lm</code> model and a tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>An instance of an <code>mlx_lm</code> model.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>An instance of an <code>mlx_lm</code> tokenizer or of a compatible transformers tokenizer.</p> required <p>Returns:</p> Type Description <code>MLXLM</code> <p>An Outlines <code>MLXLM</code> model instance.</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def from_mlxlm(model: \"nn.Module\", tokenizer: \"PreTrainedTokenizer\") -&gt; MLXLM:\n    \"\"\"Create an Outlines `MLXLM` model instance from an `mlx_lm` model and a\n    tokenizer.\n\n    Parameters\n    ----------\n    model\n        An instance of an `mlx_lm` model.\n    tokenizer\n        An instance of an `mlx_lm` tokenizer or of a compatible\n        transformers tokenizer.\n\n    Returns\n    -------\n    MLXLM\n        An Outlines `MLXLM` model instance.\n\n    \"\"\"\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api_reference/models/ollama/","title":"ollama","text":"<p>Integration with the <code>ollama</code> library.</p>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.AsyncOllama","title":"<code>AsyncOllama</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>ollama.AsyncClient</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.AsyncClient</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class AsyncOllama(AsyncModel):\n    \"\"\"Thin wrapper around the `ollama.AsyncClient` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.AsyncClient` client.\n\n    \"\"\"\n\n    def __init__(\n        self,client: \"AsyncClient\", model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    async def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        stream = await self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.AsyncOllama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(\n    self,client: \"AsyncClient\", model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.AsyncOllama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.AsyncOllama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    stream = await self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>ollama.Client</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>ollama.Client</code> client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class Ollama(Model):\n    \"\"\"Thin wrapper around the `ollama.Client` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `ollama.Client` client.\n\n    \"\"\"\n\n    def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `ollama.Client` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OllamaTypeAdapter()\n\n    def generate(self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            **kwargs,\n        )\n        return response.message.content\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `ollama` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Chat | str | list,\n        output_type: Optional[Any] = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using Ollama.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema.\n        **kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        if \"model\" not in kwargs and self.model_name is not None:\n            kwargs[\"model\"] = self.model_name\n\n        response = self.client.chat(\n            messages=self.type_adapter.format_input(model_input),\n            format=self.type_adapter.format_output_type(output_type),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in response:\n            yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.Ollama.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The <code>ollama.Client</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/ollama.py</code> <pre><code>def __init__(self, client: \"Client\", model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `ollama.Client` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OllamaTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.Ollama.generate","title":"<code>generate(model_input, output_type=None, **kwargs)</code>","text":"<p>Generate text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate(self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        **kwargs,\n    )\n    return response.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.Ollama.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **kwargs)</code>","text":"<p>Stream text using Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Chat | str | list</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Chat | str | list,\n    output_type: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using Ollama.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema.\n    **kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    if \"model\" not in kwargs and self.model_name is not None:\n        kwargs[\"model\"] = self.model_name\n\n    response = self.client.chat(\n        messages=self.type_adapter.format_input(model_input),\n        format=self.type_adapter.format_output_type(output_type),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in response:\n        yield chunk.message.content\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter","title":"<code>OllamaTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Ollama</code> model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>class OllamaTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Ollama` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        list\n            The formatted value of the `messages` argument to be passed to\n            the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            return {\n                \"role\": role,\n                \"content\": prompt,\n                \"image\": [image.image_str for image in images],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def format_output_type(\n        self, output_type: Optional[Any] = None\n    ) -&gt; Optional[dict]:\n        \"\"\"Format the output type to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        Optional[str]\n            The formatted output type to be passed to the model.\n\n        \"\"\"\n        if output_type is None:\n            return None\n        elif isinstance(output_type, Regex):\n            raise TypeError(\n                \"Regex-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not supported by Ollama. \"\n                \"Use an open source model in the meantime.\"\n            )\n        elif JsonSchema.is_json_schema(output_type):\n            return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not supported by Ollama. \"\n                \"Consider using a local model instead.\"\n            )\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted value of the <code>messages</code> argument to be passed to the client.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the value of the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    list\n        The formatted value of the `messages` argument to be passed to\n        the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"Ollama. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Format the output type to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted output type to be passed to the model.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def format_output_type(\n    self, output_type: Optional[Any] = None\n) -&gt; Optional[dict]:\n    \"\"\"Format the output type to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    Optional[str]\n        The formatted output type to be passed to the model.\n\n    \"\"\"\n    if output_type is None:\n        return None\n    elif isinstance(output_type, Regex):\n        raise TypeError(\n            \"Regex-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not supported by Ollama. \"\n            \"Use an open source model in the meantime.\"\n        )\n    elif JsonSchema.is_json_schema(output_type):\n        return cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not supported by Ollama. \"\n            \"Consider using a local model instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.OllamaTypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/ollama/#outlines.models.ollama.from_ollama","title":"<code>from_ollama(client, model_name=None)</code>","text":"<p>Create an Outlines <code>Ollama</code> model instance from an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[Client, AsyncClient]</code> <p>A <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Ollama, AsyncOllama]</code> <p>An Outlines <code>Ollama</code> or <code>AsyncOllama</code> model instance.</p> Source code in <code>outlines/models/ollama.py</code> <pre><code>def from_ollama(\n    client: Union[\"Client\", \"AsyncClient\"], model_name: Optional[str] = None\n) -&gt; Union[Ollama, AsyncOllama]:\n    \"\"\"Create an Outlines `Ollama` model instance from an `ollama.Client`\n    or `ollama.AsyncClient` instance.\n\n    Parameters\n    ----------\n    client\n        A `ollama.Client` or `ollama.AsyncClient` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[Ollama, AsyncOllama]\n        An Outlines `Ollama` or `AsyncOllama` model instance.\n\n    \"\"\"\n    from ollama import AsyncClient, Client\n\n    if isinstance(client, Client):\n        return Ollama(client, model_name)\n    elif isinstance(client, AsyncClient):\n        return AsyncOllama(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type, the client must be an instance of \"\n            \"`ollama.Client` or `ollama.AsyncClient`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/openai/","title":"openai","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api_reference/models/openai/#outlines.models.openai.AsyncOpenAI","title":"<code>AsyncOpenAI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin wrapper around the <code>openai.AsyncOpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.AsyncOpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class AsyncOpenAI(AsyncModel):\n    \"\"\"Thin wrapper around the `openai.AsyncOpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.AsyncOpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = await self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = await self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        async for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.AsyncOpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[AsyncOpenAI, AsyncAzureOpenAI]</code> <p>The <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"AsyncOpenAIClient\", \"AsyncAzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.AsyncOpenAI` or `openai.AsyncAzureOpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.AsyncOpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = await self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.AsyncOpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = await self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    async for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            The `openai.OpenAI` client.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = OpenAITypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            result = self.client.chat.completions.create(\n                messages=messages,\n                **response_format,\n                **inference_kwargs,\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        messages = [choice.message for choice in result.choices]\n        for message in messages:\n            if message.refusal is not None:\n                raise ValueError(\n                    f\"OpenAI refused to answer the request: {message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"The `openai` library does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Union[type[BaseModel], str]] = None,\n        **inference_kwargs,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. The\n            output type must be of a type that can be converted to a JSON\n            schema or an empty dictionary.\n        **inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        import openai\n\n        messages = self.type_adapter.format_input(model_input)\n        response_format = self.type_adapter.format_output_type(output_type)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        try:\n            stream = self.client.chat.completions.create(\n                stream=True,\n                messages=messages,\n                **response_format,\n                **inference_kwargs\n            )\n        except openai.BadRequestError as e:\n            if e.body[\"message\"].startswith(\"Invalid schema\"):\n                raise TypeError(\n                    f\"OpenAI does not support your schema: {e.body['message']}. \"\n                    \"Try a local model or dottxt instead.\"\n                )\n            else:\n                raise e\n\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AzureOpenAI]</code> <p>The <code>openai.OpenAI</code> client.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client: Union[\"OpenAIClient\", \"AzureOpenAIClient\"],\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        The `openai.OpenAI` client.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = OpenAITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        result = self.client.chat.completions.create(\n            messages=messages,\n            **response_format,\n            **inference_kwargs,\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    messages = [choice.message for choice in result.choices]\n    for message in messages:\n        if message.refusal is not None:\n            raise ValueError(\n                f\"OpenAI refused to answer the request: {message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Union[type[BaseModel], str]]</code> <p>The desired format of the response generated by the model. The output type must be of a type that can be converted to a JSON schema or an empty dictionary.</p> <code>None</code> <code>**inference_kwargs</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Union[type[BaseModel], str]] = None,\n    **inference_kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. The\n        output type must be of a type that can be converted to a JSON\n        schema or an empty dictionary.\n    **inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    import openai\n\n    messages = self.type_adapter.format_input(model_input)\n    response_format = self.type_adapter.format_output_type(output_type)\n\n    if \"model\" not in inference_kwargs and self.model_name is not None:\n        inference_kwargs[\"model\"] = self.model_name\n\n    try:\n        stream = self.client.chat.completions.create(\n            stream=True,\n            messages=messages,\n            **response_format,\n            **inference_kwargs\n        )\n    except openai.BadRequestError as e:\n        if e.body[\"message\"].startswith(\"Invalid schema\"):\n            raise TypeError(\n                f\"OpenAI does not support your schema: {e.body['message']}. \"\n                \"Try a local model or dottxt instead.\"\n            )\n        else:\n            raise e\n\n    for chunk in stream:\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter","title":"<code>OpenAITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>OpenAI</code> model.</p> <p><code>OpenAITypeAdapter</code> is responsible for preparing the arguments to OpenAI's <code>completions.create</code> methods: the input (prompt and possibly image), as well as the output type (only JSON).</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `OpenAI` model.\n\n    `OpenAITypeAdapter` is responsible for preparing the arguments to OpenAI's\n    `completions.create` methods: the input (prompt and possibly image), as\n    well as the output type (only JSON).\n\n    \"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the `messages` argument to pass to the client.\n\n        Parameters\n        ----------\n        model_input\n            The input provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_model_input(self, model_input: str) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user only passes a prompt.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(list)\n    def format_list_model_input(self, model_input: list) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a prompt and images.\n\n        \"\"\"\n        return [\n            self._create_message(\"user\", model_input)\n        ]\n\n    @format_input.register(Chat)\n    def format_chat_model_input(self, model_input: Chat) -&gt; list:\n        \"\"\"Generate the value of the `messages` argument to pass to the\n        client when the user passes a Chat instance.\n\n        \"\"\"\n        return [\n            self._create_message(message[\"role\"], message[\"content\"])\n            for message in model_input.messages\n        ]\n\n    def _create_message(self, role: str, content: str | list) -&gt; dict:\n        \"\"\"Create a message.\"\"\"\n\n        if isinstance(content, str):\n            return {\n                \"role\": role,\n                \"content\": content,\n            }\n\n        elif isinstance(content, list):\n            prompt = content[0]\n            images = content[1:]\n\n            if not all(isinstance(image, Image) for image in images):\n                raise ValueError(\"All assets provided must be of type Image\")\n\n            image_parts = [\n                self._create_img_content(image)\n                for image in images\n            ]\n\n            return {\n                \"role\": role,\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *image_parts,\n                ],\n            }\n\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                \"The content must be a string or a list containing a string \"\n                \"and a list of images.\"\n            )\n\n    def _create_img_content(self, image: Image) -&gt; dict:\n        \"\"\"Create the content for an image input.\"\"\"\n        return {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:{image.image_format};base64,{image.image_str}\"  # noqa: E702\n            },\n        }\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client based on the\n        output type specified by the user.\n\n        Parameters\n        ----------\n        output_type\n            The output type provided by the user.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        # Unsupported languages\n        if isinstance(output_type, Regex):\n            raise TypeError(\n                \"Neither regex-based structured outputs nor the `pattern` keyword \"\n                \"in Json Schema are available with OpenAI. Use an open source \"\n                \"model or dottxt instead.\"\n            )\n        elif isinstance(output_type, CFG):\n            raise TypeError(\n                \"CFG-based structured outputs are not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n        if output_type is None:\n            return {}\n        elif is_native_dict(output_type):\n            return self.format_json_mode_type()\n        elif JsonSchema.is_json_schema(output_type):\n            return self.format_json_output_type(\n                cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n            )\n        else:\n            type_name = getattr(output_type, \"__name__\", output_type)\n            raise TypeError(\n                f\"The type `{type_name}` is not available with OpenAI. \"\n                \"Use an open source model or dottxt instead.\"\n            )\n\n    def format_json_output_type(self, schema: dict) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified a `Json` output type.\n\n        \"\"\"\n        # OpenAI requires `additionalProperties` to be set to False\n        schema = set_additional_properties_false_json_schema(schema)\n\n        return {\n            \"response_format\": {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"name\": \"default\",\n                    \"strict\": True,\n                    \"schema\": schema,\n                },\n            }\n        }\n\n    def format_json_mode_type(self) -&gt; dict:\n        \"\"\"Generate the `response_format` argument to the client when the user\n        specified the output type should be a JSON but without specifying the\n        schema (also called \"JSON mode\").\n\n        \"\"\"\n        return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_chat_model_input","title":"<code>format_chat_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a Chat instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(Chat)\ndef format_chat_model_input(self, model_input: Chat) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a Chat instance.\n\n    \"\"\"\n    return [\n        self._create_message(message[\"role\"], message[\"content\"])\n        for message in model_input.messages\n    ]\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the <code>messages</code> argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input provided by the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the `messages` argument to pass to the client.\n\n    Parameters\n    ----------\n    model_input\n        The input provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"OpenAI. The only available types are `str`, `list` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_json_mode_type","title":"<code>format_json_mode_type()</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified the output type should be a JSON but without specifying the schema (also called \"JSON mode\").</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_mode_type(self) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified the output type should be a JSON but without specifying the\n    schema (also called \"JSON mode\").\n\n    \"\"\"\n    return {\"response_format\": {\"type\": \"json_object\"}}\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_json_output_type","title":"<code>format_json_output_type(schema)</code>","text":"<p>Generate the <code>response_format</code> argument to the client when the user specified a <code>Json</code> output type.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_json_output_type(self, schema: dict) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client when the user\n    specified a `Json` output type.\n\n    \"\"\"\n    # OpenAI requires `additionalProperties` to be set to False\n    schema = set_additional_properties_false_json_schema(schema)\n\n    return {\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"default\",\n                \"strict\": True,\n                \"schema\": schema,\n            },\n        }\n    }\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_list_model_input","title":"<code>format_list_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user passes a prompt and images.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(list)\ndef format_list_model_input(self, model_input: list) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user passes a prompt and images.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the <code>response_format</code> argument to the client based on the output type specified by the user.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The output type provided by the user.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the `response_format` argument to the client based on the\n    output type specified by the user.\n\n    Parameters\n    ----------\n    output_type\n        The output type provided by the user.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    # Unsupported languages\n    if isinstance(output_type, Regex):\n        raise TypeError(\n            \"Neither regex-based structured outputs nor the `pattern` keyword \"\n            \"in Json Schema are available with OpenAI. Use an open source \"\n            \"model or dottxt instead.\"\n        )\n    elif isinstance(output_type, CFG):\n        raise TypeError(\n            \"CFG-based structured outputs are not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n\n    if output_type is None:\n        return {}\n    elif is_native_dict(output_type):\n        return self.format_json_mode_type()\n    elif JsonSchema.is_json_schema(output_type):\n        return self.format_json_output_type(\n            cast(dict, JsonSchema.convert_to(output_type, [\"dict\"]))\n        )\n    else:\n        type_name = getattr(output_type, \"__name__\", output_type)\n        raise TypeError(\n            f\"The type `{type_name}` is not available with OpenAI. \"\n            \"Use an open source model or dottxt instead.\"\n        )\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.OpenAITypeAdapter.format_str_model_input","title":"<code>format_str_model_input(model_input)</code>","text":"<p>Generate the value of the <code>messages</code> argument to pass to the client when the user only passes a prompt.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@format_input.register(str)\ndef format_str_model_input(self, model_input: str) -&gt; list:\n    \"\"\"Generate the value of the `messages` argument to pass to the\n    client when the user only passes a prompt.\n\n    \"\"\"\n    return [\n        self._create_message(\"user\", model_input)\n    ]\n</code></pre>"},{"location":"api_reference/models/openai/#outlines.models.openai.from_openai","title":"<code>from_openai(client, model_name=None)</code>","text":"<p>Create an Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI]</code> <p>An <code>openai.OpenAI</code>, <code>openai.AsyncOpenAI</code>, <code>openai.AzureOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>An Outlines <code>OpenAI</code> or <code>AsyncOpenAI</code> model instance.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def from_openai(\n    client: Union[\n        \"OpenAIClient\",\n        \"AsyncOpenAIClient\",\n        \"AzureOpenAIClient\",\n        \"AsyncAzureOpenAIClient\",\n    ],\n    model_name: Optional[str] = None,\n) -&gt; Union[OpenAI, AsyncOpenAI]:\n    \"\"\"Create an Outlines `OpenAI` or `AsyncOpenAI` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` client.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI`, `openai.AsyncOpenAI`, `openai.AzureOpenAI` or\n        `openai.AsyncAzureOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    OpenAI\n        An Outlines `OpenAI` or `AsyncOpenAI` model instance.\n\n    \"\"\"\n    import openai\n\n    if isinstance(client, openai.OpenAI):\n        return OpenAI(client, model_name)\n    elif isinstance(client, openai.AsyncOpenAI):\n        return AsyncOpenAI(client, model_name)\n    else:\n        raise ValueError(\n            \"Invalid client type. The client must be an instance of \"\n            \"+ `openai.OpenAI` or `openai.AsyncOpenAI`.\"\n        )\n</code></pre>"},{"location":"api_reference/models/sglang/","title":"sglang","text":"<p>Integration with an SGLang server.</p>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.AsyncSGLang","title":"<code>AsyncSGLang</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class AsyncSGLang(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using `sglang`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The sglang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Return a text generator.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.AsyncSGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.AsyncSGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using <code>sglang</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using `sglang`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The sglang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.AsyncSGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Return a text generator.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Return a text generator.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLang","title":"<code>SGLang</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with an SGLang server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the SGLang server.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLang(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    an SGLang server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    SGLang server.\n\n    \"\"\"\n\n    def __init__(self, client, model_name: Optional[str] = None):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n        model_name\n            The name of the model to use.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = SGLangTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The SGLang server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\n            \"SGLang does not support batch inference.\"\n        )\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, list, str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using SGLang.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the SGLang client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLang.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>An <code>openai.OpenAI</code> client instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> Source code in <code>outlines/models/sglang.py</code> <pre><code>def __init__(self, client, model_name: Optional[str] = None):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n    model_name\n        The name of the model to use.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = SGLangTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLang.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The SGLang server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLang.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using SGLang.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, list, str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using SGLang.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLangTypeAdapter","title":"<code>SGLangTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>SGLang</code> and <code>AsyncSGLang</code> models.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>class SGLangTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `SGLang` and `AsyncSGLang` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the sglang\n        server expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the client.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The formatted output type to be passed to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            warnings.warn(\n                \"SGLang grammar-based structured outputs expects an EBNF \"\n                \"grammar instead of a Lark grammar as is generally used in \"\n                \"Outlines. The grammar cannot be used as a structured output \"\n                \"type with an outlines backend, it is only compatible with \"\n                \"the sglang and llguidance backends.\"\n            )\n            return {\"extra_body\": {\"ebnf\": term.definition}}\n        elif isinstance(term, JsonSchema):\n            return OpenAITypeAdapter().format_json_output_type(\n                json.loads(term.schema)\n            )\n        else:\n            return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLangTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the sglang server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, list, str]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_input(self, model_input: Union[Chat, list, str]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the sglang\n    server expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the client.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.SGLangTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted output type to be passed to the client.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The formatted output type to be passed to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        warnings.warn(\n            \"SGLang grammar-based structured outputs expects an EBNF \"\n            \"grammar instead of a Lark grammar as is generally used in \"\n            \"Outlines. The grammar cannot be used as a structured output \"\n            \"type with an outlines backend, it is only compatible with \"\n            \"the sglang and llguidance backends.\"\n        )\n        return {\"extra_body\": {\"ebnf\": term.definition}}\n    elif isinstance(term, JsonSchema):\n        return OpenAITypeAdapter().format_json_output_type(\n            json.loads(term.schema)\n        )\n    else:\n        return {\"extra_body\": {\"regex\": to_regex(term)}}\n</code></pre>"},{"location":"api_reference/models/sglang/#outlines.models.sglang.from_sglang","title":"<code>from_sglang(client, model_name=None)</code>","text":"<p>Create a <code>SGLang</code> or <code>AsyncSGLang</code> instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SGLang, AsyncSGLang]</code> <p>An Outlines <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> Source code in <code>outlines/models/sglang.py</code> <pre><code>def from_sglang(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[SGLang, AsyncSGLang]:\n    \"\"\"Create a `SGLang` or `AsyncSGLang` instance from an `openai.OpenAI` or\n    `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[SGLang, AsyncSGLang]\n        An Outlines `SGLang` or `AsyncSGLang` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return SGLang(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncSGLang(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/tgi/","title":"tgi","text":"<p>Integration with a TGI server.</p>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.AsyncTGI","title":"<code>AsyncTGI</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around a <code>huggingface_hub.AsyncInferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.AsyncInferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class AsyncTGI(AsyncModel):\n    \"\"\"Thin async wrapper around a `huggingface_hub.AsyncInferenceClient`\n    client used to communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.AsyncInferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `AsyncInferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    async def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.text_generation(**client_args)\n\n        return response\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.text_generation(\n            **client_args, stream=True\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.AsyncTGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>AsyncInferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `AsyncInferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.AsyncTGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.text_generation(**client_args)\n\n    return response\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.AsyncTGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.text_generation(\n        **client_args, stream=True\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGI","title":"<code>TGI</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>huggingface_hub.InferenceClient</code> client used to communicate with a <code>TGI</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>huggingface_hub.InferenceClient</code> client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGI(Model):\n    \"\"\"Thin wrapper around a `huggingface_hub.InferenceClient` client used to\n    communicate with a `TGI` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the\n    `huggingface_hub.InferenceClient` client.\n\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            A huggingface `InferenceClient` client instance.\n\n        \"\"\"\n        self.client = client\n        self.type_adapter = TGITypeAdapter()\n\n    def generate(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Generate text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        str\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        return self.client.text_generation(**client_args)\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"TGI does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using TGI.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types except `CFG` are supported provided your server uses\n            a backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.text_generation(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            yield chunk\n\n    def _build_client_args(\n        self,\n        model_input: str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the TGI client.\"\"\"\n        prompt = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        inference_kwargs.update(output_type_args)\n\n        client_args = {\n            \"prompt\": prompt,\n            **inference_kwargs,\n        }\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGI.__init__","title":"<code>__init__(client)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <p>A huggingface <code>InferenceClient</code> client instance.</p> required Source code in <code>outlines/models/tgi.py</code> <pre><code>def __init__(self, client):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        A huggingface `InferenceClient` client instance.\n\n    \"\"\"\n    self.client = client\n    self.type_adapter = TGITypeAdapter()\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGI.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; str:\n    \"\"\"Generate text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    str\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    return self.client.text_generation(**client_args)\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGI.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using TGI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>str</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types except <code>CFG</code> are supported provided your server uses a backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using TGI.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types except `CFG` are supported provided your server uses\n        a backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.text_generation(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        yield chunk\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGITypeAdapter","title":"<code>TGITypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>TGI</code> and <code>AsyncTGI</code> models.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>class TGITypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `TGI` and `AsyncTGI` models.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the client.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise NotImplementedError(\n            f\"The input type {input} is not available with TGI. \"\n            + \"The only available type is `str`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Argument\n        --------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the client.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            raise NotImplementedError(\n                \"TGI does not support CFG-based structured outputs.\"\n            )\n        elif isinstance(term, JsonSchema):\n            return {\n                \"grammar\": {\n                    \"type\": \"json\",\n                    \"value\": json.loads(term.schema),\n                }\n            }\n        else:\n            return {\n                \"grammar\": {\n                    \"type\": \"regex\",\n                    \"value\": to_regex(term),\n                }\n            }\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGITypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the client.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the client.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise NotImplementedError(\n        f\"The input type {input} is not available with TGI. \"\n        + \"The only available type is `str`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.TGITypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> Argument <p>output_type     The structured output type provided.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the client.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Argument\n    --------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the client.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        raise NotImplementedError(\n            \"TGI does not support CFG-based structured outputs.\"\n        )\n    elif isinstance(term, JsonSchema):\n        return {\n            \"grammar\": {\n                \"type\": \"json\",\n                \"value\": json.loads(term.schema),\n            }\n        }\n    else:\n        return {\n            \"grammar\": {\n                \"type\": \"regex\",\n                \"value\": to_regex(term),\n            }\n        }\n</code></pre>"},{"location":"api_reference/models/tgi/#outlines.models.tgi.from_tgi","title":"<code>from_tgi(client)</code>","text":"<p>Create an Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance from an <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[InferenceClient, AsyncInferenceClient]</code> <p>An <code>huggingface_hub.InferenceClient</code> or <code>huggingface_hub.AsyncInferenceClient</code> instance.</p> required <p>Returns:</p> Type Description <code>Union[TGI, AsyncTGI]</code> <p>An Outlines <code>TGI</code> or <code>AsyncTGI</code> model instance.</p> Source code in <code>outlines/models/tgi.py</code> <pre><code>def from_tgi(\n    client: Union[\"InferenceClient\", \"AsyncInferenceClient\"],\n) -&gt; Union[TGI, AsyncTGI]:\n    \"\"\"Create an Outlines `TGI` or `AsyncTGI` model instance from an\n    `huggingface_hub.InferenceClient` or `huggingface_hub.AsyncInferenceClient`\n    instance.\n\n    Parameters\n    ----------\n    client\n        An `huggingface_hub.InferenceClient` or\n        `huggingface_hub.AsyncInferenceClient` instance.\n\n    Returns\n    -------\n    Union[TGI, AsyncTGI]\n        An Outlines `TGI` or `AsyncTGI` model instance.\n\n    \"\"\"\n    from huggingface_hub import AsyncInferenceClient, InferenceClient\n\n    if isinstance(client, InferenceClient):\n        return TGI(client)\n    elif isinstance(client, AsyncInferenceClient):\n        return AsyncTGI(client)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            + \"Please provide an HuggingFace InferenceClient \"\n            + \"or AsyncInferenceClient instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/tokenizer/","title":"tokenizer","text":""},{"location":"api_reference/models/tokenizer/#outlines.models.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Hashable</code>, <code>Protocol</code></p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>class Tokenizer(Hashable, Protocol):\n    eos_token: str\n    eos_token_id: int\n    pad_token_id: int\n    vocabulary: Dict[str, int]\n    special_tokens: Set[str]\n\n    def encode(\n        self, prompt: Union[str, List[str]]\n    ) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n        \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n        ...\n\n    def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n        \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n        ...\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        \"\"\"Convert a token to its equivalent string.\n\n        This is for instance useful for BPE tokenizers where whitespaces are\n        represented by the special characted `\u0120`. This prevents matching a raw\n        token that includes `\u0120` with a string.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/models/tokenizer/#outlines.models.tokenizer.Tokenizer.convert_token_to_string","title":"<code>convert_token_to_string(token)</code>","text":"<p>Convert a token to its equivalent string.</p> <p>This is for instance useful for BPE tokenizers where whitespaces are represented by the special characted <code>\u0120</code>. This prevents matching a raw token that includes <code>\u0120</code> with a string.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def convert_token_to_string(self, token: str) -&gt; str:\n    \"\"\"Convert a token to its equivalent string.\n\n    This is for instance useful for BPE tokenizers where whitespaces are\n    represented by the special characted `\u0120`. This prevents matching a raw\n    token that includes `\u0120` with a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/tokenizer/#outlines.models.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids)</code>","text":"<p>Translate an array of token ids to a string or list of strings.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def decode(self, token_ids: \"NDArray[np.int64]\") -&gt; List[str]:\n    \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/tokenizer/#outlines.models.tokenizer.Tokenizer.encode","title":"<code>encode(prompt)</code>","text":"<p>Translate the input prompts into arrays of token ids and attention mask.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def encode(\n    self, prompt: Union[str, List[str]]\n) -&gt; \"Tuple['NDArray[np.int64]', 'NDArray[np.int64]']\":\n    \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/models/transformers/","title":"transformers","text":"<p>Integration with the <code>transformers</code> library.</p>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n        self.get_vocab = self.tokenizer.get_vocab\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.Transformers","title":"<code>Transformers</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> tokenizer.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and tokenizer.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers(Model):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    tokenizer.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    tokenizer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"\n        Parameters:\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        tokenizer\n            A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n            the `transformers` API for tokenizers.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        # We need to handle the cases in which jax/flax or tensorflow\n        # is not available in the environment.\n        try:\n            from transformers import FlaxPreTrainedModel\n        except ImportError:  # pragma: no cover\n            FlaxPreTrainedModel = None\n\n        try:\n            from transformers import TFPreTrainedModel\n        except ImportError:  # pragma: no cover\n            TFPreTrainedModel = None\n\n        tokenizer.padding_side = \"left\"\n        self.model = model\n        self.hf_tokenizer = tokenizer\n        self.tokenizer = TransformerTokenizer(tokenizer)\n        self.device_dtype = device_dtype\n        self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n        if (\n            FlaxPreTrainedModel is not None\n            and isinstance(model, FlaxPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"jax\"\n            warnings.warn(\"\"\"\n                Support for `jax` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `jax` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        elif (\n            TFPreTrainedModel is not None\n            and isinstance(model, TFPreTrainedModel)\n        ):  # pragma: no cover\n            self.tensor_library_name = \"tensorflow\"\n            warnings.warn(\"\"\"\n                Support for `tensorflow` has been deprecated and will be removed in\n                version 1.4.0 of Outlines. Please use `torch` instead.\n                Transformers models using `tensorflow` do not support structured\n                generation.\n                \"\"\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            self.tensor_library_name = \"torch\"\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        # Format validation\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item)\n                for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": (\n                attention_mask.to(self.model.device, dtype=self.device_dtype)\n                if self.device_dtype is not None\n                else attention_mask.to(self.model.device)\n            ),\n        }\n\n        return prompts, inputs\n\n    def generate(\n        self,\n        model_input: Union[str, dict, Chat],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response. For\n            multi-modal models, the input should be a dictionary containing the\n            `text` key with a value of type `Union[str, List[str]]` and the\n            other keys required by the model.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            of the `transformers` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, False)\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts,\n            inputs,\n            logits_processor=logits_processor,\n            **inference_kwargs,\n        )\n\n        # required for multi-modal models that return a 2D tensor even when\n        # num_return_sequences is 1\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples == 1 and len(generated_ids.shape) == 2:\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_batch(\n        self,\n        model_input: List[Union[str, dict, Chat]],\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n        **inference_kwargs: Any,\n    ) -&gt; List[Union[str, List[str]]]:\n        \"\"\"\"\"\"\n        prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n        logits_processor = self.type_adapter.format_output_type(output_type)\n\n        generated_ids = self._generate_output_seq(\n            prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n        )\n\n        # if there are multiple samples per input, convert generated_id to 3D\n        num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n        if num_samples &gt; 1:\n            generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n        return self._decode_generation(generated_ids)\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `transformers` models.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not implemented for Transformers models.\"\n        )\n\n    def _generate_output_seq(self, prompts, inputs, **inference_kwargs):\n        input_ids = inputs[\"input_ids\"]\n\n        output_ids = self.model.generate(\n            **inputs,\n            **inference_kwargs,\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Generated outputs aren't 1D, 2D or 3D, but instead are \"\n                f\"{generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.Transformers.__init__","title":"<code>__init__(model, tokenizer, *, device_dtype=None)</code>","text":"Parameters: <p>model     A <code>PreTrainedModel</code>, or any model that is compatible with the     <code>transformers</code> API for models. tokenizer     A <code>PreTrainedTokenizer</code>, or any tokenizer that is compatible with     the <code>transformers</code> API for tokenizers. device_dtype     The dtype to use for the model. If not provided, the model will use     the default dtype.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"\n    Parameters:\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    tokenizer\n        A `PreTrainedTokenizer`, or any tokenizer that is compatible with\n        the `transformers` API for tokenizers.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    # We need to handle the cases in which jax/flax or tensorflow\n    # is not available in the environment.\n    try:\n        from transformers import FlaxPreTrainedModel\n    except ImportError:  # pragma: no cover\n        FlaxPreTrainedModel = None\n\n    try:\n        from transformers import TFPreTrainedModel\n    except ImportError:  # pragma: no cover\n        TFPreTrainedModel = None\n\n    tokenizer.padding_side = \"left\"\n    self.model = model\n    self.hf_tokenizer = tokenizer\n    self.tokenizer = TransformerTokenizer(tokenizer)\n    self.device_dtype = device_dtype\n    self.type_adapter = TransformersTypeAdapter(tokenizer=tokenizer)\n\n    if (\n        FlaxPreTrainedModel is not None\n        and isinstance(model, FlaxPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"jax\"\n        warnings.warn(\"\"\"\n            Support for `jax` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `jax` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    elif (\n        TFPreTrainedModel is not None\n        and isinstance(model, TFPreTrainedModel)\n    ):  # pragma: no cover\n        self.tensor_library_name = \"tensorflow\"\n        warnings.warn(\"\"\"\n            Support for `tensorflow` has been deprecated and will be removed in\n            version 1.4.0 of Outlines. Please use `torch` instead.\n            Transformers models using `tensorflow` do not support structured\n            generation.\n            \"\"\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        self.tensor_library_name = \"torch\"\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.Transformers.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[str, dict, Chat]</code> <p>The prompt based on which the model will generate a response. For multi-modal models, the input should be a dictionary containing the <code>text</code> key with a value of type <code>Union[str, List[str]]</code> and the other keys required by the model.</p> required <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method of the <code>transformers</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[str, dict, Chat],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response. For\n        multi-modal models, the input should be a dictionary containing the\n        `text` key with a value of type `Union[str, List[str]]` and the\n        other keys required by the model.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        of the `transformers` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, False)\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts,\n        inputs,\n        logits_processor=logits_processor,\n        **inference_kwargs,\n    )\n\n    # required for multi-modal models that return a 2D tensor even when\n    # num_return_sequences is 1\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples == 1 and len(generated_ids.shape) == 2:\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.Transformers.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Union[str, dict, Chat]],\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n    **inference_kwargs: Any,\n) -&gt; List[Union[str, List[str]]]:\n    \"\"\"\"\"\"\n    prompts, inputs = self._prepare_model_inputs(model_input, True) # type: ignore\n    logits_processor = self.type_adapter.format_output_type(output_type)\n\n    generated_ids = self._generate_output_seq(\n        prompts, inputs, logits_processor=logits_processor, **inference_kwargs\n    )\n\n    # if there are multiple samples per input, convert generated_id to 3D\n    num_samples = inference_kwargs.get(\"num_return_sequences\", 1)\n    if num_samples &gt; 1:\n        generated_ids = generated_ids.view(len(model_input), num_samples, -1)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.Transformers.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>transformers</code> models.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `transformers` models.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not implemented for Transformers models.\"\n    )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersMultiModal","title":"<code>TransformersMultiModal</code>","text":"<p>               Bases: <code>Transformers</code></p> <p>Thin wrapper around a <code>transformers</code> model and a <code>transformers</code> processor.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>transformers</code> model and processor.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModal(Transformers):\n    \"\"\"Thin wrapper around a `transformers` model and a `transformers`\n    processor.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `transformers` model and\n    processor.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        processor,\n        *,\n        device_dtype: Optional[\"torch.dtype\"] = None,\n    ):\n        \"\"\"Create a TransformersMultiModal model instance\n\n        We rely on the `__init__` method of the `Transformers` class to handle\n        most of the initialization and then add elements specific to multimodal\n        models.\n\n        Parameters\n        ----------\n        model\n            A `PreTrainedModel`, or any model that is compatible with the\n            `transformers` API for models.\n        processor\n            A `ProcessorMixin` instance.\n        device_dtype\n            The dtype to use for the model. If not provided, the model will use\n            the default dtype.\n\n        \"\"\"\n        self.processor = processor\n        self.processor.padding_side = \"left\"\n        self.processor.pad_token = \"[PAD]\"\n\n        tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n        super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n        self.type_adapter = TransformersMultiModalTypeAdapter(\n            tokenizer=tokenizer\n        )\n\n    def _prepare_model_inputs(\n        self,\n        model_input,\n        is_batch: bool = False,\n    ) -&gt; Tuple[Union[str, List[str]], dict]:\n        \"\"\"Turn the user input into arguments to pass to the model\"\"\"\n        if is_batch:\n            prompts = [\n                self.type_adapter.format_input(item) for item in model_input\n            ]\n        else:\n            prompts = self.type_adapter.format_input(model_input)\n\n        # The expected format is a single dict\n        if is_batch:\n            merged_prompts = defaultdict(list)\n            for d in prompts:\n                for key, value in d.items():\n                    if key == \"text\":\n                        merged_prompts[key].append(value)\n                    else:\n                        merged_prompts[key].extend(value)\n        else:\n            merged_prompts = prompts # type: ignore\n\n        inputs = self.processor(\n            **merged_prompts, padding=True, return_tensors=\"pt\"\n        )\n        if self.device_dtype is not None:\n            inputs = inputs.to(self.model.device, dtype=self.device_dtype)\n        else:\n            inputs = inputs.to(self.model.device)\n\n        return merged_prompts[\"text\"], inputs\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersMultiModal.__init__","title":"<code>__init__(model, processor, *, device_dtype=None)</code>","text":"<p>Create a TransformersMultiModal model instance</p> <p>We rely on the <code>__init__</code> method of the <code>Transformers</code> class to handle most of the initialization and then add elements specific to multimodal models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>PreTrainedModel</code>, or any model that is compatible with the <code>transformers</code> API for models.</p> required <code>processor</code> <p>A <code>ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    processor,\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n):\n    \"\"\"Create a TransformersMultiModal model instance\n\n    We rely on the `__init__` method of the `Transformers` class to handle\n    most of the initialization and then add elements specific to multimodal\n    models.\n\n    Parameters\n    ----------\n    model\n        A `PreTrainedModel`, or any model that is compatible with the\n        `transformers` API for models.\n    processor\n        A `ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    \"\"\"\n    self.processor = processor\n    self.processor.padding_side = \"left\"\n    self.processor.pad_token = \"[PAD]\"\n\n    tokenizer: \"PreTrainedTokenizer\" = self.processor.tokenizer\n\n    super().__init__(model, tokenizer, device_dtype=device_dtype)\n\n    self.type_adapter = TransformersMultiModalTypeAdapter(\n        tokenizer=tokenizer\n    )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersMultiModalTypeAdapter","title":"<code>TransformersMultiModalTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for <code>TransformersMultiModal</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersMultiModalTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for `TransformersMultiModal` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Fomat the prompt arguments to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        dict\n            The formatted input.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available. Please \"\n            + \"provide a list containing a text prompt and assets \"\n            + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n            + \"model or a `Chat` instance.\"\n        )\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; dict:\n        conversation = []\n        assets = []\n\n        # process each message, convert if needed to standardized multimodal chat template format\n        # and collect assets for HF processor\n        for message in model_input.messages:\n            processed_message, message_assets = self._prepare_message(\n                message[\"role\"], message[\"content\"]\n            )\n            conversation.append(processed_message)\n            assets.extend(message_assets)\n\n        formatted_prompt = self.tokenizer.apply_chat_template(\n            conversation,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        # use the formatted prompt and the assets to format the input\n        return self.format_list_input([formatted_prompt, *assets])\n\n    def _prepare_message(self, role: str, content: str | list) -&gt; tuple[dict, list]:\n        \"\"\"Create a message.\"\"\"\n        if isinstance(content, str):\n            return {\"role\": role, \"content\": content}, []\n\n        elif isinstance(content, list):\n            if all(isinstance(item, dict) for item in content): # HF multimodal chat template\n                return {\"role\": role, \"content\": content}, self._extract_assets_from_content(content)\n            else: # list of string + assets\n                prompt = content[0]\n                assets = content[1:]\n                assets_dict = [self._format_asset_for_template(asset) for asset in assets]\n\n                return {\"role\": role, \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    *assets_dict\n                ]}, assets\n        else:\n            raise ValueError(\n                f\"Invalid content type: {type(content)}. \"\n                + \"The content must be a string or a list containing text and assets \"\n                + \"or a list of dict items with explicit types.\"\n            )\n\n    def _extract_assets_from_content(self, content: list) -&gt; list:\n        \"\"\"Process a list of dict items.\"\"\"\n        assets = []\n\n        for item in content:\n            if len(item) &gt; 2:\n                raise ValueError(\n                    f\"Found item with multiple keys: {item}. \"\n                    + \"Each item in the content list must be a dictionary with a 'type' key and a single asset key. \"\n                    + \"To include multiple assets, use separate dictionary items. \"\n                    + \"For example: [{{'type': 'image', 'image': image1}}, {{'type': 'image', 'image': image2}}]. \"\n                )\n\n            if \"type\" not in item:\n                raise ValueError(\n                    \"Each item in the content list must be a dictionary with a 'type' key. \"\n                    + \"Valid types are 'text', 'image', 'video', or 'audio'. \"\n                    + \"For instance {{'type': 'text', 'text': 'your message'}}. \"\n                    + f\"Found item without 'type' key: {item}\"\n                )\n            if item[\"type\"] == \"text\":\n                continue\n            elif item[\"type\"] in [\"image\", \"video\", \"audio\"]:\n                asset_key = item[\"type\"]\n                if asset_key not in item:\n                    raise ValueError(\n                        f\"Item with type '{asset_key}' must contain a '{asset_key}' key. \"\n                        + f\"Found item: {item}\"\n                    )\n                if isinstance(item[asset_key], (Image, Video, Audio)):\n                    assets.append(item[asset_key])\n                else:\n                    raise ValueError(\n                        \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                        + f\"Unsupported asset type: {type(item[asset_key])}\"\n                    )\n            else:\n                raise ValueError(\n                    \"Content must be 'text', 'image', 'video' or 'audio'. \"\n                    + f\"Unsupported content type: {item['type']}\")\n        return assets\n\n    def _format_asset_for_template(self, asset: Image | Video | Audio) -&gt; dict:\n        \"\"\"Process an asset.\"\"\"\n        if isinstance(asset, Image):\n            return {\"type\": \"image\", \"image\": asset}\n        elif isinstance(asset, Video):\n            return {\"type\": \"video\", \"video\": asset}\n        elif isinstance(asset, Audio):\n            return {\"type\": \"audio\", \"audio\": asset}\n        else:\n            raise ValueError(\n                \"Assets must be of type `Image`, `Video` or `Audio`. \"\n                + f\"Unsupported asset type: {type(asset)}\"\n            )\n\n    @format_input.register(list)\n    def format_list_input(self, model_input: list) -&gt; dict:\n        prompt = model_input[0]\n        assets = model_input[1:]\n\n        if not assets:  # handle empty assets case\n            return {\"text\": prompt}\n\n        asset_types = set(type(asset) for asset in assets)\n        if len(asset_types) &gt; 1:\n            raise ValueError(\n                \"All assets must be of the same type. \"\n                + f\"Found types: {asset_types}\"\n            )\n        asset_type = asset_types.pop()\n\n        if asset_type == Image:\n            return {\n                \"text\": prompt,\n                \"images\": [asset.image for asset in assets]\n            }\n        elif asset_type == Audio: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"audio\": [asset.audio for asset in assets]\n            }\n        elif asset_type == Video: # pragma: no cover\n            return {\n                \"text\": prompt,\n                \"videos\": [asset.video for asset in assets]\n            }\n        else:\n            raise ValueError(f\"Unsupported asset type: {asset_type}\")\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Argument\n        --------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Fomat the prompt arguments to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The formatted input.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Fomat the prompt arguments to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    dict\n        The formatted input.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available. Please \"\n        + \"provide a list containing a text prompt and assets \"\n        + \"(`Image`, `Audio` or `Video` instances) supported by your \"\n        + \"model or a `Chat` instance.\"\n    )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersMultiModalTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> Argument <p>output_type     The logits processor provided.</p> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Argument\n    --------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersTypeAdapter","title":"<code>TransformersTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>Transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformersTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `Transformers` model.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.tokenizer = kwargs.get(\"tokenizer\")\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        str\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available.\"\n            \"The only available types are `str` and `Chat`.\"\n        )\n\n    @format_input.register(str)\n    def format_str_input(self, model_input: str) -&gt; str:\n        return model_input\n\n    @format_input.register(Chat)\n    def format_chat_input(self, model_input: Chat) -&gt; str:\n        return self.tokenizer.apply_chat_template(\n            model_input.messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n\n    def format_output_type(\n        self,\n        output_type: Optional[OutlinesLogitsProcessor] = None,\n    ) -&gt; Optional[\"LogitsProcessorList\"]:\n        \"\"\"Generate the logits processor argument to pass to the model.\n\n        Parameters\n        ----------\n        output_type\n            The logits processor provided.\n\n        Returns\n        -------\n        Optional[LogitsProcessorList]\n            The logits processor to pass to the model.\n\n        \"\"\"\n        from transformers import LogitsProcessorList\n\n        if output_type is not None:\n            return LogitsProcessorList([output_type])\n        return None\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    str\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available.\"\n        \"The only available types are `str` and `Chat`.\"\n    )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.TransformersTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the logits processor argument to pass to the model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[LogitsProcessorList]</code> <p>The logits processor to pass to the model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def format_output_type(\n    self,\n    output_type: Optional[OutlinesLogitsProcessor] = None,\n) -&gt; Optional[\"LogitsProcessorList\"]:\n    \"\"\"Generate the logits processor argument to pass to the model.\n\n    Parameters\n    ----------\n    output_type\n        The logits processor provided.\n\n    Returns\n    -------\n    Optional[LogitsProcessorList]\n        The logits processor to pass to the model.\n\n    \"\"\"\n    from transformers import LogitsProcessorList\n\n    if output_type is not None:\n        return LogitsProcessorList([output_type])\n    return None\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.from_transformers","title":"<code>from_transformers(model, tokenizer_or_processor, *, device_dtype=None)</code>","text":"<p>Create an Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance from a <code>PreTrainedModel</code> instance and a <code>PreTrainedTokenizer</code> or <code>ProcessorMixin</code> instance.</p> <p><code>outlines</code> supports <code>PreTrainedModelForCausalLM</code>, <code>PreTrainedMambaForCausalLM</code>, <code>PreTrainedModelForSeq2Seq</code> and any model that implements the <code>transformers</code> model API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>A <code>transformers.PreTrainedModel</code> instance.</p> required <code>tokenizer_or_processor</code> <code>Union[PreTrainedTokenizer, ProcessorMixin]</code> <p>A <code>transformers.PreTrainedTokenizer</code> or <code>transformers.ProcessorMixin</code> instance.</p> required <code>device_dtype</code> <code>Optional[dtype]</code> <p>The dtype to use for the model. If not provided, the model will use the default dtype.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Transformers, TransformersMultiModal]</code> <p>An Outlines <code>Transformers</code> or <code>TransformersMultiModal</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def from_transformers(\n    model: \"PreTrainedModel\",\n    tokenizer_or_processor: Union[\"PreTrainedTokenizer\", \"ProcessorMixin\"],\n    *,\n    device_dtype: Optional[\"torch.dtype\"] = None,\n) -&gt; Union[Transformers, TransformersMultiModal]:\n    \"\"\"Create an Outlines `Transformers` or `TransformersMultiModal` model\n    instance from a `PreTrainedModel` instance and a `PreTrainedTokenizer` or\n    `ProcessorMixin` instance.\n\n    `outlines` supports `PreTrainedModelForCausalLM`,\n    `PreTrainedMambaForCausalLM`, `PreTrainedModelForSeq2Seq` and any model\n    that implements the `transformers` model API.\n\n    Parameters\n    ----------\n    model\n        A `transformers.PreTrainedModel` instance.\n    tokenizer_or_processor\n        A `transformers.PreTrainedTokenizer` or\n        `transformers.ProcessorMixin` instance.\n    device_dtype\n        The dtype to use for the model. If not provided, the model will use\n        the default dtype.\n\n    Returns\n    -------\n    Union[Transformers, TransformersMultiModal]\n        An Outlines `Transformers` or `TransformersMultiModal` model instance.\n\n    \"\"\"\n    from transformers import (\n        PreTrainedTokenizer, PreTrainedTokenizerFast, ProcessorMixin)\n\n    if isinstance(\n        tokenizer_or_processor, (PreTrainedTokenizer, PreTrainedTokenizerFast)\n    ):\n        tokenizer = tokenizer_or_processor\n        return Transformers(model, tokenizer, device_dtype=device_dtype)\n    elif isinstance(tokenizer_or_processor, ProcessorMixin):\n        processor = tokenizer_or_processor\n        return TransformersMultiModal(model, processor, device_dtype=device_dtype)\n    else:\n        raise ValueError(\n            \"We could determine whether the model passed to `from_transformers`\"\n            + \" is a text-2-text or a multi-modal model. Please provide a \"\n            + \"a transformers tokenizer or processor.\"\n        )\n</code></pre>"},{"location":"api_reference/models/transformers/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:  # pragma: no cover\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api_reference/models/utils/","title":"utils","text":""},{"location":"api_reference/models/utils/#outlines.models.utils.set_additional_properties_false_json_schema","title":"<code>set_additional_properties_false_json_schema(schema)</code>","text":"<p>Set additionalProperties to False to all objects in the schema using jsonpath.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON schema to modify</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The modified schema with additionalProperties set to False</p> Source code in <code>outlines/models/utils.py</code> <pre><code>def set_additional_properties_false_json_schema(schema: dict) -&gt; dict:\n    \"\"\"Set additionalProperties to False to all objects in the schema using jsonpath.\n\n    Parameters\n    ----------\n    schema\n        The JSON schema to modify\n\n    Returns\n    -------\n    dict\n        The modified schema with additionalProperties set to False\n    \"\"\"\n    # Get all nodes\n    jsonpath_expr = jsonpath_ng.parse('$..*')\n    matches = jsonpath_expr.find(schema)\n\n    # Go over all nodes and set additionalProperties to False if it's an object\n    for match in matches:\n        if match.value == 'object':\n            if 'additionalProperties' not in match.context.value:\n                match.context.value['additionalProperties'] = False\n\n    return schema\n</code></pre>"},{"location":"api_reference/models/vllm/","title":"vllm","text":"<p>Integration with a vLLM server.</p>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.AsyncVLLM","title":"<code>AsyncVLLM</code>","text":"<p>               Bases: <code>AsyncModel</code></p> <p>Thin async wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class AsyncVLLM(AsyncModel):\n    \"\"\"Thin async wrapper around the `openai.OpenAI` client used to communicate\n    with a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"AsyncOpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.AsyncOpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    async def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        response = await self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    async def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    async def generate_stream( # type: ignore\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        AsyncIterator[str]\n            An async iterator that yields the text generated by the model.\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = await self.client.chat.completions.create(\n            **client_args,\n            stream=True,\n        )\n\n        async for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.AsyncVLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncOpenAI</code> <p>An <code>openai.AsyncOpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"AsyncOpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.AsyncOpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.AsyncVLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    response = await self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.AsyncVLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>  <code>async</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[str]</code> <p>An async iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>async def generate_stream( # type: ignore\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    AsyncIterator[str]\n        An async iterator that yields the text generated by the model.\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = await self.client.chat.completions.create(\n        **client_args,\n        stream=True,\n    )\n\n    async for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLM","title":"<code>VLLM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around the <code>openai.OpenAI</code> client used to communicate with a <code>vllm</code> server.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>openai.OpenAI</code> client for the <code>vllm</code> server.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM(Model):\n    \"\"\"Thin wrapper around the `openai.OpenAI` client used to communicate with\n    a `vllm` server.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `openai.OpenAI` client for the\n    `vllm` server.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: \"OpenAI\",\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        client\n            An `openai.OpenAI` client instance.\n\n        \"\"\"\n        self.client = client\n        self.model_name = model_name\n        self.type_adapter = VLLMTypeAdapter()\n\n    def generate(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, list[str]]:\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Union[str, list[str]]\n            The text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input,\n            output_type,\n            **inference_kwargs,\n        )\n\n        response = self.client.chat.completions.create(**client_args)\n\n        messages = [choice.message for choice in response.choices]\n        for message in messages:\n            if message.refusal is not None:  # pragma: no cover\n                raise ValueError(\n                    f\"The vLLM server refused to answer the request: \"\n                    f\"{message.refusal}\"\n                )\n\n        if len(messages) == 1:\n            return messages[0].content\n        else:\n            return [message.content for message in messages]\n\n    def generate_batch(\n        self,\n        model_input,\n        output_type = None,\n        **inference_kwargs,\n    ):\n        raise NotImplementedError(\"VLLM does not support batch inference.\")\n\n    def generate_stream(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using vLLM.\n\n        Parameters\n        ----------\n        model_input\n            The prompt based on which the model will generate a response.\n        output_type\n            The desired format of the response generated by the model. All\n            output types available in Outlines are supported provided your\n            server uses a structured generation backend that supports them.\n        inference_kwargs\n            Additional keyword arguments to pass to the client.\n\n        Returns\n        -------\n        Iterator[str]\n            An iterator that yields the text generated by the model.\n\n        \"\"\"\n        client_args = self._build_client_args(\n            model_input, output_type, **inference_kwargs,\n        )\n\n        stream = self.client.chat.completions.create(\n            **client_args, stream=True,\n        )\n\n        for chunk in stream:  # pragma: no cover\n            if chunk.choices and chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def _build_client_args(\n        self,\n        model_input: Union[Chat, str, list],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; dict:\n        \"\"\"Build the arguments to pass to the OpenAI client.\"\"\"\n        messages = self.type_adapter.format_input(model_input)\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        extra_body = inference_kwargs.pop(\"extra_body\", {})\n        extra_body.update(output_type_args)\n\n        if \"model\" not in inference_kwargs and self.model_name is not None:\n            inference_kwargs[\"model\"] = self.model_name\n\n        client_args = {\n            \"messages\": messages,\n            **inference_kwargs,\n        }\n        if extra_body:\n            client_args[\"extra_body\"] = extra_body\n\n        return client_args\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLM.__init__","title":"<code>__init__(client, model_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>An <code>openai.OpenAI</code> client instance.</p> required Source code in <code>outlines/models/vllm.py</code> <pre><code>def __init__(\n    self,\n    client: \"OpenAI\",\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` client instance.\n\n    \"\"\"\n    self.client = client\n    self.model_name = model_name\n    self.type_adapter = VLLMTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLM.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, list[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, list[str]]:\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Union[str, list[str]]\n        The text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input,\n        output_type,\n        **inference_kwargs,\n    )\n\n    response = self.client.chat.completions.create(**client_args)\n\n    messages = [choice.message for choice in response.choices]\n    for message in messages:\n        if message.refusal is not None:  # pragma: no cover\n            raise ValueError(\n                f\"The vLLM server refused to answer the request: \"\n                f\"{message.refusal}\"\n            )\n\n    if len(messages) == 1:\n        return messages[0].content\n    else:\n        return [message.content for message in messages]\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLM.generate_stream","title":"<code>generate_stream(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Stream text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The desired format of the response generated by the model. All output types available in Outlines are supported provided your server uses a structured generation backend that supports them.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>An iterator that yields the text generated by the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate_stream(\n    self,\n    model_input: Union[Chat, str, list],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using vLLM.\n\n    Parameters\n    ----------\n    model_input\n        The prompt based on which the model will generate a response.\n    output_type\n        The desired format of the response generated by the model. All\n        output types available in Outlines are supported provided your\n        server uses a structured generation backend that supports them.\n    inference_kwargs\n        Additional keyword arguments to pass to the client.\n\n    Returns\n    -------\n    Iterator[str]\n        An iterator that yields the text generated by the model.\n\n    \"\"\"\n    client_args = self._build_client_args(\n        model_input, output_type, **inference_kwargs,\n    )\n\n    stream = self.client.chat.completions.create(\n        **client_args, stream=True,\n    )\n\n    for chunk in stream:  # pragma: no cover\n        if chunk.choices and chunk.choices[0].delta.content is not None:\n            yield chunk.choices[0].delta.content\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLMTypeAdapter","title":"<code>VLLMTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLM</code> and <code>AsyncVLLM</code> models.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLMTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLM` and `AsyncVLLM` models.\"\"\"\n\n    def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n        \"\"\"Generate the value of the messages argument to pass to the client.\n\n        We rely on the OpenAITypeAdapter to format the input as the vLLM server\n        expects input in the same format as OpenAI.\n\n        Parameters\n        ----------\n        model_input\n            The input passed by the user.\n\n        Returns\n        -------\n        list\n            The formatted input to be passed to the model.\n\n        \"\"\"\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the client.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The structured output argument to pass to the model.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"guided_grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            extra_body = {\"guided_json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n            return extra_body\n        else:\n            return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLMTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the value of the messages argument to pass to the client.</p> <p>We rely on the OpenAITypeAdapter to format the input as the vLLM server expects input in the same format as OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Union[Chat, str, list]</code> <p>The input passed by the user.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The formatted input to be passed to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_input(self, model_input: Union[Chat, str, list]) -&gt; list:\n    \"\"\"Generate the value of the messages argument to pass to the client.\n\n    We rely on the OpenAITypeAdapter to format the input as the vLLM server\n    expects input in the same format as OpenAI.\n\n    Parameters\n    ----------\n    model_input\n        The input passed by the user.\n\n    Returns\n    -------\n    list\n        The formatted input to be passed to the model.\n\n    \"\"\"\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.VLLMTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the client.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The structured output argument to pass to the model.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the client.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The structured output argument to pass to the model.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"guided_grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        extra_body = {\"guided_json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            extra_body[\"whitespace_pattern\"] = term.whitespace_pattern\n        return extra_body\n    else:\n        return {\"guided_regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/vllm/#outlines.models.vllm.from_vllm","title":"<code>from_vllm(client, model_name=None)</code>","text":"<p>Create an Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance from an <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Union[OpenAI, AsyncOpenAI]</code> <p>An <code>openai.OpenAI</code> or <code>openai.AsyncOpenAI</code> instance.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[VLLM, AsyncVLLM]</code> <p>An Outlines <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def from_vllm(\n    client: Union[\"OpenAI\", \"AsyncOpenAI\"],\n    model_name: Optional[str] = None,\n) -&gt; Union[VLLM, AsyncVLLM]:\n    \"\"\"Create an Outlines `VLLM` or `AsyncVLLM` model instance from an\n    `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n\n    Parameters\n    ----------\n    client\n        An `openai.OpenAI` or `openai.AsyncOpenAI` instance.\n    model_name\n        The name of the model to use.\n\n    Returns\n    -------\n    Union[VLLM, AsyncVLLM]\n        An Outlines `VLLM` or `AsyncVLLM` model instance.\n\n    \"\"\"\n    from openai import AsyncOpenAI, OpenAI\n\n    if isinstance(client, OpenAI):\n        return VLLM(client, model_name)\n    elif isinstance(client, AsyncOpenAI):\n        return AsyncVLLM(client, model_name)\n    else:\n        raise ValueError(\n            f\"Unsupported client type: {type(client)}.\\n\"\n            \"Please provide an OpenAI or AsyncOpenAI instance.\"\n        )\n</code></pre>"},{"location":"api_reference/models/vllm_offline/","title":"vllm_offline","text":"<p>Integration with the <code>vllm</code> library (offline mode).</p>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOffline","title":"<code>VLLMOffline</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thin wrapper around a <code>vllm.LLM</code> model.</p> <p>This wrapper is used to convert the input and output types specified by the users at a higher level to arguments to the <code>vllm.LLM</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOffline(Model):\n    \"\"\"Thin wrapper around a `vllm.LLM` model.\n\n    This wrapper is used to convert the input and output types specified by the\n    users at a higher level to arguments to the `vllm.LLM` model.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        \"\"\"Create a VLLM model instance.\n\n        Parameters\n        ----------\n        model\n            A `vllm.LLM` model instance.\n\n        \"\"\"\n        self.model = model\n        self.type_adapter = VLLMOfflineTypeAdapter()\n\n    def _build_generation_args(\n        self,\n        inference_kwargs: dict,\n        output_type: Optional[Any] = None,\n    ) -&gt; \"SamplingParams\":\n        \"\"\"Create the `SamplingParams` object to pass to the `generate` method\n        of the `vllm.LLM` model.\"\"\"\n        from vllm.sampling_params import StructuredOutputsParams, SamplingParams\n\n        sampling_params = inference_kwargs.pop(\"sampling_params\", None)\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        output_type_args = self.type_adapter.format_output_type(output_type)\n        if output_type_args:\n            original_sampling_params_dict = {f: getattr(sampling_params, f) for f in sampling_params.__struct_fields__}\n            sampling_params_dict = {**original_sampling_params_dict, \"structured_outputs\": StructuredOutputsParams(**output_type_args)}\n            sampling_params = SamplingParams(**sampling_params_dict)\n\n        return sampling_params\n\n    def generate(\n        self,\n        model_input: Chat | str,\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[str, List[str]]:\n        \"\"\"Generate text using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The prompt based on which the model will generate a response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[str, List[str]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if isinstance(model_input, Chat):\n            results = self.model.chat(\n                messages=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        else:\n            results = self.model.generate(\n                prompts=self.type_adapter.format_input(model_input),\n                sampling_params=sampling_params,\n                **inference_kwargs,\n            )\n        results = [completion.text for completion in results[0].outputs]\n\n        if len(results) == 1:\n            return results[0]\n        else:\n            return results\n\n    def generate_batch(\n        self,\n        model_input: List[Chat | str],\n        output_type: Optional[Any] = None,\n        **inference_kwargs: Any,\n    ) -&gt; Union[List[str], List[List[str]]]:\n        \"\"\"Generate a batch of completions using vLLM offline.\n\n        Parameters\n        ----------\n        prompt\n            The list of prompts based on which the model will generate a\n            response.\n        output_type\n            The logits processor the model will use to constrain the format of\n            the generated text.\n        inference_kwargs\n            Additional keyword arguments to pass to the `generate` method\n            in the `vllm.LLM` model.\n\n        Returns\n        -------\n        Union[List[str], List[List[str]]]\n            The text generated by the model.\n\n        \"\"\"\n        sampling_params = self._build_generation_args(\n            inference_kwargs,\n            output_type,\n        )\n\n        if any(isinstance(item, Chat) for item in model_input):\n            raise TypeError(\n                \"Batch generation is not available for the `Chat` input type.\"\n            )\n\n        results = self.model.generate(\n            prompts=[self.type_adapter.format_input(item) for item in model_input],\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n        return [[sample.text for sample in batch.outputs] for batch in results]\n\n    def generate_stream(self, model_input, output_type, **inference_kwargs):\n        \"\"\"Not available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM offline integration.\"\n        )\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOffline.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a VLLM model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> model instance.</p> required Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def __init__(self, model: \"LLM\"):\n    \"\"\"Create a VLLM model instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` model instance.\n\n    \"\"\"\n    self.model = model\n    self.type_adapter = VLLMOfflineTypeAdapter()\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOffline.generate","title":"<code>generate(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate text using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate(\n    self,\n    model_input: Chat | str,\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[str, List[str]]:\n    \"\"\"Generate text using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The prompt based on which the model will generate a response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[str, List[str]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if isinstance(model_input, Chat):\n        results = self.model.chat(\n            messages=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    else:\n        results = self.model.generate(\n            prompts=self.type_adapter.format_input(model_input),\n            sampling_params=sampling_params,\n            **inference_kwargs,\n        )\n    results = [completion.text for completion in results[0].outputs]\n\n    if len(results) == 1:\n        return results[0]\n    else:\n        return results\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOffline.generate_batch","title":"<code>generate_batch(model_input, output_type=None, **inference_kwargs)</code>","text":"<p>Generate a batch of completions using vLLM offline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The list of prompts based on which the model will generate a response.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The logits processor the model will use to constrain the format of the generated text.</p> <code>None</code> <code>inference_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>generate</code> method in the <code>vllm.LLM</code> model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>The text generated by the model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_batch(\n    self,\n    model_input: List[Chat | str],\n    output_type: Optional[Any] = None,\n    **inference_kwargs: Any,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"Generate a batch of completions using vLLM offline.\n\n    Parameters\n    ----------\n    prompt\n        The list of prompts based on which the model will generate a\n        response.\n    output_type\n        The logits processor the model will use to constrain the format of\n        the generated text.\n    inference_kwargs\n        Additional keyword arguments to pass to the `generate` method\n        in the `vllm.LLM` model.\n\n    Returns\n    -------\n    Union[List[str], List[List[str]]]\n        The text generated by the model.\n\n    \"\"\"\n    sampling_params = self._build_generation_args(\n        inference_kwargs,\n        output_type,\n    )\n\n    if any(isinstance(item, Chat) for item in model_input):\n        raise TypeError(\n            \"Batch generation is not available for the `Chat` input type.\"\n        )\n\n    results = self.model.generate(\n        prompts=[self.type_adapter.format_input(item) for item in model_input],\n        sampling_params=sampling_params,\n        **inference_kwargs,\n    )\n    return [[sample.text for sample in batch.outputs] for batch in results]\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOffline.generate_stream","title":"<code>generate_stream(model_input, output_type, **inference_kwargs)</code>","text":"<p>Not available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def generate_stream(self, model_input, output_type, **inference_kwargs):\n    \"\"\"Not available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM offline integration.\"\n    )\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter","title":"<code>VLLMOfflineTypeAdapter</code>","text":"<p>               Bases: <code>ModelTypeAdapter</code></p> <p>Type adapter for the <code>VLLMOffline</code> model.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>class VLLMOfflineTypeAdapter(ModelTypeAdapter):\n    \"\"\"Type adapter for the `VLLMOffline` model.\"\"\"\n\n    @singledispatchmethod\n    def format_input(self, model_input):\n        \"\"\"Generate the prompt argument to pass to the model.\n\n        Argument\n        --------\n        model_input\n            The input passed by the user.\n\n        \"\"\"\n        raise TypeError(\n            f\"The input type {type(model_input)} is not available with \"\n            \"VLLM offline. The only available types are `str` and \"\n            \"`Chat` (containing a prompt and images).\"\n        )\n\n    @format_input.register(str)\n    def format_input_str(self, model_input: str) -&gt; str:\n        \"\"\"Format a `str` input.\n\n        \"\"\"\n        return model_input\n\n    @format_input.register(Chat)\n    def format_input_chat(self, model_input: Chat) -&gt; list:\n        \"\"\"Format a `Chat` input.\n\n        \"\"\"\n        for message in model_input.messages:\n            content = message[\"content\"]\n            if isinstance(content, list):\n                raise ValueError(\n                    \"Assets are not supported for vLLM offline.\"\n                    \"Please only use text content in the `Chat` input.\"\n                )\n        return OpenAITypeAdapter().format_input(model_input)\n\n    def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n        \"\"\"Generate the structured output argument to pass to the model.\n\n        For vLLM, the structured output definition is set in the\n        `GuidedDecodingParams` constructor that is provided as a value to the\n        `guided_decoding` parameter of the `SamplingParams` constructor, itself\n        provided as a value to the `sampling_params` parameter of the `generate`\n        method.\n\n        Parameters\n        ----------\n        output_type\n            The structured output type provided.\n\n        Returns\n        -------\n        dict\n            The arguments to provide to the `GuidedDecodingParams` constructor.\n\n        \"\"\"\n        if output_type is None:\n            return {}\n\n        term = python_types_to_terms(output_type)\n        if isinstance(term, CFG):\n            return {\"grammar\": term.definition}\n        elif isinstance(term, JsonSchema):\n            guided_decoding_params = {\"json\": json.loads(term.schema)}\n            if term.whitespace_pattern:\n                guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n            return guided_decoding_params\n        else:\n            return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input","title":"<code>format_input(model_input)</code>","text":"<p>Generate the prompt argument to pass to the model.</p> Argument <p>model_input     The input passed by the user.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@singledispatchmethod\ndef format_input(self, model_input):\n    \"\"\"Generate the prompt argument to pass to the model.\n\n    Argument\n    --------\n    model_input\n        The input passed by the user.\n\n    \"\"\"\n    raise TypeError(\n        f\"The input type {type(model_input)} is not available with \"\n        \"VLLM offline. The only available types are `str` and \"\n        \"`Chat` (containing a prompt and images).\"\n    )\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_chat","title":"<code>format_input_chat(model_input)</code>","text":"<p>Format a <code>Chat</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(Chat)\ndef format_input_chat(self, model_input: Chat) -&gt; list:\n    \"\"\"Format a `Chat` input.\n\n    \"\"\"\n    for message in model_input.messages:\n        content = message[\"content\"]\n        if isinstance(content, list):\n            raise ValueError(\n                \"Assets are not supported for vLLM offline.\"\n                \"Please only use text content in the `Chat` input.\"\n            )\n    return OpenAITypeAdapter().format_input(model_input)\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_input_str","title":"<code>format_input_str(model_input)</code>","text":"<p>Format a <code>str</code> input.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>@format_input.register(str)\ndef format_input_str(self, model_input: str) -&gt; str:\n    \"\"\"Format a `str` input.\n\n    \"\"\"\n    return model_input\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.VLLMOfflineTypeAdapter.format_output_type","title":"<code>format_output_type(output_type=None)</code>","text":"<p>Generate the structured output argument to pass to the model.</p> <p>For vLLM, the structured output definition is set in the <code>GuidedDecodingParams</code> constructor that is provided as a value to the <code>guided_decoding</code> parameter of the <code>SamplingParams</code> constructor, itself provided as a value to the <code>sampling_params</code> parameter of the <code>generate</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>Optional[Any]</code> <p>The structured output type provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The arguments to provide to the <code>GuidedDecodingParams</code> constructor.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def format_output_type(self, output_type: Optional[Any] = None) -&gt; dict:\n    \"\"\"Generate the structured output argument to pass to the model.\n\n    For vLLM, the structured output definition is set in the\n    `GuidedDecodingParams` constructor that is provided as a value to the\n    `guided_decoding` parameter of the `SamplingParams` constructor, itself\n    provided as a value to the `sampling_params` parameter of the `generate`\n    method.\n\n    Parameters\n    ----------\n    output_type\n        The structured output type provided.\n\n    Returns\n    -------\n    dict\n        The arguments to provide to the `GuidedDecodingParams` constructor.\n\n    \"\"\"\n    if output_type is None:\n        return {}\n\n    term = python_types_to_terms(output_type)\n    if isinstance(term, CFG):\n        return {\"grammar\": term.definition}\n    elif isinstance(term, JsonSchema):\n        guided_decoding_params = {\"json\": json.loads(term.schema)}\n        if term.whitespace_pattern:\n            guided_decoding_params[\"whitespace_pattern\"] = term.whitespace_pattern\n        return guided_decoding_params\n    else:\n        return {\"regex\": to_regex(term)}\n</code></pre>"},{"location":"api_reference/models/vllm_offline/#outlines.models.vllm_offline.from_vllm_offline","title":"<code>from_vllm_offline(model)</code>","text":"<p>Create an Outlines <code>VLLMOffline</code> model instance from a <code>vllm.LLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LLM</code> <p>A <code>vllm.LLM</code> instance.</p> required <p>Returns:</p> Type Description <code>VLLMOffline</code> <p>An Outlines <code>VLLMOffline</code> model instance.</p> Source code in <code>outlines/models/vllm_offline.py</code> <pre><code>def from_vllm_offline(model: \"LLM\") -&gt; VLLMOffline:\n    \"\"\"Create an Outlines `VLLMOffline` model instance from a `vllm.LLM`\n    instance.\n\n    Parameters\n    ----------\n    model\n        A `vllm.LLM` instance.\n\n    Returns\n    -------\n    VLLMOffline\n        An Outlines `VLLMOffline` model instance.\n\n    \"\"\"\n    return VLLMOffline(model)\n</code></pre>"},{"location":"api_reference/processors/","title":"processors","text":"<p>Processors to control generation in steerable models.</p>"},{"location":"api_reference/processors/#outlines.processors.OutlinesLogitsProcessor","title":"<code>OutlinesLogitsProcessor</code>","text":"<p>Base class for logits processors. This class implements a shared <code>__call__</code> method is called by the models and returns the processed logits. It relies on the <code>process_logits</code> method that must be implemented by the subclasses to do the actual processing. The <code>tensor_adapter</code> attribute, created at initialization based on the tensor library name specified in the constructor, is used to manipulate the tensors using the appropriate library for the model (numpy, torch...).</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>class OutlinesLogitsProcessor:\n    \"\"\"Base class for logits processors.\n    This class implements a shared `__call__` method is called by the models\n    and returns the processed logits. It relies on the `process_logits` method\n    that must be implemented by the subclasses to do the actual processing. The\n    `tensor_adapter` attribute, created at initialization based on the\n    tensor library name specified in the constructor, is used to manipulate the\n    tensors using the appropriate library for the model (numpy, torch...).\n    \"\"\"\n    tensor_adapter: TensorAdapterImplementation\n\n    def __init__(self, tensor_library_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        tensor_library_name\n            The name of the library to use to manipulate tensors. Possible\n            values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n            that your model is using.\n        \"\"\"\n        # Temporary fix as torch raises a warning that can cause can an error\n        # with python 3.12.\n        if tensor_library_name == \"torch\":\n            import torch._dynamo\n\n            torch._dynamo.config.suppress_errors = True\n\n        tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n        if tensor_adapter_class is None:\n            raise NotImplementedError(\n                f\"Library {tensor_library_name} is not available\"\n            )\n        self.tensor_adapter = tensor_adapter_class()  # type: ignore\n\n    def reset(self):\n        \"\"\"Reset the logits processor for a new generation\n\n        Only implement this method in subclasses if the logits processor\n        needs to be reset for a new generation.\n\n        \"\"\"\n        pass # pragma: no cover\n\n    @abstractmethod\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Main method to implement for logits processors subclasses.\n        This method applies a mask on the logits to bias the generation.\n        It is called by the `__call__` method that standardizes the shape of\n        `input_ids` and `logits` to ensure they are 2D tensors.\n        Elements to keep in mind when designing universal logits processors:\n        - logits processors are only used once and never re-applied for a new\n        sequence generator\n        - Some models only pass output_ids, some models such as llamacpp and\n        transformers prefix with input_ids\n        - Some sampling methods, such as beam search, result in unstable\n        sequence ordering in models like vLLM\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a 2D tensor.\n        logits\n            The logits for the current generation step in a 2D tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a 2D tensor.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Entrypoint for logits processors, this is the method that is\n        called by the model.\n        Because different models use different structures to store the\n        input_ids and logits, we standardize their format to 2D tensors\n        before calling the `process_logits` method. After processing, the\n        logits are cast back to the original array library type before being\n        returned.\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a tensor.\n        logits\n            The logits for the current generation step in a tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a tensor.\n        \"\"\"\n        # if input_ids is 1D and logits is 2D with a single sequence,\n        # reshape input_ids to 2D (needed for mlx-lm)\n        if (\n            len(self.tensor_adapter.shape(input_ids)) == 1\n            and len(self.tensor_adapter.shape(logits)) == 2\n            and self.tensor_adapter.shape(logits)[0] == 1\n        ):\n            input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n        assert (\n            self.tensor_adapter.shape(logits)[:-1]\n            == self.tensor_adapter.shape(input_ids)[:-1]\n        )\n\n        # Guarantee passed as 2D Tensors, then covert back to original\n        # (1D or 2D) shape\n        if len(self.tensor_adapter.shape(logits)) == 2:\n            processed_logits = self.process_logits(input_ids, logits)\n        elif len(self.tensor_adapter.shape(logits)) == 1:\n            processed_logits = self.tensor_adapter.squeeze(\n                self.process_logits(\n                    self.tensor_adapter.unsqueeze(input_ids),\n                    self.tensor_adapter.unsqueeze(logits),\n                ),\n            )\n        else:\n            raise ValueError(\n                f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n                + \"supported\"\n            )\n\n        return processed_logits\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.OutlinesLogitsProcessor.__call__","title":"<code>__call__(input_ids, logits)</code>","text":"<p>Entrypoint for logits processors, this is the method that is called by the model. Because different models use different structures to store the input_ids and logits, we standardize their format to 2D tensors before calling the <code>process_logits</code> method. After processing, the logits are cast back to the original array library type before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __call__(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Entrypoint for logits processors, this is the method that is\n    called by the model.\n    Because different models use different structures to store the\n    input_ids and logits, we standardize their format to 2D tensors\n    before calling the `process_logits` method. After processing, the\n    logits are cast back to the original array library type before being\n    returned.\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a tensor.\n    logits\n        The logits for the current generation step in a tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a tensor.\n    \"\"\"\n    # if input_ids is 1D and logits is 2D with a single sequence,\n    # reshape input_ids to 2D (needed for mlx-lm)\n    if (\n        len(self.tensor_adapter.shape(input_ids)) == 1\n        and len(self.tensor_adapter.shape(logits)) == 2\n        and self.tensor_adapter.shape(logits)[0] == 1\n    ):\n        input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n    assert (\n        self.tensor_adapter.shape(logits)[:-1]\n        == self.tensor_adapter.shape(input_ids)[:-1]\n    )\n\n    # Guarantee passed as 2D Tensors, then covert back to original\n    # (1D or 2D) shape\n    if len(self.tensor_adapter.shape(logits)) == 2:\n        processed_logits = self.process_logits(input_ids, logits)\n    elif len(self.tensor_adapter.shape(logits)) == 1:\n        processed_logits = self.tensor_adapter.squeeze(\n            self.process_logits(\n                self.tensor_adapter.unsqueeze(input_ids),\n                self.tensor_adapter.unsqueeze(logits),\n            ),\n        )\n    else:\n        raise ValueError(\n            f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n            + \"supported\"\n        )\n\n    return processed_logits\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.OutlinesLogitsProcessor.__init__","title":"<code>__init__(tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor_library_name</code> <code>str</code> <p>The name of the library to use to manipulate tensors. Possible values are \"mlx\", \"numpy\" and \"torch\". You must choose the library that your model is using.</p> required Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __init__(self, tensor_library_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    tensor_library_name\n        The name of the library to use to manipulate tensors. Possible\n        values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n        that your model is using.\n    \"\"\"\n    # Temporary fix as torch raises a warning that can cause can an error\n    # with python 3.12.\n    if tensor_library_name == \"torch\":\n        import torch._dynamo\n\n        torch._dynamo.config.suppress_errors = True\n\n    tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n    if tensor_adapter_class is None:\n        raise NotImplementedError(\n            f\"Library {tensor_library_name} is not available\"\n        )\n    self.tensor_adapter = tensor_adapter_class()  # type: ignore\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.OutlinesLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>  <code>abstractmethod</code>","text":"<p>Main method to implement for logits processors subclasses. This method applies a mask on the logits to bias the generation. It is called by the <code>__call__</code> method that standardizes the shape of <code>input_ids</code> and <code>logits</code> to ensure they are 2D tensors. Elements to keep in mind when designing universal logits processors: - logits processors are only used once and never re-applied for a new sequence generator - Some models only pass output_ids, some models such as llamacpp and transformers prefix with input_ids - Some sampling methods, such as beam search, result in unstable sequence ordering in models like vLLM</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a 2D tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a 2D tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a 2D tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>@abstractmethod\ndef process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Main method to implement for logits processors subclasses.\n    This method applies a mask on the logits to bias the generation.\n    It is called by the `__call__` method that standardizes the shape of\n    `input_ids` and `logits` to ensure they are 2D tensors.\n    Elements to keep in mind when designing universal logits processors:\n    - logits processors are only used once and never re-applied for a new\n    sequence generator\n    - Some models only pass output_ids, some models such as llamacpp and\n    transformers prefix with input_ids\n    - Some sampling methods, such as beam search, result in unstable\n    sequence ordering in models like vLLM\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a 2D tensor.\n    logits\n        The logits for the current generation step in a 2D tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a 2D tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.OutlinesLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor for a new generation</p> <p>Only implement this method in subclasses if the logits processor needs to be reset for a new generation.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the logits processor for a new generation\n\n    Only implement this method in subclasses if the logits processor\n    needs to be reset for a new generation.\n\n    \"\"\"\n    pass # pragma: no cover\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor","title":"<code>base_logits_processor</code>","text":"<p>Base class for logits processors.</p>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor","title":"<code>OutlinesLogitsProcessor</code>","text":"<p>Base class for logits processors. This class implements a shared <code>__call__</code> method is called by the models and returns the processed logits. It relies on the <code>process_logits</code> method that must be implemented by the subclasses to do the actual processing. The <code>tensor_adapter</code> attribute, created at initialization based on the tensor library name specified in the constructor, is used to manipulate the tensors using the appropriate library for the model (numpy, torch...).</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>class OutlinesLogitsProcessor:\n    \"\"\"Base class for logits processors.\n    This class implements a shared `__call__` method is called by the models\n    and returns the processed logits. It relies on the `process_logits` method\n    that must be implemented by the subclasses to do the actual processing. The\n    `tensor_adapter` attribute, created at initialization based on the\n    tensor library name specified in the constructor, is used to manipulate the\n    tensors using the appropriate library for the model (numpy, torch...).\n    \"\"\"\n    tensor_adapter: TensorAdapterImplementation\n\n    def __init__(self, tensor_library_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        tensor_library_name\n            The name of the library to use to manipulate tensors. Possible\n            values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n            that your model is using.\n        \"\"\"\n        # Temporary fix as torch raises a warning that can cause can an error\n        # with python 3.12.\n        if tensor_library_name == \"torch\":\n            import torch._dynamo\n\n            torch._dynamo.config.suppress_errors = True\n\n        tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n        if tensor_adapter_class is None:\n            raise NotImplementedError(\n                f\"Library {tensor_library_name} is not available\"\n            )\n        self.tensor_adapter = tensor_adapter_class()  # type: ignore\n\n    def reset(self):\n        \"\"\"Reset the logits processor for a new generation\n\n        Only implement this method in subclasses if the logits processor\n        needs to be reset for a new generation.\n\n        \"\"\"\n        pass # pragma: no cover\n\n    @abstractmethod\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Main method to implement for logits processors subclasses.\n        This method applies a mask on the logits to bias the generation.\n        It is called by the `__call__` method that standardizes the shape of\n        `input_ids` and `logits` to ensure they are 2D tensors.\n        Elements to keep in mind when designing universal logits processors:\n        - logits processors are only used once and never re-applied for a new\n        sequence generator\n        - Some models only pass output_ids, some models such as llamacpp and\n        transformers prefix with input_ids\n        - Some sampling methods, such as beam search, result in unstable\n        sequence ordering in models like vLLM\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a 2D tensor.\n        logits\n            The logits for the current generation step in a 2D tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a 2D tensor.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Entrypoint for logits processors, this is the method that is\n        called by the model.\n        Because different models use different structures to store the\n        input_ids and logits, we standardize their format to 2D tensors\n        before calling the `process_logits` method. After processing, the\n        logits are cast back to the original array library type before being\n        returned.\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a tensor.\n        logits\n            The logits for the current generation step in a tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a tensor.\n        \"\"\"\n        # if input_ids is 1D and logits is 2D with a single sequence,\n        # reshape input_ids to 2D (needed for mlx-lm)\n        if (\n            len(self.tensor_adapter.shape(input_ids)) == 1\n            and len(self.tensor_adapter.shape(logits)) == 2\n            and self.tensor_adapter.shape(logits)[0] == 1\n        ):\n            input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n        assert (\n            self.tensor_adapter.shape(logits)[:-1]\n            == self.tensor_adapter.shape(input_ids)[:-1]\n        )\n\n        # Guarantee passed as 2D Tensors, then covert back to original\n        # (1D or 2D) shape\n        if len(self.tensor_adapter.shape(logits)) == 2:\n            processed_logits = self.process_logits(input_ids, logits)\n        elif len(self.tensor_adapter.shape(logits)) == 1:\n            processed_logits = self.tensor_adapter.squeeze(\n                self.process_logits(\n                    self.tensor_adapter.unsqueeze(input_ids),\n                    self.tensor_adapter.unsqueeze(logits),\n                ),\n            )\n        else:\n            raise ValueError(\n                f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n                + \"supported\"\n            )\n\n        return processed_logits\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__call__","title":"<code>__call__(input_ids, logits)</code>","text":"<p>Entrypoint for logits processors, this is the method that is called by the model. Because different models use different structures to store the input_ids and logits, we standardize their format to 2D tensors before calling the <code>process_logits</code> method. After processing, the logits are cast back to the original array library type before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __call__(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Entrypoint for logits processors, this is the method that is\n    called by the model.\n    Because different models use different structures to store the\n    input_ids and logits, we standardize their format to 2D tensors\n    before calling the `process_logits` method. After processing, the\n    logits are cast back to the original array library type before being\n    returned.\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a tensor.\n    logits\n        The logits for the current generation step in a tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a tensor.\n    \"\"\"\n    # if input_ids is 1D and logits is 2D with a single sequence,\n    # reshape input_ids to 2D (needed for mlx-lm)\n    if (\n        len(self.tensor_adapter.shape(input_ids)) == 1\n        and len(self.tensor_adapter.shape(logits)) == 2\n        and self.tensor_adapter.shape(logits)[0] == 1\n    ):\n        input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n    assert (\n        self.tensor_adapter.shape(logits)[:-1]\n        == self.tensor_adapter.shape(input_ids)[:-1]\n    )\n\n    # Guarantee passed as 2D Tensors, then covert back to original\n    # (1D or 2D) shape\n    if len(self.tensor_adapter.shape(logits)) == 2:\n        processed_logits = self.process_logits(input_ids, logits)\n    elif len(self.tensor_adapter.shape(logits)) == 1:\n        processed_logits = self.tensor_adapter.squeeze(\n            self.process_logits(\n                self.tensor_adapter.unsqueeze(input_ids),\n                self.tensor_adapter.unsqueeze(logits),\n            ),\n        )\n    else:\n        raise ValueError(\n            f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n            + \"supported\"\n        )\n\n    return processed_logits\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__init__","title":"<code>__init__(tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor_library_name</code> <code>str</code> <p>The name of the library to use to manipulate tensors. Possible values are \"mlx\", \"numpy\" and \"torch\". You must choose the library that your model is using.</p> required Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __init__(self, tensor_library_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    tensor_library_name\n        The name of the library to use to manipulate tensors. Possible\n        values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n        that your model is using.\n    \"\"\"\n    # Temporary fix as torch raises a warning that can cause can an error\n    # with python 3.12.\n    if tensor_library_name == \"torch\":\n        import torch._dynamo\n\n        torch._dynamo.config.suppress_errors = True\n\n    tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n    if tensor_adapter_class is None:\n        raise NotImplementedError(\n            f\"Library {tensor_library_name} is not available\"\n        )\n    self.tensor_adapter = tensor_adapter_class()  # type: ignore\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>  <code>abstractmethod</code>","text":"<p>Main method to implement for logits processors subclasses. This method applies a mask on the logits to bias the generation. It is called by the <code>__call__</code> method that standardizes the shape of <code>input_ids</code> and <code>logits</code> to ensure they are 2D tensors. Elements to keep in mind when designing universal logits processors: - logits processors are only used once and never re-applied for a new sequence generator - Some models only pass output_ids, some models such as llamacpp and transformers prefix with input_ids - Some sampling methods, such as beam search, result in unstable sequence ordering in models like vLLM</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a 2D tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a 2D tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a 2D tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>@abstractmethod\ndef process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Main method to implement for logits processors subclasses.\n    This method applies a mask on the logits to bias the generation.\n    It is called by the `__call__` method that standardizes the shape of\n    `input_ids` and `logits` to ensure they are 2D tensors.\n    Elements to keep in mind when designing universal logits processors:\n    - logits processors are only used once and never re-applied for a new\n    sequence generator\n    - Some models only pass output_ids, some models such as llamacpp and\n    transformers prefix with input_ids\n    - Some sampling methods, such as beam search, result in unstable\n    sequence ordering in models like vLLM\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a 2D tensor.\n    logits\n        The logits for the current generation step in a 2D tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a 2D tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor for a new generation</p> <p>Only implement this method in subclasses if the logits processor needs to be reset for a new generation.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the logits processor for a new generation\n\n    Only implement this method in subclasses if the logits processor\n    needs to be reset for a new generation.\n\n    \"\"\"\n    pass # pragma: no cover\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters","title":"<code>tensor_adapters</code>","text":"<p>Library specific objects to manipulate tensors.</p>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base","title":"<code>base</code>","text":"<p>Base class for tensor adapters.</p>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter","title":"<code>TensorAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tensor adapters.</p> <p>This class defines the interface for tensor adapters that are used to manipulate tensors in different libraries. Concrete implementations of this class should provide specific implementations for each method as well as providing a <code>library_name</code> attribute.</p> <p>TODO: Update the version of outlines-core used to receive plain arrays instead of torch tensors. In the meantime, implementations of this class must make sure that their <code>full_like</code> and <code>concatenate</code> methods can handle torch tensors.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>class TensorAdapter(ABC):\n    \"\"\"Abstract base class for tensor adapters.\n\n    This class defines the interface for tensor adapters that are used to\n    manipulate tensors in different libraries. Concrete implementations of\n    this class should provide specific implementations for each method as\n    well as providing a `library_name` attribute.\n\n    TODO: Update the version of outlines-core used to receive plain arrays\n    instead of torch tensors. In the meantime, implementations of this class\n    must make sure that their `full_like` and `concatenate` methods can\n    handle torch tensors.\n\n    \"\"\"\n    library_name: str\n\n    @abstractmethod\n    def shape(self, tensor: TensorType) -&gt; list[int]:\n        \"\"\"Get the shape of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the shape of.\n\n        Returns\n        -------\n        list[int]\n            The shape of the tensor. The list contains as many elements as\n            there are dimensions in the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Add a dimension to the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to add a dimension to.\n\n        Returns\n        -------\n        TensorType\n            The tensor with an additional dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def squeeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Remove a dimension from the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to remove a dimension from.\n\n        Returns\n        -------\n        TensorType\n            The tensor with one less dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_list(self, tensor: TensorType) -&gt; list:\n        \"\"\"Convert the tensor to a list.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to convert to a list.\n\n        Returns\n        -------\n        list\n            The tensor as a list.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_scalar(self, tensor: TensorType) -&gt; Any:\n        \"\"\"Return the only element of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to return the only element of.\n\n        Returns\n        -------\n        Any\n            The only element of the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n        \"\"\"Create a tensor with the same shape as the input tensor filled\n        with a scalar value.\n\n        ATTENTION: This method receives a torch tensor regardless of the\n        library used.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a new tensor with the same shape.\n        fill_value\n            The value to fill the new tensor with.\n\n        Returns\n        -------\n        TensorType\n            A tensor with the same shape as the input tensor filled with the\n            specified value.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def concatenate(\n        self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n    ) -&gt; TensorType:\n        \"\"\"Concatenate a list of tensors along axis 0.\n\n        ATTENTION: This method can either receive a list of torch tensors or\n        a list of tensors from the library used.\n\n        Parameters\n        ----------\n        tensors\n            The list of tensors to concatenate.\n\n        Returns\n        -------\n        TensorType\n            The concatenated tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_device(self, tensor: TensorType) -&gt; str:\n        \"\"\"Get the name of the tensor's device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the device of.\n\n        Returns\n        -------\n        str\n            The name of the tensor's device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n        \"\"\"Move the tensor to a specified device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to move to a specified device.\n        device\n            The name of the device to move the tensor to.\n\n        Returns\n        -------\n        TensorType\n            The tensor moved to the specified device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Create a boolean ones tensor with the same shape as the input\n        tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a boolean ones tensor with the same shape.\n\n        Returns\n        -------\n        TensorType\n            A boolean ones tensor with the same shape as the input tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply_mask(\n        self, tensor: TensorType, mask: TensorType, value: Any\n    ) -&gt; TensorType:\n        \"\"\"Fill the elements of the tensor where the mask is True with the\n        specified value.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to fill.\n        mask\n            The mask to apply to the tensor.\n        value\n            The value to fill the tensor with.\n\n        Returns\n        -------\n        TensorType\n            The tensor with the mask applied.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def argsort_descending(\n        self, tensor: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Return the indices that would sort the tensor in descending order\n        along axis -1.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to sort.\n\n        Returns\n        -------\n        TensorType\n            The indices that would sort the tensor in descending order along\n            axis -1.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.apply_mask","title":"<code>apply_mask(tensor, mask, value)</code>  <code>abstractmethod</code>","text":"<p>Fill the elements of the tensor where the mask is True with the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to fill.</p> required <code>mask</code> <code>TensorType</code> <p>The mask to apply to the tensor.</p> required <code>value</code> <code>Any</code> <p>The value to fill the tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with the mask applied.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef apply_mask(\n    self, tensor: TensorType, mask: TensorType, value: Any\n) -&gt; TensorType:\n    \"\"\"Fill the elements of the tensor where the mask is True with the\n    specified value.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to fill.\n    mask\n        The mask to apply to the tensor.\n    value\n        The value to fill the tensor with.\n\n    Returns\n    -------\n    TensorType\n        The tensor with the mask applied.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.argsort_descending","title":"<code>argsort_descending(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the indices that would sort the tensor in descending order along axis -1.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to sort.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The indices that would sort the tensor in descending order along axis -1.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef argsort_descending(\n    self, tensor: TensorType\n) -&gt; TensorType:\n    \"\"\"Return the indices that would sort the tensor in descending order\n    along axis -1.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to sort.\n\n    Returns\n    -------\n    TensorType\n        The indices that would sort the tensor in descending order along\n        axis -1.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.boolean_ones_like","title":"<code>boolean_ones_like(tensor)</code>  <code>abstractmethod</code>","text":"<p>Create a boolean ones tensor with the same shape as the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to create a boolean ones tensor with the same shape.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A boolean ones tensor with the same shape as the input tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Create a boolean ones tensor with the same shape as the input\n    tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a boolean ones tensor with the same shape.\n\n    Returns\n    -------\n    TensorType\n        A boolean ones tensor with the same shape as the input tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.concatenate","title":"<code>concatenate(tensors)</code>  <code>abstractmethod</code>","text":"<p>Concatenate a list of tensors along axis 0.</p> <p>ATTENTION: This method can either receive a list of torch tensors or a list of tensors from the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Union[Tensor, TensorType]]</code> <p>The list of tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The concatenated tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef concatenate(\n    self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n) -&gt; TensorType:\n    \"\"\"Concatenate a list of tensors along axis 0.\n\n    ATTENTION: This method can either receive a list of torch tensors or\n    a list of tensors from the library used.\n\n    Parameters\n    ----------\n    tensors\n        The list of tensors to concatenate.\n\n    Returns\n    -------\n    TensorType\n        The concatenated tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.full_like","title":"<code>full_like(tensor, fill_value)</code>  <code>abstractmethod</code>","text":"<p>Create a tensor with the same shape as the input tensor filled with a scalar value.</p> <p>ATTENTION: This method receives a torch tensor regardless of the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to create a new tensor with the same shape.</p> required <code>fill_value</code> <code>Any</code> <p>The value to fill the new tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A tensor with the same shape as the input tensor filled with the specified value.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n    \"\"\"Create a tensor with the same shape as the input tensor filled\n    with a scalar value.\n\n    ATTENTION: This method receives a torch tensor regardless of the\n    library used.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a new tensor with the same shape.\n    fill_value\n        The value to fill the new tensor with.\n\n    Returns\n    -------\n    TensorType\n        A tensor with the same shape as the input tensor filled with the\n        specified value.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.get_device","title":"<code>get_device(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the name of the tensor's device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the device of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the tensor's device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef get_device(self, tensor: TensorType) -&gt; str:\n    \"\"\"Get the name of the tensor's device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the device of.\n\n    Returns\n    -------\n    str\n        The name of the tensor's device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.shape","title":"<code>shape(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the shape of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the shape of.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The shape of the tensor. The list contains as many elements as there are dimensions in the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef shape(self, tensor: TensorType) -&gt; list[int]:\n    \"\"\"Get the shape of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the shape of.\n\n    Returns\n    -------\n    list[int]\n        The shape of the tensor. The list contains as many elements as\n        there are dimensions in the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.squeeze","title":"<code>squeeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Remove a dimension from the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to remove a dimension from.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with one less dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef squeeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Remove a dimension from the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to remove a dimension from.\n\n    Returns\n    -------\n    TensorType\n        The tensor with one less dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.to_device","title":"<code>to_device(tensor, device)</code>  <code>abstractmethod</code>","text":"<p>Move the tensor to a specified device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to move to a specified device.</p> required <code>device</code> <code>str</code> <p>The name of the device to move the tensor to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor moved to the specified device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n    \"\"\"Move the tensor to a specified device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to move to a specified device.\n    device\n        The name of the device to move the tensor to.\n\n    Returns\n    -------\n    TensorType\n        The tensor moved to the specified device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.to_list","title":"<code>to_list(tensor)</code>  <code>abstractmethod</code>","text":"<p>Convert the tensor to a list.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to convert to a list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The tensor as a list.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_list(self, tensor: TensorType) -&gt; list:\n    \"\"\"Convert the tensor to a list.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to convert to a list.\n\n    Returns\n    -------\n    list\n        The tensor as a list.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.to_scalar","title":"<code>to_scalar(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the only element of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to return the only element of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The only element of the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_scalar(self, tensor: TensorType) -&gt; Any:\n    \"\"\"Return the only element of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to return the only element of.\n\n    Returns\n    -------\n    Any\n        The only element of the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.base.TensorAdapter.unsqueeze","title":"<code>unsqueeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Add a dimension to the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to add a dimension to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with an additional dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Add a dimension to the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to add a dimension to.\n\n    Returns\n    -------\n    TensorType\n        The tensor with an additional dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.mlx","title":"<code>mlx</code>","text":"<p>Tensor adapter for the <code>mlx</code> library.</p>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.numpy","title":"<code>numpy</code>","text":"<p>Tensor adapter for the <code>numpy</code> library.</p>"},{"location":"api_reference/processors/#outlines.processors.tensor_adapters.torch","title":"<code>torch</code>","text":"<p>Tensor adapter for the <code>torch</code> library.</p>"},{"location":"api_reference/processors/base_logits_processor/","title":"base_logits_processor","text":"<p>Base class for logits processors.</p>"},{"location":"api_reference/processors/base_logits_processor/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor","title":"<code>OutlinesLogitsProcessor</code>","text":"<p>Base class for logits processors. This class implements a shared <code>__call__</code> method is called by the models and returns the processed logits. It relies on the <code>process_logits</code> method that must be implemented by the subclasses to do the actual processing. The <code>tensor_adapter</code> attribute, created at initialization based on the tensor library name specified in the constructor, is used to manipulate the tensors using the appropriate library for the model (numpy, torch...).</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>class OutlinesLogitsProcessor:\n    \"\"\"Base class for logits processors.\n    This class implements a shared `__call__` method is called by the models\n    and returns the processed logits. It relies on the `process_logits` method\n    that must be implemented by the subclasses to do the actual processing. The\n    `tensor_adapter` attribute, created at initialization based on the\n    tensor library name specified in the constructor, is used to manipulate the\n    tensors using the appropriate library for the model (numpy, torch...).\n    \"\"\"\n    tensor_adapter: TensorAdapterImplementation\n\n    def __init__(self, tensor_library_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        tensor_library_name\n            The name of the library to use to manipulate tensors. Possible\n            values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n            that your model is using.\n        \"\"\"\n        # Temporary fix as torch raises a warning that can cause can an error\n        # with python 3.12.\n        if tensor_library_name == \"torch\":\n            import torch._dynamo\n\n            torch._dynamo.config.suppress_errors = True\n\n        tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n        if tensor_adapter_class is None:\n            raise NotImplementedError(\n                f\"Library {tensor_library_name} is not available\"\n            )\n        self.tensor_adapter = tensor_adapter_class()  # type: ignore\n\n    def reset(self):\n        \"\"\"Reset the logits processor for a new generation\n\n        Only implement this method in subclasses if the logits processor\n        needs to be reset for a new generation.\n\n        \"\"\"\n        pass # pragma: no cover\n\n    @abstractmethod\n    def process_logits(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Main method to implement for logits processors subclasses.\n        This method applies a mask on the logits to bias the generation.\n        It is called by the `__call__` method that standardizes the shape of\n        `input_ids` and `logits` to ensure they are 2D tensors.\n        Elements to keep in mind when designing universal logits processors:\n        - logits processors are only used once and never re-applied for a new\n        sequence generator\n        - Some models only pass output_ids, some models such as llamacpp and\n        transformers prefix with input_ids\n        - Some sampling methods, such as beam search, result in unstable\n        sequence ordering in models like vLLM\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a 2D tensor.\n        logits\n            The logits for the current generation step in a 2D tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a 2D tensor.\n        \"\"\"\n        ...\n\n    def __call__(\n        self, input_ids: TensorType, logits: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Entrypoint for logits processors, this is the method that is\n        called by the model.\n        Because different models use different structures to store the\n        input_ids and logits, we standardize their format to 2D tensors\n        before calling the `process_logits` method. After processing, the\n        logits are cast back to the original array library type before being\n        returned.\n        Parameters\n        ----------\n        input_ids\n            The ids of the tokens of the existing sequences in a tensor.\n        logits\n            The logits for the current generation step in a tensor.\n        Returns\n        -------\n        TensorType\n            The processed logits as a tensor.\n        \"\"\"\n        # if input_ids is 1D and logits is 2D with a single sequence,\n        # reshape input_ids to 2D (needed for mlx-lm)\n        if (\n            len(self.tensor_adapter.shape(input_ids)) == 1\n            and len(self.tensor_adapter.shape(logits)) == 2\n            and self.tensor_adapter.shape(logits)[0] == 1\n        ):\n            input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n        assert (\n            self.tensor_adapter.shape(logits)[:-1]\n            == self.tensor_adapter.shape(input_ids)[:-1]\n        )\n\n        # Guarantee passed as 2D Tensors, then covert back to original\n        # (1D or 2D) shape\n        if len(self.tensor_adapter.shape(logits)) == 2:\n            processed_logits = self.process_logits(input_ids, logits)\n        elif len(self.tensor_adapter.shape(logits)) == 1:\n            processed_logits = self.tensor_adapter.squeeze(\n                self.process_logits(\n                    self.tensor_adapter.unsqueeze(input_ids),\n                    self.tensor_adapter.unsqueeze(logits),\n                ),\n            )\n        else:\n            raise ValueError(\n                f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n                + \"supported\"\n            )\n\n        return processed_logits\n</code></pre>"},{"location":"api_reference/processors/base_logits_processor/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__call__","title":"<code>__call__(input_ids, logits)</code>","text":"<p>Entrypoint for logits processors, this is the method that is called by the model. Because different models use different structures to store the input_ids and logits, we standardize their format to 2D tensors before calling the <code>process_logits</code> method. After processing, the logits are cast back to the original array library type before being returned.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __call__(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Entrypoint for logits processors, this is the method that is\n    called by the model.\n    Because different models use different structures to store the\n    input_ids and logits, we standardize their format to 2D tensors\n    before calling the `process_logits` method. After processing, the\n    logits are cast back to the original array library type before being\n    returned.\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a tensor.\n    logits\n        The logits for the current generation step in a tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a tensor.\n    \"\"\"\n    # if input_ids is 1D and logits is 2D with a single sequence,\n    # reshape input_ids to 2D (needed for mlx-lm)\n    if (\n        len(self.tensor_adapter.shape(input_ids)) == 1\n        and len(self.tensor_adapter.shape(logits)) == 2\n        and self.tensor_adapter.shape(logits)[0] == 1\n    ):\n        input_ids = self.tensor_adapter.unsqueeze(input_ids)\n\n    assert (\n        self.tensor_adapter.shape(logits)[:-1]\n        == self.tensor_adapter.shape(input_ids)[:-1]\n    )\n\n    # Guarantee passed as 2D Tensors, then covert back to original\n    # (1D or 2D) shape\n    if len(self.tensor_adapter.shape(logits)) == 2:\n        processed_logits = self.process_logits(input_ids, logits)\n    elif len(self.tensor_adapter.shape(logits)) == 1:\n        processed_logits = self.tensor_adapter.squeeze(\n            self.process_logits(\n                self.tensor_adapter.unsqueeze(input_ids),\n                self.tensor_adapter.unsqueeze(logits),\n            ),\n        )\n    else:\n        raise ValueError(\n            f\"Logits shape {self.tensor_adapter.shape(logits)} is not \"\n            + \"supported\"\n        )\n\n    return processed_logits\n</code></pre>"},{"location":"api_reference/processors/base_logits_processor/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.__init__","title":"<code>__init__(tensor_library_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor_library_name</code> <code>str</code> <p>The name of the library to use to manipulate tensors. Possible values are \"mlx\", \"numpy\" and \"torch\". You must choose the library that your model is using.</p> required Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def __init__(self, tensor_library_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    tensor_library_name\n        The name of the library to use to manipulate tensors. Possible\n        values are \"mlx\", \"numpy\" and \"torch\". You must choose the library\n        that your model is using.\n    \"\"\"\n    # Temporary fix as torch raises a warning that can cause can an error\n    # with python 3.12.\n    if tensor_library_name == \"torch\":\n        import torch._dynamo\n\n        torch._dynamo.config.suppress_errors = True\n\n    tensor_adapter_class = tensor_adapters.get(tensor_library_name)\n    if tensor_adapter_class is None:\n        raise NotImplementedError(\n            f\"Library {tensor_library_name} is not available\"\n        )\n    self.tensor_adapter = tensor_adapter_class()  # type: ignore\n</code></pre>"},{"location":"api_reference/processors/base_logits_processor/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.process_logits","title":"<code>process_logits(input_ids, logits)</code>  <code>abstractmethod</code>","text":"<p>Main method to implement for logits processors subclasses. This method applies a mask on the logits to bias the generation. It is called by the <code>__call__</code> method that standardizes the shape of <code>input_ids</code> and <code>logits</code> to ensure they are 2D tensors. Elements to keep in mind when designing universal logits processors: - logits processors are only used once and never re-applied for a new sequence generator - Some models only pass output_ids, some models such as llamacpp and transformers prefix with input_ids - Some sampling methods, such as beam search, result in unstable sequence ordering in models like vLLM</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>TensorType</code> <p>The ids of the tokens of the existing sequences in a 2D tensor.</p> required <code>logits</code> <code>TensorType</code> <p>The logits for the current generation step in a 2D tensor.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The processed logits as a 2D tensor.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>@abstractmethod\ndef process_logits(\n    self, input_ids: TensorType, logits: TensorType\n) -&gt; TensorType:\n    \"\"\"Main method to implement for logits processors subclasses.\n    This method applies a mask on the logits to bias the generation.\n    It is called by the `__call__` method that standardizes the shape of\n    `input_ids` and `logits` to ensure they are 2D tensors.\n    Elements to keep in mind when designing universal logits processors:\n    - logits processors are only used once and never re-applied for a new\n    sequence generator\n    - Some models only pass output_ids, some models such as llamacpp and\n    transformers prefix with input_ids\n    - Some sampling methods, such as beam search, result in unstable\n    sequence ordering in models like vLLM\n    Parameters\n    ----------\n    input_ids\n        The ids of the tokens of the existing sequences in a 2D tensor.\n    logits\n        The logits for the current generation step in a 2D tensor.\n    Returns\n    -------\n    TensorType\n        The processed logits as a 2D tensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/base_logits_processor/#outlines.processors.base_logits_processor.OutlinesLogitsProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset the logits processor for a new generation</p> <p>Only implement this method in subclasses if the logits processor needs to be reset for a new generation.</p> Source code in <code>outlines/processors/base_logits_processor.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the logits processor for a new generation\n\n    Only implement this method in subclasses if the logits processor\n    needs to be reset for a new generation.\n\n    \"\"\"\n    pass # pragma: no cover\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/","title":"tensor_adapters","text":"<p>Library specific objects to manipulate tensors.</p>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base","title":"<code>base</code>","text":"<p>Base class for tensor adapters.</p>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter","title":"<code>TensorAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tensor adapters.</p> <p>This class defines the interface for tensor adapters that are used to manipulate tensors in different libraries. Concrete implementations of this class should provide specific implementations for each method as well as providing a <code>library_name</code> attribute.</p> <p>TODO: Update the version of outlines-core used to receive plain arrays instead of torch tensors. In the meantime, implementations of this class must make sure that their <code>full_like</code> and <code>concatenate</code> methods can handle torch tensors.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>class TensorAdapter(ABC):\n    \"\"\"Abstract base class for tensor adapters.\n\n    This class defines the interface for tensor adapters that are used to\n    manipulate tensors in different libraries. Concrete implementations of\n    this class should provide specific implementations for each method as\n    well as providing a `library_name` attribute.\n\n    TODO: Update the version of outlines-core used to receive plain arrays\n    instead of torch tensors. In the meantime, implementations of this class\n    must make sure that their `full_like` and `concatenate` methods can\n    handle torch tensors.\n\n    \"\"\"\n    library_name: str\n\n    @abstractmethod\n    def shape(self, tensor: TensorType) -&gt; list[int]:\n        \"\"\"Get the shape of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the shape of.\n\n        Returns\n        -------\n        list[int]\n            The shape of the tensor. The list contains as many elements as\n            there are dimensions in the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Add a dimension to the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to add a dimension to.\n\n        Returns\n        -------\n        TensorType\n            The tensor with an additional dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def squeeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Remove a dimension from the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to remove a dimension from.\n\n        Returns\n        -------\n        TensorType\n            The tensor with one less dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_list(self, tensor: TensorType) -&gt; list:\n        \"\"\"Convert the tensor to a list.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to convert to a list.\n\n        Returns\n        -------\n        list\n            The tensor as a list.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_scalar(self, tensor: TensorType) -&gt; Any:\n        \"\"\"Return the only element of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to return the only element of.\n\n        Returns\n        -------\n        Any\n            The only element of the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n        \"\"\"Create a tensor with the same shape as the input tensor filled\n        with a scalar value.\n\n        ATTENTION: This method receives a torch tensor regardless of the\n        library used.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a new tensor with the same shape.\n        fill_value\n            The value to fill the new tensor with.\n\n        Returns\n        -------\n        TensorType\n            A tensor with the same shape as the input tensor filled with the\n            specified value.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def concatenate(\n        self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n    ) -&gt; TensorType:\n        \"\"\"Concatenate a list of tensors along axis 0.\n\n        ATTENTION: This method can either receive a list of torch tensors or\n        a list of tensors from the library used.\n\n        Parameters\n        ----------\n        tensors\n            The list of tensors to concatenate.\n\n        Returns\n        -------\n        TensorType\n            The concatenated tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_device(self, tensor: TensorType) -&gt; str:\n        \"\"\"Get the name of the tensor's device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the device of.\n\n        Returns\n        -------\n        str\n            The name of the tensor's device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n        \"\"\"Move the tensor to a specified device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to move to a specified device.\n        device\n            The name of the device to move the tensor to.\n\n        Returns\n        -------\n        TensorType\n            The tensor moved to the specified device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Create a boolean ones tensor with the same shape as the input\n        tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a boolean ones tensor with the same shape.\n\n        Returns\n        -------\n        TensorType\n            A boolean ones tensor with the same shape as the input tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply_mask(\n        self, tensor: TensorType, mask: TensorType, value: Any\n    ) -&gt; TensorType:\n        \"\"\"Fill the elements of the tensor where the mask is True with the\n        specified value.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to fill.\n        mask\n            The mask to apply to the tensor.\n        value\n            The value to fill the tensor with.\n\n        Returns\n        -------\n        TensorType\n            The tensor with the mask applied.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def argsort_descending(\n        self, tensor: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Return the indices that would sort the tensor in descending order\n        along axis -1.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to sort.\n\n        Returns\n        -------\n        TensorType\n            The indices that would sort the tensor in descending order along\n            axis -1.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.apply_mask","title":"<code>apply_mask(tensor, mask, value)</code>  <code>abstractmethod</code>","text":"<p>Fill the elements of the tensor where the mask is True with the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to fill.</p> required <code>mask</code> <code>TensorType</code> <p>The mask to apply to the tensor.</p> required <code>value</code> <code>Any</code> <p>The value to fill the tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with the mask applied.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef apply_mask(\n    self, tensor: TensorType, mask: TensorType, value: Any\n) -&gt; TensorType:\n    \"\"\"Fill the elements of the tensor where the mask is True with the\n    specified value.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to fill.\n    mask\n        The mask to apply to the tensor.\n    value\n        The value to fill the tensor with.\n\n    Returns\n    -------\n    TensorType\n        The tensor with the mask applied.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.argsort_descending","title":"<code>argsort_descending(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the indices that would sort the tensor in descending order along axis -1.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to sort.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The indices that would sort the tensor in descending order along axis -1.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef argsort_descending(\n    self, tensor: TensorType\n) -&gt; TensorType:\n    \"\"\"Return the indices that would sort the tensor in descending order\n    along axis -1.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to sort.\n\n    Returns\n    -------\n    TensorType\n        The indices that would sort the tensor in descending order along\n        axis -1.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.boolean_ones_like","title":"<code>boolean_ones_like(tensor)</code>  <code>abstractmethod</code>","text":"<p>Create a boolean ones tensor with the same shape as the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to create a boolean ones tensor with the same shape.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A boolean ones tensor with the same shape as the input tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Create a boolean ones tensor with the same shape as the input\n    tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a boolean ones tensor with the same shape.\n\n    Returns\n    -------\n    TensorType\n        A boolean ones tensor with the same shape as the input tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.concatenate","title":"<code>concatenate(tensors)</code>  <code>abstractmethod</code>","text":"<p>Concatenate a list of tensors along axis 0.</p> <p>ATTENTION: This method can either receive a list of torch tensors or a list of tensors from the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Union[Tensor, TensorType]]</code> <p>The list of tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The concatenated tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef concatenate(\n    self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n) -&gt; TensorType:\n    \"\"\"Concatenate a list of tensors along axis 0.\n\n    ATTENTION: This method can either receive a list of torch tensors or\n    a list of tensors from the library used.\n\n    Parameters\n    ----------\n    tensors\n        The list of tensors to concatenate.\n\n    Returns\n    -------\n    TensorType\n        The concatenated tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.full_like","title":"<code>full_like(tensor, fill_value)</code>  <code>abstractmethod</code>","text":"<p>Create a tensor with the same shape as the input tensor filled with a scalar value.</p> <p>ATTENTION: This method receives a torch tensor regardless of the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to create a new tensor with the same shape.</p> required <code>fill_value</code> <code>Any</code> <p>The value to fill the new tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A tensor with the same shape as the input tensor filled with the specified value.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n    \"\"\"Create a tensor with the same shape as the input tensor filled\n    with a scalar value.\n\n    ATTENTION: This method receives a torch tensor regardless of the\n    library used.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a new tensor with the same shape.\n    fill_value\n        The value to fill the new tensor with.\n\n    Returns\n    -------\n    TensorType\n        A tensor with the same shape as the input tensor filled with the\n        specified value.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.get_device","title":"<code>get_device(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the name of the tensor's device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the device of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the tensor's device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef get_device(self, tensor: TensorType) -&gt; str:\n    \"\"\"Get the name of the tensor's device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the device of.\n\n    Returns\n    -------\n    str\n        The name of the tensor's device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.shape","title":"<code>shape(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the shape of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the shape of.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The shape of the tensor. The list contains as many elements as there are dimensions in the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef shape(self, tensor: TensorType) -&gt; list[int]:\n    \"\"\"Get the shape of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the shape of.\n\n    Returns\n    -------\n    list[int]\n        The shape of the tensor. The list contains as many elements as\n        there are dimensions in the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.squeeze","title":"<code>squeeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Remove a dimension from the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to remove a dimension from.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with one less dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef squeeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Remove a dimension from the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to remove a dimension from.\n\n    Returns\n    -------\n    TensorType\n        The tensor with one less dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.to_device","title":"<code>to_device(tensor, device)</code>  <code>abstractmethod</code>","text":"<p>Move the tensor to a specified device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to move to a specified device.</p> required <code>device</code> <code>str</code> <p>The name of the device to move the tensor to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor moved to the specified device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n    \"\"\"Move the tensor to a specified device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to move to a specified device.\n    device\n        The name of the device to move the tensor to.\n\n    Returns\n    -------\n    TensorType\n        The tensor moved to the specified device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.to_list","title":"<code>to_list(tensor)</code>  <code>abstractmethod</code>","text":"<p>Convert the tensor to a list.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to convert to a list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The tensor as a list.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_list(self, tensor: TensorType) -&gt; list:\n    \"\"\"Convert the tensor to a list.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to convert to a list.\n\n    Returns\n    -------\n    list\n        The tensor as a list.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.to_scalar","title":"<code>to_scalar(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the only element of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to return the only element of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The only element of the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_scalar(self, tensor: TensorType) -&gt; Any:\n    \"\"\"Return the only element of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to return the only element of.\n\n    Returns\n    -------\n    Any\n        The only element of the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.base.TensorAdapter.unsqueeze","title":"<code>unsqueeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Add a dimension to the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to add a dimension to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with an additional dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Add a dimension to the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to add a dimension to.\n\n    Returns\n    -------\n    TensorType\n        The tensor with an additional dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.mlx","title":"<code>mlx</code>","text":"<p>Tensor adapter for the <code>mlx</code> library.</p>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.numpy","title":"<code>numpy</code>","text":"<p>Tensor adapter for the <code>numpy</code> library.</p>"},{"location":"api_reference/processors/tensor_adapters/#outlines.processors.tensor_adapters.torch","title":"<code>torch</code>","text":"<p>Tensor adapter for the <code>torch</code> library.</p>"},{"location":"api_reference/processors/tensor_adapters/base/","title":"base","text":"<p>Base class for tensor adapters.</p>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter","title":"<code>TensorAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tensor adapters.</p> <p>This class defines the interface for tensor adapters that are used to manipulate tensors in different libraries. Concrete implementations of this class should provide specific implementations for each method as well as providing a <code>library_name</code> attribute.</p> <p>TODO: Update the version of outlines-core used to receive plain arrays instead of torch tensors. In the meantime, implementations of this class must make sure that their <code>full_like</code> and <code>concatenate</code> methods can handle torch tensors.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>class TensorAdapter(ABC):\n    \"\"\"Abstract base class for tensor adapters.\n\n    This class defines the interface for tensor adapters that are used to\n    manipulate tensors in different libraries. Concrete implementations of\n    this class should provide specific implementations for each method as\n    well as providing a `library_name` attribute.\n\n    TODO: Update the version of outlines-core used to receive plain arrays\n    instead of torch tensors. In the meantime, implementations of this class\n    must make sure that their `full_like` and `concatenate` methods can\n    handle torch tensors.\n\n    \"\"\"\n    library_name: str\n\n    @abstractmethod\n    def shape(self, tensor: TensorType) -&gt; list[int]:\n        \"\"\"Get the shape of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the shape of.\n\n        Returns\n        -------\n        list[int]\n            The shape of the tensor. The list contains as many elements as\n            there are dimensions in the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Add a dimension to the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to add a dimension to.\n\n        Returns\n        -------\n        TensorType\n            The tensor with an additional dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def squeeze(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Remove a dimension from the tensor at axis 0.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to remove a dimension from.\n\n        Returns\n        -------\n        TensorType\n            The tensor with one less dimension.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_list(self, tensor: TensorType) -&gt; list:\n        \"\"\"Convert the tensor to a list.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to convert to a list.\n\n        Returns\n        -------\n        list\n            The tensor as a list.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_scalar(self, tensor: TensorType) -&gt; Any:\n        \"\"\"Return the only element of the tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to return the only element of.\n\n        Returns\n        -------\n        Any\n            The only element of the tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n        \"\"\"Create a tensor with the same shape as the input tensor filled\n        with a scalar value.\n\n        ATTENTION: This method receives a torch tensor regardless of the\n        library used.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a new tensor with the same shape.\n        fill_value\n            The value to fill the new tensor with.\n\n        Returns\n        -------\n        TensorType\n            A tensor with the same shape as the input tensor filled with the\n            specified value.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def concatenate(\n        self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n    ) -&gt; TensorType:\n        \"\"\"Concatenate a list of tensors along axis 0.\n\n        ATTENTION: This method can either receive a list of torch tensors or\n        a list of tensors from the library used.\n\n        Parameters\n        ----------\n        tensors\n            The list of tensors to concatenate.\n\n        Returns\n        -------\n        TensorType\n            The concatenated tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_device(self, tensor: TensorType) -&gt; str:\n        \"\"\"Get the name of the tensor's device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to get the device of.\n\n        Returns\n        -------\n        str\n            The name of the tensor's device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n        \"\"\"Move the tensor to a specified device.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to move to a specified device.\n        device\n            The name of the device to move the tensor to.\n\n        Returns\n        -------\n        TensorType\n            The tensor moved to the specified device.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n        \"\"\"Create a boolean ones tensor with the same shape as the input\n        tensor.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to create a boolean ones tensor with the same shape.\n\n        Returns\n        -------\n        TensorType\n            A boolean ones tensor with the same shape as the input tensor.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def apply_mask(\n        self, tensor: TensorType, mask: TensorType, value: Any\n    ) -&gt; TensorType:\n        \"\"\"Fill the elements of the tensor where the mask is True with the\n        specified value.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to fill.\n        mask\n            The mask to apply to the tensor.\n        value\n            The value to fill the tensor with.\n\n        Returns\n        -------\n        TensorType\n            The tensor with the mask applied.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def argsort_descending(\n        self, tensor: TensorType\n    ) -&gt; TensorType:\n        \"\"\"Return the indices that would sort the tensor in descending order\n        along axis -1.\n\n        Parameters\n        ----------\n        tensor\n            The tensor to sort.\n\n        Returns\n        -------\n        TensorType\n            The indices that would sort the tensor in descending order along\n            axis -1.\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.apply_mask","title":"<code>apply_mask(tensor, mask, value)</code>  <code>abstractmethod</code>","text":"<p>Fill the elements of the tensor where the mask is True with the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to fill.</p> required <code>mask</code> <code>TensorType</code> <p>The mask to apply to the tensor.</p> required <code>value</code> <code>Any</code> <p>The value to fill the tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with the mask applied.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef apply_mask(\n    self, tensor: TensorType, mask: TensorType, value: Any\n) -&gt; TensorType:\n    \"\"\"Fill the elements of the tensor where the mask is True with the\n    specified value.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to fill.\n    mask\n        The mask to apply to the tensor.\n    value\n        The value to fill the tensor with.\n\n    Returns\n    -------\n    TensorType\n        The tensor with the mask applied.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.argsort_descending","title":"<code>argsort_descending(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the indices that would sort the tensor in descending order along axis -1.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to sort.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The indices that would sort the tensor in descending order along axis -1.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef argsort_descending(\n    self, tensor: TensorType\n) -&gt; TensorType:\n    \"\"\"Return the indices that would sort the tensor in descending order\n    along axis -1.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to sort.\n\n    Returns\n    -------\n    TensorType\n        The indices that would sort the tensor in descending order along\n        axis -1.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.boolean_ones_like","title":"<code>boolean_ones_like(tensor)</code>  <code>abstractmethod</code>","text":"<p>Create a boolean ones tensor with the same shape as the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to create a boolean ones tensor with the same shape.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A boolean ones tensor with the same shape as the input tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef boolean_ones_like(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Create a boolean ones tensor with the same shape as the input\n    tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a boolean ones tensor with the same shape.\n\n    Returns\n    -------\n    TensorType\n        A boolean ones tensor with the same shape as the input tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.concatenate","title":"<code>concatenate(tensors)</code>  <code>abstractmethod</code>","text":"<p>Concatenate a list of tensors along axis 0.</p> <p>ATTENTION: This method can either receive a list of torch tensors or a list of tensors from the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Union[Tensor, TensorType]]</code> <p>The list of tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The concatenated tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef concatenate(\n    self, tensors: list[Union[\"torch.Tensor\", TensorType]]\n) -&gt; TensorType:\n    \"\"\"Concatenate a list of tensors along axis 0.\n\n    ATTENTION: This method can either receive a list of torch tensors or\n    a list of tensors from the library used.\n\n    Parameters\n    ----------\n    tensors\n        The list of tensors to concatenate.\n\n    Returns\n    -------\n    TensorType\n        The concatenated tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.full_like","title":"<code>full_like(tensor, fill_value)</code>  <code>abstractmethod</code>","text":"<p>Create a tensor with the same shape as the input tensor filled with a scalar value.</p> <p>ATTENTION: This method receives a torch tensor regardless of the library used.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to create a new tensor with the same shape.</p> required <code>fill_value</code> <code>Any</code> <p>The value to fill the new tensor with.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>A tensor with the same shape as the input tensor filled with the specified value.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef full_like(self, tensor: \"torch.Tensor\", fill_value: Any) -&gt; TensorType: # type: ignore\n    \"\"\"Create a tensor with the same shape as the input tensor filled\n    with a scalar value.\n\n    ATTENTION: This method receives a torch tensor regardless of the\n    library used.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to create a new tensor with the same shape.\n    fill_value\n        The value to fill the new tensor with.\n\n    Returns\n    -------\n    TensorType\n        A tensor with the same shape as the input tensor filled with the\n        specified value.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.get_device","title":"<code>get_device(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the name of the tensor's device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the device of.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the tensor's device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef get_device(self, tensor: TensorType) -&gt; str:\n    \"\"\"Get the name of the tensor's device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the device of.\n\n    Returns\n    -------\n    str\n        The name of the tensor's device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.shape","title":"<code>shape(tensor)</code>  <code>abstractmethod</code>","text":"<p>Get the shape of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to get the shape of.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The shape of the tensor. The list contains as many elements as there are dimensions in the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef shape(self, tensor: TensorType) -&gt; list[int]:\n    \"\"\"Get the shape of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to get the shape of.\n\n    Returns\n    -------\n    list[int]\n        The shape of the tensor. The list contains as many elements as\n        there are dimensions in the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.squeeze","title":"<code>squeeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Remove a dimension from the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to remove a dimension from.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with one less dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef squeeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Remove a dimension from the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to remove a dimension from.\n\n    Returns\n    -------\n    TensorType\n        The tensor with one less dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.to_device","title":"<code>to_device(tensor, device)</code>  <code>abstractmethod</code>","text":"<p>Move the tensor to a specified device.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to move to a specified device.</p> required <code>device</code> <code>str</code> <p>The name of the device to move the tensor to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor moved to the specified device.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_device(self, tensor: TensorType, device: str) -&gt; TensorType:\n    \"\"\"Move the tensor to a specified device.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to move to a specified device.\n    device\n        The name of the device to move the tensor to.\n\n    Returns\n    -------\n    TensorType\n        The tensor moved to the specified device.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.to_list","title":"<code>to_list(tensor)</code>  <code>abstractmethod</code>","text":"<p>Convert the tensor to a list.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to convert to a list.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The tensor as a list.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_list(self, tensor: TensorType) -&gt; list:\n    \"\"\"Convert the tensor to a list.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to convert to a list.\n\n    Returns\n    -------\n    list\n        The tensor as a list.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.to_scalar","title":"<code>to_scalar(tensor)</code>  <code>abstractmethod</code>","text":"<p>Return the only element of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to return the only element of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The only element of the tensor.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef to_scalar(self, tensor: TensorType) -&gt; Any:\n    \"\"\"Return the only element of the tensor.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to return the only element of.\n\n    Returns\n    -------\n    Any\n        The only element of the tensor.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/base/#outlines.processors.tensor_adapters.base.TensorAdapter.unsqueeze","title":"<code>unsqueeze(tensor)</code>  <code>abstractmethod</code>","text":"<p>Add a dimension to the tensor at axis 0.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>TensorType</code> <p>The tensor to add a dimension to.</p> required <p>Returns:</p> Type Description <code>TensorType</code> <p>The tensor with an additional dimension.</p> Source code in <code>outlines/processors/tensor_adapters/base.py</code> <pre><code>@abstractmethod\ndef unsqueeze(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"Add a dimension to the tensor at axis 0.\n\n    Parameters\n    ----------\n    tensor\n        The tensor to add a dimension to.\n\n    Returns\n    -------\n    TensorType\n        The tensor with an additional dimension.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/processors/tensor_adapters/mlx/","title":"mlx","text":"<p>Tensor adapter for the <code>mlx</code> library.</p>"},{"location":"api_reference/processors/tensor_adapters/numpy/","title":"numpy","text":"<p>Tensor adapter for the <code>numpy</code> library.</p>"},{"location":"api_reference/processors/tensor_adapters/torch/","title":"torch","text":"<p>Tensor adapter for the <code>torch</code> library.</p>"},{"location":"api_reference/types/","title":"types","text":"<p>Output types for structured generation and regex DSL.</p>"},{"location":"api_reference/types/#outlines.types.AirportImportError","title":"<code>AirportImportError</code>","text":"<p>Dummy module that raises an error when accessed.</p> Source code in <code>outlines/types/__init__.py</code> <pre><code>class AirportImportError:\n    \"\"\"Dummy module that raises an error when accessed.\"\"\"\n    def __getattr__(self, name):\n        raise ImportError(\n            \"The 'airportsdata' package is required to use airport types. \"\n            \"Install it with: pip install 'outlines[airports]'\"\n        )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.CFG","title":"<code>CFG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>str</code> <p>The definition of the context-free grammar as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass CFG(Term):\n    \"\"\"Class representing a context-free grammar.\n\n    Parameters\n    ----------\n    definition\n        The definition of the context-free grammar as a string.\n\n    \"\"\"\n    definition: str\n\n    def _display_node(self) -&gt; str:\n        return f\"CFG('{self.definition}')\"\n\n    def __repr__(self):\n        return f\"CFG(definition='{self.definition}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, CFG):\n            return False\n        return self.definition == other.definition\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"CFG\":\n        \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n        Parameters\n        ----------\n        path : str\n            The path to the file containing the CFG definition.\n        Returns\n        -------\n        CFG\n            A CFG instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            definition = f.read()\n        return cls(definition)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.CFG.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a CFG instance from a file containing a CFG definition.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the CFG definition.</p> required <p>Returns:</p> Type Description <code>CFG</code> <p>A CFG instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"CFG\":\n    \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n    Parameters\n    ----------\n    path : str\n        The path to the file containing the CFG definition.\n    Returns\n    -------\n    CFG\n        A CFG instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        definition = f.read()\n    return cls(definition)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.Choice","title":"<code>Choice</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a choice between different items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>The items to choose from.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Choice(Term):\n    \"\"\"Class representing a choice between different items.\n\n    Parameters\n    ----------\n    items\n        The items to choose from.\n\n    \"\"\"\n    items: List[Any]\n\n    def _display_node(self) -&gt; str:\n        return f\"Choice({repr(self.items)})\"\n\n    def __repr__(self):\n        return f\"Choice(items={repr(self.items)})\"\n</code></pre>"},{"location":"api_reference/types/#outlines.types.CountryImportError","title":"<code>CountryImportError</code>","text":"<p>Dummy module that raises an error when accessed.</p> Source code in <code>outlines/types/__init__.py</code> <pre><code>class CountryImportError:\n    \"\"\"Dummy module that raises an error when accessed.\"\"\"\n    def __getattr__(self, name):\n        raise ImportError(\n            \"The 'iso3166' package is required to use country types. \"\n            \"Install it with: pip install 'outlines[countries]'\"\n        )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.JsonSchema","title":"<code>JsonSchema</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a JSON schema.</p> <p>The JSON schema object from which to instantiate the class can be a dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a genSON schema builder.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>class JsonSchema(Term):\n    \"\"\"Class representing a JSON schema.\n\n    The JSON schema object from which to instantiate the class can be a\n    dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a\n    genSON schema builder.\n\n    \"\"\"\n    schema: str\n    whitespace_pattern: OptionalType[str]\n\n    def __init__(\n        self,\n        schema: Union[\n            dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n        ],\n        whitespace_pattern: OptionalType[str] = None,\n        ensure_ascii: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        schema\n            The object containing the JSON schema.\n        whitespace_pattern\n            The pattern to use to match whitespace characters.\n        ensure_ascii\n            Whether to ensure the schema is ASCII-only.\n\n        \"\"\"\n        schema_str: str\n\n        if is_dict_instance(schema):\n            schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n        elif is_str_instance(schema):\n            schema_str = str(schema)\n        elif is_pydantic_model(schema):\n            schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n        elif is_typed_dict(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_dataclass(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_genson_schema_builder(schema):\n            schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {schema}. The schema must be either \"\n                + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n                + \"builder or a string or dict that contains the JSON schema \"\n                + \"specification\"\n            )\n\n        self.schema = schema_str\n        self.whitespace_pattern = whitespace_pattern\n\n    def __post_init__(self):\n        jsonschema.Draft7Validator.check_schema(json.loads(self.schema))\n\n    @classmethod\n    def is_json_schema(cls, obj: Any) -&gt; bool:\n        \"\"\"Check if the object provided is a JSON schema type.\n\n        Parameters\n        ----------\n        obj: Any\n            The object to check\n\n        Returns\n        -------\n        bool\n            True if the object is a JSON schema type, False otherwise\n\n        \"\"\"\n        return (\n            isinstance(obj, cls)\n            or is_pydantic_model(obj)\n            or is_typed_dict(obj)\n            or is_dataclass(obj)\n            or is_genson_schema_builder(obj)\n        )\n\n    @classmethod\n    def convert_to(\n        cls,\n        schema: Union[\n            \"JsonSchema\",\n            type[BaseModel],\n            _TypedDictMeta,\n            type,\n            SchemaBuilder,\n        ],\n        target_types: List[Literal[\n            \"str\",\n            \"dict\",\n            \"pydantic\",\n            \"typeddict\",\n            \"dataclass\",\n            \"genson\",\n        ]],\n    ) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n        \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n        If the schema provided is already of a type in the target_types, return\n        it unchanged.\n\n        Parameters\n        ----------\n        schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n            The schema to convert\n        target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n            The target types to convert to\n\n        \"\"\"\n        # If the schema provided is already of a type in the target_types,\n        # just return it\n        if isinstance(schema, cls):\n            if \"str\" in target_types:\n                return schema.schema\n            elif \"dict\" in target_types:\n                return json.loads(schema.schema)\n        elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n            return schema\n        elif is_typed_dict(schema) and \"typeddict\" in target_types:\n            return schema\n        elif is_dataclass(schema) and \"dataclass\" in target_types:\n            return schema\n        elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n            return schema\n\n        # Convert the schema to a JSON schema string/dict\n        if isinstance(schema, cls):\n            schema_str = schema.schema\n        else:\n            schema_str = cls(schema).schema\n        schema_dict = json.loads(schema_str)\n\n        for target_type in target_types:\n            try:\n                # Convert the JSON schema string to the target type\n                if target_type == \"str\":\n                    return schema_str\n                elif target_type == \"dict\":\n                    return schema_dict\n                elif target_type == \"pydantic\":\n                    return json_schema_dict_to_pydantic(schema_dict)\n                elif target_type == \"typeddict\":\n                    return json_schema_dict_to_typeddict(schema_dict)\n                elif target_type == \"dataclass\":\n                    return json_schema_dict_to_dataclass(schema_dict)\n                # No conversion available for genson\n            except Exception as e:  # pragma: no cover\n                warnings.warn(\n                    f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n                )\n                continue\n\n        raise ValueError(\n            f\"Cannot convert schema type {type(schema)} to any of the target \"\n            f\"types {target_types}\"\n        )\n\n    def _display_node(self) -&gt; str:\n        return f\"JsonSchema('{self.schema}')\"\n\n    def __repr__(self):\n        return f\"JsonSchema(schema='{self.schema}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, JsonSchema):\n            return False\n        try:\n            self_dict = json.loads(self.schema)\n            other_dict = json.loads(other.schema)\n            return self_dict == other_dict\n        except json.JSONDecodeError:  # pragma: no cover\n            return self.schema == other.schema\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"JsonSchema\":\n        \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n        schema.\n\n        Parameters\n        ----------\n        path:\n            The path to the file containing the JSON schema.\n        Returns\n        -------\n        JsonSchema\n            A JsonSchema instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            schema = json.load(f)\n        return cls(schema)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.JsonSchema.__init__","title":"<code>__init__(schema, whitespace_pattern=None, ensure_ascii=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The object containing the JSON schema.</p> required <code>whitespace_pattern</code> <code>Optional[str]</code> <p>The pattern to use to match whitespace characters.</p> <code>None</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to ensure the schema is ASCII-only.</p> <code>True</code> Source code in <code>outlines/types/dsl.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[\n        dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n    ],\n    whitespace_pattern: OptionalType[str] = None,\n    ensure_ascii: bool = True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    schema\n        The object containing the JSON schema.\n    whitespace_pattern\n        The pattern to use to match whitespace characters.\n    ensure_ascii\n        Whether to ensure the schema is ASCII-only.\n\n    \"\"\"\n    schema_str: str\n\n    if is_dict_instance(schema):\n        schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n    elif is_str_instance(schema):\n        schema_str = str(schema)\n    elif is_pydantic_model(schema):\n        schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n    elif is_typed_dict(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_dataclass(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_genson_schema_builder(schema):\n        schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n            + \"builder or a string or dict that contains the JSON schema \"\n            + \"specification\"\n        )\n\n    self.schema = schema_str\n    self.whitespace_pattern = whitespace_pattern\n</code></pre>"},{"location":"api_reference/types/#outlines.types.JsonSchema.convert_to","title":"<code>convert_to(schema, target_types)</code>  <code>classmethod</code>","text":"<p>Convert a JSON schema type to a different JSON schema type.</p> <p>If the schema provided is already of a type in the target_types, return it unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The schema to convert</p> required <code>target_types</code> <code>List[Literal['str', 'dict', 'pydantic', 'typeddict', 'dataclass', 'genson']]</code> <p>The target types to convert to</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef convert_to(\n    cls,\n    schema: Union[\n        \"JsonSchema\",\n        type[BaseModel],\n        _TypedDictMeta,\n        type,\n        SchemaBuilder,\n    ],\n    target_types: List[Literal[\n        \"str\",\n        \"dict\",\n        \"pydantic\",\n        \"typeddict\",\n        \"dataclass\",\n        \"genson\",\n    ]],\n) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n    \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n    If the schema provided is already of a type in the target_types, return\n    it unchanged.\n\n    Parameters\n    ----------\n    schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n        The schema to convert\n    target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n        The target types to convert to\n\n    \"\"\"\n    # If the schema provided is already of a type in the target_types,\n    # just return it\n    if isinstance(schema, cls):\n        if \"str\" in target_types:\n            return schema.schema\n        elif \"dict\" in target_types:\n            return json.loads(schema.schema)\n    elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n        return schema\n    elif is_typed_dict(schema) and \"typeddict\" in target_types:\n        return schema\n    elif is_dataclass(schema) and \"dataclass\" in target_types:\n        return schema\n    elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n        return schema\n\n    # Convert the schema to a JSON schema string/dict\n    if isinstance(schema, cls):\n        schema_str = schema.schema\n    else:\n        schema_str = cls(schema).schema\n    schema_dict = json.loads(schema_str)\n\n    for target_type in target_types:\n        try:\n            # Convert the JSON schema string to the target type\n            if target_type == \"str\":\n                return schema_str\n            elif target_type == \"dict\":\n                return schema_dict\n            elif target_type == \"pydantic\":\n                return json_schema_dict_to_pydantic(schema_dict)\n            elif target_type == \"typeddict\":\n                return json_schema_dict_to_typeddict(schema_dict)\n            elif target_type == \"dataclass\":\n                return json_schema_dict_to_dataclass(schema_dict)\n            # No conversion available for genson\n        except Exception as e:  # pragma: no cover\n            warnings.warn(\n                f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n            )\n            continue\n\n    raise ValueError(\n        f\"Cannot convert schema type {type(schema)} to any of the target \"\n        f\"types {target_types}\"\n    )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.JsonSchema.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a JsonSchema instance from a .json file containing a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the JSON schema.</p> required <p>Returns:</p> Type Description <code>JsonSchema</code> <p>A JsonSchema instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"JsonSchema\":\n    \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n    schema.\n\n    Parameters\n    ----------\n    path:\n        The path to the file containing the JSON schema.\n    Returns\n    -------\n    JsonSchema\n        A JsonSchema instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        schema = json.load(f)\n    return cls(schema)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.JsonSchema.is_json_schema","title":"<code>is_json_schema(obj)</code>  <code>classmethod</code>","text":"<p>Check if the object provided is a JSON schema type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a JSON schema type, False otherwise</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef is_json_schema(cls, obj: Any) -&gt; bool:\n    \"\"\"Check if the object provided is a JSON schema type.\n\n    Parameters\n    ----------\n    obj: Any\n        The object to check\n\n    Returns\n    -------\n    bool\n        True if the object is a JSON schema type, False otherwise\n\n    \"\"\"\n    return (\n        isinstance(obj, cls)\n        or is_pydantic_model(obj)\n        or is_typed_dict(obj)\n        or is_dataclass(obj)\n        or is_genson_schema_builder(obj)\n    )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.Regex","title":"<code>Regex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Regex(Term):\n    \"\"\"Class representing a regular expression.\n\n    Parameters\n    ----------\n    pattern\n        The regular expression as a string.\n\n    \"\"\"\n    pattern: str\n\n    def _display_node(self) -&gt; str:\n        return f\"Regex('{self.pattern}')\"\n\n    def __repr__(self):\n        return f\"Regex(pattern='{self.pattern}')\"\n</code></pre>"},{"location":"api_reference/types/#outlines.types.at_least","title":"<code>at_least(count, term)</code>","text":"<p>Repeat the term at least <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_least(count: int, term: Union[Term, str]) -&gt; QuantifyMinimum:\n    \"\"\"Repeat the term at least `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMinimum(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.at_most","title":"<code>at_most(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_most(count: int, term: Union[Term, str]) -&gt; QuantifyMaximum:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMaximum(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.either","title":"<code>either(*terms)</code>","text":"<p>Represents an alternative between different terms or strings.</p> <p>This factory function automatically translates string arguments into <code>String</code> objects.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def either(*terms: Union[str, Term]):\n    \"\"\"Represents an alternative between different terms or strings.\n\n    This factory function automatically translates string arguments\n    into `String` objects.\n\n    \"\"\"\n    terms = [String(arg) if isinstance(arg, str) else arg for arg in terms]\n    return Alternatives(terms)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.exactly","title":"<code>exactly(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def exactly(count: int, term: Union[Term, str]) -&gt; QuantifyExact:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyExact(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.airports","title":"<code>airports</code>","text":"<p>Generate valid airport codes.</p>"},{"location":"api_reference/types/#outlines.types.countries","title":"<code>countries</code>","text":"<p>Generate valid country codes and names.</p>"},{"location":"api_reference/types/#outlines.types.countries.get_country_flags","title":"<code>get_country_flags()</code>","text":"<p>Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.</p> Source code in <code>outlines/types/countries.py</code> <pre><code>def get_country_flags():\n    \"\"\"Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.\"\"\"\n    base = ord(\"\ud83c\udde6\")\n    return {\n        code.name: chr(base + ord(code.name[0]) - ord(\"A\"))\n        + chr(base + ord(code.name[1]) - ord(\"A\"))\n        for code in Alpha2\n    }\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl","title":"<code>dsl</code>","text":"<p>Regular expression DSL and output types for structured generation.</p> <p>This module contains elements related to three logical steps in the use of output types for structured generation:</p> <ol> <li>Definition of <code>Term</code> classes that contain output type definitions. That    includes both terms intended to be used by themselves such as <code>JsonSchema</code>    or <code>CFG</code> and terms that are part of the regular expression DSL such as    <code>Alternatives</code> or <code>KleeneStar</code> (and the related functions).</li> <li>Conversion of Python types into <code>Term</code> instances (<code>python_types_to_terms</code>).</li> <li>Conversion of a <code>Term</code> instance into a regular expression (<code>to_regex</code>).</li> </ol>"},{"location":"api_reference/types/#outlines.types.dsl.CFG","title":"<code>CFG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>str</code> <p>The definition of the context-free grammar as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass CFG(Term):\n    \"\"\"Class representing a context-free grammar.\n\n    Parameters\n    ----------\n    definition\n        The definition of the context-free grammar as a string.\n\n    \"\"\"\n    definition: str\n\n    def _display_node(self) -&gt; str:\n        return f\"CFG('{self.definition}')\"\n\n    def __repr__(self):\n        return f\"CFG(definition='{self.definition}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, CFG):\n            return False\n        return self.definition == other.definition\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"CFG\":\n        \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n        Parameters\n        ----------\n        path : str\n            The path to the file containing the CFG definition.\n        Returns\n        -------\n        CFG\n            A CFG instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            definition = f.read()\n        return cls(definition)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.CFG.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a CFG instance from a file containing a CFG definition.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the CFG definition.</p> required <p>Returns:</p> Type Description <code>CFG</code> <p>A CFG instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"CFG\":\n    \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n    Parameters\n    ----------\n    path : str\n        The path to the file containing the CFG definition.\n    Returns\n    -------\n    CFG\n        A CFG instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        definition = f.read()\n    return cls(definition)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.Choice","title":"<code>Choice</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a choice between different items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>The items to choose from.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Choice(Term):\n    \"\"\"Class representing a choice between different items.\n\n    Parameters\n    ----------\n    items\n        The items to choose from.\n\n    \"\"\"\n    items: List[Any]\n\n    def _display_node(self) -&gt; str:\n        return f\"Choice({repr(self.items)})\"\n\n    def __repr__(self):\n        return f\"Choice(items={repr(self.items)})\"\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.JsonSchema","title":"<code>JsonSchema</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a JSON schema.</p> <p>The JSON schema object from which to instantiate the class can be a dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a genSON schema builder.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>class JsonSchema(Term):\n    \"\"\"Class representing a JSON schema.\n\n    The JSON schema object from which to instantiate the class can be a\n    dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a\n    genSON schema builder.\n\n    \"\"\"\n    schema: str\n    whitespace_pattern: OptionalType[str]\n\n    def __init__(\n        self,\n        schema: Union[\n            dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n        ],\n        whitespace_pattern: OptionalType[str] = None,\n        ensure_ascii: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        schema\n            The object containing the JSON schema.\n        whitespace_pattern\n            The pattern to use to match whitespace characters.\n        ensure_ascii\n            Whether to ensure the schema is ASCII-only.\n\n        \"\"\"\n        schema_str: str\n\n        if is_dict_instance(schema):\n            schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n        elif is_str_instance(schema):\n            schema_str = str(schema)\n        elif is_pydantic_model(schema):\n            schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n        elif is_typed_dict(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_dataclass(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_genson_schema_builder(schema):\n            schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {schema}. The schema must be either \"\n                + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n                + \"builder or a string or dict that contains the JSON schema \"\n                + \"specification\"\n            )\n\n        self.schema = schema_str\n        self.whitespace_pattern = whitespace_pattern\n\n    def __post_init__(self):\n        jsonschema.Draft7Validator.check_schema(json.loads(self.schema))\n\n    @classmethod\n    def is_json_schema(cls, obj: Any) -&gt; bool:\n        \"\"\"Check if the object provided is a JSON schema type.\n\n        Parameters\n        ----------\n        obj: Any\n            The object to check\n\n        Returns\n        -------\n        bool\n            True if the object is a JSON schema type, False otherwise\n\n        \"\"\"\n        return (\n            isinstance(obj, cls)\n            or is_pydantic_model(obj)\n            or is_typed_dict(obj)\n            or is_dataclass(obj)\n            or is_genson_schema_builder(obj)\n        )\n\n    @classmethod\n    def convert_to(\n        cls,\n        schema: Union[\n            \"JsonSchema\",\n            type[BaseModel],\n            _TypedDictMeta,\n            type,\n            SchemaBuilder,\n        ],\n        target_types: List[Literal[\n            \"str\",\n            \"dict\",\n            \"pydantic\",\n            \"typeddict\",\n            \"dataclass\",\n            \"genson\",\n        ]],\n    ) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n        \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n        If the schema provided is already of a type in the target_types, return\n        it unchanged.\n\n        Parameters\n        ----------\n        schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n            The schema to convert\n        target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n            The target types to convert to\n\n        \"\"\"\n        # If the schema provided is already of a type in the target_types,\n        # just return it\n        if isinstance(schema, cls):\n            if \"str\" in target_types:\n                return schema.schema\n            elif \"dict\" in target_types:\n                return json.loads(schema.schema)\n        elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n            return schema\n        elif is_typed_dict(schema) and \"typeddict\" in target_types:\n            return schema\n        elif is_dataclass(schema) and \"dataclass\" in target_types:\n            return schema\n        elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n            return schema\n\n        # Convert the schema to a JSON schema string/dict\n        if isinstance(schema, cls):\n            schema_str = schema.schema\n        else:\n            schema_str = cls(schema).schema\n        schema_dict = json.loads(schema_str)\n\n        for target_type in target_types:\n            try:\n                # Convert the JSON schema string to the target type\n                if target_type == \"str\":\n                    return schema_str\n                elif target_type == \"dict\":\n                    return schema_dict\n                elif target_type == \"pydantic\":\n                    return json_schema_dict_to_pydantic(schema_dict)\n                elif target_type == \"typeddict\":\n                    return json_schema_dict_to_typeddict(schema_dict)\n                elif target_type == \"dataclass\":\n                    return json_schema_dict_to_dataclass(schema_dict)\n                # No conversion available for genson\n            except Exception as e:  # pragma: no cover\n                warnings.warn(\n                    f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n                )\n                continue\n\n        raise ValueError(\n            f\"Cannot convert schema type {type(schema)} to any of the target \"\n            f\"types {target_types}\"\n        )\n\n    def _display_node(self) -&gt; str:\n        return f\"JsonSchema('{self.schema}')\"\n\n    def __repr__(self):\n        return f\"JsonSchema(schema='{self.schema}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, JsonSchema):\n            return False\n        try:\n            self_dict = json.loads(self.schema)\n            other_dict = json.loads(other.schema)\n            return self_dict == other_dict\n        except json.JSONDecodeError:  # pragma: no cover\n            return self.schema == other.schema\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"JsonSchema\":\n        \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n        schema.\n\n        Parameters\n        ----------\n        path:\n            The path to the file containing the JSON schema.\n        Returns\n        -------\n        JsonSchema\n            A JsonSchema instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            schema = json.load(f)\n        return cls(schema)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.JsonSchema.__init__","title":"<code>__init__(schema, whitespace_pattern=None, ensure_ascii=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The object containing the JSON schema.</p> required <code>whitespace_pattern</code> <code>Optional[str]</code> <p>The pattern to use to match whitespace characters.</p> <code>None</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to ensure the schema is ASCII-only.</p> <code>True</code> Source code in <code>outlines/types/dsl.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[\n        dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n    ],\n    whitespace_pattern: OptionalType[str] = None,\n    ensure_ascii: bool = True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    schema\n        The object containing the JSON schema.\n    whitespace_pattern\n        The pattern to use to match whitespace characters.\n    ensure_ascii\n        Whether to ensure the schema is ASCII-only.\n\n    \"\"\"\n    schema_str: str\n\n    if is_dict_instance(schema):\n        schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n    elif is_str_instance(schema):\n        schema_str = str(schema)\n    elif is_pydantic_model(schema):\n        schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n    elif is_typed_dict(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_dataclass(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_genson_schema_builder(schema):\n        schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n            + \"builder or a string or dict that contains the JSON schema \"\n            + \"specification\"\n        )\n\n    self.schema = schema_str\n    self.whitespace_pattern = whitespace_pattern\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.JsonSchema.convert_to","title":"<code>convert_to(schema, target_types)</code>  <code>classmethod</code>","text":"<p>Convert a JSON schema type to a different JSON schema type.</p> <p>If the schema provided is already of a type in the target_types, return it unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The schema to convert</p> required <code>target_types</code> <code>List[Literal['str', 'dict', 'pydantic', 'typeddict', 'dataclass', 'genson']]</code> <p>The target types to convert to</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef convert_to(\n    cls,\n    schema: Union[\n        \"JsonSchema\",\n        type[BaseModel],\n        _TypedDictMeta,\n        type,\n        SchemaBuilder,\n    ],\n    target_types: List[Literal[\n        \"str\",\n        \"dict\",\n        \"pydantic\",\n        \"typeddict\",\n        \"dataclass\",\n        \"genson\",\n    ]],\n) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n    \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n    If the schema provided is already of a type in the target_types, return\n    it unchanged.\n\n    Parameters\n    ----------\n    schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n        The schema to convert\n    target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n        The target types to convert to\n\n    \"\"\"\n    # If the schema provided is already of a type in the target_types,\n    # just return it\n    if isinstance(schema, cls):\n        if \"str\" in target_types:\n            return schema.schema\n        elif \"dict\" in target_types:\n            return json.loads(schema.schema)\n    elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n        return schema\n    elif is_typed_dict(schema) and \"typeddict\" in target_types:\n        return schema\n    elif is_dataclass(schema) and \"dataclass\" in target_types:\n        return schema\n    elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n        return schema\n\n    # Convert the schema to a JSON schema string/dict\n    if isinstance(schema, cls):\n        schema_str = schema.schema\n    else:\n        schema_str = cls(schema).schema\n    schema_dict = json.loads(schema_str)\n\n    for target_type in target_types:\n        try:\n            # Convert the JSON schema string to the target type\n            if target_type == \"str\":\n                return schema_str\n            elif target_type == \"dict\":\n                return schema_dict\n            elif target_type == \"pydantic\":\n                return json_schema_dict_to_pydantic(schema_dict)\n            elif target_type == \"typeddict\":\n                return json_schema_dict_to_typeddict(schema_dict)\n            elif target_type == \"dataclass\":\n                return json_schema_dict_to_dataclass(schema_dict)\n            # No conversion available for genson\n        except Exception as e:  # pragma: no cover\n            warnings.warn(\n                f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n            )\n            continue\n\n    raise ValueError(\n        f\"Cannot convert schema type {type(schema)} to any of the target \"\n        f\"types {target_types}\"\n    )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.JsonSchema.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a JsonSchema instance from a .json file containing a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the JSON schema.</p> required <p>Returns:</p> Type Description <code>JsonSchema</code> <p>A JsonSchema instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"JsonSchema\":\n    \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n    schema.\n\n    Parameters\n    ----------\n    path:\n        The path to the file containing the JSON schema.\n    Returns\n    -------\n    JsonSchema\n        A JsonSchema instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        schema = json.load(f)\n    return cls(schema)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.JsonSchema.is_json_schema","title":"<code>is_json_schema(obj)</code>  <code>classmethod</code>","text":"<p>Check if the object provided is a JSON schema type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a JSON schema type, False otherwise</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef is_json_schema(cls, obj: Any) -&gt; bool:\n    \"\"\"Check if the object provided is a JSON schema type.\n\n    Parameters\n    ----------\n    obj: Any\n        The object to check\n\n    Returns\n    -------\n    bool\n        True if the object is a JSON schema type, False otherwise\n\n    \"\"\"\n    return (\n        isinstance(obj, cls)\n        or is_pydantic_model(obj)\n        or is_typed_dict(obj)\n        or is_dataclass(obj)\n        or is_genson_schema_builder(obj)\n    )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.Regex","title":"<code>Regex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Regex(Term):\n    \"\"\"Class representing a regular expression.\n\n    Parameters\n    ----------\n    pattern\n        The regular expression as a string.\n\n    \"\"\"\n    pattern: str\n\n    def _display_node(self) -&gt; str:\n        return f\"Regex('{self.pattern}')\"\n\n    def __repr__(self):\n        return f\"Regex(pattern='{self.pattern}')\"\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.Term","title":"<code>Term</code>","text":"<p>Represents types defined with a regular expression.</p> <p><code>Regex</code> instances can be used as a type in a Pydantic model definittion. They will be translated to JSON Schema as a \"string\" field with the \"pattern\" keyword set to the regular expression this class represents. The class also handles validation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from outlines.types import Regex\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n&gt;&gt;&gt;     name: str\n&gt;&gt;&gt;     age: age_type\n</code></pre> Source code in <code>outlines/types/dsl.py</code> <pre><code>class Term:\n    \"\"\"Represents types defined with a regular expression.\n\n    `Regex` instances can be used as a type in a Pydantic model definittion.\n    They will be translated to JSON Schema as a \"string\" field with the\n    \"pattern\" keyword set to the regular expression this class represents. The\n    class also handles validation.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from outlines.types import Regex\n    &gt;&gt;&gt; from pydantic import BaseModel\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; class User(BaseModel):\n    &gt;&gt;&gt;     name: str\n    &gt;&gt;&gt;     age: age_type\n\n    \"\"\"\n\n    def __add__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([self, other])\n\n    def __radd__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([other, self])\n\n    def __or__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([self, other])\n\n    def __ror__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([other, self])\n\n    def __get_validator__(self, _core_schema):\n        def validate(input_value):\n            return self.validate(input_value)\n\n        return validate\n\n    def __get_pydantic_core_schema__(\n        self, source_type: Any, handler: GetCoreSchemaHandler\n    ) -&gt; cs.CoreSchema:\n        return cs.no_info_plain_validator_function(lambda value: self.validate(value))\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: cs.CoreSchema, handler: GetJsonSchemaHandler\n    ) -&gt; JsonSchemaValue:\n        return {\"type\": \"string\", \"pattern\": to_regex(self)}\n\n    def validate(self, value: str) -&gt; str:\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if not compiled.fullmatch(str(value)):\n            raise ValueError(\n                f\"Input should be in the language of the regular expression {pattern}\"\n            )\n        return value\n\n    def matches(self, value: str) -&gt; bool:\n        \"\"\"Check that a given value is in the language defined by the Term.\n\n        We make the assumption that the language defined by the term can\n        be defined with a regular expression.\n\n        \"\"\"\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if compiled.fullmatch(str(value)):\n            return True\n        return False\n\n    def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n        \"\"\"Display the regex tree in ASCII format.\"\"\"\n        branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n        result = indent + branch + self._display_node() + \"\\n\"\n\n        # Calculate the new indent for children\n        new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n        # Let each subclass handle its children\n        result += self._display_children(new_indent)\n        return result\n\n    def _display_node(self):\n        raise NotImplementedError\n\n    def _display_children(self, indent: str) -&gt; str:\n        \"\"\"Display the children of this node. Override in subclasses with children.\"\"\"\n        return \"\"\n\n    def __str__(self):\n        return self.display_ascii_tree()\n\n    def optional(self) -&gt; \"Optional\":\n        return optional(self)\n\n    def exactly(self, count: int) -&gt; \"QuantifyExact\":\n        return exactly(count, self)\n\n    def at_least(self, count: int) -&gt; \"QuantifyMinimum\":\n        return at_least(count, self)\n\n    def at_most(self, count: int) -&gt; \"QuantifyMaximum\":\n        return at_most(count, self)\n\n    def between(self, min_count: int, max_count: int) -&gt; \"QuantifyBetween\":\n        return between(min_count, max_count, self)\n\n    def one_or_more(self) -&gt; \"KleenePlus\":\n        return one_or_more(self)\n\n    def zero_or_more(self) -&gt; \"KleeneStar\":\n        return zero_or_more(self)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.Term.display_ascii_tree","title":"<code>display_ascii_tree(indent='', is_last=True)</code>","text":"<p>Display the regex tree in ASCII format.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n    \"\"\"Display the regex tree in ASCII format.\"\"\"\n    branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n    result = indent + branch + self._display_node() + \"\\n\"\n\n    # Calculate the new indent for children\n    new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n    # Let each subclass handle its children\n    result += self._display_children(new_indent)\n    return result\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.Term.matches","title":"<code>matches(value)</code>","text":"<p>Check that a given value is in the language defined by the Term.</p> <p>We make the assumption that the language defined by the term can be defined with a regular expression.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def matches(self, value: str) -&gt; bool:\n    \"\"\"Check that a given value is in the language defined by the Term.\n\n    We make the assumption that the language defined by the term can\n    be defined with a regular expression.\n\n    \"\"\"\n    pattern = to_regex(self)\n    compiled = re.compile(pattern)\n    if compiled.fullmatch(str(value)):\n        return True\n    return False\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.at_least","title":"<code>at_least(count, term)</code>","text":"<p>Repeat the term at least <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_least(count: int, term: Union[Term, str]) -&gt; QuantifyMinimum:\n    \"\"\"Repeat the term at least `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMinimum(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.at_most","title":"<code>at_most(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_most(count: int, term: Union[Term, str]) -&gt; QuantifyMaximum:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMaximum(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.either","title":"<code>either(*terms)</code>","text":"<p>Represents an alternative between different terms or strings.</p> <p>This factory function automatically translates string arguments into <code>String</code> objects.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def either(*terms: Union[str, Term]):\n    \"\"\"Represents an alternative between different terms or strings.\n\n    This factory function automatically translates string arguments\n    into `String` objects.\n\n    \"\"\"\n    terms = [String(arg) if isinstance(arg, str) else arg for arg in terms]\n    return Alternatives(terms)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.exactly","title":"<code>exactly(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def exactly(count: int, term: Union[Term, str]) -&gt; QuantifyExact:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyExact(term, count)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.python_types_to_terms","title":"<code>python_types_to_terms(ptype, recursion_depth=0)</code>","text":"<p>Convert Python types to Outlines DSL terms that constrain LLM output.</p> <p>Parameters:</p> Name Type Description Default <code>ptype</code> <code>Any</code> <p>The Python type to convert</p> required <code>recursion_depth</code> <code>int</code> <p>Current recursion depth to prevent infinite recursion</p> <code>0</code> <p>Returns:</p> Type Description <code>Term</code> <p>The corresponding DSL <code>Term</code> instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def python_types_to_terms(ptype: Any, recursion_depth: int = 0) -&gt; Term:\n    \"\"\"Convert Python types to Outlines DSL terms that constrain LLM output.\n\n    Parameters\n    ----------\n    ptype\n        The Python type to convert\n    recursion_depth\n        Current recursion depth to prevent infinite recursion\n\n    Returns\n    -------\n    Term\n        The corresponding DSL `Term` instance.\n\n    \"\"\"\n    if recursion_depth &gt; 10:\n        raise RecursionError(\n            f\"Maximum recursion depth exceeded when converting {ptype}. \"\n            \"This might be due to a recursive type definition.\"\n        )\n\n    # First handle Term instances\n    if isinstance(ptype, Term):\n        return ptype\n\n    # Basic types\n    if is_int(ptype):\n        return types.integer\n    elif is_float(ptype):\n        return types.number\n    elif is_bool(ptype):\n        return types.boolean\n    elif is_str(ptype):\n        return types.string\n    elif is_native_dict(ptype):\n        return CFG(grammars.json)\n    elif is_time(ptype):\n        return types.time\n    elif is_date(ptype):\n        return types.date\n    elif is_datetime(ptype):\n        return types.datetime\n\n    # Basic type instances\n    if is_str_instance(ptype):\n        return String(ptype)\n    elif is_int_instance(ptype) or is_float_instance(ptype):\n        return Regex(str(ptype))\n\n    # Structured types\n    structured_type_checks = [\n        lambda x: is_dataclass(x),\n        lambda x: is_typed_dict(x),\n        lambda x: is_pydantic_model(x),\n    ]\n    if any(check(ptype) for check in structured_type_checks):\n        schema = TypeAdapter(ptype).json_schema()\n        return JsonSchema(schema)\n\n    elif is_genson_schema_builder(ptype):\n        schema = ptype.to_json()\n        return JsonSchema(schema)\n\n    if is_enum(ptype):\n        return Alternatives(\n            [\n                python_types_to_terms(member, recursion_depth + 1)\n                for member in _get_enum_members(ptype)\n            ]\n        )\n\n    args = get_args(ptype)\n    if is_literal(ptype):\n        return _handle_literal(args)\n    elif is_union(ptype):\n        return _handle_union(args, recursion_depth)\n    elif is_typing_list(ptype):\n        return _handle_list(args, recursion_depth)\n    elif is_typing_tuple(ptype):\n        return _handle_tuple(args, recursion_depth)\n    elif is_typing_dict(ptype):\n        return _handle_dict(args, recursion_depth)\n\n    if is_callable(ptype):\n        return JsonSchema(get_schema_from_signature(ptype))\n\n    type_name = getattr(ptype, \"__name__\", ptype)\n    raise TypeError(\n        f\"Type {type_name} is currently not supported. Please open an issue: \"\n        \"https://github.com/dottxt-ai/outlines/issues\"\n    )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.dsl.to_regex","title":"<code>to_regex(term)</code>","text":"<p>Convert a term to a regular expression.</p> <p>We only consider self-contained terms that do not refer to another rule.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>Term</code> <p>The term to convert to a regular expression.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The regular expression as a string.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def to_regex(term: Term) -&gt; str:\n    \"\"\"Convert a term to a regular expression.\n\n    We only consider self-contained terms that do not refer to another rule.\n\n    Parameters\n    ----------\n    term\n        The term to convert to a regular expression.\n\n    Returns\n    -------\n    str\n        The regular expression as a string.\n\n    \"\"\"\n    if isinstance(term, String):\n        return re.escape(term.value)\n    elif isinstance(term, Regex):\n        return f\"({term.pattern})\"\n    elif isinstance(term, JsonSchema):\n        regex_str = outlines_core.json_schema.build_regex_from_schema(term.schema, term.whitespace_pattern)\n        return f\"({regex_str})\"\n    elif isinstance(term, Choice):\n        regexes = [to_regex(python_types_to_terms(item)) for item in term.items]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, KleeneStar):\n        return f\"({to_regex(term.term)})*\"\n    elif isinstance(term, KleenePlus):\n        return f\"({to_regex(term.term)})+\"\n    elif isinstance(term, Optional):\n        return f\"({to_regex(term.term)})?\"\n    elif isinstance(term, Alternatives):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, Sequence):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"{''.join(regexes)}\"\n    elif isinstance(term, QuantifyExact):\n        return f\"({to_regex(term.term)}){{{term.count}}}\"\n    elif isinstance(term, QuantifyMinimum):\n        return f\"({to_regex(term.term)}){{{term.min_count},}}\"\n    elif isinstance(term, QuantifyMaximum):\n        return f\"({to_regex(term.term)}){{,{term.max_count}}}\"\n    elif isinstance(term, QuantifyBetween):\n        return f\"({to_regex(term.term)}){{{term.min_count},{term.max_count}}}\"\n    else:\n        raise TypeError(\n            f\"Cannot convert object {repr(term)} to a regular expression.\"\n        )\n</code></pre>"},{"location":"api_reference/types/#outlines.types.json_schema_utils","title":"<code>json_schema_utils</code>","text":"<p>Convert JSON Schema dicts to Python types.</p>"},{"location":"api_reference/types/#outlines.types.json_schema_utils.json_schema_dict_to_dataclass","title":"<code>json_schema_dict_to_dataclass(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a dataclass.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a dataclass</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the dataclass</p> <code>None</code> <p>Returns:</p> Type Description <code>type</code> <p>The dataclass</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_dataclass(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type:\n    \"\"\"Convert a JSON Schema dict into a dataclass.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a dataclass\n    name: Optional[str]\n        The name of the dataclass\n\n    Returns\n    -------\n    type\n        The dataclass\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n    defaults: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"dataclass\")\n        annotations[property] = typ\n\n        if property not in required:\n            defaults[property] = None\n\n    class_dict = {\n        '__annotations__': annotations,\n        '__module__': __name__,\n    }\n\n    for property, default_val in defaults.items():\n        class_dict[property] = field(default=default_val)\n\n    cls = type(name or \"AnonymousDataclass\", (), class_dict)\n    return dataclass(cls)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.json_schema_utils.json_schema_dict_to_pydantic","title":"<code>json_schema_dict_to_pydantic(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a Pydantic BaseModel class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Pydantic BaseModel</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the Pydantic BaseModel</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic BaseModel class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_pydantic(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type[BaseModel]:\n    \"\"\"Convert a JSON Schema dict into a Pydantic BaseModel class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Pydantic BaseModel\n    name: Optional[str]\n        The name of the Pydantic BaseModel\n\n    Returns\n    -------\n    type[BaseModel]\n        The Pydantic BaseModel class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    field_definitions: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"pydantic\")\n        if property not in required:\n            field_definitions[property] = (Optional[typ], None)\n        else:\n            field_definitions[property] = (typ, ...)\n\n    return create_model(name or \"AnonymousPydanticModel\", **field_definitions)\n</code></pre>"},{"location":"api_reference/types/#outlines.types.json_schema_utils.json_schema_dict_to_typeddict","title":"<code>json_schema_dict_to_typeddict(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a TypedDict class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a TypedDict</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the TypedDict</p> <code>None</code> <p>Returns:</p> Type Description <code>_TypedDictMeta</code> <p>The TypedDict class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_typeddict(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; _TypedDictMeta:\n    \"\"\"Convert a JSON Schema dict into a TypedDict class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a TypedDict\n    name: Optional[str]\n        The name of the TypedDict\n\n    Returns\n    -------\n    _TypedDictMeta\n        The TypedDict class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"typeddict\")\n        if property not in required:\n            typ = Optional[typ]\n        annotations[property] = typ\n\n    return TypedDict(name or \"AnonymousTypedDict\", annotations)  # type: ignore\n</code></pre>"},{"location":"api_reference/types/#outlines.types.json_schema_utils.schema_type_to_python","title":"<code>schema_type_to_python(schema, caller_target_type)</code>","text":"<p>Get a Python type from a JSON Schema dict.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Python type</p> required <code>caller_target_type</code> <code>Literal['pydantic', 'typeddict', 'dataclass']</code> <p>The type of the caller</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The Python type</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def schema_type_to_python(\n    schema: dict,\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n) -&gt; Any:\n    \"\"\"Get a Python type from a JSON Schema dict.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Python type\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n        The type of the caller\n\n    Returns\n    -------\n    Any\n        The Python type\n\n    \"\"\"\n    if \"enum\" in schema:\n        values = schema[\"enum\"]\n        return Literal[tuple(values)]\n\n    t = schema.get(\"type\")\n\n    if t == \"string\":\n        return str\n    elif t == \"integer\":\n        return int\n    elif t == \"number\":\n        return float\n    elif t == \"boolean\":\n        return bool\n    elif t == \"array\":\n        items = schema.get(\"items\", {})\n        if items:\n            item_type = schema_type_to_python(items, caller_target_type)\n        else:\n            item_type = Any\n        return List[item_type]  # type: ignore\n    elif t == \"object\":\n        name = schema.get(\"title\")\n        if caller_target_type == \"pydantic\":\n            return json_schema_dict_to_pydantic(schema, name)\n        elif caller_target_type == \"typeddict\":\n            return json_schema_dict_to_typeddict(schema, name)\n        elif caller_target_type == \"dataclass\":\n            return json_schema_dict_to_dataclass(schema, name)\n\n    return Any\n</code></pre>"},{"location":"api_reference/types/#outlines.types.locale","title":"<code>locale</code>","text":"<p>Locale-specific regex patterns.</p>"},{"location":"api_reference/types/#outlines.types.locale.us","title":"<code>us</code>","text":"<p>Locale-specific regex patterns for the United States.</p>"},{"location":"api_reference/types/#outlines.types.utils","title":"<code>utils</code>","text":"<p>Utility functions for the types module.</p>"},{"location":"api_reference/types/#outlines.types.utils.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/types/utils.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; dict:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    try:\n        fn_name = fn.__name__\n    except Exception as e:\n        fn_name = \"Arguments\"\n        warnings.warn(\n            f\"The function name could not be determined. Using default name 'Arguments' instead. For debugging, here is exact error:\\n{e}\",\n            category=UserWarning,\n        )\n    model = create_model(fn_name, **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api_reference/types/airports/","title":"airports","text":"<p>Generate valid airport codes.</p>"},{"location":"api_reference/types/countries/","title":"countries","text":"<p>Generate valid country codes and names.</p>"},{"location":"api_reference/types/countries/#outlines.types.countries.get_country_flags","title":"<code>get_country_flags()</code>","text":"<p>Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.</p> Source code in <code>outlines/types/countries.py</code> <pre><code>def get_country_flags():\n    \"\"\"Generate Unicode flags for all ISO 3166-1 alpha-2 country codes in Alpha2 Enum.\"\"\"\n    base = ord(\"\ud83c\udde6\")\n    return {\n        code.name: chr(base + ord(code.name[0]) - ord(\"A\"))\n        + chr(base + ord(code.name[1]) - ord(\"A\"))\n        for code in Alpha2\n    }\n</code></pre>"},{"location":"api_reference/types/dsl/","title":"dsl","text":"<p>Regular expression DSL and output types for structured generation.</p> <p>This module contains elements related to three logical steps in the use of output types for structured generation:</p> <ol> <li>Definition of <code>Term</code> classes that contain output type definitions. That    includes both terms intended to be used by themselves such as <code>JsonSchema</code>    or <code>CFG</code> and terms that are part of the regular expression DSL such as    <code>Alternatives</code> or <code>KleeneStar</code> (and the related functions).</li> <li>Conversion of Python types into <code>Term</code> instances (<code>python_types_to_terms</code>).</li> <li>Conversion of a <code>Term</code> instance into a regular expression (<code>to_regex</code>).</li> </ol>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.CFG","title":"<code>CFG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a context-free grammar.</p> <p>Parameters:</p> Name Type Description Default <code>definition</code> <code>str</code> <p>The definition of the context-free grammar as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass CFG(Term):\n    \"\"\"Class representing a context-free grammar.\n\n    Parameters\n    ----------\n    definition\n        The definition of the context-free grammar as a string.\n\n    \"\"\"\n    definition: str\n\n    def _display_node(self) -&gt; str:\n        return f\"CFG('{self.definition}')\"\n\n    def __repr__(self):\n        return f\"CFG(definition='{self.definition}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, CFG):\n            return False\n        return self.definition == other.definition\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"CFG\":\n        \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n        Parameters\n        ----------\n        path : str\n            The path to the file containing the CFG definition.\n        Returns\n        -------\n        CFG\n            A CFG instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            definition = f.read()\n        return cls(definition)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.CFG.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a CFG instance from a file containing a CFG definition.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the CFG definition.</p> required <p>Returns:</p> Type Description <code>CFG</code> <p>A CFG instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"CFG\":\n    \"\"\"Create a CFG instance from a file containing a CFG definition.\n\n    Parameters\n    ----------\n    path : str\n        The path to the file containing the CFG definition.\n    Returns\n    -------\n    CFG\n        A CFG instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        definition = f.read()\n    return cls(definition)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.Choice","title":"<code>Choice</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a choice between different items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>The items to choose from.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Choice(Term):\n    \"\"\"Class representing a choice between different items.\n\n    Parameters\n    ----------\n    items\n        The items to choose from.\n\n    \"\"\"\n    items: List[Any]\n\n    def _display_node(self) -&gt; str:\n        return f\"Choice({repr(self.items)})\"\n\n    def __repr__(self):\n        return f\"Choice(items={repr(self.items)})\"\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.JsonSchema","title":"<code>JsonSchema</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a JSON schema.</p> <p>The JSON schema object from which to instantiate the class can be a dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a genSON schema builder.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>class JsonSchema(Term):\n    \"\"\"Class representing a JSON schema.\n\n    The JSON schema object from which to instantiate the class can be a\n    dictionary, a string, a Pydantic model, a typed dict, a dataclass, or a\n    genSON schema builder.\n\n    \"\"\"\n    schema: str\n    whitespace_pattern: OptionalType[str]\n\n    def __init__(\n        self,\n        schema: Union[\n            dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n        ],\n        whitespace_pattern: OptionalType[str] = None,\n        ensure_ascii: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        schema\n            The object containing the JSON schema.\n        whitespace_pattern\n            The pattern to use to match whitespace characters.\n        ensure_ascii\n            Whether to ensure the schema is ASCII-only.\n\n        \"\"\"\n        schema_str: str\n\n        if is_dict_instance(schema):\n            schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n        elif is_str_instance(schema):\n            schema_str = str(schema)\n        elif is_pydantic_model(schema):\n            schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n        elif is_typed_dict(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_dataclass(schema):\n            schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n        elif is_genson_schema_builder(schema):\n            schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n        else:\n            raise ValueError(\n                f\"Cannot parse schema {schema}. The schema must be either \"\n                + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n                + \"builder or a string or dict that contains the JSON schema \"\n                + \"specification\"\n            )\n\n        self.schema = schema_str\n        self.whitespace_pattern = whitespace_pattern\n\n    def __post_init__(self):\n        jsonschema.Draft7Validator.check_schema(json.loads(self.schema))\n\n    @classmethod\n    def is_json_schema(cls, obj: Any) -&gt; bool:\n        \"\"\"Check if the object provided is a JSON schema type.\n\n        Parameters\n        ----------\n        obj: Any\n            The object to check\n\n        Returns\n        -------\n        bool\n            True if the object is a JSON schema type, False otherwise\n\n        \"\"\"\n        return (\n            isinstance(obj, cls)\n            or is_pydantic_model(obj)\n            or is_typed_dict(obj)\n            or is_dataclass(obj)\n            or is_genson_schema_builder(obj)\n        )\n\n    @classmethod\n    def convert_to(\n        cls,\n        schema: Union[\n            \"JsonSchema\",\n            type[BaseModel],\n            _TypedDictMeta,\n            type,\n            SchemaBuilder,\n        ],\n        target_types: List[Literal[\n            \"str\",\n            \"dict\",\n            \"pydantic\",\n            \"typeddict\",\n            \"dataclass\",\n            \"genson\",\n        ]],\n    ) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n        \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n        If the schema provided is already of a type in the target_types, return\n        it unchanged.\n\n        Parameters\n        ----------\n        schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n            The schema to convert\n        target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n            The target types to convert to\n\n        \"\"\"\n        # If the schema provided is already of a type in the target_types,\n        # just return it\n        if isinstance(schema, cls):\n            if \"str\" in target_types:\n                return schema.schema\n            elif \"dict\" in target_types:\n                return json.loads(schema.schema)\n        elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n            return schema\n        elif is_typed_dict(schema) and \"typeddict\" in target_types:\n            return schema\n        elif is_dataclass(schema) and \"dataclass\" in target_types:\n            return schema\n        elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n            return schema\n\n        # Convert the schema to a JSON schema string/dict\n        if isinstance(schema, cls):\n            schema_str = schema.schema\n        else:\n            schema_str = cls(schema).schema\n        schema_dict = json.loads(schema_str)\n\n        for target_type in target_types:\n            try:\n                # Convert the JSON schema string to the target type\n                if target_type == \"str\":\n                    return schema_str\n                elif target_type == \"dict\":\n                    return schema_dict\n                elif target_type == \"pydantic\":\n                    return json_schema_dict_to_pydantic(schema_dict)\n                elif target_type == \"typeddict\":\n                    return json_schema_dict_to_typeddict(schema_dict)\n                elif target_type == \"dataclass\":\n                    return json_schema_dict_to_dataclass(schema_dict)\n                # No conversion available for genson\n            except Exception as e:  # pragma: no cover\n                warnings.warn(\n                    f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n                )\n                continue\n\n        raise ValueError(\n            f\"Cannot convert schema type {type(schema)} to any of the target \"\n            f\"types {target_types}\"\n        )\n\n    def _display_node(self) -&gt; str:\n        return f\"JsonSchema('{self.schema}')\"\n\n    def __repr__(self):\n        return f\"JsonSchema(schema='{self.schema}')\"\n\n    def __eq__(self, other):\n        if not isinstance(other, JsonSchema):\n            return False\n        try:\n            self_dict = json.loads(self.schema)\n            other_dict = json.loads(other.schema)\n            return self_dict == other_dict\n        except json.JSONDecodeError:  # pragma: no cover\n            return self.schema == other.schema\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"JsonSchema\":\n        \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n        schema.\n\n        Parameters\n        ----------\n        path:\n            The path to the file containing the JSON schema.\n        Returns\n        -------\n        JsonSchema\n            A JsonSchema instance.\n\n        \"\"\"\n        with open(path, \"r\") as f:\n            schema = json.load(f)\n        return cls(schema)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.JsonSchema.__init__","title":"<code>__init__(schema, whitespace_pattern=None, ensure_ascii=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The object containing the JSON schema.</p> required <code>whitespace_pattern</code> <code>Optional[str]</code> <p>The pattern to use to match whitespace characters.</p> <code>None</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to ensure the schema is ASCII-only.</p> <code>True</code> Source code in <code>outlines/types/dsl.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[\n        dict, str, type[BaseModel], _TypedDictMeta, type, SchemaBuilder\n    ],\n    whitespace_pattern: OptionalType[str] = None,\n    ensure_ascii: bool = True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    schema\n        The object containing the JSON schema.\n    whitespace_pattern\n        The pattern to use to match whitespace characters.\n    ensure_ascii\n        Whether to ensure the schema is ASCII-only.\n\n    \"\"\"\n    schema_str: str\n\n    if is_dict_instance(schema):\n        schema_str = json.dumps(schema, ensure_ascii=ensure_ascii)\n    elif is_str_instance(schema):\n        schema_str = str(schema)\n    elif is_pydantic_model(schema):\n        schema_str = json.dumps(schema.model_json_schema(), ensure_ascii=ensure_ascii) # type: ignore\n    elif is_typed_dict(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_dataclass(schema):\n        schema_str = json.dumps(TypeAdapter(schema).json_schema(), ensure_ascii=ensure_ascii)\n    elif is_genson_schema_builder(schema):\n        schema_str = schema.to_json(ensure_ascii=ensure_ascii)  # type: ignore\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {schema}. The schema must be either \"\n            + \"a Pydantic class, typed dict, a dataclass, a genSON schema \"\n            + \"builder or a string or dict that contains the JSON schema \"\n            + \"specification\"\n        )\n\n    self.schema = schema_str\n    self.whitespace_pattern = whitespace_pattern\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.JsonSchema.convert_to","title":"<code>convert_to(schema, target_types)</code>  <code>classmethod</code>","text":"<p>Convert a JSON schema type to a different JSON schema type.</p> <p>If the schema provided is already of a type in the target_types, return it unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]</code> <p>The schema to convert</p> required <code>target_types</code> <code>List[Literal['str', 'dict', 'pydantic', 'typeddict', 'dataclass', 'genson']]</code> <p>The target types to convert to</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef convert_to(\n    cls,\n    schema: Union[\n        \"JsonSchema\",\n        type[BaseModel],\n        _TypedDictMeta,\n        type,\n        SchemaBuilder,\n    ],\n    target_types: List[Literal[\n        \"str\",\n        \"dict\",\n        \"pydantic\",\n        \"typeddict\",\n        \"dataclass\",\n        \"genson\",\n    ]],\n) -&gt; Union[str, dict, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]:\n    \"\"\"Convert a JSON schema type to a different JSON schema type.\n\n    If the schema provided is already of a type in the target_types, return\n    it unchanged.\n\n    Parameters\n    ----------\n    schema: Union[JsonSchema, type[BaseModel], _TypedDictMeta, type, SchemaBuilder]\n        The schema to convert\n    target_types: List[Literal[\"str\", \"dict\", \"pydantic\", \"typeddict\", \"dataclass\", \"genson\"]]\n        The target types to convert to\n\n    \"\"\"\n    # If the schema provided is already of a type in the target_types,\n    # just return it\n    if isinstance(schema, cls):\n        if \"str\" in target_types:\n            return schema.schema\n        elif \"dict\" in target_types:\n            return json.loads(schema.schema)\n    elif is_pydantic_model(schema) and \"pydantic\" in target_types:\n        return schema\n    elif is_typed_dict(schema) and \"typeddict\" in target_types:\n        return schema\n    elif is_dataclass(schema) and \"dataclass\" in target_types:\n        return schema\n    elif is_genson_schema_builder(schema) and \"genson\" in target_types:\n        return schema\n\n    # Convert the schema to a JSON schema string/dict\n    if isinstance(schema, cls):\n        schema_str = schema.schema\n    else:\n        schema_str = cls(schema).schema\n    schema_dict = json.loads(schema_str)\n\n    for target_type in target_types:\n        try:\n            # Convert the JSON schema string to the target type\n            if target_type == \"str\":\n                return schema_str\n            elif target_type == \"dict\":\n                return schema_dict\n            elif target_type == \"pydantic\":\n                return json_schema_dict_to_pydantic(schema_dict)\n            elif target_type == \"typeddict\":\n                return json_schema_dict_to_typeddict(schema_dict)\n            elif target_type == \"dataclass\":\n                return json_schema_dict_to_dataclass(schema_dict)\n            # No conversion available for genson\n        except Exception as e:  # pragma: no cover\n            warnings.warn(\n                f\"Cannot convert schema type {type(schema)} to {target_type}: {e}\"\n            )\n            continue\n\n    raise ValueError(\n        f\"Cannot convert schema type {type(schema)} to any of the target \"\n        f\"types {target_types}\"\n    )\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.JsonSchema.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Create a JsonSchema instance from a .json file containing a JSON schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file containing the JSON schema.</p> required <p>Returns:</p> Type Description <code>JsonSchema</code> <p>A JsonSchema instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str) -&gt; \"JsonSchema\":\n    \"\"\"Create a JsonSchema instance from a .json file containing a JSON\n    schema.\n\n    Parameters\n    ----------\n    path:\n        The path to the file containing the JSON schema.\n    Returns\n    -------\n    JsonSchema\n        A JsonSchema instance.\n\n    \"\"\"\n    with open(path, \"r\") as f:\n        schema = json.load(f)\n    return cls(schema)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.JsonSchema.is_json_schema","title":"<code>is_json_schema(obj)</code>  <code>classmethod</code>","text":"<p>Check if the object provided is a JSON schema type.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a JSON schema type, False otherwise</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>@classmethod\ndef is_json_schema(cls, obj: Any) -&gt; bool:\n    \"\"\"Check if the object provided is a JSON schema type.\n\n    Parameters\n    ----------\n    obj: Any\n        The object to check\n\n    Returns\n    -------\n    bool\n        True if the object is a JSON schema type, False otherwise\n\n    \"\"\"\n    return (\n        isinstance(obj, cls)\n        or is_pydantic_model(obj)\n        or is_typed_dict(obj)\n        or is_dataclass(obj)\n        or is_genson_schema_builder(obj)\n    )\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.Regex","title":"<code>Regex</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Term</code></p> <p>Class representing a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression as a string.</p> required Source code in <code>outlines/types/dsl.py</code> <pre><code>@dataclass\nclass Regex(Term):\n    \"\"\"Class representing a regular expression.\n\n    Parameters\n    ----------\n    pattern\n        The regular expression as a string.\n\n    \"\"\"\n    pattern: str\n\n    def _display_node(self) -&gt; str:\n        return f\"Regex('{self.pattern}')\"\n\n    def __repr__(self):\n        return f\"Regex(pattern='{self.pattern}')\"\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.Term","title":"<code>Term</code>","text":"<p>Represents types defined with a regular expression.</p> <p><code>Regex</code> instances can be used as a type in a Pydantic model definittion. They will be translated to JSON Schema as a \"string\" field with the \"pattern\" keyword set to the regular expression this class represents. The class also handles validation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from outlines.types import Regex\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n&gt;&gt;&gt;     name: str\n&gt;&gt;&gt;     age: age_type\n</code></pre> Source code in <code>outlines/types/dsl.py</code> <pre><code>class Term:\n    \"\"\"Represents types defined with a regular expression.\n\n    `Regex` instances can be used as a type in a Pydantic model definittion.\n    They will be translated to JSON Schema as a \"string\" field with the\n    \"pattern\" keyword set to the regular expression this class represents. The\n    class also handles validation.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; from outlines.types import Regex\n    &gt;&gt;&gt; from pydantic import BaseModel\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; age_type = Regex(\"[0-9]+\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; class User(BaseModel):\n    &gt;&gt;&gt;     name: str\n    &gt;&gt;&gt;     age: age_type\n\n    \"\"\"\n\n    def __add__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([self, other])\n\n    def __radd__(self: \"Term\", other: \"Term\") -&gt; \"Sequence\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Sequence([other, self])\n\n    def __or__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([self, other])\n\n    def __ror__(self: \"Term\", other: \"Term\") -&gt; \"Alternatives\":\n        if is_str_instance(other):\n            other = String(str(other))\n\n        return Alternatives([other, self])\n\n    def __get_validator__(self, _core_schema):\n        def validate(input_value):\n            return self.validate(input_value)\n\n        return validate\n\n    def __get_pydantic_core_schema__(\n        self, source_type: Any, handler: GetCoreSchemaHandler\n    ) -&gt; cs.CoreSchema:\n        return cs.no_info_plain_validator_function(lambda value: self.validate(value))\n\n    def __get_pydantic_json_schema__(\n        self, core_schema: cs.CoreSchema, handler: GetJsonSchemaHandler\n    ) -&gt; JsonSchemaValue:\n        return {\"type\": \"string\", \"pattern\": to_regex(self)}\n\n    def validate(self, value: str) -&gt; str:\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if not compiled.fullmatch(str(value)):\n            raise ValueError(\n                f\"Input should be in the language of the regular expression {pattern}\"\n            )\n        return value\n\n    def matches(self, value: str) -&gt; bool:\n        \"\"\"Check that a given value is in the language defined by the Term.\n\n        We make the assumption that the language defined by the term can\n        be defined with a regular expression.\n\n        \"\"\"\n        pattern = to_regex(self)\n        compiled = re.compile(pattern)\n        if compiled.fullmatch(str(value)):\n            return True\n        return False\n\n    def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n        \"\"\"Display the regex tree in ASCII format.\"\"\"\n        branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n        result = indent + branch + self._display_node() + \"\\n\"\n\n        # Calculate the new indent for children\n        new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n        # Let each subclass handle its children\n        result += self._display_children(new_indent)\n        return result\n\n    def _display_node(self):\n        raise NotImplementedError\n\n    def _display_children(self, indent: str) -&gt; str:\n        \"\"\"Display the children of this node. Override in subclasses with children.\"\"\"\n        return \"\"\n\n    def __str__(self):\n        return self.display_ascii_tree()\n\n    def optional(self) -&gt; \"Optional\":\n        return optional(self)\n\n    def exactly(self, count: int) -&gt; \"QuantifyExact\":\n        return exactly(count, self)\n\n    def at_least(self, count: int) -&gt; \"QuantifyMinimum\":\n        return at_least(count, self)\n\n    def at_most(self, count: int) -&gt; \"QuantifyMaximum\":\n        return at_most(count, self)\n\n    def between(self, min_count: int, max_count: int) -&gt; \"QuantifyBetween\":\n        return between(min_count, max_count, self)\n\n    def one_or_more(self) -&gt; \"KleenePlus\":\n        return one_or_more(self)\n\n    def zero_or_more(self) -&gt; \"KleeneStar\":\n        return zero_or_more(self)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.Term.display_ascii_tree","title":"<code>display_ascii_tree(indent='', is_last=True)</code>","text":"<p>Display the regex tree in ASCII format.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def display_ascii_tree(self, indent=\"\", is_last=True) -&gt; str:\n    \"\"\"Display the regex tree in ASCII format.\"\"\"\n    branch = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n    result = indent + branch + self._display_node() + \"\\n\"\n\n    # Calculate the new indent for children\n    new_indent = indent + (\"    \" if is_last else \"\u2502   \")\n\n    # Let each subclass handle its children\n    result += self._display_children(new_indent)\n    return result\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.Term.matches","title":"<code>matches(value)</code>","text":"<p>Check that a given value is in the language defined by the Term.</p> <p>We make the assumption that the language defined by the term can be defined with a regular expression.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def matches(self, value: str) -&gt; bool:\n    \"\"\"Check that a given value is in the language defined by the Term.\n\n    We make the assumption that the language defined by the term can\n    be defined with a regular expression.\n\n    \"\"\"\n    pattern = to_regex(self)\n    compiled = re.compile(pattern)\n    if compiled.fullmatch(str(value)):\n        return True\n    return False\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.at_least","title":"<code>at_least(count, term)</code>","text":"<p>Repeat the term at least <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_least(count: int, term: Union[Term, str]) -&gt; QuantifyMinimum:\n    \"\"\"Repeat the term at least `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMinimum(term, count)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.at_most","title":"<code>at_most(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def at_most(count: int, term: Union[Term, str]) -&gt; QuantifyMaximum:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyMaximum(term, count)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.either","title":"<code>either(*terms)</code>","text":"<p>Represents an alternative between different terms or strings.</p> <p>This factory function automatically translates string arguments into <code>String</code> objects.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def either(*terms: Union[str, Term]):\n    \"\"\"Represents an alternative between different terms or strings.\n\n    This factory function automatically translates string arguments\n    into `String` objects.\n\n    \"\"\"\n    terms = [String(arg) if isinstance(arg, str) else arg for arg in terms]\n    return Alternatives(terms)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.exactly","title":"<code>exactly(count, term)</code>","text":"<p>Repeat the term exactly <code>count</code> times.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def exactly(count: int, term: Union[Term, str]) -&gt; QuantifyExact:\n    \"\"\"Repeat the term exactly `count` times.\"\"\"\n    term = String(term) if isinstance(term, str) else term\n    return QuantifyExact(term, count)\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.python_types_to_terms","title":"<code>python_types_to_terms(ptype, recursion_depth=0)</code>","text":"<p>Convert Python types to Outlines DSL terms that constrain LLM output.</p> <p>Parameters:</p> Name Type Description Default <code>ptype</code> <code>Any</code> <p>The Python type to convert</p> required <code>recursion_depth</code> <code>int</code> <p>Current recursion depth to prevent infinite recursion</p> <code>0</code> <p>Returns:</p> Type Description <code>Term</code> <p>The corresponding DSL <code>Term</code> instance.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def python_types_to_terms(ptype: Any, recursion_depth: int = 0) -&gt; Term:\n    \"\"\"Convert Python types to Outlines DSL terms that constrain LLM output.\n\n    Parameters\n    ----------\n    ptype\n        The Python type to convert\n    recursion_depth\n        Current recursion depth to prevent infinite recursion\n\n    Returns\n    -------\n    Term\n        The corresponding DSL `Term` instance.\n\n    \"\"\"\n    if recursion_depth &gt; 10:\n        raise RecursionError(\n            f\"Maximum recursion depth exceeded when converting {ptype}. \"\n            \"This might be due to a recursive type definition.\"\n        )\n\n    # First handle Term instances\n    if isinstance(ptype, Term):\n        return ptype\n\n    # Basic types\n    if is_int(ptype):\n        return types.integer\n    elif is_float(ptype):\n        return types.number\n    elif is_bool(ptype):\n        return types.boolean\n    elif is_str(ptype):\n        return types.string\n    elif is_native_dict(ptype):\n        return CFG(grammars.json)\n    elif is_time(ptype):\n        return types.time\n    elif is_date(ptype):\n        return types.date\n    elif is_datetime(ptype):\n        return types.datetime\n\n    # Basic type instances\n    if is_str_instance(ptype):\n        return String(ptype)\n    elif is_int_instance(ptype) or is_float_instance(ptype):\n        return Regex(str(ptype))\n\n    # Structured types\n    structured_type_checks = [\n        lambda x: is_dataclass(x),\n        lambda x: is_typed_dict(x),\n        lambda x: is_pydantic_model(x),\n    ]\n    if any(check(ptype) for check in structured_type_checks):\n        schema = TypeAdapter(ptype).json_schema()\n        return JsonSchema(schema)\n\n    elif is_genson_schema_builder(ptype):\n        schema = ptype.to_json()\n        return JsonSchema(schema)\n\n    if is_enum(ptype):\n        return Alternatives(\n            [\n                python_types_to_terms(member, recursion_depth + 1)\n                for member in _get_enum_members(ptype)\n            ]\n        )\n\n    args = get_args(ptype)\n    if is_literal(ptype):\n        return _handle_literal(args)\n    elif is_union(ptype):\n        return _handle_union(args, recursion_depth)\n    elif is_typing_list(ptype):\n        return _handle_list(args, recursion_depth)\n    elif is_typing_tuple(ptype):\n        return _handle_tuple(args, recursion_depth)\n    elif is_typing_dict(ptype):\n        return _handle_dict(args, recursion_depth)\n\n    if is_callable(ptype):\n        return JsonSchema(get_schema_from_signature(ptype))\n\n    type_name = getattr(ptype, \"__name__\", ptype)\n    raise TypeError(\n        f\"Type {type_name} is currently not supported. Please open an issue: \"\n        \"https://github.com/dottxt-ai/outlines/issues\"\n    )\n</code></pre>"},{"location":"api_reference/types/dsl/#outlines.types.dsl.to_regex","title":"<code>to_regex(term)</code>","text":"<p>Convert a term to a regular expression.</p> <p>We only consider self-contained terms that do not refer to another rule.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>Term</code> <p>The term to convert to a regular expression.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The regular expression as a string.</p> Source code in <code>outlines/types/dsl.py</code> <pre><code>def to_regex(term: Term) -&gt; str:\n    \"\"\"Convert a term to a regular expression.\n\n    We only consider self-contained terms that do not refer to another rule.\n\n    Parameters\n    ----------\n    term\n        The term to convert to a regular expression.\n\n    Returns\n    -------\n    str\n        The regular expression as a string.\n\n    \"\"\"\n    if isinstance(term, String):\n        return re.escape(term.value)\n    elif isinstance(term, Regex):\n        return f\"({term.pattern})\"\n    elif isinstance(term, JsonSchema):\n        regex_str = outlines_core.json_schema.build_regex_from_schema(term.schema, term.whitespace_pattern)\n        return f\"({regex_str})\"\n    elif isinstance(term, Choice):\n        regexes = [to_regex(python_types_to_terms(item)) for item in term.items]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, KleeneStar):\n        return f\"({to_regex(term.term)})*\"\n    elif isinstance(term, KleenePlus):\n        return f\"({to_regex(term.term)})+\"\n    elif isinstance(term, Optional):\n        return f\"({to_regex(term.term)})?\"\n    elif isinstance(term, Alternatives):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"({'|'.join(regexes)})\"\n    elif isinstance(term, Sequence):\n        regexes = [to_regex(subterm) for subterm in term.terms]\n        return f\"{''.join(regexes)}\"\n    elif isinstance(term, QuantifyExact):\n        return f\"({to_regex(term.term)}){{{term.count}}}\"\n    elif isinstance(term, QuantifyMinimum):\n        return f\"({to_regex(term.term)}){{{term.min_count},}}\"\n    elif isinstance(term, QuantifyMaximum):\n        return f\"({to_regex(term.term)}){{,{term.max_count}}}\"\n    elif isinstance(term, QuantifyBetween):\n        return f\"({to_regex(term.term)}){{{term.min_count},{term.max_count}}}\"\n    else:\n        raise TypeError(\n            f\"Cannot convert object {repr(term)} to a regular expression.\"\n        )\n</code></pre>"},{"location":"api_reference/types/json_schema_utils/","title":"json_schema_utils","text":"<p>Convert JSON Schema dicts to Python types.</p>"},{"location":"api_reference/types/json_schema_utils/#outlines.types.json_schema_utils.json_schema_dict_to_dataclass","title":"<code>json_schema_dict_to_dataclass(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a dataclass.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a dataclass</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the dataclass</p> <code>None</code> <p>Returns:</p> Type Description <code>type</code> <p>The dataclass</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_dataclass(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type:\n    \"\"\"Convert a JSON Schema dict into a dataclass.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a dataclass\n    name: Optional[str]\n        The name of the dataclass\n\n    Returns\n    -------\n    type\n        The dataclass\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n    defaults: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"dataclass\")\n        annotations[property] = typ\n\n        if property not in required:\n            defaults[property] = None\n\n    class_dict = {\n        '__annotations__': annotations,\n        '__module__': __name__,\n    }\n\n    for property, default_val in defaults.items():\n        class_dict[property] = field(default=default_val)\n\n    cls = type(name or \"AnonymousDataclass\", (), class_dict)\n    return dataclass(cls)\n</code></pre>"},{"location":"api_reference/types/json_schema_utils/#outlines.types.json_schema_utils.json_schema_dict_to_pydantic","title":"<code>json_schema_dict_to_pydantic(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a Pydantic BaseModel class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Pydantic BaseModel</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the Pydantic BaseModel</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic BaseModel class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_pydantic(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; type[BaseModel]:\n    \"\"\"Convert a JSON Schema dict into a Pydantic BaseModel class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Pydantic BaseModel\n    name: Optional[str]\n        The name of the Pydantic BaseModel\n\n    Returns\n    -------\n    type[BaseModel]\n        The Pydantic BaseModel class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    field_definitions: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"pydantic\")\n        if property not in required:\n            field_definitions[property] = (Optional[typ], None)\n        else:\n            field_definitions[property] = (typ, ...)\n\n    return create_model(name or \"AnonymousPydanticModel\", **field_definitions)\n</code></pre>"},{"location":"api_reference/types/json_schema_utils/#outlines.types.json_schema_utils.json_schema_dict_to_typeddict","title":"<code>json_schema_dict_to_typeddict(schema, name=None)</code>","text":"<p>Convert a JSON Schema dict into a TypedDict class.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a TypedDict</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the TypedDict</p> <code>None</code> <p>Returns:</p> Type Description <code>_TypedDictMeta</code> <p>The TypedDict class</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def json_schema_dict_to_typeddict(\n    schema: dict,\n    name: Optional[str] = None\n) -&gt; _TypedDictMeta:\n    \"\"\"Convert a JSON Schema dict into a TypedDict class.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a TypedDict\n    name: Optional[str]\n        The name of the TypedDict\n\n    Returns\n    -------\n    _TypedDictMeta\n        The TypedDict class\n\n    \"\"\"\n    required = set(schema.get(\"required\", []))\n    properties = schema.get(\"properties\", {})\n\n    annotations: Dict[str, Any] = {}\n\n    for property, details in properties.items():\n        typ = schema_type_to_python(details, \"typeddict\")\n        if property not in required:\n            typ = Optional[typ]\n        annotations[property] = typ\n\n    return TypedDict(name or \"AnonymousTypedDict\", annotations)  # type: ignore\n</code></pre>"},{"location":"api_reference/types/json_schema_utils/#outlines.types.json_schema_utils.schema_type_to_python","title":"<code>schema_type_to_python(schema, caller_target_type)</code>","text":"<p>Get a Python type from a JSON Schema dict.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON Schema dict to convert to a Python type</p> required <code>caller_target_type</code> <code>Literal['pydantic', 'typeddict', 'dataclass']</code> <p>The type of the caller</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The Python type</p> Source code in <code>outlines/types/json_schema_utils.py</code> <pre><code>def schema_type_to_python(\n    schema: dict,\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n) -&gt; Any:\n    \"\"\"Get a Python type from a JSON Schema dict.\n\n    Parameters\n    ----------\n    schema: dict\n        The JSON Schema dict to convert to a Python type\n    caller_target_type: Literal[\"pydantic\", \"typeddict\", \"dataclass\"]\n        The type of the caller\n\n    Returns\n    -------\n    Any\n        The Python type\n\n    \"\"\"\n    if \"enum\" in schema:\n        values = schema[\"enum\"]\n        return Literal[tuple(values)]\n\n    t = schema.get(\"type\")\n\n    if t == \"string\":\n        return str\n    elif t == \"integer\":\n        return int\n    elif t == \"number\":\n        return float\n    elif t == \"boolean\":\n        return bool\n    elif t == \"array\":\n        items = schema.get(\"items\", {})\n        if items:\n            item_type = schema_type_to_python(items, caller_target_type)\n        else:\n            item_type = Any\n        return List[item_type]  # type: ignore\n    elif t == \"object\":\n        name = schema.get(\"title\")\n        if caller_target_type == \"pydantic\":\n            return json_schema_dict_to_pydantic(schema, name)\n        elif caller_target_type == \"typeddict\":\n            return json_schema_dict_to_typeddict(schema, name)\n        elif caller_target_type == \"dataclass\":\n            return json_schema_dict_to_dataclass(schema, name)\n\n    return Any\n</code></pre>"},{"location":"api_reference/types/utils/","title":"utils","text":"<p>Utility functions for the types module.</p>"},{"location":"api_reference/types/utils/#outlines.types.utils.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/types/utils.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; dict:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    try:\n        fn_name = fn.__name__\n    except Exception as e:\n        fn_name = \"Arguments\"\n        warnings.warn(\n            f\"The function name could not be determined. Using default name 'Arguments' instead. For debugging, here is exact error:\\n{e}\",\n            category=UserWarning,\n        )\n    model = create_model(fn_name, **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api_reference/types/locale/","title":"locale","text":"<p>Locale-specific regex patterns.</p>"},{"location":"api_reference/types/locale/#outlines.types.locale.us","title":"<code>us</code>","text":"<p>Locale-specific regex patterns for the United States.</p>"},{"location":"api_reference/types/locale/us/","title":"us","text":"<p>Locale-specific regex patterns for the United States.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"community/","title":"Community","text":"<p>Outlines exists for a community of users who believe software doesn't need to be complicated. Who share the same passion for Large Language Models but don't want to compromise on robustness. Together, we are bringing these powerful models back to the world of software.</p>"},{"location":"community/#connect-on-discord","title":"Connect on Discord","text":"<p>The Outlines community lives on our Discord server. There you can ask questions, share ideas or just chat with people like you. Don't be a stranger and join us.</p>"},{"location":"community/contribute/","title":"Contribute","text":""},{"location":"community/contribute/#what-contributions","title":"What contributions?","text":"<ul> <li>Documentation contributions are very valuable to us!</li> <li>Examples. Show us what you did with Outlines :)</li> <li>Bug reports with a minimum working examples in the issue tracker</li> <li>Bug fixes are always a pleasure to review.</li> <li>New features. Please start a new discussion, or come chat with us beforehand!</li> </ul> <p>Note that the issue tracker is only intended for actionable items. In doubt, open a discussion or come talk to us.</p>"},{"location":"community/contribute/#how-to-contribute","title":"How to contribute?","text":""},{"location":"community/contribute/#setup","title":"Setup","text":"<p>First, fork the repository on GitHub and clone the fork locally:</p> <pre><code>git clone git@github.com/YourUserName/outlines.git\ncd outlines\n</code></pre> <p>Create a new virtual environment:</p> <p>If you are using <code>uv</code>:</p> <pre><code>uv venv\nsource .venv/bin/activate\nalias pip=\"uv pip\" # ... or just remember to prepend any pip command with uv in the rest of this guide\n</code></pre> <p>If you are using <code>venv</code>:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>If you are using <code>conda</code>:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Then install the dependencies in editable mode, and install the <code>pre-commit</code> hooks:</p> <p><pre><code>pip install -e \".[test]\"\npre-commit install\n</code></pre> If you own a GPU and want to run the vLLM tests you will have to run:</p> <pre><code>pip install -e \".[test-gpu]\"\n</code></pre> <p>instead.</p> <p>Outlines provides optional dependencies for different supported backends, which you can install with</p> <pre><code>pip install \".[vllm]\"\n</code></pre> <p>A list of supported optional dependencies can be found in the installation guide.</p>"},{"location":"community/contribute/#using-vscode-devcontainer-github-codespaces","title":"Using VSCode DevContainer / GitHub Codespaces","text":"<p>If you want a fully pre-configured development environment, you can use VSCode DevContainers or GitHub Codespaces.</p>"},{"location":"community/contribute/#vscode-devcontainer","title":"VSCode DevContainer","text":"<ol> <li>Ensure that the Docker daemon is running on your machine.</li> <li>Install the Dev Containers extension in VSCode.</li> <li>Open the Outlines repository in VSCode. When prompted, Reopen in Container (or press <code>F1</code> and select \"Remote-Containers: Reopen in Container\").</li> <li>Run the normal setup steps. Your environment will not complain about missing system dependencies!</li> </ol>"},{"location":"community/contribute/#github-codespaces","title":"GitHub Codespaces","text":"<ol> <li>Navigate to the Outlines repository on GitHub.</li> <li>Click on the Code button and select the Codespaces tab.</li> <li>Click Create codespace on main (or another branch you are working on).</li> <li>GitHub will launch a pre-configured cloud development environment.</li> </ol> <p>You will not have access to a GPU, but you'll be able to make basic contributions to the project on the go while using a fully featured web-based IDE.</p>"},{"location":"community/contribute/#before-pushing-your-code","title":"Before pushing your code","text":"<p>Run the tests:</p> <pre><code>pytest\n</code></pre> <p>And run the code style checks:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"community/contribute/#benchmarking","title":"Benchmarking","text":"<p>Outlines uses asv for automated benchmark testing. Benchmarks are run automatically before pull requests are merged to prevent performance degradation.</p> <p>You can run the benchmark test suite locally with the following command:</p> <pre><code>asv run --config benchmarks/asv.conf.json\n</code></pre> <p>Caveats:</p> <ul> <li>If you're on a device with CUDA, you must add the argument <code>--launch-method spawn</code></li> <li>Uncommitted code will not be benchmarked, you must first commit your changes.</li> </ul>"},{"location":"community/contribute/#run-a-specific-test","title":"Run a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#profile-a-specific-test","title":"Profile a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json --profile -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#compare-to-originmain","title":"Compare to <code>origin/main</code>","text":"<pre><code>get fetch origin\nasv continuous origin/main HEAD --config benchmarks/asv.conf.json\n</code></pre>"},{"location":"community/contribute/#asv-pr-behavior","title":"ASV PR Behavior","text":"<ul> <li>View ASV Benchmark Results: Open the workflow, view <code>BENCHMARK RESULTS</code> section.</li> <li>Merging is blocked unless benchmarks are run for the latest commit.</li> <li>Benchmarks fail if performance degrades by more than 10% for any individual benchmark.</li> <li>The \"Benchmark PR\" workflow runs when it is manually dispatched, or if the <code>run_benchmarks</code> label is added to the PR they run for every commit.</li> </ul>"},{"location":"community/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>To work on the documentation you will need to install the related dependencies:</p> <pre><code>pip install -r requirements-doc.txt\n</code></pre> <p>To build the documentation and serve it locally, run the following command in the repository's root folder:</p> <pre><code>mkdocs serve\n</code></pre> <p>By following the instruction you will be able to view the documentation locally. It will be updated every time you make a change.</p>"},{"location":"community/contribute/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a new branch on your fork, commit and push the changes:</p> <pre><code>git checkout -b new-branch\ngit add .\ngit commit -m \"Changes I made\"\ngit push origin new-branch\n</code></pre> <p>Then you can open a pull request on GitHub. It should prompt you to do so. Every subsequent change that you make on your branch will update the pull request.</p> <p>Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback. If you need help, come tell us on Discord.</p>"},{"location":"community/examples/","title":"Community projects and articles","text":"<p>Publishing examples and articles about Outlines are a meaningful way to contribute to the community. Here is a list of projects we are aware of. Drop us a line if we forgot yours!</p> <p>MMSG is a Python library for generating interleaved text and image content in a structured format you can directly pass to downstream APIs.</p> <p>Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report shows that Structured Generation can outperform finetuning, and maybe even multimodality, in document-image understanding tasks as part of CVPR's 2nd MMFM Challenge.</p> <p>Chess LLM Arena is a HuggingFace Space where you can make LLMs compete in a chess match.</p> <p>LLM Data Gen is a HuggingFace Space that generates synthetic dataset files in JSONLines format.</p> <p>Fast, High-Fidelity LLM Decoding with Regex Constraints  presents an efficient alternative to Outlines's structured generation.</p> <p>gigax is an Open-Source library that allows to create real-time LLM-powered NPCs for video games.</p> <p>Improving Prompt Consistency with Structured Generations shows how structured generation can improve consistency of evaluation runs by reducing sensitivity to changes in prompt format.</p> <p>AskNews is a news curation service processing 300k news articles per day in a structured way, with Outlines.</p>"},{"location":"community/feedback/","title":"Feedback","text":"<p>If Outlines has been helpful to you, let us know on Discord or give us a shoutout on Twitter! It's always heartwarming \u2764\ufe0f</p> <p> <p></p> <p>I am once again reminding you that structured extraction using LLMs is going to transform every single industry in the next 10 years https://t.co/xQ3tcWnrZ8</p>\u2014 Sam Hogan (@0xSamHogan) April 17, 2024 <p>outline's growth is insane, using is an understatement! https://t.co/rHCNWhZdCs</p>\u2014 jason liu (@jxnlco) April 17, 2024 <p>Outlines is an amazing lib and more popular than @remilouf\u2019s modesty will admit. https://t.co/DfHbMPIlX1 https://t.co/mDHIWJrD0C</p>\u2014 Delip Rao e/\u03c3 (@deliprao) April 18, 2024 <p>Impressive implementation of a true regex / json / grammar guided text generation pic.twitter.com/RX5RVYaVIx</p>\u2014 Rohan Paul (@rohanpaul_ai) December 30, 2023 <p>Most underrated Github Repo in AI + LLM JSON guided Generation: https://t.co/lSB8KIet1H</p>\u2014 \ud83c\udf99Jean-Louis Queguiner (@JiliJeanlouis) December 18, 2023 <p>Nice and useful. https://t.co/LX72AE0lgt</p>\u2014 Dan Roy (@roydanroy) August 15, 2023 <p>HUGE dub for open source AI https://t.co/bYKuiEUZ1j</p>\u2014 kenneth \ud83d\udd87 (@k3nnethfrancis) August 15, 2023 <p>This is amazing - glad to see more outp guidance modules! Will try this out soon I'm wondering how they translate from regex automatons to token boundariesAlso why Open Source will succeed. Even today I don't see any guided output functionality from the big providers. https://t.co/Ity2H25Klf</p>\u2014 Hrishi (@hrishioa) August 14, 2023 <p>Outlines - a library to help LLM developers guide text generation in a fast and reliable way.\"Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.\"Need to check this out. Reliable JSON output is a common use\u2026 pic.twitter.com/Bkbh8vKogN</p>\u2014 elvis (@omarsar0) August 14, 2023 <p>Woah this is cool! Makes open source models more usable.Give any LLM Function Call capability (and more) with Outlines: https://t.co/PtPykR5ZGR https://t.co/RRQjWHnIxv pic.twitter.com/BwNnH8SMwv</p>\u2014 Yohei (@yoheinakajima) August 14, 2023 <p>This is awesome! Being able to guarantee the output's structure unblocks so many applications. This is a great milestone and a fundamental building block for more advanced AI apps. https://t.co/WdwMOc7hE8</p>\u2014 Guilherme Castro (@skastr052) August 15, 2023 <p>Juggling with the unpredictable outputs of ChatGPT API lately while building my product. \ud83d\ude13 Tried prompt engineering to channel its wisdom into a neat JSON, but it's like asking a cat to fetch. \ud83d\udc31Luckily, stumbled upon \"Outlines\" \u2013 looks like a promising way to tame the LLM\u2026 pic.twitter.com/oYQ6q8exAS</p>\u2014 Charlie (@14435635Sun) August 15, 2023 <p>A complex system of LLM input-outputs interacting with non-LLM agents and models benefits immeasurably from structured outputs. The outlines package saves so much time, https://t.co/NhVQ6NpKDR</p>\u2014 Amir Sani (@amirsani) November 26, 2023"},{"location":"community/feedback/#let-us-know","title":"Let us know!","text":"<p>We highly value the insights of our users, and we would love to hear from you. If you are using Outlines for your projects and would like to share your experience with us, let's connect:</p> <ul> <li>What are you building with it?</li> <li>What do you like about it?</li> <li>What challenges are you facing?</li> <li>What do you think could be improved?</li> </ul> <p>To schedule an appointment follow this link. This is exclusively intended to share your experience, please go on Discord or GitHub for support.</p>"},{"location":"community/versioning/","title":"Versioning Guide","text":"<p>The Outlines project follows a structured versioning scheme designed to provide clarity and minimize risk for downstream dependents.</p> <p>Each part of the version number (<code>major.minor.patch</code>) conveys information about the nature and impact of the changes included in the release.</p> <ul> <li>Major Releases includes compatibility-breaking changes to core interfaces, such as <code>LogitsProcessor</code>s and <code>Guides</code>.</li> <li>Minor Releases introduce changes of substance to internal or unexposed functionality. These changes are well tested and intended to maintain compatibility with existing use of core interfaces.</li> <li>Patch Releases address bug fixes and incorporate low-risk changes to improve stability and performance.</li> </ul> <p>Breaking Changes</p> <p>Outlines v1.0 introduced several breaking changes to the core interface. See the migration guide for more details.</p>"},{"location":"community/versioning/#releases","title":"Releases","text":"<p>Releases along with release notes can be found on the Outlines Releases GitHub Page.</p>"},{"location":"community/versioning/#version-pinning-recommendations","title":"Version Pinning Recommendations","text":"<p>Here are our recommendations for managing dependencies on the Outlines package:</p> <p>Small, Risk-Tolerant Projects: Pin to a specific major version.</p> <p>Large, Conservative Projects: Pin to a specific minor version.</p>"},{"location":"examples/","title":"Examples","text":"<p>This part of the documentation provides a few cookbooks that you can browse to get acquainted with the library and get some inspiration about what you could do with structured generation. Remember that you can easily change the model that is being used!</p> <ul> <li>Classification: Classify customer requests.</li> <li>Named Entity Extraction: Extract information from pizza orders.</li> <li>Dating Profiles: Build dating profiles from descriptions using prompt templating and JSON-structured generation.</li> <li>Chain Of Density: Summarize documents using chain of density prompting and JSON-structured generation.</li> <li>Playing Chess: Make Phi-3 Mini play chess against itself using regex-structured generation.</li> <li>SimToM: Improve LLMs' Theory of Mind capabilities with perspective-taking prompting and JSON-structured generation.</li> <li>Q&amp;A with Citations: Answer questions and provide citations using JSON-structured generation.</li> <li>Knowledge Graph Generation: Generate a Knowledge Graph from unstructured text using JSON-structured generation.</li> <li>Structured Generation Workflow:</li> <li>Chain Of Thought (CoT): Generate a series of intermediate reasoning steps using regex-structured generation.</li> <li>ReAct Agent: Build an agent with open weights models using regex-structured generation.</li> <li>Structured Generation from PDFs: Use Outlines with vision-language models to read PDFs and produce structured output.</li> <li>Earnings reports to CSV: Extract data from earnings reports to CSV using regex-structured generation.</li> <li>Receipt Digitization: Extract information from a picture of a receipt using structured generation.</li> <li>Extract Events Details:</li> </ul> <p>Run Outlines on the cloud:</p> <ul> <li>BentoML</li> <li>Cerebrium</li> <li>Modal</li> </ul>"},{"location":"examples/chain_of_density/","title":"Summarize documents using Chain of Density prompting","text":"<p>A good summary should be informative, concise and clear. While large language models are generally good at summarizing documents, their summaries tend to be long and contain redundant information; their information density tends to be on the lower end. This is where chain of Density, a new prompting technique, comes in. In this example we will show how one can implement chain of density with a few lines of code using Outlines, leveraging both Outline's prompt templating and its structured generation capabilities.</p> <p>The article we will try to summarize is the first three paragraphs of the Alan Turing page on Wikipedia:</p> <pre><code>article = \"\"\"\nAlan Mathison Turing OBE FRS (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] He is widely considered to be the father of theoretical computer science and artificial intelligence.[9]\n\nBorn in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes\u2013no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.[10][11]\n\nAfter the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.[13]\n\"\"\"\n</code></pre>"},{"location":"examples/chain_of_density/#how-chain-of-density-works","title":"How Chain Of Density works","text":"<p>Chain Of Density starts with asking the model to generate a first long and non-specific summary. Then it asks the model to generate 4 extra summaries by proceeding in the following way:</p> <ol> <li>Identify 1-3 entities missing in the previous summary;</li> <li>Add all entities marked as missing in the previous step, while not dropping entities;</li> <li>Make the summary more concise;</li> </ol> <p>The prompt also asks the model to return a list of JSON objects that contain the missing entities and the new summary. This is where structured generation will come in handy :) The paper provides the prompt and an example:</p> <p></p> <p>We can now implement the prompt provided in the paper. We stored the prompt template in a text file, and we can load it using the <code>Template</code> class:</p> <pre><code>from outlines import Template\n\nchain_of_density = Template.from_file(\"prompt_templates/chain_of_density.txt\")\n</code></pre> Note <p>Note that we modified the prompt slightly so it returns a JSON object that contains the summaries, instead of a list of summaries.</p>"},{"location":"examples/chain_of_density/#outlines-implementation","title":"Outlines implementation","text":"<p>We will use Outline's JSON-structured generation to ensure that the model's output is consistent with the format specified in the prompt. We start with defining the JSON objects that the model is asked to return using Pydantic. One JSON object that contains a list of <code>Summary</code> objects that contain the missing entities and new summary:</p> <pre><code>from pydantic import BaseModel, conlist\n\nclass Summary(BaseModel):\n    missing_entities: str\n    denser_summary: str\n\nclass Summaries(BaseModel):\n    summaries: conlist(Summary, max_length=5, min_length=5)\n</code></pre> <p>We now generate the prompt by passing the article we want to summarize to the prompt template previously loaded. We load a quantized version of Mistral-7B using the AutoAWQ library, and then use the <code>Summaries</code> schema to generate the summaries with structured generation:</p> <pre><code>import outlines\nimport transformers\n\nMODEL_NAME = \"TheBloke/Mistral-7B-OpenOrca-AWQ\"\n\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n    transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n)\nprompt = chain_of_density(article=article)\nresult = model(prompt, Summaries, max_new_tokens=2000)\n</code></pre> <p>We can now check the results:</p> <pre><code>print(result)\n# {'summaries': [\n#     {\n#       'missing_entities': 'English mathematician, cryptanalyst, philosopher',\n#       'denser_summary': 'Alan Mathison Turing was an English mathematician, cryptanalyst, philosopher.'\n#     },\n#     {\n#       'missing_entities': '',\n#       'denser_summary': \"Alan Mathison Turing was an English mathematician who was a crucial figure in WW2's Bletchley Park codebreaking centre and designed one of the first computers.\"\n#     },\n#     {\n#       'missing_entities': 'cryptanalyst, studied, biology, father',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and contributed to mathematical biology.'\n#     },\n#     {\n#       'missing_entities': 'biology, morphogenesis, chemical',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and predicted chemical reactions in morphogenesis.\n#     '},\n#     {\n#       'missing_entities': '',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, developed computer science, and made strides in mathematical biology research.'\n#       }\n# ]}\n</code></pre> <p>Not bad, considering we used a smallish model to generate the summary! Chain of Density seems to be a very effective prompting technique to generate dense summaries, even with small quantized models. Its implementation in Outlines is also very short.</p> <p>Note that this is the first article I tried and it worked out of the box. Try it out on other articles, and please share the results on Twitter, or by opening a new discussion on the Outlines repository!</p>"},{"location":"examples/chain_of_thought/","title":"Chain of thought","text":"<p>Chain of thought is a prompting technique introduced in the paper \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" where throught prompting the authors generate a series of intermediate reasoning steps which improves the ability of LLMs to perform complex reasoning.</p> <p>In this guide, we use outlines to apply chain of thought through structured output.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>To create an outlines <code>LlamaCpp</code> model, you first need to create a <code>Llama</code> object from the <code>llama-cpp-python</code> library. Then you can create the outlines model by calling <code>models.from_llamacpp</code> with the <code>Llama</code> object instance as argument. To create the <code>Llama</code> object, you need to provide the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames or glob pattern (it will automatically download the weights from the hub):</p> <pre><code>import llama_cpp\nimport outlines\n\nllm = llama_cpp.Llama(\n    \"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\nmodel = outlines.from_llamacpp(llm)\n</code></pre> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>from llama_cpp import Llama\n\nllm = Llama(\"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\", ...)\n</code></pre>"},{"location":"examples/chain_of_thought/#chain-of-thought_1","title":"Chain of thought","text":"<p>We first define our Pydantic class for a reasoning step:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reasoning_Step(BaseModel):\n    reasoning_step: str = Field(..., description=\"Reasoning step\")\n</code></pre> <p>We then define the Pydantic class for reasoning which will consist on a list of reasoning steps and a conclusion, and we get its JSON schema:</p> <pre><code>from typing import List\n\nclass Reasoning(BaseModel):\n    reasoning: List[Reasoning_Step] = Field(..., description=\"List of reasoning steps\")\n    conclusion: str = Field(..., description=\"Conclusion\")\n\njson_schema = Reasoning.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>from outlines import Template\n\ngenerate_hermes_prompt = Template.from_string(\n    \"\"\"\n    &lt;|im_start|&gt;system\n    You are a world class AI model who answers questions in JSON\n    Here's the json schema you must adhere to:\n    &lt;schema&gt;\n    {{ json_schema }}\n    &lt;/schema&gt;\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;user\n    {{ user_prompt }}\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\n    &lt;schema&gt;\n    \"\"\"\n)\n</code></pre> <p>For a given user prompt:</p> <pre><code>user_prompt = \"9.11 and 9.9 -- which is bigger?\"\n</code></pre> <p>We can use <code>outlines.Generator</code> with the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>generator = outlines.Generator(model, regex_str)\nprompt = generate_hermes_prompt(json_schema=json_schema, user_prompt=user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain a series of intermediate reasoning steps as well as the conclusion:</p> <pre><code>import json\n\njson_response = json.loads(response)\n\nprint(json_response[\"reasoning\"])\nprint(json_response[\"conclusion\"])\n# [{'reasoning_step': 'Both 9.11 and 9.9 are decimal numbers.'},\n#  {'reasoning_step': 'When comparing decimal numbers, we look at the numbers after the decimal point.'},\n#  {'reasoning_step': 'In this case, 9.11 has the number 1 after the decimal point, while 9.9 has the number 9.'},\n#  {'reasoning_step': 'Since 1 is greater than 9, 9.11 is greater than 9.9.'}]\n# '9.11 is bigger.'\n</code></pre> <p>We notice that the 4th reasoning step is wrong ``Since 1 is greater than 9, 9.11 is greater than 9.9.'', so we should probably give the model some examples for this particular task.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"examples/classification/","title":"Classification","text":"<p>Classification is a classic problem in NLP and finds many applications: spam detection, sentiment analysis, triaging of incoming requests, etc. We will use the example of a company that wants to sort support requests between those that require immediate attention (<code>URGENT</code>), those that can wait a little (<code>STANDARD</code>). You could easily extend the example by adding new labels.</p> <p>This tutorial shows how one can implement multi-label classification using Outlines.</p> <p>As always, we start with initializing the model. Since we are GPU poor we will be using a quantized version of Mistal-7B-v0.1:</p> <pre><code>import outlines\nimport transformers\n\nMODEL_NAME = \"TheBloke/Mistral-7B-OpenOrca-AWQ\"\n\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n    transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n)\n</code></pre> <p>We will use a prompt template stored in a text file:</p> <pre><code>from outlines import Template\n\ncustomer_support = Template.from_file(\"prompt_templates/classification.txt\")\n</code></pre>"},{"location":"examples/classification/#choosing-between-multiple-choices","title":"Choosing between multiple choices","text":"<p>Outlines provides a convenient way to do multi-label classification, passing a Literal type hint to the <code>outlines.Generator</code> object:</p> <p><pre><code>from typing import Literal\nimport outlines\n\ngenerator = outlines.Generator(model, Literal[\"URGENT\", \"STANDARD\"])\n</code></pre> Outlines supports batched requests, so we will pass two requests to the model:</p> <pre><code>requests = [\n    \"My hair is one fire! Please help me!!!\",\n    \"Just wanted to say hi\"\n]\n\nprompts = [customer_support(request=request) for request in requests]\n</code></pre> <p>We can now ask the model to classify the requests:</p> <pre><code>labels = generator(prompts)\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre>"},{"location":"examples/classification/#using-json-structured-generation","title":"Using JSON-structured generation","text":"<p>Another (convoluted) way to do multi-label classification is to JSON-structured generation in Outlines. We first need to define our Pydantic schema that contains the labels:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\n\nclass Label(str, Enum):\n    urgent = \"URGENT\"\n    standard = \"STANDARD\"\n\n\nclass Classification(BaseModel):\n    label: Label\n</code></pre> <p>We can then create a generator with the Pydantic model we just defined and call it:</p> <pre><code>generator = outlines.Generator(model, Classification)\nlabels = generator(prompts)\nprint(labels)\n# ['{\"label\":\"URGENT\"}', '{ \"label\": \"STANDARD\" }']\n</code></pre>"},{"location":"examples/dating_profiles/","title":"Generate a synthetic dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>import json\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines\n</code></pre>"},{"location":"examples/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"examples/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>from outlines import Template\n\ndating_profile_prompt = Template.from_string(\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n)\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"examples/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>MODEL_NAME = \"mosaicml/mpt-7b-8k-instruct\"\n\nconfig = transformers.AutoConfig.from_pretrained(\n    MODEL_NAME, trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel_kwargs = {\n    \"config\": config,\n    \"trust_remote_code\": True,\n    \"torch_dtype\": torch.bfloat16,\n    \"device_map\": \"cuda\",\n}\ntf_model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\ntf_tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = outlines.from_transformers(tf_model, tokenizer=tf_tokenizer)\n</code></pre>"},{"location":"examples/dating_profiles/#json-structured-generation-of-profiles","title":"JSON-structured generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(description=new_description, examples=samples)\nprofile = model(prompt, DatingProfile)\nparsed_profile = DatingProfile.model_validate_json(json.loads(profile))\n</code></pre>"},{"location":"examples/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"examples/deploy-using-bentoml/","title":"Run Outlines using BentoML","text":"<p>BentoML is an open-source model serving library for building performant and scalable AI applications with Python. It comes with tools that you need for serving optimization, model packaging, and production deployment.</p> <p>In this guide, we will show you how to use BentoML to run programs written with Outlines on GPU locally and in BentoCloud, an AI Inference Platform for enterprise AI teams. The example source code in this guide is also available in the examples/bentoml/ directory.</p>"},{"location":"examples/deploy-using-bentoml/#import-a-model","title":"Import a model","text":"<p>First we need to download an LLM (Mistral-7B-v0.1 in this example and you can use any other LLM) and import the model into BentoML's Model Store. Let's install BentoML and other dependencies from PyPi (preferably in a virtual environment):</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Then save the code snippet below as <code>import_model.py</code> and run <code>python import_model.py</code>.</p> <p>Note: You need to accept related conditions on Hugging Face first to gain access to Mistral-7B-v0.1.</p> <pre><code>import bentoml\n\nMODEL_ID = \"mistralai/Mistral-7B-v0.1\"\nBENTO_MODEL_TAG = MODEL_ID.lower().replace(\"/\", \"--\")\n\ndef import_model(model_id, bento_model_tag):\n\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n\n    with bentoml.models.create(bento_model_tag) as bento_model_ref:\n        tokenizer.save_pretrained(bento_model_ref.path)\n        model.save_pretrained(bento_model_ref.path)\n\n\nif __name__ == \"__main__\":\n    import_model(MODEL_ID, BENTO_MODEL_TAG)\n</code></pre> <p>You can verify the download is successful by running:</p> <pre><code>$ bentoml models list\n\nTag                                          Module  Size        Creation Time\nmistralai--mistral-7b-v0.1:m7lmf5ac2cmubnnz          13.49 GiB   2024-04-25 06:52:39\n</code></pre>"},{"location":"examples/deploy-using-bentoml/#define-a-bentoml-service","title":"Define a BentoML Service","text":"<p>As the model is ready, we can define a BentoML Service to wrap the capabilities of the model.</p> <p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code>DEFAULT_SCHEMA = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>First, we need to define a BentoML service by decorating an ordinary class (<code>Outlines</code> here) with <code>@bentoml.service</code> decorator. We pass to this decorator some configuration and GPU on which we want this service to run in BentoCloud (here an L4 with 24GB memory):</p> <pre><code>import typing as t\nimport bentoml\n\nfrom import_model import BENTO_MODEL_TAG\n\n@bentoml.service(\n    traffic={\n        \"timeout\": 300,\n    },\n    resources={\n        \"gpu\": 1,\n        \"gpu_type\": \"nvidia-l4\",\n    },\n)\nclass Outlines:\n\n    bento_model_ref = bentoml.models.get(BENTO_MODEL_TAG)\n\n    def __init__(self) -&gt; None:\n        import outlines\n        import torch\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n\n        # Load tokenizer and model from the BentoML model reference path\n        hf_tokenizer = AutoTokenizer.from_pretrained(self.bento_model_ref.path)\n        hf_model = AutoModelForCausalLM.from_pretrained(\n            self.bento_model_ref.path,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n            device_map=\"cuda\"\n        )\n\n        # Then use the loaded model with Outlines\n        self.model = outlines.from_transformers(hf_model, hf_tokenizer)\n\n    ...\n</code></pre> <p>We then need to define an HTTP endpoint using <code>@bentoml.api</code> to decorate the method <code>generate</code> of <code>Outlines</code> class:</p> <pre><code>    ...\n\n    @bentoml.api\n    async def generate(\n        self,\n        prompt: str = \"Give me a character description.\",\n        json_schema: t.Optional[str] = DEFAULT_SCHEMA,\n    ) -&gt; t.Dict[str, t.Any]:\n        import json\n        import outlines\n        from outlines.types import JsonSchema\n\n        generator = outlines.Generator(self.model, JsonSchema(json_schema))\n        character = generator(prompt)\n\n        return json.loads(character)\n</code></pre> <p>Here <code>@bentoml.api</code> decorator defines <code>generate</code> as an HTTP endpoint that accepts a JSON request body with two fields: <code>prompt</code> and <code>json_schema</code> (optional, which allows HTTP clients to provide their own JSON schema). The type hints in the function signature will be used to validate incoming JSON requests. You can define as many HTTP endpoints as you want by using <code>@bentoml.api</code> to decorate other methods of <code>Outlines</code> class.</p> <p>Now you can save the above code to <code>service.py</code> (or use this implementation), and run the code using the BentoML CLI.</p>"},{"location":"examples/deploy-using-bentoml/#run-locally-for-testing-and-debugging","title":"Run locally for testing and debugging","text":"<p>Then you can run a server locally by:</p> <pre><code>bentoml serve .\n</code></pre> <p>The server is now active at http://localhost:3000. You can interact with it using the Swagger UI or in other different ways:</p> CURL <pre><code>curl -X 'POST' \\\n  'http://localhost:3000/generate' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"prompt\": \"Give me a character description.\"\n}'\n</code></pre> Python client <pre><code>import bentoml\n\nwith bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n    response = client.generate(\n        prompt=\"Give me a character description\"\n    )\n    print(response)\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"name\": \"Aura\",\n  \"age\": 15,\n  \"armor\": \"plate\",\n  \"weapon\": \"sword\",\n  \"strength\": 20\n}\n</code></pre>"},{"location":"examples/deploy-using-bentoml/#deploy-to-bentocloud","title":"Deploy to BentoCloud","text":"<p>After the Service is ready, you can deploy it to BentoCloud for better management and scalability. Sign up if you haven't got a BentoCloud account.</p> <p>Make sure you have logged in to BentoCloud, then run the following command to deploy it.</p> <pre><code>bentoml deploy .\n</code></pre> <p>Once the application is up and running on BentoCloud, you can access it via the exposed URL.</p> <p>Note: For custom deployment in your own infrastructure, use BentoML to generate an OCI-compliant image.</p>"},{"location":"examples/deploy-using-cerebrium/","title":"Run Outlines using Cerebrium","text":"<p>Cerebrium is a serverless AI infrastructure platform that makes it easier for companies to build and deploy AI based applications. They offer Serverless GPU's\u00a0with low cold start times with over 12 varieties of GPU chips that auto scale and you only pay for the compute you use.</p> <p>In this guide we will show you how you can use Cerebrium to run programs written with Outlines on GPUs in the cloud.</p>"},{"location":"examples/deploy-using-cerebrium/#setup-cerebrium","title":"Setup Cerebrium","text":"<p>First, we install Cerebrium and login to get authenticated.</p> <pre><code>pip install cerebrium\ncerebrium login\n</code></pre> <p>Then let us create our first project</p> <pre><code>cerebrium init outlines-project\n</code></pre>"},{"location":"examples/deploy-using-cerebrium/#setup-environment-and-hardware","title":"Setup Environment and Hardware","text":"<p>You set up your environment and hardware in the cerebrium.toml file that was created using the init function above.</p> <pre><code>[cerebrium.deployment]\ndocker_base_image_url = \"nvidia/cuda:12.1.1-runtime-ubuntu22.04\"\n\n[cerebrium.hardware]\ncpu = 2\nmemory = 14.0\ngpu = \"AMPERE A10\"\ngpu_count = 1\nprovider = \"aws\"\nregion = \"us-east-1\"\n\n[cerebrium.dependencies.pip]\noutline = \"==1.0.0\"\ntransformers = \"==4.38.2\"\ndatasets = \"==2.18.0\"\naccelerate = \"==0.27.2\"\n</code></pre>"},{"location":"examples/deploy-using-cerebrium/#setup-inference","title":"Setup inference","text":"<p>Running code in Cerebrium is like writing normal python with no special syntax. In a <code>main.py</code> file specify the following:</p> <pre><code>import outlines\nimport transformers\nfrom outlines.types import JsonSchema\n\n\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n\ngenerator = outlines.Generator(model, JsonSchema(schema))\n</code></pre> <p>On first deploy, it will download the model and store it on disk therefore for subsequent calls it will load the model from disk.</p> <p>Every function in Cerebrium is callable through an API endpoint. Code at the top most layer (ie: not in a function) is instantiated only when the container is spun up the first time so for subsequent calls, it will simply run the code defined in the function you call.</p> <p>To deploy an API that creates a new character when called with a prompt you can add the following code to <code>main.py</code>:</p> <pre><code>def generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    return character\n</code></pre>"},{"location":"examples/deploy-using-cerebrium/#run-on-the-cloud","title":"Run on the cloud","text":"<pre><code>cerebrium deploy\n</code></pre> <p>You will see your application deploy, install pip packages and download the model. Once completed it will output a CURL request you can use to call your endpoint. Just remember to end the url with the function you would like to call - in this case /generate. You should see your response returned!</p>"},{"location":"examples/deploy-using-modal/","title":"Run Outlines using Modal","text":"<p>Modal is a serverless platform that allows you to easily run code on the cloud, including GPUs. It can come very handy for those of us who don't have a monster GPU at home and want to be able to quickly and easily provision, configure and orchestrate cloud infrastructure.</p> <p>In this guide we will show you how you can use Modal to run programs written with Outlines on GPU in the cloud.</p>"},{"location":"examples/deploy-using-modal/#requirements","title":"Requirements","text":"<p>We recommend installing <code>modal</code> and <code>outlines</code> in a virtual environment. You can create one with:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Then install the required packages:</p> <pre><code>pip install modal outlines\n</code></pre>"},{"location":"examples/deploy-using-modal/#build-the-image","title":"Build the image","text":"<p>First we need to define our container image. If you need to access a gated model, you will need to provide an access token. See the <code>.env</code> call below for how to provide a HuggingFace token.</p> <p>Setting a token is best done by setting an environment variable <code>HF_TOKEN</code> with your token. If you do not wish to do this, we provide a commented-out line in the code to set the token directly in the code.</p> <pre><code>from modal import Image, App, gpu\nimport os\n\n# This creates a modal App object. Here we set the name to \"outlines-app\".\n# There are other optional parameters like modal secrets, schedules, etc.\n# See the documentation here: https://modal.com/docs/reference/modal.App\napp = App(name=\"outlines-app\")\n\n# Specify a language model to use.\n# Another good model to use is \"NousResearch/Hermes-2-Pro-Mistral-7B\"\nlanguage_model = \"mistral-community/Mistral-7B-v0.2\"\n\n# Please set an environment variable HF_TOKEN with your Hugging Face API token.\n# The code below (the .env({...}) part) will copy the token from your local\n# environment to the container.\n# More info on Image here: https://modal.com/docs/reference/modal.Image\noutlines_image = Image.debian_slim(python_version=\"3.11\").pip_install(\n    \"outlines\",\n    \"transformers\",\n    \"datasets\",\n    \"accelerate\",\n    \"sentencepiece\",\n).env({\n    # This will pull in your HF_TOKEN environment variable if you have one.\n    'HF_TOKEN':os.environ['HF_TOKEN']\n\n    # To set the token directly in the code, uncomment the line below and replace\n    # 'YOUR_TOKEN' with the HuggingFace access token.\n    # 'HF_TOKEN':'YOUR_TOKEN'\n})\n</code></pre>"},{"location":"examples/deploy-using-modal/#setting-the-container-up","title":"Setting the container up","text":"<p>When running longer Modal apps, it's recommended to download your language model when the container starts, rather than when the function is called. This will cache the model for future runs.</p> <pre><code># This function imports the model from Hugging Face. The modal container\n# will call this function when it starts up. This is useful for\n# downloading models, setting up environment variables, etc.\ndef import_model():\n    import outlines\n    import transformers\n\n    outlines.from_transformers(\n        transformers.AutoModelForCausalLM.from_pretrained(language_model),\n        transformers.AutoTokenizer.from_pretrained(language_model)\n    )\n\n# This line tells the container to run the import_model function when it starts.\noutlines_image = outlines_image.run_function(import_model)\n</code></pre>"},{"location":"examples/deploy-using-modal/#define-a-schema","title":"Define a schema","text":"<p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code># Specify a schema for the character description. In this case,\n# we want to generate a character with a name, age, armor, weapon, and strength.\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>To make the inference work on Modal we need to wrap the corresponding function in a <code>@app.function</code> decorator. We pass to this decorator the image and GPU on which we want this function to run.</p> <p>Let's choose an A100 with 80GB memory. Valid GPUs can be found here.</p> <pre><code># Define a function that uses the image we chose, and specify the GPU\n# and memory we want to use.\n@app.function(image=outlines_image, gpu=gpu.A100(size='80GB'))\ndef generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # Remember, this function is being executed in the container,\n    # so we need to import the necessary libraries here. You should\n    # do this with any other libraries you might need.\n    import outlines\n    import transformers\n    from outlines.types import JsonSchema\n\n    # Load the model into memory. The import_model function above\n    # should have already downloaded the model, so this call\n    # only loads the model into GPU memory.\n    outlines.from_transformers(\n        transformers.AutoModelForCausalLM.from_pretrained(language_model, device_map=\"cuda\"),\n        transformers.AutoTokenizer.from_pretrained(language_model)\n    )\n\n    # Generate a character description based on the prompt.\n    # We use the .json generation method -- we provide the\n    # - model: the model we loaded above\n    # - schema: the JSON schema we defined above\n    generator = outlines.Generator(model, JsonSchema(schema))\n\n    # Make sure you wrap your prompt in instruction tags ([INST] and [/INST])\n    # to indicate that the prompt is an instruction. Instruction tags can vary\n    # by models, so make sure to check the model's documentation.\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    # Print out the generated character.\n    print(character)\n</code></pre> <p>We then need to define a <code>local_entrypoint</code> to call our function <code>generate</code> remotely.</p> <pre><code>@app.local_entrypoint()\ndef main(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # We use the \"generate\" function defined above -- note too that we are calling\n    # .remote() on the function. This tells modal to run the function in our cloud\n    # machine. If you want to run the function locally, you can call .local() instead,\n    # though this will require additional setup.\n    generate.remote(prompt)\n</code></pre> <p>Here <code>@app.local_entrypoint()</code> decorator defines <code>main</code> as the function to start from locally when using the Modal CLI. You can save above code to <code>example.py</code> (or use this implementation). Let's now see how to run the code on the cloud using the Modal CLI.</p>"},{"location":"examples/deploy-using-modal/#run-on-the-cloud","title":"Run on the cloud","text":"<p>First install the Modal client from PyPi, if you have not already:</p> <pre><code>pip install modal\n</code></pre> <p>You then need to obtain a token from Modal. Run the following command:</p> <pre><code>modal setup\n</code></pre> <p>Once that is set you can run inference on the cloud using:</p> <pre><code>modal run example.py\n</code></pre> <p>You should see the Modal app initialize, and soon after see the result of the <code>print</code> function in your terminal. That's it!</p>"},{"location":"examples/earnings-reports/","title":"Extracting financial data from earnings reports","text":"<p>A common task in finance is to extract financial data from earnings reports. Earnings reports are infamously poorly formatted, as the SEC does not have requirements for producing machine-readable documents.</p> <p>Earnings reports are often provided as HTML documents, which can be difficult to parse. Investors often use complicated parsing systems or manual review to extract data. Entire companies are built around automating this task.</p> <p>This cookbook is a proof of concept about how we can use LLMs to extract financial data directly into CSV. Comma-separated values are well-structured and can be defined by a regular expression, which Outlines can use to guide the LLM's output.</p> <p>The example is a smaller subset of a full demo found here. The demo contains the full set of pre-processing steps needed to convert raw HTML into a structured CSV file, and tests the results across three company's 10k reports.</p>"},{"location":"examples/earnings-reports/#setup","title":"Setup","text":"<p>Install outlines and required dependencies:</p> <pre><code># Later versions of torch can have difficulty with certain CUDA drivers.\n# We recommend using 2.4.0 for now, but you may wish to experiment with\n# other versions.\npip install outlines pandas transformers torch==2.4.0 accelerate\n</code></pre>"},{"location":"examples/earnings-reports/#load-the-model","title":"Load the model","text":"<p>Choose your language model. We'll use Phi-3 mini, which is small enough to run on reasonably small machines.</p> <pre><code>import outlines\nimport torch\nimport transformers\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\ntf_model = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16\n)\ntf_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\nmodel = outlines.from_transformers(tf_model, tf_tokenizer)\n</code></pre>"},{"location":"examples/earnings-reports/#set-up-the-data","title":"Set up the data","text":"<p>For brevity, we've attached the markdown version of Nvidia's 10k report. The full demonstration processes the raw HTML version of the report to these markdown tables. Pages are filtered by whether they seem to contain income statements, and then compacted into the string you see below.</p> <pre><code>income_statement = \"\"\"\nTable of ContentsNVIDIA Corporation and SubsidiariesConsolidated Statements of Income(In millions, except per share data)\n\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | | | Year Ended | | | | | | | | | | | | | | |\n|  | | | Jan 28, 2024 | | |  | | | Jan 29, 2023 | | |  | | | Jan 30, 2022 | | |\n| Revenue | | | $ | 60,922 |  |  | | | $ | 26,974 |  |  | | | $ | 26,914 |  |\n| Cost of revenue | | | 16,621 | |  |  | | | 11,618 | |  |  | | | 9,439 | |  |\n| Gross profit | | | 44,301 | |  |  | | | 15,356 | |  |  | | | 17,475 | |  |\n| Operating expenses | | |  | | |  | | |  | | |  | | |  | | |\n| Research and development | | | 8,675 | |  |  | | | 7,339 | |  |  | | | 5,268 | |  |\n| Sales, general and administrative | | | 2,654 | |  |  | | | 2,440 | |  |  | | | 2,166 | |  |\n| Acquisition termination cost | | | \u0097 | |  |  | | | 1,353 | |  |  | | | \u0097 | |  |\n| Total operating expenses | | | 11,329 | |  |  | | | 11,132 | |  |  | | | 7,434 | |  |\n| Operating income | | | 32,972 | |  |  | | | 4,224 | |  |  | | | 10,041 | |  |\n| Interest income | | | 866 | |  |  | | | 267 | |  |  | | | 29 | |  |\n| Interest expense | | | (257) | |  |  | | | (262) | |  |  | | | (236) | |  |\n| Other, net | | | 237 | |  |  | | | (48) | |  |  | | | 107 | |  |\n| Other income (expense), net | | | 846 | |  |  | | | (43) | |  |  | | | (100) | |  |\n| Income before income tax | | | 33,818 | |  |  | | | 4,181 | |  |  | | | 9,941 | |  |\n| Income tax expense (benefit) | | | 4,058 | |  |  | | | (187) | |  |  | | | 189 | |  |\n| Net income | | | $ | 29,760 |  |  | | | $ | 4,368 |  |  | | | $ | 9,752 |  |\n|  | | |  | | |  | | |  | | |  | | |  | | |\n| Net income per share: | | |  | | |  | | |  | | |  | | |  | | |\n| Basic | | | $ | 12\\.05 |  |  | | | $ | 1\\.76 |  |  | | | $ | 3\\.91 |  |\n| Diluted | | | $ | 11\\.93 |  |  | | | $ | 1\\.74 |  |  | | | $ | 3\\.85 |  |\n|  | | |  | | |  | | |  | | |  | | |  | | |\n| Weighted average shares used in per share computation: | | |  | | |  | | |  | | |  | | |  | | |\n| Basic | | | 2,469 | |  |  | | | 2,487 | |  |  | | | 2,496 | |  |\n| Diluted | | | 2,494 | |  |  | | | 2,507 | |  |  | | | 2,535 | |  |\n\"\"\"\n</code></pre> <p>The markdown tables extracted from the earnings reports can vary widely in row names, column counts, data types, etc. The advantage of LLMs here is that we can define the data we want in terms of the data types, and the LLM will output the data in the desired format.</p> <p>For comparison, here is how the income statement looks in the original HTML:</p> <p></p>"},{"location":"examples/earnings-reports/#define-the-data-we-want","title":"Define the data we want","text":"<p>Outlines is often used for JSON output, but it can also be used for CSV. We know the columns we want to extract, and we know the data types of the columns. Year for example is always a four-digit number, revenue is a number with commas, and so on.</p> <p>We can define a regex pattern for each column type:</p> <pre><code># Define the column type regex patterns\ncolumn_types = {\n    # Year is always a four-digit number\n    \"year\": r\"\\d{4}\",\n\n    # Revenue, operating income, and net income are always numbers with commas.\n    # This regex permits integers that may begin with a minus sign, and may have\n    # commas separating the thousands, millions, etc.\n    \"integer_comma\": r\"((-?\\d+),?\\d+|(-?\\d+))\",\n    # Number is currently not used, but it represents a number with up to two decimal places.\n    \"number\": r\"(-?\\d+(?:\\.\\d{1,2})?)\",\n}\n</code></pre> <p>Next, let's choose the columns we want to extract. We want</p> <ul> <li>Year, always a four-digit number</li> <li>Revenue, a number with commas</li> <li>Operating income, a number with commas</li> <li>Net income, a number with commas</li> </ul> <pre><code># Define the columns to extract, and their data types.\ncolumns_to_extract = {\n    \"year\": \"year\",\n    \"revenue\": \"integer_comma\",\n    \"operating_income\": \"integer_comma\",\n    \"net_income\": \"integer_comma\",\n}\n</code></pre> <p>You can modify <code>column_type_regex</code> to match the data types of the columns you want to extract.  Adding a new financial metric to extract is as simple as adding a new key/value pair to <code>columns_to_extract</code>:</p> <pre><code>columns_to_extract[\"diluted_earnings_per_share\"] = \"number\"\n</code></pre> <p>Additional columns are not well tested for accuracy, so use with caution.</p>"},{"location":"examples/earnings-reports/#create-the-regex-describing-the-data-we-want","title":"Create the regex describing the data we want","text":"<pre><code># Create the header line. This is the requested column names\n# separated by commas, i.e. \"year,revenue,...\"\nheader = \",\".join(columns_to_extract.keys())\n\n# Create the data capture patterns. These are the regex patterns\n# that will be used to capture the data in each column\ndata_patterns = [column_types[dtype] for dtype in columns_to_extract.values()]\ndata_line = \",\".join(data_patterns)\n\n# Our final regex pattern.\nmax_rows = 3 # We expect 3 rows of data, firms usually report 3 years of income statements\ncsv_regex = f\"{header}(\\n{data_line}){{,{max_rows}}}\\n\\n\"\n\nprint(csv_regex)\n</code></pre> <p>which gives us</p> <pre><code>year,revenue,operating_income,net_income,basic_earnings_per_share(\n\\d{4},((-?\\d+),?\\d+|(-?\\d+)),((-?\\d+),?\\d+|(-?\\d+)),((-?\\d+),?\\d+|(-?\\d+)),(-?\\d+(?:\\.\\d{1,2})?)){,3}\n</code></pre> <p>Pretty hairy, right? Thankfully, we have a simple function to construct this regex for you. The regex defines a header line, followed by a data line that repeats for each row of data we want to extract. Passing the regex to <code>outlines.Generator</code> will produce a function that will always produce a CSV string that is consistent with the regex.</p>"},{"location":"examples/earnings-reports/#prompting-the-model","title":"Prompting the model","text":"<p>Outlines does not add system or instruction tokens by default, so we need to use <code>transformers.AutoTokenizer</code> to add them for whatever model we're using.</p> <p><pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef add_instruction(prompt):\n    return tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n\nprint(add_instruction(\"Howdy\"))\n</code></pre> <pre><code>&lt;|user|&gt;\nHowdy&lt;|end|&gt;\n&lt;|assistant|&gt;\n</code></pre></p> <p>Our prompt roughly describes the task we want the model to perform, and a few pieces of information it may need to know about income statements.</p> <pre><code>def extract_financial_data_prompt(columns_to_extract, income_statement):\n    user_prompt = f\"\"\"\n    Extract annual financial data from this set of pages. Pages\n    are from a 10k filing and were chosen because they may contain\n    a comprehensive income statement. Note that selected pages may\n    be incorrectly extracted, so you should verify that you are extracting\n    from the comprehensive income statement and not some other financial\n    statement.\n\n    Create a row for each year available in the income statement with the\n    following columns: {', '.join(columns_to_extract.keys())}. Firms typically report the\n    most recent 3 years of data, but this can vary.\n\n    Each column has types: {', '.join(columns_to_extract.values())}.\n\n    # Relevant pages:\n\n    {income_statement}\n\n    # Key instructions:\n\n    1. Look ONLY at the \"Consolidated Statements of Income\" table\n    2. For operating income, look for \"Income from operations\" or \"Operating income\"\n    3. For net income, use the TOTAL net income figure, not amounts allocated to specific share classes\n    4. Use NULL for missing values\n    5. Operating income must be less than revenue\n    6. Net income must be less than operating income\n    7. Ignore segment breakdowns, quarterly data, or per-share amounts\n\n    # Output format:\n\n    - CSV format with headers: {','.join(columns_to_extract.keys())}\n    - Use NULL for missing values\n    - If no data are found, do not create a row.\n    - Enter two newline characters to terminate the CSV when no more data are found.\n\n    # Definitions:\n    - Revenue: Total sales of goods and services. Usually this is at the top of the\n    income statement.\n    - Operating income: Revenue minus operating expenses for the entire company. This is revenue\n    minus costs. Operating income is also called operating profit, EBIT, or income from\n    operations.\n    - Net income: Operating income minus taxes. This is the bottom line of the\n    income statement.\n    \"\"\"\n\n    return add_instruction(user_prompt)\n</code></pre>"},{"location":"examples/earnings-reports/#running-the-model","title":"Running the model","text":"<p>Now that we have our prompt and regular expression, we can run the model.</p> <p>Construct our regex extractor function.</p> <pre><code>from outlines.types import Regex\n\ncsv_extractor = outlines.Generator(model, Regex(csv_regex))\n</code></pre> <p>Provide the prompt to the model and run it:</p> <p><pre><code>csv_data = csv_extractor(\n    extract_financial_data_prompt(columns_to_extract, income_statement),\n    max_new_tokens=1024,\n)\n\nprint(csv_data)\n</code></pre> <pre><code>year,revenue,operating_income,net_income\n2024,60922,32972,29760\n2023,26974,4224,4368\n2022,26914,10041,9752\n</code></pre></p> <p>Voila! We've extracted the financial data from the income statement, and it's correct upon inspection.</p> <p>You can even load this into a <code>pandas</code> DataFrame for further analysis:</p> <p><pre><code>import pandas as pd\nfrom io import StringIO\n\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n</code></pre> <pre><code>   year  revenue  operating_income  net_income\n0  2024    60922             32972       29760\n1  2023    26974              4224        4368\n2  2022    26914             10041        9752\n</code></pre></p>"},{"location":"examples/extract_event_details/","title":"Extract Events Details","text":"<p>This recipe demonstrates how to use the <code>outlines</code> library to extract structured event details from a text message. We will extract the title, location, and start date and time from messages like the following:</p> <pre><code>Hello Kitty, my grandmother will be here, I think it's better to postpone\nour appointment to review math lessons to next Monday at 2pm at the same\nplace, 3 avenue des tanneurs, one hour will be enough see you \ud83d\ude18\n</code></pre> <p>Let see how to extract the event details from the message with the MLX library dedicated to Apple Silicon processor (M series).</p> <pre><code>\n</code></pre> <p>The output will be:</p> <pre><code>Today: Saturday 16 November 2024 and it's 10:55\n</code></pre> <p>and the extracted event information will be:</p> <pre><code>{\n  \"title\":\"Math Review\",\n  \"location\":\"3 avenue des tanneurs\",\n  \"start\":\"2024-11-22T14:00:00Z\"\n}\n</code></pre> <p>To find out more about this use case, we recommend the project developped by Joseph Rudoler the ICS Generator</p>"},{"location":"examples/extraction/","title":"Named entity extraction","text":"<p>Named Entity Extraction is a fundamental problem in NLP. It involves identifying and categorizing named entities within a document: people, organization, dates, places, etc. It is usually the first step in a more complex NLP worklow. Here we will use the example of a pizza restaurant that receives orders via their website and need to identify the number and types of pizzas that are being ordered.</p> <p>Getting LLMs to output the extracted entities in a structured format can be challenging. In this tutorial we will see how we can use Outlines' JSON-structured generation to extract entities from a document and return them in a valid JSON data structure 100% of the time.</p> <p>As always, we start with initializing the model. We will be using a quantized version of Mistal-7B-v0.1 (we're GPU poor):</p> <pre><code>import transformers\nimport outlines\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\"),\n    transformers.AutoTokenizer.from_pretrained(model_name),\n)\n</code></pre> <p>And we will be using the following prompt template:</p> <pre><code>from outlines import Template\n\ntake_order = Template.from_string(\n    \"\"\"You are the owner of a pizza parlor. Customers \\\n    send you orders from which you need to extract:\n\n    1. The pizza that is ordered\n    2. The number of pizzas\n\n    # EXAMPLE\n\n    ORDER: I would like one Margherita pizza\n    RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n\n    # OUTPUT INSTRUCTIONS\n\n    Answer in valid JSON. Here are the different objects relevant for the output:\n\n    Order:\n        pizza (str): name of the pizza\n        number (int): number of pizzas\n\n    Return a valid JSON of type \"Order\"\n\n    # OUTPUT\n\n    ORDER: {{ order }}\n    RESULT: \"\"\"\n)\n</code></pre> <p>We now define our data model using Pydantic:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Pizza(str, Enum):\n    margherita = \"Margherita\"\n    pepperonni = \"Pepperoni\"\n    calzone = \"Calzone\"\n\nclass Order(BaseModel):\n    pizza: Pizza\n    number: int\n</code></pre> <p>We can now define our generator and call it on several incoming orders:</p> <pre><code>orders = [\n    \"Hi! I would like to order two pepperonni pizzas and would like them in 30mins.\",\n    \"Is it possible to get 12 margheritas?\"\n]\nprompts = [take_order(order=order) for order in orders]\n\ngenerator = outlines.Generator(model, Order)\n\nresults = generator(prompts)\nprint(results)\n# ['{\"pizza\": \"Pepperoni\", \"number\": 2}',\n# '{\"pizza\": \"Margherita\", \"number\": 12}']\n</code></pre> <p>There are several ways you could improve this example:</p> <ul> <li>Clients may order several types of pizzas.</li> <li>Clients may order drinks as well.</li> <li>If the pizza place has a delivery service we need to extract the client's address and phone number</li> <li>Clients may specify the time for which they want the pizza. We could then check against a queuing system and reply to them with the estimated delivery time.</li> </ul> <p>How would you change the Pydantic model to account for these use cases?</p>"},{"location":"examples/knowledge_graph_extraction/","title":"Knowledge Graph Extraction","text":"<p>In this guide, we use outlines to extract a knowledge graph from unstructured text.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>To create an outlines <code>LlamaCpp</code> model, you first need to create a <code>Llama</code> object from the <code>llama-cpp-python</code> library. Then you can create the outlines model by calling <code>models.from_llamacpp</code> with the <code>Llama</code> object instance as argument. To create the <code>Llama</code> object, you need to provide the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames or glob pattern (it will automatically download the weights from the hub):</p> <pre><code>import llama_cpp\nimport outlines\n\nllm = llama_cpp.Llama(\n    \"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\nmodel = outlines.from_llamacpp(llm)\n</code></pre> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>from llama_cpp import Llama\n\nllm = Llama(\"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\", ...)\n</code></pre>"},{"location":"examples/knowledge_graph_extraction/#knowledge-graph-extraction_1","title":"Knowledge Graph Extraction","text":"<p>We first need to define our Pydantic class for each node and each edge of the knowledge graph:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Node(BaseModel):\n    \"\"\"Node of the Knowledge Graph\"\"\"\n\n    id: int = Field(..., description=\"Unique identifier of the node\")\n    label: str = Field(..., description=\"Label of the node\")\n    property: str = Field(..., description=\"Property of the node\")\n\n\nclass Edge(BaseModel):\n    \"\"\"Edge of the Knowledge Graph\"\"\"\n\n    source: int = Field(..., description=\"Unique source of the edge\")\n    target: int = Field(..., description=\"Unique target of the edge\")\n    label: str = Field(..., description=\"Label of the edge\")\n    property: str = Field(..., description=\"Property of the edge\")\n</code></pre> <p>We then define the Pydantic class for the knowledge graph and get its JSON schema:</p> <pre><code>from typing import List\n\nclass KnowledgeGraph(BaseModel):\n    \"\"\"Generated Knowledge Graph\"\"\"\n\n    nodes: List[Node] = Field(..., description=\"List of nodes of the knowledge graph\")\n    edges: List[Edge] = Field(..., description=\"List of edges of the knowledge graph\")\n\nschema = KnowledgeGraph.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>from outlines import Template\n\ngenerate_hermes_prompt = Template.from_string(\n    \"\"\"\n    &lt;|im_start|&gt;system\n    You are a world class AI model who answers questions in JSON\n    Here's the json schema you must adhere to:\n    &lt;schema&gt;\n    {{ schema }}\n    &lt;/schema&gt;\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;user\n    {{ user_prompt }}\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\n    &lt;schema&gt;\n    \"\"\"\n)\n</code></pre> <p>For a given user prompt, for example:</p> <pre><code>user_prompt = \"Alice loves Bob and she hates Charlie.\"\n</code></pre> <p>We can use <code>outlines.Generator</code> by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>from outlines import Generator\n\ngenerator = Generator(model, KnowledgeGraph)\nprompt = generate_hermes_prompt(schema=schema, user_prompt=user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain the nodes and edges of the knowledge graph:</p> <pre><code>print(response)\n# {\"nodes\":[{\"id\":1,\"label\":\"Alice\",\"property\":\"loves,hates\"},\n# {\"id\":2,\"label\":\"Bob\",\"property\":\"loved_by\"},\n# {\"id\":3,\"label\":\"Charlie\",\"property\":\"hated_by\"}],\n# \"edges\":[{\"source\":1,\"target\":2,\"label\":\"loves\",\"property\":\"love\"},\n# {\"source\":1,\"target\":3,\"label\":\"hates\",\"property\":\"hate\"}]}\n</code></pre>"},{"location":"examples/knowledge_graph_extraction/#optional-visualizing-the-knowledge-graph","title":"(Optional) Visualizing the Knowledge Graph","text":"<p>We can use the Graphviz library to visualize the generated knowledge graph. For detailed installation instructions, see here.</p> <pre><code>from graphviz import Digraph\n\ndot = Digraph()\nfor node in response[\"nodes\"]:\n    dot.node(str(node[\"id\"]), node[\"label\"], shape='circle', width='1', height='1')\nfor edge in response[\"edges\"]:\n    dot.edge(str(edge[\"source\"]), str(edge[\"target\"]), label=edge[\"label\"])\n\ndot.render('knowledge-graph.gv', view=True)\n</code></pre> <p></p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"examples/models_playing_chess/","title":"Large language models playing chess","text":"<p>In this example we will make a Phi-3 model play chess against itself. On its own the model easily generates invalid moves, so we will give it a little help. At each step we will generate a regex that only matches valid move, and use it to help the model only generating valid moves.</p>"},{"location":"examples/models_playing_chess/#the-chessboard","title":"The chessboard","text":"<p>The game will be played on a standard checkboard. We will use the <code>chess</code> library to track the opponents' moves, and check that the moves are valid.</p> <pre><code>%pip install outlines -q\n%pip install chess -q\n%pip install transformers accelerate einops -q\n\nimport chess\n\nboard = chess.Board(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n</code></pre>"},{"location":"examples/models_playing_chess/#the-opponents","title":"The opponents","text":"<p>Phi-3 will be playing against itself:</p> <pre><code>import transformers\nimport outlines\n\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(model_name),\n    transformers.AutoTokenizer.from_pretrained(model_name),\n)\n</code></pre>"},{"location":"examples/models_playing_chess/#a-little-help-for-the-language-model","title":"A little help for the language model","text":"<p>To make sure Phi-3 generates valid chess moves we will use Outline's regex-structured generation. We define a function that takes the current state of the board and returns a regex that matches all possible legal moves:</p> <pre><code>import re\nfrom outlines.types.dsl import either, String\n\ndef legal_moves_regex(board):\n    \"\"\"Build a regex that only matches valid moves.\"\"\"\n    legal_moves = list(board.legal_moves)\n    legal_modes_str = [board.san(move) for move in legal_moves]\n    legal_modes_str = [re.sub(r\"[+#]\", \"\", move) for move in legal_modes_str]\n    regex_pattern = either(*[String(move) for move in legal_modes_str])\n    return regex_pattern\n</code></pre>"},{"location":"examples/models_playing_chess/#prompting-the-language-model","title":"Prompting the language model","text":"<p>The prompt corresponds to the current state of the board, so we start with:</p> <pre><code>prompt = \"Let's play Chess. Moves: \"\n</code></pre> <p>We update the prompt at each step so it reflects the state of the board after the previous move.</p>"},{"location":"examples/models_playing_chess/#lets-play","title":"Let's play","text":"<pre><code>board_state = \" \"\nturn_number = 0\nwhile not board.is_game_over():\n    regex_pattern = legal_moves_regex(board)\n    structured = model(prompt + board_state, regex_pattern)\n    move = board.parse_san(structured)\n\n    if turn_number % 2 == 0 :  # It's White's turn\n        board_state += board.san(move) + \" \"\n    else:\n        board_state += board.san(move) + \" \" + str(turn_number) + \".\"\n\n    turn_number += 1\n\n    board.push(move)\n\n    print(board_state)\n</code></pre> <p>Interestingly enough, Phi-3 hates capturing.</p> <pre><code> e4 e5 1.Nf3 Ne7 3.b4 Nf5 5.Nc3 Ne7 7.Bb5 a6 9.Na4 b6 11.c3 Nec6 13.c4 a5 15.d4 Qg5 17.Nd2 Bb7 19.dxe5\n</code></pre> <p>This example was originally authored by @903124S in this gist.</p>"},{"location":"examples/qa-with-citations/","title":"Generate Synthetic Data and Q&amp;A with Citations","text":"<p>This tutorial is adapted from the instructor-ollama notebook. We start with a simple example to generate synthetic data and then we approach the problem of question answering by providing citations.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nimport outlines\n\nllm = llama_cpp.Llama(\n    \"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\nmodel = outlines.from_llamacpp(llm)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>from llama_cpp import Llama\n\nllm = Llama(\"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\", ...)\n</code></pre>"},{"location":"examples/qa-with-citations/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>We first need to define our Pydantic class for a user:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier\") # so the model keeps track of the number of users\n    first_name: str\n    last_name: str\n    age: int\n</code></pre> <p>We then define a Pydantic class for a list of users:</p> <pre><code>from typing import List\n\nclass Users(BaseModel):\n    users: List[UserDetail]\n</code></pre> <p>We can use a <code>outlines.Generator</code> by passing this Pydantic class we just defined, and call the generator:</p> <pre><code>import json\n\ngenerator = outlines.Generator(model, Users)\nresponse = generator(\"Create 5 fake users\", max_tokens=1024, temperature=0, seed=42)\nresponse = json.loads(response)\nprint(response['users'])\n# [{'id': 1, 'first_name': 'John', 'last_name': 'Doe', 'age': 25},\n# {'id': 2, 'first_name': 'Jane', 'last_name': 'Doe', 'age': 30},\n# {'id': 3, 'first_name': 'Bob', 'last_name': 'Smith', 'age': 40},\n# {'id': 4, 'first_name': 'Alice', 'last_name': 'Smith', 'age': 35},\n# {'id': 5, 'first_name': 'John', 'last_name': 'Smith', 'age': 20}]\n</code></pre> <pre><code>for user in response['users']:\n    print(user['first_name'])\n    print(user['last_name'])\n    print(user['age'])\n    print(\"#####\")\n# John\n# Doe\n# 25\n# #####\n# Jane\n# Doe\n# 30\n# #####\n# Bob\n# Smith\n# 40\n# #####\n# Alice\n# Smith\n# 35\n# #####\n# John\n# Smith\n# 20\n# #####\n</code></pre>"},{"location":"examples/qa-with-citations/#qa-with-citations","title":"QA with Citations","text":"<p>We first need to define our Pydantic class for QA with citations:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n    citations: List[str]\n\nschema = QuestionAnswer.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>from outlines import Template\n\nhermes_prompt = Template.from_string(\n    \"\"\"\n    &lt;|im_start|&gt;system\n    You are a world class AI model who answers questions in JSON with correct and exact citations\n    extracted from the `Context`.\n    Here's the json schema you must adhere to:\n    &lt;schema&gt;\n    {{ schema }}\n    &lt;/schema&gt;\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;user\n    `Context`:\n    {{ context }}\n    `Question`:\n    {{ question }}\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\n    \"\"\"\n)\n</code></pre> <p>We can use <code>outlines.Generator</code> by passing the Pydantic class we previously defined, and call the generator with Hermes prompt:</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\ngenerator = outlines.Generator(model, QuestionAnswer)\nprompt = hermes_prompt(question=question, context=context, schema=schema)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\nprint(response)\n# {\"question\": \"What did the author do during college?\", \"answer\": \"The author studied Computational Mathematics and physics in university and was also involved in starting the Data Science club, serving as its president for 2 years.\", \"citations\": [\"I went to an arts high school but in university I studied Computational Mathematics and physics.\", \"I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\"]}\n</code></pre> <p>We can do the same for a list of question-context pairs:</p> <pre><code>question1 = \"Where was John born?\"\ncontext1 = \"\"\"\nJohn Doe is a software engineer who was born in New York, USA.\nHe studied Computer Science at the Massachusetts Institute of Technology.\nDuring his studies, he interned at Google and Microsoft.\nHe also founded the Artificial Intelligence club at his university and served as its president for three years.\n\"\"\"\n\nquestion2 = \"What did Emily study in university?\"\ncontext2 = \"\"\"\nEmily Smith is a data scientist from London, England.\nShe attended the University of Cambridge where she studied Statistics and Machine Learning.\nShe interned at IBM and Amazon during her summer breaks.\nEmily was also the head of the Women in Tech society at her university.\n\"\"\"\n\nquestion3 = \"Which companies did Robert intern at?\"\ncontext3 = \"\"\"\nRobert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert.\nHe studied Information Systems at the University of Melbourne.\nRobert interned at several cybersecurity firms including NortonLifeLock and McAfee.\nHe was also the leader of the Cybersecurity club at his university.\n\"\"\"\n\nquestion4 = \"What club did Alice start at her university?\"\ncontext4 = \"\"\"\nAlice Williams, a native of Dublin, Ireland, is a successful web developer.\nShe studied Software Engineering at Trinity College Dublin.\nAlice interned at several tech companies including Shopify and Squarespace.\nShe started the Web Development club at her university and was its president for two years.\n\"\"\"\n\nquestion5 = \"What did Michael study in high school?\"\ncontext5 = \"\"\"\nMichael Brown is a game developer from Tokyo, Japan.\nHe attended a specialized high school where he studied Game Design.\nHe later attended the University of Tokyo where he studied Computer Science.\nMichael interned at Sony and Nintendo during his university years.\nHe also started the Game Developers club at his university.\n\"\"\"\n\nfor question, context in [\n    (question1, context1),\n    (question2, context2),\n    (question3, context3),\n    (question4, context4),\n    (question5, context5),\n]:\n    prompt = hermes_prompt(question=question, context=context, schema=schema)\n    generator = outlines.Generator(model, QuestionAnswer)\n    response = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n    response = json.loads(response)\n    print(question)\n    print(response['answer'])\n    print(response['citations'])\n    print(\"\\n\\n\")\n\n# 'Where was John born?'\n# 'John Doe was born in New York, USA.'\n# ['John Doe is a software engineer who was born in New York, USA.']\n#\n#\n# 'What did Emily study in university?'\n# 'Emily studied Statistics and Machine Learning in university.'\n# ['She attended the University of Cambridge where she studied Statistics and Machine Learning.']\n#\n#\n# 'Which companies did Robert intern at?'\n# 'Robert interned at NortonLifeLock and McAfee.'\n# ['Robert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert. He interned at several cybersecurity firms including NortonLifeLock and McAfee.']\n#\n#\n# 'What club did Alice start at her university?'\n# 'Alice started the Web Development club at her university.'\n# ['Alice Williams, a native of Dublin, Ireland, is a successful web developer. She started the Web Development club at her university and was its president for two years.']\n#\n#\n# 'What did Michael study in high school?'\n# 'Michael studied Game Design in high school.'\n# ['Michael Brown is a game developer from Tokyo, Japan. He attended a specialized high school where he studied Game Design.']\n</code></pre> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"examples/react_agent/","title":"ReAct Agent","text":"<p>This example shows how to use outlines to build your own agent with open weights local models and structured outputs. It is inspired by the blog post A simple Python implementation of the ReAct pattern for LLMs by Simon Willison.</p> <p>The ReAct pattern (for Reason+Act) is described in the paper ReAct: Synergizing Reasoning and Acting in Language Models. It's a pattern where you implement additional actions that an LLM can take - searching Wikipedia or running calculations for example - and then teach it how to request the execution of those actions, and then feed their results back into the LLM.</p> <p>Additionally, we give the LLM the possibility of using a scratchpad described in the paper Show Your Work: Scratchpads for Intermediate Computation with Language Models which improves the ability of LLMs to perform multi-step computations.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nimport outlines\n\nllm = llama_cpp.Llama(\n    \"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\nmodel = outlines.from_llamacpp(llm)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>from llama_cpp import Llama\n\nllm = Llama(\"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\", ...)\n</code></pre>"},{"location":"examples/react_agent/#build-a-react-agent","title":"Build a ReAct agent","text":"<p>In this example, we use two tools:</p> <ul> <li>wikipedia: \\&lt;search term&gt; - search Wikipedia and returns the snippet of the first result</li> <li>calculate: \\&lt;expression&gt; - evaluate an expression using Python's eval() function</li> </ul> <pre><code>import httpx\n\ndef wikipedia(q):\n    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n        \"action\": \"query\",\n        \"list\": \"search\",\n        \"srsearch\": q,\n        \"format\": \"json\"\n    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n\n\ndef calculate(numexp):\n    return eval(numexp)\n</code></pre> <p>We define the logic of the agent through a Pydantic class. First, we want the LLM to decide only between the two previously defined tools:</p> <pre><code>from enum import Enum\n\nclass Action(str, Enum):\n    wikipedia = \"wikipedia\"\n    calculate = \"calculate\"\n</code></pre> <p>Our agent will loop through Thought and Action. We explicitly give the Action Input field so it doesn't forget to add the arguments of the Action. We also add a scratchpad (optional).</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reason_and_Act(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Thought: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n    Action: Action\n    Action_Input: str = Field(..., description=\"The arguments of the Action.\")\n</code></pre> <p>Our agent will reach a Final Answer. We also add a scratchpad (optional).</p> <pre><code>class Final_Answer(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Final_Answer: str = Field(..., description=\"Answer to the question grounded on the Observation\")\n</code></pre> <p>Our agent will decide when it has reached a Final Answer and therefore to stop the loop of Thought and Action.</p> <pre><code>from typing import Union\n\nclass Decision(BaseModel):\n    Decision: Union[Reason_and_Act, Final_Answer]\n\njson_schema = Decision.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema and explain the agent logic. We can load a template from a file for that:</p> <pre><code>from outlines import Template\n\nhermes_prompt = Template.from_file(\"prompt_templates/react_agent.txt\")\n</code></pre> <p>We define a ChatBot class</p> <pre><code>class ChatBot:\n    def __init__(self, prompt=\"\"):\n        self.prompt = prompt\n\n    def __call__(self, user_prompt):\n        self.prompt += user_prompt\n        result = self.execute()\n        return result\n\n    def execute(self):\n        generator = outlines.Generator(model, Decision)\n        result = generator(self.prompt, max_tokens=1024, temperature=0, seed=42)\n        return result\n</code></pre> <p>We define a query function:</p> <pre><code>import json\n\ndef query(question, max_turns=5):\n    i = 0\n    next_prompt = (\n        \"\\n&lt;|im_start|&gt;user\\n\" + question + \"&lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;assistant\\n\"\n    )\n    previous_actions = []\n    while i &lt; max_turns:\n        i += 1\n        prompt = generate_hermes_prompt(\n            question=question,\n            schema=Decision.model_json_schema(),\n            today=datetime.datetime.today().strftime('%Y-%m-%d')\n        )\n        bot = ChatBot(prompt=prompt)\n        result = bot(next_prompt)\n        json_result = json.loads(result)['Decision']\n        if \"Final_Answer\" not in list(json_result.keys()):\n            scratchpad = json_result['Scratchpad'] if i == 0 else \"\"\n            thought = json_result['Thought']\n            action = json_result['Action']\n            action_input = json_result['Action_Input']\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Thought: {thought} \\x1b[0m\")\n            print(f\"\\x1b[36m  -- running {action}: {str(action_input)}\\x1b[0m\")\n            if action + \": \" + str(action_input) in previous_actions:\n                observation = \"You already run that action. **TRY A DIFFERENT ACTION INPUT.**\"\n            else:\n                if action==\"calculate\":\n                    try:\n                        observation = eval(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n                elif action==\"wikipedia\":\n                    try:\n                        observation = wikipedia(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n            print()\n            print(f\"\\x1b[33m Observation: {observation} \\x1b[0m\")\n            print()\n            previous_actions.append(action + \": \" + str(action_input))\n            next_prompt += (\n                \"\\nScratchpad: \" + scratchpad +\n                \"\\nThought: \" + thought +\n                \"\\nAction: \" + action  +\n                \"\\nAction Input: \" + action_input +\n                \"\\nObservation: \" + str(observation)\n            )\n        else:\n            scratchpad = json_result[\"Scratchpad\"]\n            final_answer = json_result[\"Final_Answer\"]\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Final Answer: {final_answer} \\x1b[0m\")\n            return final_answer\n    print(f\"\\nFinal Answer: I am sorry, but I am unable to answer your question. Please provide more information or a different question.\")\n    return \"No answer found\"\n</code></pre> <p>We can now test our ReAct agent:</p> <pre><code>print(query(\"What's 2 to the power of 10?\"))\n# Scratchpad:\n# Thought: I need to perform a mathematical calculation to find the result of 2 to the power of 10.\n#  -- running calculate: 2**10\n#\n# Observation: 1024\n#\n# Scratchpad: 2 to the power of 10 is 1024.\n# Final Answer: 2 to the power of 10 is 1024.\n# 2 to the power of 10 is 1024.\n</code></pre> <pre><code>print(query(\"What does England share borders with?\"))\n# Scratchpad:\n# Thought: To answer this question, I will use the 'wikipedia' action to gather information about England's geographical location and its borders.\n#  -- running wikipedia: England borders\n#\n# Observation: Anglo-Scottish &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; of the United Kingdom separating Scotland and &lt;span class=\"searchmatch\"&gt;England&lt;/span&gt; which runs for\n#\n# Scratchpad: Anglo-Scottish border (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal border of the United Kingdom separating Scotland and England which runs for\n# Final Answer: England shares a border with Scotland.\n# England shares a border with Scotland.\n</code></pre> <p>As mentioned in Simon's blog post, this is not a very robust implementation at all and there's a ton of room for improvement. But it is lovely how simple it is with a few lines of Python to make these extra capabilities available to the LLM. And now you can run it locally with an open weights LLM.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"examples/read-pdfs/","title":"PDF to structured output with vision language models","text":"<p>A common task with language models is to ask language models questions about a PDF file.</p> <p>Typically, the output is unstructured text, i.e. \"talking\" to your PDF.</p> <p>In some cases, you may wish to extract structured information from the PDF, like tables, lists, citations, etc.</p> <p>PDFs are difficult to machine read. However, you can simply convert the PDF to images, and then use a vision language model to extract structured information from the images.</p> <p>This cookbook demonstrates how to</p> <ol> <li>Convert a PDF to a list of images</li> <li>Use a vision language model to extract structured information from the images</li> </ol>"},{"location":"examples/read-pdfs/#dependencies","title":"Dependencies","text":"<p>You'll need to install these dependencies:</p> <pre><code>pip install outlines pillow transformers torch==2.4.0 pdf2image\n\n# Optional, but makes the output look nicer\npip install rich\n</code></pre>"},{"location":"examples/read-pdfs/#import-the-necessary-libraries","title":"Import the necessary libraries","text":"<pre><code>from PIL import Image\nimport outlines\nimport torch\nfrom transformers import AutoProcessor\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom pdf2image import convert_from_path\nimport os\nfrom rich import print\nimport requests\n</code></pre>"},{"location":"examples/read-pdfs/#choose-a-model","title":"Choose a model","text":"<p>We've tested this example with Pixtral 12b and Qwen2-VL-7B-Instruct.</p> <p>To use Pixtral:</p> <pre><code>from transformers import LlavaForConditionalGeneration, LlavaProcessor\nmodel_name=\"mistral-community/pixtral-12b\"\nmodel_class=LlavaForConditionalGeneration\nprocessor_class = LlavaProcessor\n</code></pre> <p>To use Qwen-2-VL:</p> <pre><code>from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nmodel_name = \"Qwen/Qwen2-VL-7B-Instruct\"\nmodel_class = Qwen2VLForConditionalGeneration\nprocessor_class = AutoProcessor\n</code></pre> <p>You can load your model into memory with:</p> <pre><code># This loads the model into memory. On your first run,\n# it will have to download the model, which might take a while.\nmodel_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.bfloat16}\nprocessor_kwargs={\"device_map\": \"cpu\"}\ntf_model = model_class.from_pretrained(model_name, **model_kwargs)\ntf_processor = processor_class.from_pretrained(model_name, **processor_kwargs)\n\nmodel = outlines.from_transformers(tf_model, tf_processor)\n</code></pre>"},{"location":"examples/read-pdfs/#convert-the-pdf-to-images","title":"Convert the PDF to images","text":"<p>We'll use the <code>pdf2image</code> library to convert each page of the PDF to an image.</p> <p><code>convert_pdf_to_images</code> is a convenience function that converts each page of the PDF to an image, and optionally saves the images to disk when <code>output_dir</code> is provided.</p> <p>Note: the <code>dpi</code> argument is important. It controls the resolution of the images. High DPI images are higher quality and may yield better results, but they are also larger, slower to process, and require more memory.</p> <pre><code>from pdf2image import convert_from_path\nfrom PIL import Image\nimport os\nfrom typing import List, Optional\n\ndef convert_pdf_to_images(\n    pdf_path: str,\n    output_dir: Optional[str] = None,\n    dpi: int = 120,\n    fmt: str = 'PNG'\n) -&gt; List[Image.Image]:\n    \"\"\"\n    Convert a PDF file to a list of PIL Image objects.\n\n    Args:\n        pdf_path: Path to the PDF file\n        output_dir: Optional directory to save the images\n        dpi: Resolution for the conversion. High DPI is high quality, but also slow and memory intensive.\n        fmt: Output format (PNG recommended for quality)\n\n    Returns:\n        List of PIL Image objects\n    \"\"\"\n    # Convert PDF to list of images\n    images = convert_from_path(\n        pdf_path,\n        dpi=dpi,\n        fmt=fmt\n    )\n\n    # Optionally save images\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n        for i, image in enumerate(images):\n            image.save(os.path.join(output_dir, f'page_{i+1}.{fmt.lower()}'))\n\n    return images\n</code></pre> <p>We're going to use the Louf &amp; Willard paper that described the method that Outlines uses for structured generation.</p> <p>To download the PDF, run:</p> <pre><code># Download the PDF file\npdf_url = \"https://arxiv.org/pdf/2307.09702\"\nresponse = requests.get(pdf_url)\n\n# Save the PDF locally\nwith open(\"louf-willard.pdf\", \"wb\") as f:\n    f.write(response.content)\n</code></pre> <p>Now, we can convert the PDF to a list of images:</p> <pre><code># Load the pdf\nimages = convert_pdf_to_images(\n    \"louf-willard.pdf\",\n    dpi=120,\n    output_dir=\"output_images\"\n)\n</code></pre>"},{"location":"examples/read-pdfs/#extract-structured-information-from-the-images","title":"Extract structured information from the images","text":"<p>The structured output you can extract is exactly the same as everywhere else in Outlines -- you can use regular expressions, JSON schemas, selecting from a list of options, etc.</p>"},{"location":"examples/read-pdfs/#extracting-data-into-json","title":"Extracting data into JSON","text":"<p>Suppose you wished to go through each page of the PDF, and extract the page description, key takeaways, and page number.</p> <p>You can do this by defining a JSON schema, and then using <code>outlines.Generator</code> to extract the data.</p> <p>First, define the structure you want to extract:</p> <pre><code>class PageSummary(BaseModel):\n    description: str\n    key_takeaways: List[str]\n    page_number: int\n</code></pre> <p>Second, we need to set up the prompt. Adding special tokens can be tricky, so we use the transformers processor to apply the special tokens for us. To do so, we specify a list of messages, where each message is a dictionary with a <code>role</code> and <code>content</code> key.</p> <p>Images are denoted with <code>type: \"image\"</code>, and text is denoted with <code>type: \"text\"</code>.</p> <pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            # The text you're passing to the model --\n            # this is where you do your standard prompting.\n            {\"type\": \"text\", \"text\": f\"\"\"\n                Describe the page in a way that is easy for a PhD student to understand.\n\n                Return the information in the following JSON schema:\n                {PageSummary.model_json_schema()}\n\n                Here is the page:\n                \"\"\"\n            },\n\n            # This a placeholder, the actual image is passed in when\n            # we call the generator function down below.\n            {\"type\": \"image\", \"image\": \"\"},\n        ],\n    }\n]\n\n# Convert the messages to the final prompt\nprompt = tf_processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n</code></pre> <p>Now we iterate through each image, and extract the structured information:</p> <pre><code># Page summarizer function\npage_summary_generator = outlines.Generator(model, PageSummary)\n\nfor image in images:\n    result = page_summary_generator({\"text\": prompt, \"images\": image})\n    print(result)\n</code></pre>"},{"location":"examples/read-pdfs/#regular-expressions-to-extract-the-arxiv-paper-identifier","title":"Regular expressions to extract the arxiv paper identifier","text":"<p>The arXiv paper identifier is a unique identifier for each paper. These identifiers have the format <code>arXiv:YYMM.NNNNN</code> (five end digits) or <code>arXiv:YYMM.NNNN</code> (four end digits). arXiv identifiers are typically watermarked on papers uploaded to arXiv.</p> <p>arXiv identifiers are optionally followed by a version number, i.e. <code>arXiv:YYMM.NNNNNvX</code>.</p> <p>We can use a regular expression to define this patter:</p> <pre><code>from outlines.types import Regex\n\npaper_regex = Regex(r'arXiv:\\d{2}[01]\\d\\.\\d{4,5}(v\\d)?')\n</code></pre> <p>We can build an extractor function from the regex:</p> <pre><code>id_extractor = outlines.Generator(model, paper_regex)\n</code></pre> <p>Now, we can extract the arxiv paper identifier from the first image:</p> <pre><code>arxiv_instruction = tf_processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"\"\"\n                Extract the arxiv paper identifier from the page.\n\n                Here is the page:\n                \"\"\"},\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Extract the arxiv paper identifier\npaper_id = id_extractor({\"text\": arxiv_instruction, \"images\": images[0]})\n</code></pre> <p>As of the time of this writing, the arxiv paper identifier is</p> <pre><code>arXiv:2307.09702v4\n</code></pre> <p>Your version number may be different, but the part before <code>vX</code> should match.</p>"},{"location":"examples/read-pdfs/#categorize-the-paper-into-one-of-several-categories","title":"Categorize the paper into one of several categories","text":"<p><code>outlines.Generator</code> also allows the model to select one of several options by providing a Literal type hint with the categories.</p> <p>Suppose we wanted to categorize the paper into being about \"language models\", \"cell biology\", or \"other\". We would then define the output type as <code>Literal[\"llms\", \"cell biology\", \"other\"]</code>.</p> <p>Let's define a few categories we might be interested in:</p> <pre><code>categories = [\n    \"llms\",\n    \"cell biology\",\n    \"other\"\n]\n</code></pre> <p>Now we can construct the prompt:</p> <pre><code>categorization_instruction = tf_processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"\"\"\n                Please choose one of the following categories\n                that best describes the paper.\n\n                {categories}\n\n                Here is the paper:\n                \"\"\"},\n\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n</code></pre> <p>Now we can show the model the first page and extract the category:</p> <pre><code>from typing import Literal\n\n# Build the choice extractor\ncategorizer = outlines.Generator(model, Literal[\"llms\", \"cell biology\", \"other\"])\n\n# Categorize the paper\ncategory = categorizer({\"text\": categorization_instruction, \"images\": images[0]})\nprint(category)\n</code></pre> <p>Which should return:</p> <pre><code>llms\n</code></pre>"},{"location":"examples/read-pdfs/#additional-notes","title":"Additional notes","text":"<p>You can provide multiple images to the model by</p> <ol> <li>Adding additional image messages</li> <li>Providing a list of images to the generator</li> </ol> <p>For example, to have two images, you can do:</p> <pre><code>two_image_prompt = tf_processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"are both of these images of hot dogs?\"},\n\n                # Tell the model there are two images\n                {\"type\": \"image\", \"image\": \"\"},\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Pass two images to the model\ngenerator = outlines.Generator(model, Literal[\"hot dog\", \"not hot dog\"])\n\nresult = generator({\"text\": two_image_prompt, \"images\": [images[0], images[1]]})\nprint(result)\n</code></pre> <p>Using the first to pages of the paper (they are not images of hot dogs), we should get</p> <pre><code>not hot dog\n</code></pre>"},{"location":"examples/receipt-digitization/","title":"Receipt Data Extraction with VLMs","text":""},{"location":"examples/receipt-digitization/#setup","title":"Setup","text":"<p>You'll need to install the dependencies:</p> <pre><code>pip install outlines torch==2.4.0 transformers accelerate pillow rich\n</code></pre>"},{"location":"examples/receipt-digitization/#import-libraries","title":"Import libraries","text":"<p>Load all the necessary libraries:</p> <pre><code># LLM stuff\nimport outlines\nimport torch\nfrom transformers import AutoProcessor\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional, List\n\n# Image stuff\nfrom PIL import Image\nimport requests\n\n# Rich for pretty printing\nfrom rich import print\n</code></pre>"},{"location":"examples/receipt-digitization/#choose-a-model","title":"Choose a model","text":"<p>This example has been tested with <code>mistral-community/pixtral-12b</code> (HF link) and <code>Qwen/Qwen2-VL-7B-Instruct</code> (HF link).</p> <p>We recommend Qwen-2-VL as we have found it to be more accurate than Pixtral.</p> <p>If you want to use Qwen-2-VL, you can do the following:</p> <pre><code># To use Qwen-2-VL:\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nmodel_name = \"Qwen/Qwen2-VL-7B-Instruct\"\nmodel_class = Qwen2VLForConditionalGeneration\nprocessor_class = AutoProcessor\n</code></pre> <p>If you want to use Pixtral, you can do the following:</p> <pre><code># To use Pixtral:\nfrom transformers import LlavaForConditionalGeneration, LlavaProcessor\nmodel_name=\"mistral-community/pixtral-12b\"\nmodel_class=LlavaForConditionalGeneration\nprocessor_class = LlavaProcessor\n</code></pre>"},{"location":"examples/receipt-digitization/#load-the-model","title":"Load the model","text":"<p>Load the model into memory:</p> <pre><code>model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": torch.bfloat16}\nprocessor_kwargs={\"device_map\": \"cuda\"}\ntf_model = model_class.from_pretrained(model_name, **model_kwargs)\ntf_processor = processor_class.from_pretrained(model_name, **processor_kwargs)\n\nmodel = outlines.from_transformers(tf_model, tf_processor)\n</code></pre>"},{"location":"examples/receipt-digitization/#image-processing","title":"Image processing","text":"<p>Images can be quite large. In GPU-poor environments, you may need to resize the image to a smaller size.</p> <p>Here's a helper function to do that:</p> <pre><code>def load_and_resize_image(image_path, max_size=1024):\n    \"\"\"\n    Load and resize an image while maintaining aspect ratio\n\n    Args:\n        image_path: Path to the image file\n        max_size: Maximum dimension (width or height) of the output image\n\n    Returns:\n        PIL Image: Resized image\n    \"\"\"\n    image = Image.open(image_path)\n\n    # Get current dimensions\n    width, height = image.size\n\n    # Calculate scaling factor\n    scale = min(max_size / width, max_size / height)\n\n    # Only resize if image is larger than max_size\n    if scale &lt; 1:\n        new_width = int(width * scale)\n        new_height = int(height * scale)\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n\n    return image\n</code></pre> <p>You can change the resolution of the image by changing the <code>max_size</code> argument. Small max sizes will make the image more blurry, but processing will be faster and require less memory.</p>"},{"location":"examples/receipt-digitization/#load-an-image","title":"Load an image","text":"<p>Load an image and resize it. We've provided a sample image of a Trader Joe's receipt, but you can use any image you'd like.</p> <p>Here's what the image looks like:</p> <p></p> <pre><code># Path to the image\nimage_path = \"https://raw.githubusercontent.com/dottxt-ai/outlines/refs/heads/main/docs/cookbook/images/trader-joes-receipt.jpg\"\n\n# Download the image\nresponse = requests.get(image_path)\nwith open(\"receipt.png\", \"wb\") as f:\n    f.write(response.content)\n\n# Load + resize the image\nimage = load_and_resize_image(\"receipt.png\")\n</code></pre>"},{"location":"examples/receipt-digitization/#define-the-output-structure","title":"Define the output structure","text":"<p>We'll define a Pydantic model to describe the data we want to extract from the image.</p> <p>In our case, we want to extract the following information:</p> <ul> <li>The store name</li> <li>The store address</li> <li>The store number</li> <li>A list of items, including the name, quantity, price per unit, and total price</li> <li>The tax</li> <li>The total</li> <li>The date</li> <li>The payment method</li> </ul> <p>Most fields are optional, as not all receipts contain all information.</p> <pre><code>class Item(BaseModel):\n    name: str\n    quantity: Optional[int]\n    price_per_unit: Optional[float]\n    total_price: Optional[float]\n\nclass ReceiptSummary(BaseModel):\n    store_name: str\n    store_address: str\n    store_number: Optional[int]\n    items: List[Item]\n    tax: Optional[float]\n    total: Optional[float]\n    # Date is in the format YYYY-MM-DD. We can apply a regex pattern to ensure it's formatted correctly.\n    date: Optional[str] = Field(pattern=r'\\d{4}-\\d{2}-\\d{2}', description=\"Date in the format YYYY-MM-DD\")\n    payment_method: Literal[\"cash\", \"credit\", \"debit\", \"check\", \"other\"]\n</code></pre>"},{"location":"examples/receipt-digitization/#prepare-the-prompt","title":"Prepare the prompt","text":"<p>We'll use the <code>tf_processor</code> to convert the image and the text prompt into a format that the model can understand. Practically, this is the code that adds user, system, assistant, and image tokens to the prompt.</p> <pre><code># Set up the content you want to send to the model\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                # The image is provided as a PIL Image object\n                \"type\": \"image\",\n                \"image\": image,\n            },\n            {\n                \"type\": \"text\",\n                \"text\": f\"\"\"You are an expert at extracting information from receipts.\n                Please extract the information from the receipt. Be as detailed as possible --\n                missing or misreporting information is a crime.\n\n                Return the information in the following JSON schema:\n                {ReceiptSummary.model_json_schema()}\n            \"\"\"},\n        ],\n    }\n]\n\n# Convert the messages to the final prompt\nprompt = tf_processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n</code></pre> <p>If you are curious, the final prompt that is sent to the model looks (roughly) like this:</p> <pre><code>&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;\nYou are an expert at extracting information from receipts.\nPlease extract the information from the receipt. Be as detailed as\npossible -- missing or misreporting information is a crime.\n\nReturn the information in the following JSON schema:\n\n&lt;JSON SCHEMA OMITTED&gt;\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>"},{"location":"examples/receipt-digitization/#run-the-model","title":"Run the model","text":"<pre><code># Prepare a function to process receipts\nreceipt_summary_generator = outlines.Generator(model, ReceiptSummary)\n\n# Generate the receipt summary\nresult = receipt_summary_generator(\n    {\"text\": prompt, \"images\": image},\n    max_new_tokens=1024\n)\nprint(result)\n</code></pre>"},{"location":"examples/receipt-digitization/#output","title":"Output","text":"<p>The output should look like this:</p> <pre><code>{\n  \"store_name\": \"Trader Joe's\",\n  \"store_address\": \"401 Bay Street, San Francisco, CA 94133\",\n  \"store_number\": 0,\n  \"items\": [\n    {\"name\": \"BANANA EACH\", \"quantity\": 7, \"price_per_unit\": 0.23, \"total_price\": 1.61},\n    {\"name\": \"BAREBELLS CHOCOLATE DOUG\", \"quantity\": 1, \"price_per_unit\": 2.29, \"total_price\": 2.29},\n    {\"name\": \"BAREBELLS CREAMY CRISP\", \"quantity\": 1, \"price_per_unit\": 2.29, \"total_price\": 2.29},\n    {\"name\": \"BAREBELLS CHOCOLATE DOUG\", \"quantity\": 1, \"price_per_unit\": 2.29, \"total_price\": 2.29},\n    {\"name\": \"BAREBELLS CARAMEL CASHEW\", \"quantity\": 2, \"price_per_unit\": 2.29, \"total_price\": 4.58},\n    {\"name\": \"BAREBELLS CREAMY CRISP\", \"quantity\": 1, \"price_per_unit\": 2.29, \"total_price\": 2.29},\n    {\"name\": \"SPINDRIFT ORANGE MANGO 8\", \"quantity\": 1, \"price_per_unit\": 7.49, \"total_price\": 7.49},\n    {\"name\": \"Bottle Deposit\", \"quantity\": 8, \"price_per_unit\": 0.05, \"total_price\": 0.4},\n    {\"name\": \"MILK ORGANIC GALLON WHOL\", \"quantity\": 1,\"price_per_unit\": 6.79,\"total_price\": 6.79},\n    {\"name\": \"CLASSIC GREEK SALAD\", \"quantity\": 1, \"price_per_unit\": 3.49, \"total_price\": 3.49},\n    {\"name\": \"COBB SALAD\", \"quantity\": 1, \"price_per_unit\": 5.99, \"total_price\": 5.99},\n    {\"name\": \"PEPPER BELL RED XL EACH\", \"quantity\": 1, \"price_per_unit\": 1.29, \"total_price\": 1.29},\n    {\"name\": \"BAG FEE.\", \"quantity\": 1, \"price_per_unit\": 0.25, \"total_price\": 0.25},\n    {\"name\": \"BAG FEE.\", \"quantity\": 1, \"price_per_unit\": 0.25, \"total_price\": 0.25},\n  ],\n  \"tax\": 0.68,\n  \"total\": 41.98,\n  \"date\": \"2023-11-04\",\n  \"payment_method\": \"debit\"\n}\n</code></pre> <p>Voila! You've successfully extracted information from a receipt using an LLM.</p>"},{"location":"examples/receipt-digitization/#bonus-roasting-the-user-for-their-receipt","title":"Bonus: roasting the user for their receipt","text":"<p>You can roast the user for their receipt by adding a <code>roast</code> field to the end of the  <code>ReceiptSummary</code> model.</p> <pre><code>class ReceiptSummary(BaseModel):\n    ...\n    roast: str\n</code></pre> <p>which gives you a result like</p> <pre><code>{\n    ...\n    \"roast\": \"You must be a fan of Trader Joe's because you bought enough\n    items to fill a small grocery bag and still had to pay for a bag fee.\n    Maybe you should start using reusable bags to save some money and the\n    environment.\"\n}\n</code></pre> <p>Qwen is not particularly funny, but worth a shot.</p>"},{"location":"examples/simtom/","title":"Build perspective-taking agents with SimToM","text":"<p>Prompting strategies like Chain-of-Thought (CoT) can improve LLMs' reasoning capabilities. However, they underwhelm in tasks that require keeping track of inconsistent world states. SimToM proposes a simple, two-stage prompting framework for LLMs inspired by Simulation Theory. The authors showed that this approach outperforms zero-shot prompting and CoT on ToMI and BigToM, two benchmarks with Theory of Mind questions.</p> <p>In this example, we will implement SimToM with a few lines of code using Outlines' prompt templating and structured generation capabilities.</p>"},{"location":"examples/simtom/#how-simtom-works","title":"How SimToM works","text":"<p>SimToM calls an LLM with two consecutive prompts:</p> <ol> <li>Perspective-taking: The first prompt receives a <code>story</code> and a <code>character</code>. The goal is to understand the situation based on the character's point of view and filter out the rest of the story.</li> <li>Question-Answering: The second prompt receives the character's point of view from the previous step and tasks the LLM to answer a question using that context.</li> </ol> <p></p>"},{"location":"examples/simtom/#outlines-implementation","title":"Outlines implementation","text":"<p>To implement SimToM with Outlines, we will need to:</p> <ol> <li>Write the prompts with prompt templates.</li> <li>Define the JSON object each prompt will return using Pydantic.</li> <li>Generate responses with a Mistral model using the transformers integration.</li> </ol> <p>Let's dive into it!</p>"},{"location":"examples/simtom/#using-prompt-templates","title":"Using Prompt Templates","text":"<p>The authors have shared their code, prompts and data in this GitHub repository. Below, we define in Outlines the prompts they used for the ToMI dataset:</p> <pre><code>from outlines import Template\n\nperspective_taking = Template.from_file(\"prompt_templates/simtom_prospective_taking.txt\")\nsimulation = Template.from_file(\"prompt_templates/simtom_simulation.txt\")\n</code></pre>"},{"location":"examples/simtom/#json-structured-generation","title":"JSON Structured Generation","text":"<p>Outlines guarantees that the LLM will return a valid JSON object, which we can specify as a Pydantic model.</p> <p>We will need two Pydantic models for SimToM, one for each prompt:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass PerspectiveTaking(BaseModel):\n    \"\"\"This is for the first prompt.\"\"\"\n    character: str = Field(description=\"The character we extract the events for.\")\n    events: List[str] = Field(description=\"All events that the character knows about.\")\n\nclass Simulation(BaseModel):\n    \"\"\"This is for the second prompt.\"\"\"\n    answer: str\n</code></pre>"},{"location":"examples/simtom/#calling-an-llm","title":"Calling an LLM","text":"<p>Let's try SimToM with an example from the ToMI dataset:</p> <pre><code>story = \"\"\"\n1 Aria entered the front_yard.\n2 Aiden entered the front_yard.\n3 The grapefruit is in the green_bucket.\n4 Aria moved the grapefruit to the blue_container.\n5 Aiden exited the front_yard.\n6 Noah entered the playroom.\n\"\"\"\nquestion = \"7 Where was the grapefruit at the beginning?\"\ncharacter = \"Aria\"\n</code></pre> <p>We load <code>Mistral-7B-Instruct-v0.3</code>, create the prompt using the template we defined earlier, and generate a structured response. As a reminder, the goal of the first call is to get all the events a character, <code>Aria</code>, knows about.</p> <pre><code>import transformers\nimport outlines\n# Load an LLM from Hugging Face\nMODEL_NAME = \"mistral-community/Mistral-7B-Instruct-v0.3\"\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n    transformers.AutoTokenizer.from_pretrained(MODEL_NAME),\n)\n\nperspective_prompt = perspective_taking(story=story, character=character)\n\n# Call Mistral 7B with the first prompt\ngenerator = outlines.Generator(model, PerspectiveTaking)\nperspective = generator(perspective_prompt, max_new_tokens=1024)\n\nprint(perspective)\n# {'character': 'Aria', 'events': ['1 Aria entered the front_yard.', '3 The grapefruit is in the green_bucket.', '4 Aria moved the grapefruit to the blue_container.']}\n</code></pre> <p>Not bad! We will now generate the second prompt with those events.</p> <pre><code>import json\n\nsim_prompt = simulation(events=json.loads(perspective)[\"events\"], name=character, question=question)\n\n# Call Mistral 7B with the second prompt\ngenerator = outlines.Generator(model, Simulation)\nresult = generator(sim_prompt, max_new_tokens=1024)\n\nprint(result)\n# {'answer': 'green_bucket'}\n</code></pre> <p>And this is it! SimToM could be useful in agentic workflows, where agents must act based on what they know, not all available information. One caveat of SimToM is that the perspective-taking step may remove important information, leading to wrong results. As the authors note in their paper, it can feature as a simple and effective baseline for evaluating LLMs on Theory of Mind reasoning tasks.</p>"},{"location":"examples/structured_generation_workflow/","title":"Structured Generation Workflow: Generating Synthetic Phone Numbers","text":"<p>This is a condensed version of Coding for Structured Generation with LLMs.</p> <p>For this example we're going to be building an LLM program to generate synthetic data in the form of realistic looking phone numbers for Washington State. Using an LLM for this task is a bit overkill since we could just as easily accomplish this with a tool like Faker, but this example still serves as a useful way to demonstrate a workflow for using structured generation.</p>"},{"location":"examples/structured_generation_workflow/#unstructured-approach","title":"Unstructured approach","text":"<p>Before diving into how to use structure generation for this task let's start with an unstructured example. We begin by loading our model:</p> <pre><code>import outlines\nimport transformers\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\nmodel = outlines.from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(model_name),\n    transformers.AutoTokenizer.from_pretrained(model_name)\n)\n</code></pre> <p>Next we need a prompt for this model. Since we're focusing on structured generation, we won't be engaging in any form of \"prompt hacking\" and will be leaving this prompt untouched for the rest of this example.</p> <pre><code>prompt_phone = \"\"\"\n    Please generate a realistic phone number for Washington State in the following format\n    (555) 555-5555\n\"\"\"\n</code></pre> <p>With our prompt ready we can now generate 10 example phone numbers</p> <pre><code>phone_generator_unstruct = outlines.Generator(model)\nfor _ in range(3):\n    print(phone_generator_unstruct(prompt_phone, max_new_tokens=12))\n</code></pre> <p>I'd be happy to help you generate a realistic phone\\ I cannot generate a real phone number as I'm just\\ I'm an AI and don't have the ability\\ Sure! Here is a randomly generated phone number in the format\\ Here's a phone number that fits the format for a\\ In Washington State, phone numbers typically have a three-dig\\ Here are a few examples of phone numbers that could be considered\\ I'd be happy to help generate a realistic phone number\\ I'd be happy to help you generate a random phone\\ Based on the format you provided, a realistic phone number for\\</p> <p>As we can see, none of these outputs are even phone numbers!</p> <p>Let's see  if we can improve this using structured generation.</p>"},{"location":"examples/structured_generation_workflow/#the-structured-generation-workflow","title":"The Structured Generation Workflow","text":"<p>In order to solve this problem we're going to introduce a Structured Generation Workflow outlined in this image:</p> <p></p> <p>Let's step through this:</p>"},{"location":"examples/structured_generation_workflow/#real-example","title":"Real example","text":"<p>We start with a real example phone number, in this case for the Seattle Public Library, that we can use to verify the structure we are creating.</p> <pre><code>phone_number = \"(206) 386-4636\"\n</code></pre> <p>For a simple example like this, we'll just be using a single phone number, for more complex examples it can be helpful to have more examples.</p>"},{"location":"examples/structured_generation_workflow/#draft-structure","title":"Draft Structure","text":"<p>The next step in the process is for use to define a simple regex that we feel correctly models our real data.</p> <pre><code>from outlines.types import Regex\n\nphone_regex_1 = Regex(r'\\([0-9]{3}\\) [0-9]{3}-[0-9]{4}')\n</code></pre> <p>Next we need to validate this regex against our real data.</p>"},{"location":"examples/structured_generation_workflow/#validate-by-matching-examples","title":"Validate by matching examples","text":"<p>Whenever writing non-trivial code with structured generation it is essential that you first validate the code against your real data example(s).</p> <p>We'll start with a simple method of validation: just checking that our regex matches the data.</p> <pre><code>import re\n\nre.match(phone_regex_1.pattern, phone_number)\n# &lt;re.Match object; span=(0, 14), match='(206) 386-4636'&gt;\n</code></pre> <p>Now that we have a match, we can move on to generating structured output!</p>"},{"location":"examples/structured_generation_workflow/#generate-structure","title":"Generate Structure","text":"<p>We're ready to see if structured generation can make an improvement over our initial unstructured approach:</p> <pre><code>phone_generator_v1 = outlines.Generator(model, phone_regex_1)\n\nfor _ in range(3):\n    print(phone_generator_v1(prompt_phone))\n</code></pre> <p>(206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 123-4567\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234</p> <p>At least we have phone numbers! But I think we can do better!</p>"},{"location":"examples/structured_generation_workflow/#inspect-output","title":"Inspect output","text":"<p>In this case the model did create phone numbers and, impressively, got the area code correct. So using structured generation did improve things. However these numbers are pretty boring. Let's improve that structure!</p>"},{"location":"examples/structured_generation_workflow/#iteration","title":"Iteration","text":"<p>We've walked through the loop once, so we can go quickly now through each iteration.</p> <p>We start by improving our structure:</p> <pre><code>phone_regex_2 = Regex(r'\\([0-9]{3}\\) [2-46-9]{3}-[02-9]{4}')\n</code></pre> <p>Before rushing to another round of generation, let's validate this new regex. We'll add just a bit more sophistication over our last check:</p> <p><pre><code>re.match(phone_regex_2.pattern, phone_number)[0] == phone_number\n# True\n</code></pre> Now that we've validated, let's generate with this new regex!</p> <pre><code>phone_generator_v2 = outlines.Generator(model, phone_regex_2)\n\nfor _ in range(3):\n    print(phone_generator_v2(prompt_phone))\n</code></pre> <p>(206) 867-5309\\ (206) 666-7777\\ (206) 444-3333\\ (206) 444-3333\\ (206) 943-2222\\ (206) 323-6789\\ (206) 444-3333\\ (206) 867-5309\\ (206) 466-2255\\ (206) 222-3333</p> <p>Better, but I don't like those repeated sequences. Like good software developers, let's iterate again!</p>"},{"location":"examples/structured_generation_workflow/#reiteration-with-debugging","title":"Reiteration - with debugging","text":"<p>Here's a fancier regex that should give us more interesting results:</p> <pre><code>phone_regex_3_error = r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4]'\n</code></pre> <p>This looks good to me, but there's a subtle bug, that's why we always need to validate our structure against real data. This time we'll make our validator do a bit more work to verify the correct string is matched:</p> <p><pre><code>if not re.match(phone_regex_3_error, phone_number):\n    print(\"Regex fails match\")\nelse:\n    matched_string = re.match(phone_regex_3_error, phone_number)[0]\n    if matched_string == phone_number:\n    print(\"Successful match\")\n    else:\n    print(f\"Error {matched_string} != {phone_number}\")\n</code></pre> This prints out:</p> <p>Error (206) 386-463 != (206) 386-4636</p> <p>Ah! We were missing the last digit, let's fix that and regenerate:</p> <pre><code>phone_regex_3_fixed = Regex(r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4][6-9]')\nphone_generator_v3 = outlines.Generator(model, phone_regex_3_fixed)\n\nfor _ in range(3):\n    print(phone_generator_v3(prompt_phone))\n</code></pre> <p>(206) 494-3216\\ (206) 374-6218\\ (206) 494-3337\\ (206) 476-3216\\ (206) 484-3548\\ (206) 495-3218\\ (206) 494-5517\\ (206) 375-4636\\ (206) 384-6216\\ (206) 385-6218</p> <p>Much better!</p> <p>Now you've seen a quick example of the structured generation workflow that can be used at the basis for building and iteration on much larger structured generation tasks!</p>"},{"location":"features/","title":"Features","text":"<p>This section presents in details the different features of Outlines.</p>"},{"location":"features/#core-concepts","title":"Core Concepts","text":"<ul> <li>Models</li> <li>Model Inputs</li> <li>Output Types</li> <li>Generators</li> </ul>"},{"location":"features/#utilities","title":"Utilities","text":"<ul> <li>Applications</li> <li>Templates</li> <li>Regex DSL</li> </ul>"},{"location":"features/#advanced","title":"Advanced","text":"<ul> <li>Logits Processors</li> </ul>"},{"location":"features/advanced/backends/","title":"Structured Generation Backends","text":"<p>Outlines relies on a structured generation backend to control text generation for steerable models such thah they conform to the output type provided. One of those backends is of course <code>outlines-core</code>, but you also have access to two other libraries that fulfill the same purpose: <code>llguidance</code> and <code>xgrammar</code>.</p>"},{"location":"features/advanced/backends/#overview","title":"Overview","text":"<p>To select the backend to use for your generation, provide a value for the <code>backend</code> argument when calling a model or a generator.</p> <p>For instance:</p> <pre><code>from typing import Literal\nimport outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"What is the capital of France?\", output_type, backend=\"llguidance\")\nprint(result) # 'Paris'\n\ngenerator = outlines.Generaor(model, output_type)\nresult = generator(\"What is the capital of France?\", backend=\"xgrammar\")\nprint(result) # 'Paris'\n</code></pre> <p>If you do not provide a value for the <code>backend</code> argument, the default value will be used. The default value depends on the type of output type:</p> <ul> <li>JSON schema: <code>outlines_core</code></li> <li>Regex: <code>outlines_core</code></li> <li>Context-free grammar: <code>llguidance</code></li> </ul>"},{"location":"features/advanced/backends/#features-matrix","title":"Features matrix","text":"<p>As mentioned previously, selecting the structured generation backend is only applicable to steerable models, so <code>Transformers</code>, <code>LlmaCpp</code> and <code>MLXLM</code>. Additionaly, some backends do not support some models within those or some output types.</p> outlines_core llguidance xgrammar Models Transformers \u2705 \u2705 \u2705 LlamaCpp \u2705 \u2705 \u274c MLXLM \u2705 \u2705 \u2705 Output Types JSON Schema \u2705 \u2705 \u2705 Regex \u2705 \u2705 \u2705 Grammar \u274c \u2705 \u2705"},{"location":"features/advanced/logits_processors/","title":"Logits Processors","text":"<p>Logits processors are objects that control text generation by modifying the probability distribution of possible next tokens. They do this by adjusting the logits (raw model outputs) at each generation step, effectively biasing the model's token selection.</p> <p>Processors can be used to:</p> <ol> <li>Generate structured output (e.g., JSON that follows a specific schema)</li> <li>Prevent the model from generating specific words or tokens</li> <li>Implement custom token sampling strategies</li> </ol>"},{"location":"features/advanced/logits_processors/#overview","title":"Overview","text":"<p>Outlines uses logits processors with steerable models \u2014 models that run locally and allow fine-grained control over the generation process. When using such models in Outlines, the output type provided is turned into a logits processor that is then passed to the inference engine.</p> <p>There are three models that support logits processors:</p> <ul> <li>LlamaCpp</li> <li>MLXLM</li> <li>Transformers</li> </ul> <p>Instead of providing an output type that will be turned into a logits processor, it is possible to directly provide a logits processor. To do so, you must create a <code>Generator</code> instance using the <code>processor</code> keyword argument. You cannot directly call the model with a logits processor.</p> <p>For instance:</p> <pre><code>import transformers\nfrom outlines import Generator, from_transformers\nfrom outlines.processors import RegexLogitsProcessor\n\n# Create a model\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"NousResearch/Hermes-2-Pro-Llama-3-8B\"),\n    transformers.AutoTokenizer.from_pretrained(\"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n)\n\n# Create a regex logits processor that only returns hex unicode notations\nlogits_processor = RegexLogitsProcessor(r\"U\\+[0-9A-Fa-f]{4,6}\", model.tokenizer, model.tensor_library_name)\n\n# Create a generator with the logits processor and use it to generate text\ngenerator = Generator(model, processor=logits_processor)\nresponse = generator(\"What's the unicode for the hugging face emoji\")\n\nprint(response) # U+1F917\n</code></pre>"},{"location":"features/advanced/logits_processors/#creating-custom-logits-processors","title":"Creating Custom Logits Processors","text":"<p>You can create your own logits processor by subclassing the <code>OutlinesLogitsProcessor</code> class. This allows you to implement specific logic to modify logits as needed. Your logits processor needs to implement the <code>process_logits</code> method to modify the logits. <code>process_logits</code> accepts: - <code>input_ids</code>: the ids of the tokens of the existing sequences in a 2D tensor. - <code>logits</code>: the logits for the current generation step in a 2D tensor.</p> <p>In the example below, we create a custom logits processor to force the model to provide a response using only binary representation (so only the tokens for 0 and 1 are allowed):</p> <pre><code>from outlines.processors.base_logits_processor import OutlinesLogitsProcessor, TensorType\nfrom outlines import Generator, from_transformers\nimport transformers\n\nALLOWED_TOKENS = [15, 16]  # token IDs corresponding to '0' and '1' in the model's vocabulary\n\n# Subclass OutlinesLogitsProcessor\nclass BinaryLogitsProcessor(OutlinesLogitsProcessor):\n\n    def process_logits(self, input_ids: TensorType, logits: TensorType) -&gt; TensorType:\n        # Create a mask for all tokens\n        mask = self.tensor_adapter.boolean_ones_like(logits)\n        # Set mask to False for the allowed tokens\n        for token_id in ALLOWED_TOKENS:\n            mask[:, token_id] = False\n        # Set non-allowed tokens to -inf so they are not selected\n        logits[mask] = float(\"-inf\")\n        return logits\n\n# Create a regular model\ntf_tokenizer = transformers.AutoTokenizer.from_pretrained(\"NousResearch/Hermes-2-Pro-Llama-3-8B\")\ntf_model = transformers.AutoModelForCausalLM.from_pretrained(\"NousResearch/Hermes-2-Pro-Llama-3-8B\")\nmodel = from_transformers(tf_model, tf_tokenizer)\n\n# Instantiate your custom logits processor\nlogits_processor = BinaryLogitsProcessor(model.tensor_library_name)\n\nprompt = \"Write the number 47 in binary. For example, 1010 is the binary representation of 10. Answer just with the binary number composed of 0s and 1s.\"\nformatted_prompt = tf_tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": prompt}],\n    tokenize=False\n)\n\n# Create a generator with the custom logits processor instance and use it to generate text\ngenerator = Generator(model, processor=logits_processor)\nresponse = generator(formatted_prompt)\n\nprint(response) # \"101111\"\n</code></pre>"},{"location":"features/core/generator/","title":"Generator","text":"<p>The <code>Generator</code> class is the core component of Outlines v1. <code>Generator</code> accepts a model and an optional output type. If no output type is provided, the <code>Generator</code> will return unstructured text.</p> <p>Note</p> <p><code>Generator</code> is new as of Outlines v1, and replaces previous generator constructors:</p> <ul> <li><code>generate.cfg</code></li> <li><code>generate.choice</code></li> <li><code>generate.format</code></li> <li><code>generate.fsm</code></li> <li><code>generate.json</code></li> <li><code>generate.regex</code></li> <li><code>generate.text</code></li> </ul>"},{"location":"features/core/generator/#methods","title":"Methods","text":"<p>Generators implement the same methods as models:</p> <ul> <li><code>__call__</code></li> <li><code>batch</code></li> <li><code>stream</code></li> </ul> <p>All of them take a single positional argument: the model input from which text is generated. Contrarily to the equivalent methods of models, you do not need to provide an output type as it has already been defined when initializing the generator.</p>"},{"location":"features/core/generator/#basic-usage","title":"Basic Usage","text":"<pre><code>from outlines import Generator, from_transformers\nimport transformers\n\n# Initialize a model\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(model_name),\n    transformers.AutoTokenizer.from_pretrained(model_name),\n)\n\n# Create a generator for plain text\ngenerator = Generator(model)\nresult = generator(\"Write a short poem about AI.\")\n\n# Print the result\nprint(result)\n</code></pre>"},{"location":"features/core/generator/#structured-generation","title":"Structured Generation","text":"<pre><code>from pydantic import BaseModel\nfrom outlines import Generator, from_transformers\nimport transformers\n\n# Define a Pydantic model for structured output\nclass BookRecommendation(BaseModel):\n    title: str\n    author: str\n    year: int\n\n# Initialize a model\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(model_name),\n    transformers.AutoTokenizer.from_pretrained(model_name),\n)\n\n# Create a generator for JSON output\ngenerator = Generator(model, BookRecommendation)\n\n# Generate a book recommendation\nresult = generator(\"Recommend a science fiction book.\")\n\n# Parse the JSON result into a Pydantic model\nbook = BookRecommendation.model_validate_json(result)\nprint(f\"{book.title} by {book.author} ({book.year})\")\n</code></pre>"},{"location":"features/core/generator/#parameters","title":"Parameters","text":"<ul> <li><code>model</code>: The language model to use for generation</li> <li><code>output_type</code>: Optional. The type of output to generate</li> </ul>"},{"location":"features/core/generator/#generation-parameters","title":"Generation Parameters","text":"<p>When calling the generator, you can pass additional parameters to control the generation process. These parameters are passed through to the underlying model, so they depend on the specific model being used.</p> <p>Common parameters for most models include: - <code>max_new_tokens</code>: Maximum number of tokens to generate - <code>temperature</code>: Controls randomness (higher values = more random) - <code>top_p</code>: Controls diversity via nucleus sampling - <code>stop_strings</code>: String or list of strings at which to stop generation</p> <p>Example: <pre><code>result = generator(\n    \"Write a short story.\",\n    max_new_tokens=200,\n    temperature=0.7,\n    top_p=0.9,\n    stop_strings=[\"THE END\", \"###\"]\n)\n</code></pre></p>"},{"location":"features/core/generator/#return-value","title":"Return Value","text":"<p>The generator always returns a raw string containing the generated text. When generating structured outputs, you need to parse this string into the desired format.</p> <p>Unlike in Outlines v0, where the return type could be a parsed object, in v1 you are responsible for parsing the output when needed:</p> <pre><code># Outlines v1 approach\nfrom pydantic import BaseModel\nfrom outlines import Generator\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\ngenerator = Generator(model, Person)\nresult = generator(\"Generate a person:\")\n\n# Parse the result yourself\nperson = Person.model_validate_json(result)\n</code></pre> <p>Create a generator for the given model and output parameters.</p> <p>The 2 parameters output_type and processor are mutually exclusive. The parameters processor is only supported for SteerableModel instances (typically local models) and is intended to be only used by advanced users.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Model, AsyncModel]</code> <p>An instance of an Outlines model.</p> required <code>output_type</code> <code>Optional[Any]</code> <p>The output type expressed as a Python type or a type defined in the outlines.types.dsl module.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The name of the backend to use to create the logits processor. Only used for steerable models if there is an output type and <code>processor</code> is not provided.</p> <code>None</code> <code>processor</code> <code>Optional[LogitsProcessorType]</code> <p>An instance of a logits processor.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]</code> <p>A generator instance.</p> Source code in <code>outlines/generator.py</code> <pre><code>def Generator(\n    model: Union[Model, AsyncModel],\n    output_type: Optional[Any] = None,\n    backend: Optional[str] = None,\n    *,\n    processor: Optional[LogitsProcessorType] = None,\n) -&gt; Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]:\n    \"\"\"Create a generator for the given model and output parameters.\n\n    The 2 parameters output_type and processor are mutually exclusive. The\n    parameters processor is only supported for SteerableModel instances\n    (typically local models) and is intended to be only used by advanced users.\n\n    Parameters\n    ----------\n    model\n        An instance of an Outlines model.\n    output_type\n        The output type expressed as a Python type or a type defined in the\n        outlines.types.dsl module.\n    backend\n        The name of the backend to use to create the logits processor. Only\n        used for steerable models if there is an output type and `processor` is\n        not provided.\n    processor\n        An instance of a logits processor.\n\n    Returns\n    -------\n    Union[SteerableGenerator, BlackBoxGenerator, AsyncBlackBoxGenerator]\n        A generator instance.\n\n    \"\"\"\n    provided_output_params = sum(\n        param is not None\n        for param in [output_type, processor]\n    )\n    if provided_output_params &gt; 1:\n        raise ValueError(\n            \"At most one of output_type or processor can be provided\"\n        )\n\n    if isinstance(model, SteerableModel): # type: ignore\n        if processor is not None:\n            return SteerableGenerator.from_processor(model, processor) # type: ignore\n        else:\n            return SteerableGenerator(model, output_type, backend) # type: ignore\n    else:\n        if processor is not None:\n            raise NotImplementedError(\n                \"This model does not support logits processors\"\n            )\n        if isinstance(model, AsyncBlackBoxModel): # type: ignore\n            return AsyncBlackBoxGenerator(model, output_type) # type: ignore\n        elif isinstance(model, BlackBoxModel): # type: ignore\n            return BlackBoxGenerator(model, output_type) # type: ignore\n        else:\n            raise ValueError(\n                \"The model argument must be an instance of \"\n                \"SteerableModel, BlackBoxModel or AsyncBlackBoxModel\"\n            )\n</code></pre>"},{"location":"features/core/inputs/","title":"Model Inputs","text":"<p>Outlines models accept various types of inputs to generate text. The input format depends on the capabilities of the underlying model and the type of task you want to perform. The most basic type of input is a single string prompt, it's accepted by all models.</p>"},{"location":"features/core/inputs/#overview","title":"Overview","text":"<p>The model input is the first argument of the <code>__call__</code>, <code>stream</code> and <code>batch</code> methods of both models and generators.</p> <p>There are 3 types of model inputs:</p> <ul> <li>Text prompts - Simple strings</li> <li>Multimodal inputs - List containning a string prompt along with assets</li> <li>Chat inputs - <code>Chat</code> instances containing messages</li> </ul>"},{"location":"features/core/inputs/#text-prompts","title":"Text Prompts","text":"<p>The simplest form of input is a plain text string. This works with all models and is suitable for standard text generation tasks.</p> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create a model\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n)\n\n# Simple text prompt\nresponse = model(\"What's the capital of France?\", max_new_tokens=20)\nprint(response)  # 'Paris'\n</code></pre>"},{"location":"features/core/inputs/#multimodal-inputs-vision","title":"Multimodal Inputs (Vision)","text":"<p>For models that support them, you can provide a list containing a text prompt and one or more assets.</p> <p>There are 3 types of assets defined in Outlines:</p> <ul> <li><code>Image</code>: contains a PIL Image</li> <li><code>Video</code>: contains any object (you must choose a format that is supported by your model)</li> <li><code>Audio</code>: contains any object (you must choose a format that is supported by your model)</li> </ul> <p>Among those, <code>Image</code> is by far the most important as multiple models support vision inputs.</p> <p>For instance with vision input:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nimport openai\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/core/inputs/#chat-inputs","title":"Chat Inputs","text":"<p>For conversational models, you can use the <code>Chat</code> class to provide a conversation history with multiple messages.</p> <p>A <code>Chat</code> instance is instantiated with an optional list of messages. Each message must be a dictionary containing two mandatory keys: - <code>role</code>: must be one of <code>system</code>, <code>assistant</code> or <code>user</code> - <code>content</code>: must be either a string or a multimodal input (if the model supports it)</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nfrom outlines.inputs import Chat, Image\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\nprint(prompt)\n# {'role': 'system', 'content': 'You are a helpful assistant.'}\n# {'role': 'user', 'content': ['Describe the image', Image(image=&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=400x300 at 0x7FFA526CCC40&gt;)]}\n</code></pre> <p>After having created a <code>Chat</code> instance, you can add one or several messages thanks to the <code>append</code> and <code>extend</code> methods. You can also remove the last message of the Chat with the <code>pop</code> method.</p> <p>For instance:</p> <pre><code>from outlines.inputs import Chat\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n])\n\n# Add a message\nprompt.append({\"role\": \"user\", \"content\": \"How are you doing today?\"})\nprint(prompt)\n# {'role': 'system', 'content': 'You are a helpful assistant.'}\n# {'role': 'user', 'content': 'How are you doing today?'}\n\n# Remove the last messsage\nlast_message = prompt.pop()\nprint(last_message)\n# {'role': 'user', 'content': 'How are you doing today?'}\nprint(prompt)\n# {'role': 'system', 'content': 'You are a helpful assistant.'}\n\n# RAdd several messages\nprompt.extend([\n    {\"role\": \"user\", \"content\": \"How are you doing today?\"},\n    {\"role\": \"assistant\", \"content\": \"Excellent, thanks!\"}\n])\nprint(prompt)\n# {'role': 'system', 'content': 'You are a helpful assistant.'}\n# {'role': 'user', 'content': 'How are you doing today?'}\n# {'role': 'assistant', 'content': 'Excellent, thanks!'}\n</code></pre> <p>Finally, there are three convenience method to easily add a message:</p> <ul> <li>add_system_message</li> <li>add_user_message</li> <li>add_assistant_message</li> </ul> <p>As the role is already set, you only need to provide the content.</p> <p>For instance:</p> <pre><code>from outlines.inputs import Chat\n\n# Create the chat input\nprompt = Chat()\n\nprompt.add_system_message(\"You are a helpful assistant.\")\nprompt.add_system_message(\"How are you doing today?\")\nprompt.add_system_message(\"Excellent, thanks!\")\n\nprint(prompt)\n# {'role': 'system', 'content': 'You are a helpful assistant.'}\n# {'role': 'user', 'content': 'How are you doing today?'}\n# {'role': 'assistant', 'content': 'Excellent, thanks!'}\n</code></pre>"},{"location":"features/core/inputs/#batching","title":"Batching","text":"<p>In the case of batching, for models that support it, you just have to provide several instances of the model inputs described above in a list.</p> <p>For instance:</p> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create model\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create a list of prompts that will be used in a single batch\nprompts = [\n    \"What's the capital of Lithuania?\",\n    \"What's the capital of Latvia?\",\n    \"What's the capital of Estonia?\"\n]\n\n# Call it to generate text\nresult = model.batch(prompts, max_new_tokens=20)\nprint(result) # ['Vilnius', 'Riga', 'Tallinn']\n</code></pre>"},{"location":"features/core/output_types/","title":"Output Types","text":"<p>Outlines provides a simple and intuitive way of defining the output structure of text generation. Possible output formats include basic Python types, multiple-choices, JSON schemas, regular expressions and context-free grammars.</p>"},{"location":"features/core/output_types/#overview","title":"Overview","text":"<p>Outlines models accept a prompt and an output type when they are invoked, as well as additional inference keyword arguments that are forwarded on to the underlying model.</p> <p>Output types can be from the general Python ecosystem, including: - Most native Python types, such as <code>int</code> or <code>str</code> - Types from the <code>typing</code> module, such as <code>Literal</code>, <code>List</code>, <code>Dict</code>, <code>Enum</code>, etc - Types from popular third party libraries such as Pydantic or GenSON.</p> <p>Outlines also provides special classes for certain output structures (more details below): - Multiple choices with <code>Choice</code> - JSON schemas with <code>JsonSchema</code> - Regular expressions with <code>Regex</code> - Context-free grammars with <code>CFG</code></p> <p>The general idea is that you should provide as an output type what you would give as the type hint of the return type of a function.</p> <p>Consider the following functions for instance:</p> <pre><code>from datetime import date\nfrom typing import Dict, List, Literal, Union\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    name: str\n    birth_date: date\n    skills: Union[Dict, List[str]]\n\ndef give_int() -&gt; int:\n    ...\n\ndef pizza_or_burger() -&gt; Literal[\"pizza\", \"burger\"]:\n    ...\n\ndef create_character() -&gt; Character:\n    ...\n</code></pre> <p>With an Outlines model, you can generate text that respects the type hints above by providing those as the output type:</p> <pre><code>model(\"How many minutes are there in one hour\", int) # \"60\"\nmodel(\"Pizza or burger\", Literal[\"pizza\", \"burger\"]) # \"pizza\"\nmodel(\"Create a character\", Character, max_new_tokens=100) # '{\"name\": \"James\", \"birth_date\": \"1980-05-10)\", \"skills\": [\"archery\", \"negotiation\"]}'\n</code></pre> <p>An important difference with function type hints though is that an Outlines generator always returns a string. You have to cast the response into the type you want yourself.</p> <p>For instance:</p> <pre><code>result = model(\"Create a character\", Character, max_new_tokens=100)\ncasted_result = Character.model_validate_json(result)\nprint(result) # '{\"name\": \"Aurora\", \"birth_date\": \"1990-06-15\", \"skills\": [\"Stealth\", \"Diplomacy\"]}'\nprint(casted_result) # name=Aurora birth_date=datetime.date(1990, 6, 15) skills=['Stealth', 'Diplomacy']\n</code></pre>"},{"location":"features/core/output_types/#output-type-categories","title":"Output Type Categories","text":"<p>We can group possible output types in several categories based on the use case they correspond to. While most of those types are native python or types coming from well-known third-party libraries, there are three Outlines-specific types: <code>JsonSchema</code>, <code>Regex</code> and <code>CFG</code>. Their use is explained below.</p>"},{"location":"features/core/output_types/#basic-python-types","title":"Basic Python Types","text":"<p>The most straightforward form of structured generation is to return an answer that conforms to a given basic type such as an int or a python list. You can use the basic Python types and the types from the <code>typing</code> library. For instance:</p> <pre><code>from typing import Dict\n\noutput_type = float # example of valid value: \"0.05\"\noutput_type = bool # example of valid value: \"True\"\noutput_type = Dict[int, str] # example of valid value: \"{1: 'hello', 2: 'there'}\"\n</code></pre> <p>You can combine types to create more complex response formats by relying on collection types and types such as <code>Union</code> and <code>Optional</code>. Let's consider for instance the output type below used to represent semi-structured data:</p> <pre><code>from typing import Dict, List, Optional, Tuple, Union\n\noutput_type = Dict[str, Union[int, str, List[Tuple[str, Optional[float]]]]]\n</code></pre> <p>Values created with this output type would be dictionaries with string as keys and values made of either an integer, a string or a list of two elements tuples: a string and either a float or None. Example of a valid response for text generated with this output type (it would be contained in a string):</p> <pre><code>{\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"metrics\": [(\"engagement\", 0.85), (\"satisfaction\", None)]\n}\n</code></pre>"},{"location":"features/core/output_types/#multiple-choices","title":"Multiple Choices","text":"<p>Outlines supports multiple choice classification by using the <code>Literal</code> or <code>Enum</code> output types. For instance:</p> <pre><code>from enum import Enum\nfrom typing import Literal\n\nclass PizzaOrBurger(Enum):\n    pizza = \"pizza\"\n    burger = \"burger\"\n\n# Equivalent multiple-choice output types\noutput_type = Literal[\"pizza\", \"burger\"]\noutput_type = PizzaOrBurger\n</code></pre> <p>Additionally, you can use the Outlines-specific type <code>Choice</code> that takes a <code>list</code> as an argument. This type is useful in situations in which the list of choices is dynamic.</p> <p>For instance:</p> <pre><code>from outlines.types import Choice\n\ndef get_multiple_choices() -&gt; list:\n    # we could have something complex here\n    return [\"pizza\", \"burger\"]\n\noutput_type = Choice(get_multiple_choices())\n</code></pre>"},{"location":"features/core/output_types/#json-schemas","title":"JSON Schemas","text":"<p>Multiple different common Python types are often used to store information equivalent to a JSON schema. The following can be used in Outlines to generate text that respects a JSON schema:</p> <ul> <li>A Pydantic class</li> <li>A Dataclass</li> <li>A TypedDict</li> <li>A GenSON <code>SchemaBuilder</code></li> <li>A Callable (the parameters are turned into the keys and the type hinting is used to define the types of the values)</li> </ul> <p>For instance:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Character:\n    name: str\n    age: int\n\noutput_type = Character\n\ndef character(name: str, age: int):\n    return None\n\noutput_type = character\n</code></pre> <p>There are two other JSON schema formats that require Outlines-specific classes: JSON schema strings and dictionaries.</p> <p>As those are contained in regular Python strings or dictionaries, the associated output format would be ambiguous if they were to be provided directly. As a result, Outlines requires them to be wrapped in a <code>outlines.types.JsonSchema</code> object. For instance:</p> <pre><code>from outlines.types import JsonSchema\n\nschema_string = '{\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"number\"}}}'\noutput_type = JsonSchema(schema_string)\n\nschema_dict = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"answer\": {\"type\": \"number\"}\n    }\n}\noutput_type = JsonSchema(schema_dict)\n</code></pre> <p><code>JsonSchema</code> accepts two optional parameters: - <code>whitespace_pattern</code> (defaults to <code>None</code>): specifies the pattern to use for JSON syntactic whitespace. If none is provided, the default permissive JSON whitespace rules are used. - <code>ensure_ascii</code> (defaults to <code>True</code>): defines the value to use for the argument <code>ensure_ascii</code> of the <code>json.dumps</code> method. If false, non-ASCII characters will be turned into unicodes.</p>"},{"location":"features/core/output_types/#regex-patterns","title":"Regex Patterns","text":"<p>Outlines provides support for text generation constrained by regular expressions. Since regular expressions are expressed as simple raw string literals, regex strings must wrapped in an <code>outlines.types.Regex</code> object.</p> <pre><code>from outlines.types import Regex\n\nregex = r\"[0-9]{3}\"\noutput_type = Regex(regex)\n</code></pre> <p>The <code>outlines.types</code> module contains a few common regex patterns stored in variables you can import and directly use as output types. Common patterns include a sentence, an email address and an ISBN reference. For instance:</p> <pre><code>from outlines.types import sentence\n\nprint(type(sentence)) # outlines.types.dsl.Regex\nprint(sentence.pattern) # [A-Z].*\\s*[.!?]\n</code></pre> <p>To help you create complex regex patterns yourself, you can use the Outlines regex DSL.</p>"},{"location":"features/core/output_types/#context-free-grammars","title":"Context-Free Grammars","text":"<p>Outlines allows you to generate text that respects the syntax of a context-free grammar. Context-free grammars are defined using Lark, a grammar language. Since grammars are expressed as a string, Large CFG strings should be be wrapped in an <code>outlines.types.CFG</code> object. For instance:</p> <pre><code>from outlines.types import CFG\n\ngrammar_string = \"\"\"\n    start: expr\n    expr: \"{\" expr \"}\" | \"[\" expr \"]\" |\n\"\"\"\noutput_type = CFG(grammar_string)\n</code></pre> <p>You can find a few Lark grammar examples in the grammars module.</p>"},{"location":"features/core/output_types/#output-type-availability","title":"Output type availability","text":"<p>The output types presented above are not available for all models as some have only limited support for structured outputs. Please refer to the documentation of the specific model you wish to use to know what output types it supports.</p>"},{"location":"features/models/","title":"Models","text":""},{"location":"features/models/#overview","title":"Overview","text":"<p>Outlines models are objects that wrap an inference client or engine. Models provide a standardized interface to generate structured text.</p> <p>All Outlines model classes have an associated loader function to facilitate initializing a model instance. The name of this function is <code>from_</code> plus the name of the model in lower-case letters. For instance, Outlines has a <code>Transformers</code> model and an associated <code>from_transformers</code> loader function. The parameters to load a model are specific to each provider, please consult the documentation of the model you want to use for more information.</p> <p>After having created a model instance, you can either directly call it to generate text or first create a reusable generator that you would then call.</p> <p>The input you must provide to a model to generate text can be a simple text prompt or a vision or chat input for models that support them. See the model inputs section for more information on model inputs formats.</p> <p>In all cases, you can provide an <code>output_type</code> to constrain the format of the generation output. See the output types section for more information on constrained generation.</p> <p>For instance:</p> <pre><code>from outlines import from_transformers, Generator\nimport transformers\n\n# Create a model\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n)\n\n# Call it directly\nresponse = model(\"How many countries are there in the world\", max_new_tokens=20)\nprint(response) # 'There are 200 countries in the world.'\n\n# Call it directly with an output_type\nresponse = model(\"How many countries are there in the world\", int, max_new_tokens=20)\nprint(response) # '200'\n\n# Create a generator first and then call it\ngenerator = Generator(model, int)\nresponse = generator(\"How many countries are there in the world\")\nprint(response) # '200'\n</code></pre> <p>Some models support streaming through a <code>stream</code> method. It takes the same argument as the <code>__call__</code> method, but returns an iterator instead of a string.</p> <p>For instance:</p> <pre><code>from outlines import from_openai, Generator\nimport openai\n\n# Create the model\nmodel = from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell a short story about a cat.\", max_tokens=50):\n    print(chunk) # 'This...'\n</code></pre> <p>Additionally, some models support batch processing through a <code>batch</code> method. It's similar to the <code>__call__</code> method, but takes a list of prompts instead of a single prompt and returns a list of strings.</p> <p>For instance:</p> <pre><code>from outlines import from_transformers, Generator\nimport transformers\n\n# Create a model\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n)\n\n# Call it directly\nresponse = model.batch([\"What's the capital of Latvia?\", \"What's the capital of Estonia?\"], max_new_tokens=20)\nprint(response) # ['Riga', 'Tallinn']\n</code></pre>"},{"location":"features/models/#features-matrix","title":"Features Matrix","text":"<p>In alphabetical order:</p> Anthropic Dottxt Gemini LlamaCpp MLXLM Mistral Ollama OpenAI SGLang TGI Transformers Transformers MultiModal VLLM VLLMOffline Output Types Simple Types \u274c \u274c \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 JSON Schema \u274c \u2705 \ud83d\udfe0 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Multiple Choice \u274c \u274c \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Regex \u274c \u274c \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Grammar \u274c \u274c \u274c \u274c \u2705 \u274c \u274c \u274c \ud83d\udfe0 \u274c \u2705 \u2705 \u2705 \u2705 Generation Features Async \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c Streaming \u2705 \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c Vision \u2705 \u274c \u2705 \u274c \u274c \u2705 \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c \u274c Batching \u274c \u274c \u274c \u274c \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u2705 \u274c \u2705"},{"location":"features/models/#model-types","title":"Model Types","text":"<p>Models can be divided into two categories: local models and server-based models.</p> <p>In the case of local models, the text generation happens within the inference library object used to instantiate the model. This gives Outlines direct access to the generation process (through a logits processor) and means all structured generation output types are available.</p> <p>The local models available are the following:</p> <ul> <li>LlamaCpp</li> <li>MLXLM</li> <li>Transformers</li> <li>TransformersMultiModal</li> <li>VLLMOffline</li> </ul> <p>In the case of server-based models, the model is initialized with a client that sends a request to a server that is in charge of the actual text generation. As a result, we have limited control over text generation and some output types are not supported. The server on which the text generation happens can either be remote (with OpenAI or Anthopic for instance) or local (with SGLang for instance).</p> <p>The server-based models available are the following:</p> <ul> <li>Anthropic</li> <li>Dottxt</li> <li>Gemini</li> <li>Mistral</li> <li>Ollama</li> <li>OpenAI</li> <li>SgLang</li> <li>TGI</li> <li>VLLM</li> </ul> <p>Some models have an async version. To use them, just pass the async version of the provider object to their loading function. It will then return a <code>Async&lt;ModelName&gt;</code> instance with the same methods and features as the regular sync instance.</p> <p>For instance:</p> <pre><code>from outlines import from_tgi\nfrom huggingface_hub import AsyncInferenceClient\n\nmodel = from_tgi(\n    AsyncInferenceClient(\"http://localhost:8000/v1\")\n)\nprint(type(model)) # outlines.models.tgi.AsyncTGI\n</code></pre> <p>The models that have an async version are the following:</p> <ul> <li>Mistral</li> <li>Ollama</li> <li>OpenAI</li> <li>SgLang</li> <li>TGI</li> <li>VLLM</li> </ul>"},{"location":"features/models/anthropic/","title":"Anthropic","text":"<p>Installation</p> <p>You need to install the <code>anthropic</code> library to be able to use the Anthropic API in Outlines. Install all optional dependencies of the <code>Anthropic</code> model with: <code>pip install \"outlines[anthropic]\"</code>.</p> <p>You also need to have an Anthropic API key. This API key must either be set as an environment variable called <code>ANTHROPIC_API_KEY</code> or be provided to the <code>anthropic.Anthropic</code> class when instantiating it.</p>"},{"location":"features/models/anthropic/#model-initialization","title":"Model Initialization","text":"<p>To create an Anthropic model instance, you can use the <code>from_anthropic</code> function. It takes 2 arguments:</p> <ul> <li><code>client</code>: an <code>anthropic.Anthropic</code> instance</li> <li><code>model_name</code>: the name of the model you want to use in subsequent model calls (optional)</li> </ul> <p>For instance:</p> <pre><code>from anthropic import Anthropic\nimport outlines\n\n# Create the Anthropic client\nclient = Anthropic()\n\n# Create the model\nmodel = outlines.from_anthropic(\n    client,\n    \"claude-3-5-sonnet-latest\"\n)\n</code></pre> <p>Check the Anthropic documentation for an up-to-date list of available models.</p>"},{"location":"features/models/anthropic/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>Anthropic</code> model instance, you're all set to generate text with this provider. You can simply call the model with a text prompt.</p> <p>For instance:</p> <pre><code>from anthropic import Anthropic\nimport outlines\n\n# Create the model\nmodel = outlines.from_anthropic(\n    Anthropic(),\n    \"claude-3-5-sonnet-latest\"\n)\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/anthropic/#vision","title":"Vision","text":"<p>Some Anthropic models support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nfrom anthropic import Anthropic\nfrom outlines import from_anthropic\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = from_anthropic(\n    Anthropic(),\n    \"claude-3-5-sonnet-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/anthropic/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>Anthropic</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nfrom anthropic import Anthropic\nfrom outlines import from_anthropic\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = from_anthropic(\n    Anthropic(),\n    \"claude-3-5-sonnet-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"user\", \"content\": \"You are a helpful assistant that helps me described pictures.\"},\n    {\"role\": \"assistant\", \"content\": \"I'd be happy to help you describe pictures! Please go ahead and share an image\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/anthropic/#streaming","title":"Streaming","text":"<p>Finally, the <code>Anthropic</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>from anthropic import Anthropic\nimport outlines\n\n# Create the model\nmodel = outlines.from_anthropic(\n    Anthropic(),\n    \"claude-3-5-sonnet-latest\"\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n    print(chunk) # 'Once...'\n</code></pre>"},{"location":"features/models/anthropic/#inference-arguments","title":"Inference arguments","text":"<p>When calling the model or streaming, you can provide keyword arguments that will be passed down to the Anthropic client. Make sure to include all the arguments you need to configure the client's behavior to your expected behavior. Some of the most common arguments include <code>max_tokens</code>, <code>temperature</code>, <code>stop_sequences</code> and <code>top_k</code>.</p> <p>See the Anthropic API documentation for the full list of available arguments.</p> <p>Warning</p> <p>You must set a value for <code>max_tokens</code> with Anthropic models.</p>"},{"location":"features/models/dottxt/","title":"Dottxt","text":"<p>Installation</p> <p>You need to install the <code>dottxt</code> python sdk to be able to use the Dottxt API in Outlines. Install all optional dependencies of the <code>Dottxt</code> model with: <code>pip install \"outlines[dottxt]\"</code>.</p> <p>You also need to have a Dottxt API key. This API key must either be set as an environment variable called <code>DOTTXT_API_KEY</code> or be provided to the <code>dottxt.client.Dottxt</code> class when instantiating it.</p>"},{"location":"features/models/dottxt/#model-initialization","title":"Model Initialization","text":"<p>To create an Dottxt model instance, you can use the <code>from_dottxt</code> function. It takes 3 arguments:</p> <ul> <li><code>client</code>: a <code>dottxt.client.Dottxt</code> instance</li> <li><code>model_name</code>: the name of the model you want to use in subsequent model calls (optional)</li> <li><code>model_revision</code>: the name of the revision to use for the model selected (optional)</li> </ul> <p>For instance:</p> <pre><code>from dottxt.client import Dottxt\nimport outlines\n\n# Create client\nclient = Dottxt(api_key=\"...\")\n\n# Create the model\nmodel = outlines.from_dottxt(\n    client,\n    \"meta-llama/Llama-3.1-8B\",\n    \"d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n)\n</code></pre> <p>Use the <code>list_models</code> method of the Dottxt client to get a list of available model names and revisions for your account.</p>"},{"location":"features/models/dottxt/#text-generation","title":"Text Generation","text":"<p>Dottxt only supports constrained generation with JSON schema output types. You must always provide a value for the <code>output_type</code> parameter as unconstrained generation is not available.</p> <p>For instance:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom dottxt.client import Dottxt\nimport outlines\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_dottxt(\n    Dottxt(),\n    \"meta-llama/Llama-3.1-8B\",\n    \"d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n)\n\n# Generate structured text\nresult = model(\"Create a character\", Character)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/dottxt/#inference-arguments","title":"Inference arguments","text":"<p>You can provide the same optional parameters you would pass to the <code>dottxt</code> sdk's client both during the initialization of the <code>Dottxt</code> class and when generating text. Some of the common inference arguments include <code>max_tokens</code>, <code>frequency_penalty</code>, <code>presence_penalty</code> and <code>temperature</code>.</p> <p>Consult the dottxt python sdk GitHub repository for the full list of parameters.</p>"},{"location":"features/models/gemini/","title":"Gemini","text":"<p>Installation</p> <p>You need to install the <code>google.genai</code> libray to be able to use the Gemini API in Outlines. Install all optional dependencies of the <code>Gemini</code> model with: <code>pip install \"outlines[gemini]\"</code>.</p> <p>You also need to have a Gemini API key. This API key must either be set as an environment variable called <code>GEMINI_API_KEY</code> or be provided to the <code>google.genai.Client</code> class when instantiating it.</p>"},{"location":"features/models/gemini/#model-initialization","title":"Model Initialization","text":"<p>To create a Gemini model instance, you can use the <code>from_gemini</code> function. It takes 2 arguments:</p> <ul> <li><code>client</code>: a <code>google.genai.Client</code> instance</li> <li><code>model_name</code>: the name of the model you want to use in subsequent model calls (optional)</li> </ul> <p>For instance:</p> <pre><code>import outlines\nfrom google import genai\n\n# Create the client\nclient = genai.Client()\n\n# Create the model\nmodel = outlines.from_gemini(\n    client,\n    \"gemini-1.5-flash-latest\"\n)\n</code></pre> <p>Check the Gemini documentation for an up-to-date list of available models.</p>"},{"location":"features/models/gemini/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>Gemini</code> model instance, you're all set to generate text with this provider. You can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nfrom google.genai import Client\n\n# Create the model\nmodel = outlines.from_gemini(\n    Client(),\n    \"gemini-1.5-flash-latest\"\n)\n\n# Call it to generate text\nresult = model(\"What's the capital of Latvia?\", max_output_tokens=20)\nprint(result) # 'Riga'\n</code></pre>"},{"location":"features/models/gemini/#vision","title":"Vision","text":"<p>Some Gemini models support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nfrom google.genai import Client\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_gemini(\n    Client(),\n    \"gemini-1.5-flash-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_output_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/gemini/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>Gemini</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nfrom google.genai import Client\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_gemini(\n    Client(),\n    \"gemini-1.5-flash-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"user\", \"content\": \"You are a helpful assistant that helps me described pictures.\"},\n    {\"role\": \"assistant\", \"content\": \"I'd be happy to help you describe pictures! Please go ahead and share an image\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_output_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/gemini/#streaming","title":"Streaming","text":"<p>Finally, the <code>Gemini</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import outlines\nfrom google.genai import Client\n\n# Create the model\nmodel = outlines.from_gemini(\n    Client(),\n    \"gemini-1.5-flash-latest\"\n)\n\n# Stream text\nfor chunk in model.stream(\"Write a short story about a cat.\", max_output_tokens=20):\n    print(chunk) # 'In...'\n</code></pre>"},{"location":"features/models/gemini/#structured-generation","title":"Structured Generation","text":"<p>Gemini provides supports for some forms of structured output: multiple choice, JSON schema (with caveats) and lists of structured objects. To use it, call the model with an <code>output_type</code> on top of your prompt.</p>"},{"location":"features/models/gemini/#multiple-choice","title":"Multiple Choice","text":"<pre><code>import outlines\nfrom google import genai\nfrom enum import Enum\n\nclass PizzaOrBurger(Enum):\n    pizza = \"pizza\"\n    burger = \"burger\"\n\n# Create the model\nmodel = outlines.from_gemini(genai.Client(), \"gemini-1.5-flash-latest\")\n\n# Call it with the ouput type to generate structured text\nresult = model(\"Pizza or burger?\", PizzaOrBurger, max_output_tokens=20)\nprint(result) # 'pizza'\n</code></pre>"},{"location":"features/models/gemini/#json-schema","title":"JSON Schema","text":"<p>Gemini supports only three types of objects used to define a JSON Schema:</p> <ul> <li>Pydantic classes</li> <li>Dataclasses</li> <li>TypedDicts</li> </ul> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom google import genai\nimport outlines\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_gemini(genai.Client(), \"gemini-1.5-flash-latest\")\n\n# Call it with the ouput type to generate structured text\nresult = model(\"Create a character\", Character)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/gemini/#lists-of-structured-objects","title":"Lists of Structured Objects","text":"<p>A specificity of Gemini is that, despite not supporting regex, it does support a list of structured objects as an output type. To use it, put any of three available types described above in the typing <code>List</code> class</p> <pre><code>from dataclasses import dataclass\nfrom google import genai\nimport outlines\n\n@dataclass\nclass Character:\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_gemini(genai.Client(), \"gemini-1.5-flash-latest\")\n\n# Call it with the ouput type to generate structured text\nresult = model(\"Create a character\", list[Character])\nprint(result) # '[{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}, {[\"name\":...'\n</code></pre> <p>Attention</p> <p>The structured objects must be in a built-in <code>list</code>, not a <code>List</code> from the <code>typing</code> library</p>"},{"location":"features/models/gemini/#inference-arguments","title":"Inference arguments","text":"<p>You can provide the same optional parameters you would pass to the <code>google.genai.Client</code> client both during the initialization of the Gemini model and when generating text. Some of the common inference arguments include <code>max_output_tokens</code>, <code>temperature</code>, and other generation parameters.</p> <p>Consult the Google Generative AI documentation for the full list of parameters.</p>"},{"location":"features/models/llamacpp/","title":"llama.cpp","text":"<p>Outlines provides an integration with Llama.cpp using the llama-cpp-python library. Llamacpp allows to run quantized models on machines with limited compute.</p> <p>Installation</p> <p>You need to install the <code>llama-cpp-python</code> library to use the llama.cpp integration. Install all optional dependencies of the <code>LlamaCpp</code> model with: <code>\"pip install \"outlines[llamacpp]\"</code>.</p> <p>See the llama-cpp-python Github page for instructions on installing with CUDA, Metal, ROCm and other backends.</p>"},{"location":"features/models/llamacpp/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_llamacpp</code> function. The single argument of the function is a <code>Llama</code> model instance from the <code>llama_cpp</code> library. Consult the Llama class API reference for detailed information on how to create a model instance and on the various available parameters.</p> <p>For instance:</p> <pre><code>import outlines\nfrom llama_cpp import Llama\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n</code></pre>"},{"location":"features/models/llamacpp/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nfrom llama_cpp import Llama\n\n# Create the model\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\n# Call it to generate text\nresult = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(result) # 'Riga'\n</code></pre>"},{"location":"features/models/llamacpp/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>LlamaCpp</code> model. To do so, call the model with a <code>Chat</code> instance.</p> <p>For instance:</p> <pre><code>import outlines\nfrom llama_cpp import Llama\nfrom outlines.inputs import Chat\n\n# Create the model\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\n# Create the prompt containing the text and the image\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"assistant\", \"content\": \"What's the capital of Latvia?\"},\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'Riga.'\n</code></pre>"},{"location":"features/models/llamacpp/#streaming","title":"Streaming","text":"<p>The <code>LlamaCpp</code> model also supports streaming.</p> <p>For instance:</p> <pre><code>import outlines\nfrom llama_cpp import Llama\n\n# Create the model\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\n# Stream text\nfor chunk in model.stream(\"Write a short story about a cat.\", max_tokens=100):\n    print(chunk) # 'In...'\n</code></pre>"},{"location":"features/models/llamacpp/#structured-generation","title":"Structured Generation","text":"<p>The <code>LlamaCpp</code> model supports all output types available in Outlines. Simply provide an <code>output_type</code> after the prompt when calling the model.</p>"},{"location":"features/models/llamacpp/#basic-type","title":"Basic Type","text":"<pre><code>import outlines\nfrom llama_cpp import Llama\n\noutput_type = int\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/llamacpp/#json-schema","title":"JSON Schema","text":"<pre><code>from typing import List\nfrom pydantic import BaseModel\nimport outlines\nfrom llama_cpp import Llama\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\nresult = model(\"Create a character.\", output_type=Character, max_tokens=200)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/llamacpp/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport outlines\nfrom llama_cpp import Llama\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\nresult = model(\"What is the capital of France?\", output_type)\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/llamacpp/#regex","title":"Regex","text":"<pre><code>from outlines.types import Regex\nimport outlines\nfrom llama_cpp import Llama\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\nresult = model(\"Generate a fake social security number.\", output_type)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/llamacpp/#context-free-grammar","title":"Context-free grammar","text":"<pre><code>from outlines.types import CFG\nimport outlines\nfrom llama_cpp import Llama\n\noutput_type = CFG(\"\"\"\nroot ::= answer\nanswer ::= \"yes\" | \"no\"\n\"\"\")\n\nmodel = outlines.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        filename=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n    )\n)\n\nresult = model(\"Are you feeling good today?\", output_type)\nprint(result) # 'yes'\n</code></pre>"},{"location":"features/models/llamacpp/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional inference parameters on top of the prompt and the output type. These parameters will be passed on to the <code>__call__</code> method of the <code>llama_cpp.Llama</code> model. Some common inference arguments include <code>max_tokens</code>, <code>temperature</code>, <code>frequency_penalty</code> and <code>top_p</code>.</p> <p>See the llama-cpp-python documentation for more information on inference parameters.</p>"},{"location":"features/models/mistral/","title":"Mistral","text":"<p>Installation</p> <p>You need to install the <code>mistralai</code> library to be able to use the Mistral API in Outlines. Install all optional dependencies of the <code>Mistral</code> model with: <code>pip install \"outlines[mistral]\"</code>.</p> <p>You also need to have an Mistral API key. This API key must either be set as an environment variable called <code>MISTRAL_API_KEY</code> or be provided to the <code>mistralai.Mistral</code> class when instantiating it.</p>"},{"location":"features/models/mistral/#model-initialization","title":"Model Initialization","text":"<p>To create an <code>Mistral</code> or <code>AsyncMistral</code> model instance, you can use the <code>from_mistral</code> function. It takes 3 arguments:</p> <ul> <li><code>client</code>: a <code>mistralai.Mistral</code> instance</li> <li><code>model_name</code> (optional): the name of the model you want to use</li> <li><code>async_client</code> (optional): whether it should create a sync or an async model</li> </ul> <p>As the <code>mistralai</code> library uses a single class to handle both sync and async requests, you must set the <code>async_client</code> argument to True to get an <code>AsyncMistral</code> model.</p> <p>For instance:</p> <pre><code>import mistralai\nimport outlines\n\n# Create the Mistral client\nclient = mistral.Mistral()\n\n# Create a sync model\nmodel = outlines.from_mistral(\n    client,\n    \"mistral-large-latest\"\n)\n\n# Create aa async model\nmodel = outlines.from_mistral(\n    client,\n    \"mistral-large-latest\",\n    True\n)\n</code></pre> <p>The mistralai python SDK provides methods to query the API for a list of all available models, including paid endpoints for premium models in addition to open weights.</p>"},{"location":"features/models/mistral/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>Mistral</code> model instance, you're all set to generate text with this provider. You can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import mistralai\nimport outlines\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/mistral/#vision","title":"Vision","text":"<p>Some Mistral models support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nimport mistralai\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/mistral/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>Mistral</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport mistralai\nimport outlines\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/mistral/#streaming","title":"Streaming","text":"<p>Finally, the <code>Mistral</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import mistralai\nimport outlines\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n    print(chunk) # 'Once...'\n</code></pre>"},{"location":"features/models/mistral/#structured-generation","title":"Structured Generation","text":"<p>Mistral provides supports for some forms of structured output: JSON schemas and JSON syntax. To use it, call the model with an <code>output_type</code> on top of your prompt.</p>"},{"location":"features/models/mistral/#json-schema","title":"JSON Schema","text":"<pre><code>from typing import List\nfrom pydantic import BaseModel\nimport mistralai\nimport outlines\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n# Call it with the output type to generate structured text\nresult = model(\"Create a character, use the json format.\", Character, top_p=0.1)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/mistral/#json-syntax","title":"JSON Syntax","text":"<p>What we mean by JSON syntax is what is sometimes called JSON mode, meaning that the model will return a valid JSON, but you do not get to specify its structure. To use this JSON mode, provide the <code>dict</code> type as an output type.</p> <pre><code>import mistralai\nimport outlines\n\n## Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\"\n)\n\n\n# Call it with the output type to generate structured text\nresult = model(\"Create a character, use the json format.\", dict, temperature=0.5)\nprint(result) # '{\"first_name\": \"Henri\", \"last_name\": \"Smith\", \"height\": \"170\"}'\n</code></pre>"},{"location":"features/models/mistral/#asynchronous-calls","title":"Asynchronous Calls","text":"<p>All features presented above for the sync model are also available for the async model.</p> <p>For instance:</p> <pre><code>import asyncio\nimport mistralai\nimport outlines\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_mistral(\n    mistralai.Mistral(),\n    \"mistral-large-latest\",\n    True\n)\n\nasync def text_generation():\n    # Regular generation\n    response = await model(\"What's the capital of Latvia?\", max_tokens=20)\n    print(response) # 'Riga'\n\n    # Streaming\n    async for chunk in  model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n        print(chunk, end=\"\") # 'Once...'\n\n    # Structured generation\n    result = await model(\"Create a character, use the json format.\", Character, top_p=0.1)\n    print(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\n    print(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n\nasyncio.run(text_generation())\n</code></pre>"},{"location":"features/models/mistral/#inference-arguments","title":"Inference arguments","text":"<p>When calling the model, you can provide keyword arguments that will be passed down to the <code>chat.complete</code> method of the Mistral client and its async and streaming equivalents. Some of the most common arguments include <code>max_tokens</code>, <code>temperature</code>, <code>stop</code> and <code>top_p</code>.</p> <p>Another keyword argument of interest is <code>n</code>. If set with an integer value superior to 1, Mistral will generate several sample responses and you will receive a list of strings as a response to your model call.</p> <p>See the Mistral API documentation for the full list of available arguments.</p>"},{"location":"features/models/mistral/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>ImportError: No module named 'mistralai'   \u2192 Run <code>pip install mistralai</code>.</p> </li> <li> <p>Authentication Error   \u2192 Verify <code>MISTRAL_API_KEY</code> is set and valid. Test with the Mistral Playground.</p> </li> <li> <p>Schema Error (e.g., \"Mistral does not support your schema\")   \u2192 Ensure no <code>pattern</code> fields in Pydantic (Outlines sets <code>additionalProperties: false</code>); try a simpler schema or a different Outlines model (local models in particular).</p> </li> <li> <p>Model Not Found Error   \u2192 Confirm the model name (e.g., <code>\"mistral-small-latest\"</code>) and your subscription tier. Check docs.</p> </li> <li> <p>Rate Limits or Quotas   \u2192 Monitor usage in the Mistral console; upgrade your plan for higher limits.</p> </li> <li> <p>Input Validation Errors   \u2192 Ensure Chat messages use valid roles (<code>system</code>, <code>user</code>, <code>assistant</code>); list inputs start with strings.</p> </li> </ul> <p>Last updated: October 2, 2025</p>"},{"location":"features/models/mlxlm/","title":"mlx-lm","text":"<p>Outlines provides an integration with mlx-lm, allowing models to be run quickly on Apple Silicon via the mlx library.</p> <p>Installation</p> <p>You need a device that supports Metal to use the mlx-lm integration.</p> <p>You need to install the <code>mlx</code> and <code>mlx-lm</code> libraries to be able to use mlx in Outlines. Install all optional dependencies of the <code>MLXLM</code> model with: <code>pip install \"outlines[mlxlm]\"</code>.</p>"},{"location":"features/models/mlxlm/#model-initialization","title":"Model Initialization","text":"<p>To create a MLXLM model instance, you can use the <code>from_mlxlm</code> function. It takes 2 arguments:</p> <ul> <li><code>model</code>: an <code>mlx.nn.Module</code> instance</li> <li><code>tokenizer</code>: a <code>transformers.PreTrainedTokenizer</code> instance</li> </ul> <p>However, we recommend you simply pass on the output of the <code>mlx_lm.load</code> function (it takes a model name as an argument).</p> <p>For instance:</p> <pre><code>import outlines\nimport mlx_lm\n\n# Create the model\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n</code></pre>"},{"location":"features/models/mlxlm/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nimport mlx_lm\n\n# Load the model\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\n# Call it to generate text\nresult = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(result) # 'Riga'\n</code></pre>"},{"location":"features/models/mlxlm/#chat","title":"Chat","text":"<p>You can use chat inputs with the <code>MLXLM</code> model. To do so, call the model with a <code>Chat</code> instance.</p> <p>For instance:</p> <pre><code>import outlines\nimport mlx_lm\nfrom outlines.inputs import Chat\n\n# Load the model\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\n# Create the prompt containing the text and the image\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"assistant\", \"content\": \"What's the capital of Latvia?\"},\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'Riga.'\n</code></pre>"},{"location":"features/models/mlxlm/#streaming","title":"Streaming","text":"<p>The <code>MLXLM</code> model also supports streaming. For instance:</p> <pre><code>import outlines\nimport mlx_lm\n\n# Load the model\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\n# Stream text\nfor chunk in model.stream(\"Write a short story about a cat.\", max_tokens=100):\n    print(chunk) # 'In...'\n</code></pre>"},{"location":"features/models/mlxlm/#batch-generation","title":"Batch Generation","text":"<p>The <code>MLXLM</code> model supports generating text in batches. To do so, use the <code>batch</code> method and provide a list of strings as a model input. However, constrained generation is not supported with batching, so you cannot provide an <code>output_type</code>. For instance:</p> <pre><code>import outlines\nimport mlx_lm\n\n# Load the model\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\n# Generate text in batches\nresult = model.batch([\"What's the capital of Lithuania?\", \"What's the capital of Latvia?\"], max_tokens=20)\nprint(result) # ['Vilnius', 'Riga']\n</code></pre>"},{"location":"features/models/mlxlm/#structured-generation","title":"Structured Generation","text":"<p>As a local model, <code>MLXLM</code> supports all forms of structured generation available in Outlines.</p>"},{"location":"features/models/mlxlm/#basic-type","title":"Basic Type","text":"<pre><code>import outlines\nimport mlx_lm\n\noutput_type = int\n\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/mlxlm/#json-schema","title":"JSON Schema","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List\nimport outlines\nimport mlx_lm\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\nresult = model(\"Create a character.\", output_type=Character)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/mlxlm/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport outlines\nimport mlx_lm\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\nresult = model(\"What is the capital of France?\", output_type)\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/mlxlm/#regex","title":"Regex","text":"<pre><code>from outlines.types import Regex\nimport outlines\nimport mlx_lm\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\nresult = model(\"Generate a fake social security number.\", output_type)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/mlxlm/#context-free-grammar","title":"Context-Free Grammar","text":"<pre><code>from outlines.types import CFG\nimport outlines\nimport mlx_lm\n\narithmetic_grammar = \"\"\"\n?start: sum\n\n?sum: product\n| sum \"+\" product   -&gt; add\n| sum \"-\" product   -&gt; sub\n\n?product: atom\n| product \"*\" atom  -&gt; mul\n| product \"/\" atom  -&gt; div\n\n?atom: NUMBER           -&gt; number\n| \"-\" atom         -&gt; neg\n| \"(\" sum \")\"\n\n%import common.NUMBER\n%import common.WS_INLINE\n\n%ignore WS_INLINE\n\"\"\"\noutput_type = CFG(arithmetic_grammar)\n\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit\")\n)\n\nresult = model(\"Write an addition.\", output_type, max_tokens=20)\nprint(result) # '23 + 48'\n</code></pre>"},{"location":"features/models/mlxlm/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional inference parameters on top of the prompt and the output type. These parameters will be passed on to the <code>mlx_lm.generate</code> function used to generate text.</p> <p>See the MLXLM documentation for more information on inference parameters.</p>"},{"location":"features/models/ollama/","title":"Ollama","text":"<p>Installation</p> <p>To be able to use Ollama in Outlines, you must install both Ollama and the optional dependency libraries of the model.</p> <ul> <li>To download Ollama: https://ollama.com/download</li> <li>To install the ollama python sdk: <code>pip install \"outlines[ollama]\"</code></li> </ul> <p>Consult the <code>ollama</code> documentation for detailed information on installation and client initialization.</p>"},{"location":"features/models/ollama/#model-initialization","title":"Model Initialization","text":"<p>To create an Ollama model instance, you can use the <code>from_ollama</code> function. It takes 2 arguments:</p> <ul> <li><code>client</code>: an <code>ollama.Client</code> or <code>ollama.AsyncClient</code> instance</li> <li><code>model_name</code>: the name of the model you want to use</li> </ul> <p>Based on whether the inference client instance is synchronous or asynchronous, you will receive an <code>Ollama</code> or an <code>AsyncOllama</code> model instance.</p> <p>For instance:</p> <pre><code>import ollama\nimport outlines\n\n# Create the client or async client\nclient = ollama.Client()\nasync_client = ollama.AsyncClient()\n\n# Create a sync model\nmodel = outlines.from_ollama(\n    client,\n    \"qwen2.5vl:3b\",\n)\n\n# Create an async model\nmodel = outlines.from_ollama(\n    async_client,\n    \"qwen2.5vl:3b\",\n)\n</code></pre> <p>You can find the list of available models on the Ollama library.</p>"},{"location":"features/models/ollama/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>Ollama</code> model instance, you're all set to generate text with this provider. You can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import ollama\nimport outlines\n\n# Create the model\nmodel = outlines.from_ollama(ollama.Client(), \"qwen2.5vl:3b\")\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\")\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/ollama/#vision","title":"Vision","text":"<p>Some Ollama models support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <pre><code>import io\nimport requests\nimport PIL\nimport ollama\nimport outlines\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_ollama(\n    ollama.Client(),\n    \"qwen2.5vl:3b\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Generate text\nresponse = model(prompt)\nprint(response) # The image shows a black puppy with a curious and attentive expression.\n</code></pre>"},{"location":"features/models/ollama/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>Ollama</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport ollama\nimport outlines\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_ollama(\n    ollama.Client(),\n    \"qwen2.5vl:3b\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/ollama/#streaming","title":"Streaming","text":"<p>Finally, the <code>Anthropic</code> model supports streaming through the <code>stream</code> method.</p> <pre><code>import ollama\nimport outlines\n\n# Create the model\nmodel = outlines.from_ollama(ollama.Client(), \"qwen2.5vl:3b\")\n\n# Stream text\nfor chunk in model.stream(\"Write a short story about a cat\"):\n    print(chunk) # 'In...'\n</code></pre>"},{"location":"features/models/ollama/#asynchronous-calls","title":"Asynchronous Calls","text":"<p>Ollama supports asynchronous operations by passing an <code>AsyncClient</code> instead of a regular <code>Client</code>. This returns an <code>AsyncOllama</code> model instance that supports async/await patterns.</p>"},{"location":"features/models/ollama/#basic-async-generation","title":"Basic Async Generation","text":"<pre><code>import asyncio\nimport outlines\nimport ollama\n\nasync def generate_text():\n    # Create an async model\n    async_client = ollama.AsyncClient()\n    async_model = outlines.from_ollama(async_client, \"qwen2.5vl:3b\")\n\n    result = await async_model(\"Write a haiku about Python.\")\n    print(result)\n\nasyncio.run(generate_text())\n</code></pre>"},{"location":"features/models/ollama/#async-streaming","title":"Async Streaming","text":"<p>The async model also supports streaming with async iteration:</p> <pre><code>import asyncio\nimport outlines\nimport ollama\n\nasync def stream_text():\n    async_client = ollama.AsyncClient()\n    async_model = outlines.from_ollama(async_client, \"qwen2.5vl:3b\")\n\n    async for chunk in async_model.stream(\"Tell me a story about a robot.\"):\n        print(chunk, end=\"\")\n\nasyncio.run(stream_text())\n</code></pre>"},{"location":"features/models/ollama/#concurrent-async-requests","title":"Concurrent Async Requests","text":"<p>One of the main benefits of async calls is the ability to make multiple concurrent requests:</p> <pre><code>import asyncio\nimport outlines\nimport ollama\n\nasync def generate_multiple():\n    async_client = ollama.AsyncClient()\n    async_model = outlines.from_ollama(async_client, \"qwen2.5vl:3b\")\n\n    # Define multiple prompts\n    prompts = [\n        \"Write a tagline for a coffee shop.\",\n        \"Write a tagline for a bookstore.\",\n        \"Write a tagline for a gym.\"\n    ]\n\n    tasks = [async_model(prompt) for prompt in prompts]\n    results = await asyncio.gather(*tasks)\n\n    for prompt, result in zip(prompts, results):\n        print(f\"{prompt}\\n{result}\\n\")\n\nasyncio.run(generate_multiple())\n</code></pre>"},{"location":"features/models/ollama/#structured-generation","title":"Structured Generation","text":"<p>Ollama only provides support for structured generation based on a JSON schema. To use it, call the model with a JSON schema object as an <code>output_type</code> on top of your prompt.</p> <p>For instance:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nimport ollama\nimport outlines\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_ollama(ollama.Client(), \"tinyllama\")\n\n# Call it with the output type to generate structured text\nresult = model(\"Create a character\", Character)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/ollama/#inference-arguments","title":"Inference arguments","text":"<p>When calling the model, you can provide keyword arguments that will be passed down to the <code>generate</code> method of the Ollama client.</p> <p>Consult the Ollama REST API documentation for the full list of inference parameters.</p>"},{"location":"features/models/openai/","title":"OpenAI","text":"<p>Installation</p> <p>You need to install the <code>openai</code> library to be able to use the OpenAI API in Outlines. Install all optional dependencies of the <code>OpenAI</code> model with: <code>pip install \"outlines[openai]\"</code>.</p> <p>You also need to have an OpenAI API key. This API key must either be set as an environment variable called <code>OPENAI_API_KEY</code> or be provided to the <code>openai.OpenAI</code> class when instantiating it.</p>"},{"location":"features/models/openai/#model-initialization","title":"Model Initialization","text":"<p>To create an OpenAI model instance, you can use the <code>from_openai</code> function. It takes 2 arguments:</p> <ul> <li><code>client</code>: an <code>openai.OpenAI</code>, <code>openai.AzureOpenAI</code>, <code>openai.AsyncOpenAI</code> or <code>openai.AsyncAzureOpenAI</code> instance</li> <li><code>model_name</code>: the name of the model you want to use</li> </ul> <p>Based on whether the inference client instance is synchronous or asynchronous, you will receive an <code>OpenAI</code> or an <code>AsyncOpenAI</code> model instance.</p> <p>For instance:</p> <pre><code>import outlines\nimport openai\n\n# Create the client or async client\nclient = openai.OpenAI()\nasync_client = openai.AsyncOpenAI()\n\n# Create a sync model\nmodel = outlines.from_openai(\n    client,\n    \"gpt-4o\"\n)\n\n# Create aa async model\nmodel = outlines.from_openai(\n    async_client,\n    \"gpt-4o\"\n)\n</code></pre> <p>Check the OpenAI documentation for an up-to-date list of available models. As shown above, you can use Azure OpenAI in Outlines the same way you would use OpenAI, just provide an <code>openai.AzureOpenAI</code> instance to the Outlines model class.</p>"},{"location":"features/models/openai/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>OpenAI</code> model instance, you're all set to generate text with this provider. You can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/openai/#vision","title":"Vision","text":"<p>Some OpenAI models support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nimport openai\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/openai/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>OpenAI</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport openai\nimport outlines\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/openai/#streaming","title":"Streaming","text":"<p>Finally, the <code>OpenAI</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.OpenAI(),\n    \"gpt-4o\"\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n    print(chunk) # 'Once...'\n</code></pre>"},{"location":"features/models/openai/#structured-generation","title":"Structured Generation","text":"<p>OpenAI provides supports for some forms of structured output: JSON schemas and JSON syntax. To use it, call the model with an <code>output_type</code> on top of your prompt.</p>"},{"location":"features/models/openai/#json-schema","title":"JSON Schema","text":"<pre><code>from typing import List\nfrom pydantic import BaseModel\nimport openai\nimport outlines\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(), \"gpt-4o\")\n\n# Call it with the output type to generate structured text\nresult = model(\"Create a character, use the json format.\", Character, top_p=0.1)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/openai/#json-syntax","title":"JSON Syntax","text":"<p>What we mean by JSON syntax is what is sometimes called JSON mode, meaning that the model will return a valid JSON, but you do not get to specify its structure. To use this JSON mode, provide the <code>dict</code> type as an output type.</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(), \"gpt-4o\")\n\n# Call it with the output type to generate structured text\nresult = model(\"Create a character, use the json format.\", dict, temperature=0.5)\nprint(result) # '{\"first_name\": \"Henri\", \"last_name\": \"Smith\", \"height\": \"170\"}'\n</code></pre>"},{"location":"features/models/openai/#asynchronous-calls","title":"Asynchronous Calls","text":"<p>All features presented above for the sync model are also available for the async model.</p> <p>For instance:</p> <pre><code>import asyncio\nimport openai\nimport outlines\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\n# Create the model\nmodel = outlines.from_openai(\n    openai.AsyncOpenAI(),\n    \"gpt-4o\"\n)\n\nasync def text_generation():\n    # Regular generation\n    response = await model(\"What's the capital of Latvia?\", max_tokens=20)\n    print(response) # 'Riga'\n\n    # Streaming\n    async for chunk in  model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n        print(chunk, end=\"\") # 'Once...'\n\n    # Structured generation\n    result = await model(\"Create a character, use the json format.\", Character, top_p=0.1)\n    print(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\n    print(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n\nasyncio.run(text_generation())\n</code></pre>"},{"location":"features/models/openai/#inference-arguments","title":"Inference arguments","text":"<p>When calling the model, you can provide keyword arguments that will be passed down to the <code>chat.completions.create</code> method of the OpenAI client. Some of the most common arguments include <code>max_tokens</code>, <code>temperature</code>, <code>stop</code> and <code>top_p</code>.</p> <p>Another keyword argument of interest is <code>n</code>. If set with an integer value superior to 1, OpenAI will generate several sample responses and you will receive a list of strings as a response to your model call.</p> <p>See the OpenAI API documentation for the full list of available arguments.</p>"},{"location":"features/models/openai_compatible/","title":"OpenAI-Compatible APIs","text":"<p>Many inference providers offer OpenAI-compatible APIs, allowing you to use the familiar OpenAI SDK while connecting to different backends. Outlines allows you can leverage various providers while maintaining consistent code.</p>"},{"location":"features/models/openai_compatible/#what-are-openai-compatible-apis","title":"What are OpenAI-Compatible APIs?","text":"<p>OpenAI-compatible APIs implement the same REST endpoints and request/response formats as OpenAI's API, but serve different models or run on different infrastructure. This allows you to use the <code>openai</code> Python library with any compatible provider by simply changing the <code>base_url</code>.</p> <p>Installation</p> <p>You need to install the <code>openai</code> library to be able to use the OpenAI-compatible APIs in Outlines. Install all optional dependencies of the <code>OpenAI</code> model with: <code>pip install \"outlines[openai]\"</code>.</p>"},{"location":"features/models/openai_compatible/#general-usage-pattern","title":"General Usage Pattern","text":"<p>The standard approach is to use the OpenAI SDK with a custom base URL:</p> <pre><code>import openai\nimport outlines\n\n# Point to your OpenAI-compatible endpoint\nclient = openai.OpenAI(\n    base_url=\"https://your-provider.com/v1\",  # Custom endpoint\n    api_key=\"your-api-key\"\n)\n\n# Use with Outlines\nmodel = outlines.from_openai(client, \"model-name\")\n</code></pre>"},{"location":"features/models/openai_compatible/#important-provider-specific-parameters","title":"Important: Provider-Specific Parameters","text":"<p>API-Specific Parameters</p> <p>Some providers require additional parameters in the API request for structured generation to work properly. These are typically passed as extra arguments when calling the model.</p> <p>For example, some providers may need special parameters in the request body to enable guided generation or specify constraints. Always consult your provider's documentation for structured generation requirements.</p>"},{"location":"features/models/openai_compatible/#popular-openai-compatible-providers","title":"Popular OpenAI-Compatible Providers","text":"<p>Many providers offer OpenAI-compatible endpoints:</p> <ul> <li>Groq</li> <li>Together AI</li> <li>Anyscale</li> <li>Fireworks AI</li> <li>Perplexity</li> <li>Local servers (LocalAI, etc.)</li> </ul>"},{"location":"features/models/openai_compatible/#configuration-examples","title":"Configuration Examples","text":""},{"location":"features/models/openai_compatible/#basic-setup","title":"Basic Setup","text":"<pre><code>import openai\nimport outlines\n\n# Generic OpenAI-compatible setup\nclient = openai.OpenAI(\n    base_url=\"https://api.your-provider.com/v1\",\n    api_key=\"your-api-key\"\n)\n\nmodel = outlines.from_openai(client, \"provider-model-name\")\n</code></pre>"},{"location":"features/models/openai_compatible/#with-authentication-headers","title":"With Authentication Headers","text":"<pre><code>import openai\nimport outlines\n\n# Some providers need custom headers\nclient = openai.OpenAI(\n    base_url=\"https://api.your-provider.com/v1\",\n    api_key=\"your-api-key\",\n    default_headers={\"Custom-Header\": \"value\"}\n)\n\nmodel = outlines.from_openai(client, \"provider-model-name\")\n</code></pre>"},{"location":"features/models/openai_compatible/#related-documentation","title":"Related Documentation","text":"<p>For specific implementations that use OpenAI-compatible APIs:</p> <ul> <li>SGLang: Local inference server with OpenAI-compatible endpoints</li> <li>vLLM: High-performance inference with OpenAI-compatible API</li> <li>OpenAI: The original OpenAI API implementation</li> </ul>"},{"location":"features/models/openrouter/","title":"Openrouter","text":"<p>Installation</p> <p>OpenRouter uses the same API as OpenAI, so both services are interoperable using the <code>openai</code> library. Install all optional dependencies of the <code>OpenAI</code> model with: <code>pip install \"outlines[openai]\"</code>.</p> <p>You also need to have an Openrouter API key. This API key must either be set as an environment variable called <code>OPENAI_API_KEY</code> or be provided to the <code>openai.OpenAI</code> class when instantiating it.</p>"},{"location":"features/models/openrouter/#model-initialization","title":"Model Initialization","text":"<p>To create a model instance, you can use the <code>from_openai</code> function. It takes 2 arguments:</p> <ul> <li><code>client</code>: an <code>openai.OpenAI</code> instance</li> <li><code>model_name</code>: the name of the model you want to use, defined as <code>provider/model</code></li> </ul> <p>For instance:</p> <pre><code>import outlines\nimport openai\n\n# Create the client\nclient = openai.OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=\"OPENAI_API_KEY\",\n)\n\n# Create the model\nmodel = outlines.from_openai(\n    client,\n    \"x-ai/grok-4\"\n)\n</code></pre> <p>Leaving an empty string in the model name field will lead OpenRouter to use your default model defined in settings.</p> <p>The OpenRouter website lists available models. Keep in mind that some models do not support <code>json_schema</code> response formats and may return a 400 error code as a result.</p>"},{"location":"features/models/openrouter/#related-documentation","title":"Related Documentation","text":"<p>For specific implementations that use OpenAI-compatible APIs:</p> <ul> <li>OpenAI: The original OpenAI API implementation</li> <li>OpenAI compatible API: Details on how to use OpenAI-compatible APIs</li> </ul>"},{"location":"features/models/sglang/","title":"SGLang","text":""},{"location":"features/models/sglang/#prerequisites","title":"Prerequisites","text":"<p>The Outlines <code>SGLang</code> model is intended to be used along with an SGLang instance running on a separate server (can be local or remote). Make sure you have a SGLang server running and accessible before using the <code>SGLang</code> model. For instance by running:</p> <pre><code>pip install \"sglang[all]\"\n\npython -m sglang.launch_server \\\n  --model-path NousResearch/Meta-Llama-3-8B-Instruct \\\n  --host 0.0.0.0 \\\n  --port 30000\n</code></pre> <p>Follow the Installation instructions for more information on how to set up a SGLang server for your particular setup.</p> <p>As the SGLang client relies on the <code>openai</code> python sdk, you need to have the <code>openai</code> package installed. Install all optional dependencies of the <code>SGLang</code> model with: <code>pip install \"outlines[sglang]\"</code>.</p> <p>When launching your SGLang server, you can specify the backend engine to use for structured generation through the <code>grammar-backend</code> cli argument. Add <code>--grammar-backend outlines</code> to your command to use Outlines instead of the default engine.</p>"},{"location":"features/models/sglang/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_sglang</code> function. The argument of the function is either an <code>OpenAI</code> or <code>AsyncOpenAI</code> instance from the <code>openai</code> library. Make sure the value of the <code>base_url</code> argument of the <code>OpenAI</code> client points to your running SGLang server. Consult the SGLang documentation on using an OpenAI client with an SGLang server for more information.</p> <p>Based on whether the <code>openai</code> client instance is synchronous or asynchronous, you will receive a <code>SGLang</code> or <code>AsyncSGLang</code> model instance.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the OpenAI client\nsync_openai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nasync_openai_client = openai.AsyncOpenAI(base_url=\"http://localhost:11434\")\n\n# Create a sync model\nsync_model = outlines.from_sglang(sync_openai_client)\nprint(type(sync_model)) # &lt;class 'outlines.models.sglang.SGLang'&gt;\n\n# Create an async model\nasync_model = outlines.from_sglang(async_openai_client)\nprint(type(async_model)) # &lt;class 'outlines.models.sglang.AsyncSGLang'&gt;\n</code></pre>"},{"location":"features/models/sglang/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(base_url=\"http://localhost:11434\"))\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/sglang/#vision","title":"Vision","text":"<p>Some models you can run with SGLang support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nimport openai\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(base_url=\"http://localhost:11434\"))\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/sglang/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>SGLang</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport openai\nimport outlines\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(base_url=\"http://localhost:11434\"))\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/sglang/#streaming","title":"Streaming","text":"<p>Finally, the <code>SGLang</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_openai(openai.OpenAI(base_url=\"http://localhost:11434\"))\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n    print(chunk) # 'Once...'\n</code></pre>"},{"location":"features/models/sglang/#structured-generation","title":"Structured Generation","text":"<p>SGLang supports all output types available in Outlines (context-free grammars with caveats though, see the subsection below for more details). Simply provide an <code>output_type</code> after the prompt when calling the model. All structured generation features work with both synchronous and asynchronous models.</p>"},{"location":"features/models/sglang/#simple-type","title":"Simple Type","text":"<pre><code>import openai\nimport outlines\n\noutput_type = int\n\nopenai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nmodel = outlines.from_sglang(openai_client)\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/sglang/#json-schema","title":"JSON Schema","text":"<pre><code>import openai\nimport outlines\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nopenai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nmodel = outlines.from_sglang(openai_client)\n\nresult = model(\"Create a character.\", Character, frequency_penalty=1.5)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/sglang/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport openai\nimport outlines\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nopenai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nmodel = outlines.from_sglang(openai_client)\n\nresult = model(\"What is the capital of France?\", output_type, temperature=0)\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/sglang/#regex","title":"Regex","text":"<pre><code>import openai\nimport outlines\nfrom outlines.types import Regex\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nopenai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nmodel = outlines.from_sglang(openai_client)\n\nresult = model(\"Generate a fake social security number.\", output_type, top_p=0.1)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/sglang/#context-free-grammar","title":"Context-Free Grammar","text":"<p>SGLang supports grammars, but expects an EBNF format instead of the Lark format Outlines uses. Thus, to use a context-free grammar with SGLang, provide a string using the EBNF syntax to the Outlines <code>CFG</code> object.</p> <pre><code>import openai\nimport outlines\nfrom outlines.types import CFG\n\nebnf_grammar = \"\"\"\nroot ::= answer\nanswer ::= \"yes\" | \"no\"\n\"\"\"\noutput_type = CFG(ebnf_grammar)\n\nopenai_client = openai.OpenAI(base_url=\"http://localhost:11434\")\nmodel = outlines.from_sglang(openai_client)\n\nresult = model(\"Is the weather good today?\", output_type)\nprint(result) # 'yes'\n</code></pre>"},{"location":"features/models/sglang/#async-structured-generation","title":"Async Structured Generation","text":"<p>All structured generation features work seamlessly with async models:</p> <pre><code>import asyncio\nimport openai\nimport outlines\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\nasync def generate_user():\n    async_client = openai.AsyncOpenAI(base_url=\"http://localhost:11434\")\n    async_model = outlines.from_sglang(async_client)\n\n    result = await async_model(\"Generate a random user profile.\", output_type=User)\n    user = User.model_validate_json(result)\n    print(f\"Name: {user.name}, Email: {user.email}, Age: {user.age}\")\n\nasyncio.run(generate_user())\n</code></pre>"},{"location":"features/models/sglang/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional parameters on top of the prompt and the output type. Those will be passed on to the <code>chat.completions.create</code> method of the OpenAI client.</p> <p>An optional parameter of particular interest is <code>extra_body</code>, which is a dictionary containing arguments that are specific to SGLang and are not part of the standard <code>openai</code> interface.</p> <p>See the SGLang documentation on parameters for the OpenAI-compatible server for more information on inference parameters.</p>"},{"location":"features/models/tgi/","title":"TGI","text":""},{"location":"features/models/tgi/#prerequisites","title":"Prerequisites","text":"<p>The Outlines <code>TGI</code> model is intended to be used along with a HuggingFace <code>Text Generation Inference</code> server (running locally or remotely). Make sure you have a TGI server running before using the <code>TGI</code> model. For instance running:</p> <pre><code>docker run \\\n  --gpus all \\\n  --shm-size 1g \\\n  -p 8080:80 \\\n  ghcr.io/huggingface/text-generation-inference:3.3.4 \\\n  --model-id NousResearch/Meta-Llama-3-8B-Instruct\n</code></pre> <p>Please consult the installation guide for more information about how to run TGI with your particular setup. As the TGI client relies on the <code>huggingface_hub</code> python package, you need to have it installed. Install all optional dependencoes of the <code>TGI</code> model with: <code>pip install \"outlines[tgi]\"</code></p>"},{"location":"features/models/tgi/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_tgi</code> function. The argument of the function is either an <code>InferenceClient</code> or <code>AsyncInferenceClient</code> instance from the <code>huggingface_hub</code> library. Consult the HuggingFace documentation for more information on their inference client.</p> <p>Based on whether the inference client instance is synchronous or asynchronous, you will receive a <code>TGI</code> or an <code>AsyncTGI</code> model instance.</p> <p>For instance:</p> <pre><code>import outlines\nimport huggingface_hub\n\n# Create the inference client\nclient = huggingface_hub.InferenceClient(\"http://localhost:11434\")\nasync_client = huggingface_hub.AsyncInferenceClient(\"http://localhost:11434\")\n\n# Create a sync model\nsync_model = outlines.from_tgi(client)\nprint(type(sync_model))  # &lt;class 'outlines.models.tgi.TGI'&gt;\n\n# Create an async model\nasync_model = outlines.from_tgi(async_client)\nprint(type(async_model))  # &lt;class 'outlines.models.tgi.AsyncTGI'&gt;\n</code></pre>"},{"location":"features/models/tgi/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nimport huggingface_hub\n\n# Create the model\nclient = huggingface_hub.InferenceClient(\"http://localhost:11434\")\nmodel = outlines.from_tgi(client)\n\n# Call it to generate text\nresult = model(\"Write a short story about a cat.\", stop_sequences=[\".\"])\nprint(result) # 'In a quiet village where the cobblestones hummed softly beneath the morning mist...'\n</code></pre> <p>The <code>TGI</code> model supports streaming. For instance:</p> <pre><code>import outlines\nimport huggingface_hub\n\n# Create the model\nclient = huggingface_hub.InferenceClient(\"http://localhost:11434\")\nmodel = outlines.from_tgi(client)\n\n# Stream text\nfor chunk in model.stream(\"Write a short story about a cat.\", stop_sequences=[\".\"]):\n    print(chunk) # 'In ...'\n</code></pre>"},{"location":"features/models/tgi/#asynchronous-calls","title":"Asynchronous Calls","text":"<p>TGI supports asynchronous operations by passing an <code>AsyncInferenceClient</code> instead of a regular <code>InferenceClient</code>. This returns an <code>AsyncTGI</code> model instance that supports async/await patterns.</p>"},{"location":"features/models/tgi/#basic-async-generation","title":"Basic Async Generation","text":"<pre><code>import asyncio\nimport outlines\nimport huggingface_hub\n\nasync def generate_text():\n    # Create an async model\n    async_client = huggingface_hub.AsyncInferenceClient(\"http://localhost:11434\")\n    async_model = outlines.from_tgi(async_client)\n\n    result = await async_model(\"Write a haiku about Python.\", max_new_tokens=50)\n    print(result)\n\nasyncio.run(generate_text())\n</code></pre>"},{"location":"features/models/tgi/#async-streaming","title":"Async Streaming","text":"<p>The async model also supports streaming with async iteration:</p> <pre><code>import asyncio\nimport outlines\nimport huggingface_hub\n\nasync def stream_text():\n    async_client = huggingface_hub.AsyncInferenceClient(\"http://localhost:11434\")\n    async_model = outlines.from_tgi(async_client)\n\n    async for chunk in async_model.stream(\"Tell me a story about a robot.\", max_new_tokens=100):\n        print(chunk, end=\"\")\n\nasyncio.run(stream_text())\n</code></pre>"},{"location":"features/models/tgi/#concurrent-async-requests","title":"Concurrent Async Requests","text":"<p>One of the main benefits of async calls is the ability to make multiple concurrent requests:</p> <pre><code>import asyncio\nimport outlines\nimport huggingface_hub\n\nasync def generate_multiple():\n    async_client = huggingface_hub.AsyncInferenceClient(\"http://localhost:11434\")\n    async_model = outlines.from_tgi(async_client)\n\n    # Define multiple prompts\n    prompts = [\n        \"Write a tagline for a coffee shop.\",\n        \"Write a tagline for a bookstore.\",\n        \"Write a tagline for a gym.\"\n    ]\n\n    tasks = [async_model(prompt, max_new_tokens=30) for prompt in prompts]\n    results = await asyncio.gather(*tasks)\n\n    for prompt, result in zip(prompts, results):\n        print(f\"{prompt}\\n{result}\\n\")\n\nasyncio.run(generate_multiple())\n</code></pre>"},{"location":"features/models/tgi/#structured-generation","title":"Structured Generation","text":"<p>TGI supports all output types available in Outlines except for context-free grammars. Simply provide an <code>output_type</code> after the prompt when calling the model. All structured generation features work with both synchronous and asynchronous models.</p>"},{"location":"features/models/tgi/#simple-type","title":"Simple Type","text":"<pre><code>import outlines\nimport huggingface_hub\n\noutput_type = int\n\ntgi_client = huggingface_hub.InferenceClient(\"http://localhost:8080\")\nmodel = outlines.from_tgi(tgi_client)\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n```### JSON Schema\n\n```python\nimport outlines\nimport huggingface_hub\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\ntgi_client = huggingface_hub.InferenceClient(\"http://localhost:8080\")\nmodel = outlines.from_tgi(tgi_client)\n\nresult = model(\"Create a character.\", output_type=Character, frequency_penalty=1.5)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n```### Multiple Choice\n\n```python\nimport outlines\nimport huggingface_hub\nfrom typing import Literal\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\ntgi_client = huggingface_hub.InferenceClient(\"http://localhost:8080\")\nmodel = outlines.from_tgi(tgi_client)\n\nresult = model(\"What is the capital of France?\", output_type, temperature=0)\nprint(result) # 'Paris'\n```### Regex\n\n```python\nimport outlines\nimport huggingface_hub\nfrom outlines.types import Regex\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\ntgi_client = huggingface_hub.InferenceClient(\"http://localhost:8080\")\nmodel = outlines.from_tgi(tgi_client)\n\nresult = model(\"Generate a fake social security number.\", output_type, top_p=0.1)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/tgi/#async-structured-generation","title":"Async Structured Generation","text":"<p>All structured generation features work seamlessly with async models:</p> <pre><code>import asyncio\nimport outlines\nimport huggingface_hub\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\nasync def generate_user():\n    async_client = huggingface_hub.AsyncInferenceClient(\"http://localhost:11434\")\n    async_model = outlines.from_tgi(async_client)\n\n    result = await async_model(\"Generate a random user profile.\", output_type=User)\n    user = User.model_validate_json(result)\n    print(f\"Name: {user.name}, Email: {user.email}, Age: {user.age}\")\n\nasyncio.run(generate_user())\n</code></pre>"},{"location":"features/models/tgi/#inference-parameters","title":"Inference parameters","text":"<p>When calling the model, you can provide optional parameters on top of the prompt and the output type. Those will be passed on to the <code>text_generation</code> method of the TGI client.</p> <p>Common parameters include <code>max_new_tokens</code>, <code>stop_sequences</code>, <code>temperature</code>, <code>top_k</code>, <code>top_p</code>, and others as specified in the TGI inference client documentation.</p>"},{"location":"features/models/transformers/","title":"Transformers","text":"<p>Installation</p> <p>You need to install the <code>transformers</code> library to be able to use the Transformers in Outlines. Install all optional dependencies of the <code>Transformers</code> model with: <code>pip install \"outlines[transformers]\"</code>.</p> <p>See the HuggingFace documentation for more information on installing <code>transformers</code> with CPU, GPU...</p>"},{"location":"features/models/transformers/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_transformers</code> function. It takes 3 arguments:</p> <ul> <li><code>model</code>: a <code>transformers</code> model (created with <code>AutoModelForCausalLM</code> for instance)</li> <li><code>tokenizer_or_processor</code>: a <code>transformers</code> tokenizer (created with <code>AutoTokenizer</code> for instance, it must be an instance of either <code>PreTrainedTokenizer</code> or <code>PreTrainedTokenizerFast</code>)</li> <li><code>device_dtype</code> (optional): the tensor dtype to use for inference. If not provided, the model will use the default dtype.</li> </ul> <p>For instance:</p> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create the transformers model and tokenizer\nhf_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\nhf_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n\n# Create the Outlines model\nmodel = outlines.from_transformers(hf_model, hf_tokenizer)\n</code></pre> <p>If you provide a processor instead of a tokenizer for the second argument of the <code>from_transformers</code> function, you would get a <code>TransformersMultiModal</code> instance. See the TransformersMultiModal model documentation for more information on using multimodal models in Outlines.</p>"},{"location":"features/models/transformers/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create model\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Call it to generate text\nresult = model(\"What's the capital of Latvia?\", max_new_tokens=20)\nprint(result) # 'Riga'\n</code></pre>"},{"location":"features/models/transformers/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>Transformers</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import outlines\nfrom outlines.inputs import Chat\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create the model\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of Latvia?\"},\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_new_tokens=50)\nprint(response) # 'This is a picture of a black dog.'\n</code></pre>"},{"location":"features/models/transformers/#batching","title":"Batching","text":"<p>Finally, the <code>Transformers</code> model supports batching through the <code>batch</code> method. To use it, provide a list of prompts (using the formats described above) to the <code>batch</code> method. You will receive as a result a list of completions.</p> <p>For instance:</p> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Create model\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create a list of prompts that will be used in a single batch\nprompts = [\n    \"What's the capital of Lithuania?\",\n    \"What's the capital of Latvia?\",\n    \"What's the capital of Estonia?\"\n]\n\n# Call it to generate text\nresult = model.batch(prompts, max_new_tokens=20)\nprint(result) # ['Vilnius', 'Riga', 'Tallinn']\n</code></pre>"},{"location":"features/models/transformers/#structured-generation","title":"Structured Generation","text":"<p>As a local model, <code>Transformers</code> supports all output types available in Outlines. Simply provide an <code>output_type</code> after the prompt when calling the model.</p>"},{"location":"features/models/transformers/#simple-type","title":"Simple Type","text":"<pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\noutput_type = int\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"How many countries are there in the world?\", output_type, max_new_tokens=5)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/transformers/#json-schema","title":"JSON Schema","text":"<pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Create a character.\", output_type=Character, max_new_tokens=200, repetition_penalty=0.5)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/transformers/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"What is the capital of France?\", output_type, max_new_tokens=10, temperature=0)\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/transformers/#regex","title":"Regex","text":"<pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines.types import Regex\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Generate a fake social security number.\", output_type, max_new_tokens=20, top_p=0.5)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/transformers/#context-free-grammar","title":"Context-Free Grammar","text":"<pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines.types import CFG\n\narithmetic_grammar = \"\"\"\n?start: sum\n\n?sum: product\n| sum \"+\" product   -&gt; add\n| sum \"-\" product   -&gt; sub\n\n?product: atom\n| product \"*\" atom  -&gt; mul\n| product \"/\" atom  -&gt; div\n\n?atom: NUMBER           -&gt; number\n| \"-\" atom         -&gt; neg\n| \"(\" sum \")\"\n\n%import common.NUMBER\n%import common.WS_INLINE\n\n%ignore WS_INLINE\n\"\"\"\noutput_type = CFG(arithmetic_grammar)\n\nmodel = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Write an addition.\", output_type, max_new_tokens=100)\nprint(result) # '23 + 48'\n</code></pre>"},{"location":"features/models/transformers/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional inference parameters on top of the prompt and the output type. These parameters will be passed on to the <code>generate</code> method of the <code>transformers</code> model. Some common inference arguments include <code>max_new_tokens</code>, <code>temperature</code>, <code>repetition_penalty</code> and <code>top_p</code>.</p> <p>See the transformers documentation for more information on inference parameters.</p> <p>Warning</p> <p>The <code>max_new_tokens</code> inference parameter has a default value of 20. This is insufficient for most tasks and will result in the generation output not respecting the output type (because the response is truncated). We recommend you always provide a value for this argument.</p>"},{"location":"features/models/transformers_multimodal/","title":"Transformers MultiModal","text":"<p>The Outlines <code>TransformersMultiModal</code> model inherits from <code>Transformers</code> and shares most of its interface. Please start by reading the Transformers documentation as this document only focuses on the specificities of <code>TransformersMultiModal</code> compared to <code>Transformers</code>.</p>"},{"location":"features/models/transformers_multimodal/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_transformers</code> function. It takes 2 arguments:</p> <ul> <li><code>model</code>: a <code>transformers</code> model (created with <code>AutoModelForImageTextToText</code> for instance)</li> <li><code>tokenizer_or_processor</code>: a <code>transformers</code> processor (created with <code>AutoProcessor</code> for instance, it must be an instance of <code>ProcessorMixin</code>)</li> <li><code>device_dtype</code> (optional): the tensor dtype to use for inference. If not provided, the model will use the default dtype.</li> </ul> <p>For instance:</p> <pre><code>import outlines\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\n\n# Create the transformers model and processor\nhf_model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\nhf_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\n# Create the Outlines model\nmodel = outlines.from_transformers(hf_model, hf_processor)\n</code></pre>"},{"location":"features/models/transformers_multimodal/#model-input","title":"Model Input","text":"<p>As with other multimodal models, you should provide a list containing a text prompt and assets (<code>Image</code>, <code>Audio</code> or <code>Video</code> instances) as the model input. The type of asset to provide depends on the capabilities of the <code>transformers</code> model you are running.</p> <p>Here's an example of using a vision multimodal model:</p> <pre><code>from io import BytesIO\nfrom urllib.request import urlopen\n\nfrom PIL import Image as PILImage\nfrom pydantic import BaseModel\nfrom transformers import (\n    LlavaForConditionalGeneration,\n    AutoProcessor,\n)\n\nimport outlines\nfrom outlines.inputs import Image\n\nTEST_MODEL = \"trl-internal-testing/tiny-LlavaForConditionalGeneration\"\nIMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\"\n\nclass Animal(BaseModel):\n    specie: str\n    color: str\n    weight: int\n\ndef get_image_from_url(image_url):\n    img_byte_stream = BytesIO(urlopen(image_url).read())\n    image = PILImage.open(img_byte_stream).convert(\"RGB\")\n    image.format = \"PNG\"\n    return image\n\n# Create a model\nmodel = outlines.from_transformers(\n    LlavaForConditionalGeneration.from_pretrained(TEST_MODEL),\n    AutoProcessor.from_pretrained(TEST_MODEL),\n)\n\n# Call it with a model input dict containing a text prompt and an image + an output type\nresult = model(\n    [\"&lt;image&gt;Describe this animal.\", Image(get_image_from_url(IMAGE_URL))],\n    Animal,\n    max_new_tokens=100\n)\nprint(result) # '{\"specie\": \"cat\", \"color\": \"white\", \"weight\": 4}'\nprint(Animal.model_validate_json(result)) # specie=cat, color=white, weight=4\n</code></pre> <p>Warning</p> <p>Make sure your prompt contains the tags expected by your processor to correctly inject the assets in the prompt. For some vision multimodal models for instance, you need to add as many <code>&lt;image&gt;</code> tags in your prompt as there are image assets included in your model input. <code>Chat</code> method, instead, does not require this step.</p>"},{"location":"features/models/transformers_multimodal/#chat","title":"Chat","text":"<p>The <code>Chat</code> interface offers a more convenient way to work with multimodal inputs. You don't need to manually add asset tags like <code>&lt;image&gt;</code>. The model's HF processor handles the chat templating and asset placement for you automatically. To do so, call the model with a <code>Chat</code> instance using a multimodal chat format. Assets must be pre-processed as <code>outlines.inputs.{Image, Audio, Video}</code> format, and only <code>image</code>, <code>video</code>, and <code>audio</code> types are supported.</p> <p>For instance:</p> <pre><code>import outlines\nfrom outlines.inputs import Chat, Image\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport torch\n\nmodel_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"attn_implementation\": \"flash_attention_2\",\n        \"device_map\": \"auto\",\n    }\n\ndef get_image_from_url(image_url):\n    img_byte_stream = BytesIO(urlopen(image_url).read())\n    image = PILImage.open(img_byte_stream).convert(\"RGB\")\n    image.format = \"PNG\"\n    return image\n\n# Create the model\nmodel = outlines.from_transformers(\n    AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", **model_kwargs),\n    AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", **model_kwargs)\n)\n\nIMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\"\n\n# Create the chat mutimodal input\nprompt = Chat([\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": Image(get_image_from_url(IMAGE_URL))},\n            {\"type\": \"text\", \"text\": \"Describe the image in few words.\"}\n        ],\n    }\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_new_tokens=50)\nprint(response) # 'A Siamese cat with blue eyes is sitting on a cat tree, looking alert and curious.'\n</code></pre> <p>Or using a list containing text and assets:</p> <pre><code>import outlines\nfrom outlines.inputs import Chat, Image\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nimport requests\nimport torch\n\n\nTEST_MODEL = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n# Function to get an image\ndef get_image(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    r = requests.get(url, headers=headers)\n    image = PILImage.open(BytesIO(r.content)).convert(\"RGB\")\n    image.format = \"PNG\"\n    return image\n\nmodel_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        # \"attn_implementation\": \"flash_attention_2\",\n        \"device_map\": \"auto\",\n    }\n\n# Create a model\nmodel = outlines.from_transformers(\n    AutoModelForImageTextToText.from_pretrained(TEST_MODEL, **model_kwargs),\n    AutoProcessor.from_pretrained(TEST_MODEL, **model_kwargs),\n)\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"user\", \"content\": \"You are a helpful assistant that helps me described pictures.\"},\n    {\"role\": \"assistant\", \"content\": \"I'd be happy to help you describe pictures! Please go ahead and share an image\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe briefly the image\", Image(get_image(\"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_new_tokens=50)\nprint(response) # 'The image shows a light-colored cat with a white chest...'\n</code></pre>"},{"location":"features/models/transformers_multimodal/#batching","title":"Batching","text":"<p>The <code>TransformersMultiModal</code> model supports batching through the <code>batch</code> method. To use it, provide a list of prompts (using the formats described above) to the <code>batch</code> method. You will receive as a result a list of completions.</p> <p>An example using the Chat format:</p> <pre><code>import outlines\nfrom outlines.inputs import Chat, Image\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom PIL import Image as PILImage\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport torch\nfrom pydantic import BaseModel\n\nmodel_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"attn_implementation\": \"flash_attention_2\",\n        \"device_map\": \"auto\",\n    }\n\nclass Animal(BaseModel):\n    animal: str\n    color: str\n\ndef get_image_from_url(image_url):\n    img_byte_stream = BytesIO(urlopen(image_url).read())\n    image = PILImage.open(img_byte_stream).convert(\"RGB\")\n    image.format = \"PNG\"\n    return image\n\n# Create the model\nmodel = outlines.from_transformers(\n    AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", **model_kwargs),\n    AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", **model_kwargs)\n)\n\nIMAGE_URL_1 = \"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\"\nIMAGE_URL_2 = \"https://upload.wikimedia.org/wikipedia/commons/a/af/Golden_retriever_eating_pigs_foot.jpg\"\n\n# Create the chat mutimodal messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the image in few words.\"},\n            {\"type\": \"image\", \"image\": Image(get_image_from_url(IMAGE_URL_1))},\n        ],\n    },\n]\n\nmessages_2 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the image in few words.\"},\n            {\"type\": \"image\", \"image\": Image(get_image_from_url(IMAGE_URL_2))},\n        ],\n    },\n]\n\nprompts = [Chat(messages), Chat(messages_2)]\n\n# Call the model to generate a response\nresponses = model.batch(prompts, output_type=Animal, max_new_tokens=100)\nprint(responses) # ['{ \"animal\": \"cat\", \"color\": \"white and gray\" }', '{ \"animal\": \"dog\", \"color\": \"white\" }']\nprint([Animal.model_validate_json(i) for i in responses]) # [Animal(animal='cat', color='white and gray'), Animal(animal='dog', color='white')]\n</code></pre> <p>An example using a list of lists with tag assets:</p> <pre><code>from io import BytesIO\nfrom urllib.request import urlopen\n\nfrom PIL import Image as PILImage\nfrom transformers import (\n    LlavaForConditionalGeneration,\n    AutoProcessor,\n)\n\nimport outlines\nfrom outlines.inputs import Image\n\nTEST_MODEL = \"trl-internal-testing/tiny-LlavaForConditionalGeneration\"\nIMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\"\nIMAGE_URL_2 =\"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\"\n\ndef get_image_from_url(image_url):\n    img_byte_stream = BytesIO(urlopen(image_url).read())\n    image = PILImage.open(img_byte_stream).convert(\"RGB\")\n    image.format = \"PNG\"\n    return image\n\n# Create a model\nmodel = outlines.from_transformers(\n    LlavaForConditionalGeneration.from_pretrained(TEST_MODEL),\n    AutoProcessor.from_pretrained(TEST_MODEL),\n)\n\n# Call the batch method with a list of model input dicts\nresult = model.batch(\n    [\n        [\"&lt;image&gt;Describe the image.\", Image(get_image_from_url(IMAGE_URL))],\n        [\"&lt;image&gt;Describe the image.\", Image(get_image_from_url(IMAGE_URL_2))],\n    ]\n)\nprint(result) # ['The image shows a cat', 'The image shows an astronaut']\n</code></pre>"},{"location":"features/models/vllm/","title":"vLLM","text":""},{"location":"features/models/vllm/#prerequisites","title":"Prerequisites","text":"<p>The Outlines <code>VLLM</code> model is intended to be used along with a vLLM instance running on a separate server (can be local or remote). Make sure you have a vLLM server running and accessible before using the <code>VLLM</code> model. For instance by running:</p> <pre><code>pip install vllm\n\nvllm serve microsoft/Phi-3-mini-4k-instruct \\\n  --dtype auto \\\n  --api-key token-abc123\n</code></pre> <p>Follow the Installation instructions for more information on how to set up a vLLM server for your particular setup.</p> <p>As the vLLM client relies on the <code>openai</code> python sdk, you need to have the <code>openai</code> package installed. Install all optional dependencies for the <code>VLLM</code> model with: <code>pip install openai</code>.</p> <p>If you want to use the vllm offline inference mode instead of the server mode, please refer to the VLLMOffline model documentation.</p>"},{"location":"features/models/vllm/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_vllm</code> function. The argument of the function is either an <code>OpenAI</code> or <code>AsyncOpenAI</code> instance from the <code>openai</code> library. Make sure the value of the <code>base_url</code> argument of the <code>OpenAI</code> client points to your running vLLM server. Consult the vLLM documentation on using an OpenAI client with a vLLM server for more information.</p> <p>Based on whether the <code>openai</code> client instance is synchronous or asynchronous, you will receive a <code>VLLM</code> or <code>AsyncVLLM</code> model instance.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the OpenAI client\nsync_openai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nasync_openai_client = openai.AsyncOpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\n\n# Create a sync model\nsync_model = outlines.from_vllm(sync_openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\nprint(type(sync_model)) # &lt;class 'outlines.models.vllm.VLLM'&gt;\n\n# Create an async model\nasync_model = outlines.from_vllm(async_openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\nprint(type(async_model)) # &lt;class 'outlines.models.vllm.AsyncVLLM'&gt;\n</code></pre>"},{"location":"features/models/vllm/#text-generation","title":"Text Generation","text":"<p>To generate text, you can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_vllm(openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\"), \"microsoft/Phi-3-mini-4k-instruct\")\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", max_tokens=20)\nprint(response) # 'The capital of Latvia is Riga.'\n</code></pre>"},{"location":"features/models/vllm/#vision","title":"Vision","text":"<p>Some models you can run with VLLM support vision input. To use this feature, provide a list containing a text prompt and <code>Image</code> instances.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport outlines\nimport openai\nfrom outlines.inputs import Image\n\n# Create the model\nmodel = outlines.from_vllm(\n    openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\"),\n    \"Qwen/Qwen2.5-VL-3B-Instruct\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the prompt containing the text and the image\nprompt = [\n    \"Describe the image\",\n    Image(get_image(\"https://picsum.photos/id/237/400/300\"))\n]\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'The image shows a black puppy lying on a wooden surface...'\n</code></pre>"},{"location":"features/models/vllm/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>VLLM</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import io\nimport requests\nimport PIL\nimport openai\nimport outlines\nfrom outlines.inputs import Chat, Image\n\n# Create the model\nmodel = outlines.from_vllm(\n    openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\"),\n    \"Qwen/Qwen2.5-VL-3B-Instruct\"\n)\n\n# Function to get an image\ndef get_image(url):\n    r = requests.get(url)\n    return PIL.Image.open(io.BytesIO(r.content))\n\n# Create the chat input\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\"Describe the image\", Image(get_image(\"https://picsum.photos/id/237/400/300\"))]\n    },\n])\n\n# Call the model to generate a response\nresponse = model(prompt, max_tokens=50)\nprint(response) # 'The image shows a black puppy lying on a wooden surface...'\n</code></pre>"},{"location":"features/models/vllm/#streaming","title":"Streaming","text":"<p>Finally, the <code>VLLM</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import openai\nimport outlines\n\n# Create the model\nmodel = outlines.from_vllm(\n    openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\"),\n    \"microsoft/Phi-3-mini-4k-instruct\"\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", max_tokens=50):\n    print(chunk, end=\"\") # 'Once upon a time...'\nprint()\n</code></pre>"},{"location":"features/models/vllm/#asynchronous-calls","title":"Asynchronous Calls","text":"<p>vLLM supports asynchronous operations by passing an <code>AsyncOpenAI</code> client instead of a regular <code>OpenAI</code> client. This returns an <code>AsyncVLLM</code> model instance that supports async/await patterns.</p>"},{"location":"features/models/vllm/#basic-async-generation","title":"Basic Async Generation","text":"<pre><code>import asyncio\nimport openai\nimport outlines\n\nasync def generate_text():\n    async_client = openai.AsyncOpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\n    async_model = outlines.from_vllm(async_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\n    result = await async_model(\"Write a haiku about Python.\", max_tokens=50)\n    print(result)\n\nasyncio.run(generate_text())\n</code></pre>"},{"location":"features/models/vllm/#async-streaming","title":"Async Streaming","text":"<p>The async model also supports streaming with async iteration:</p> <pre><code>import asyncio\nimport openai\nimport outlines\n\nasync def stream_text():\n    async_client = openai.AsyncOpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\n    async_model = outlines.from_vllm(async_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\n    async for chunk in async_model.stream(\"Tell me a story about a robot.\", max_tokens=100):\n        print(chunk, end=\"\")\n\nasyncio.run(stream_text())\n</code></pre>"},{"location":"features/models/vllm/#concurrent-async-requests","title":"Concurrent Async Requests","text":"<p>One of the main benefits of async calls is the ability to make multiple concurrent requests:</p> <pre><code>import asyncio\nimport openai\nimport outlines\n\nasync def generate_multiple():\n    async_client = openai.AsyncOpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\n    async_model = outlines.from_vllm(async_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\n    prompts = [\n        \"Write a tagline for a coffee shop.\",\n        \"Write a tagline for a bookstore.\",\n        \"Write a tagline for a gym.\"\n    ]\n\n    tasks = [async_model(prompt, max_tokens=30) for prompt in prompts]\n    results = await asyncio.gather(*tasks)\n\n    for prompt, result in zip(prompts, results):\n        print(f\"{prompt}\\n{result}\\n\")\n\nasyncio.run(generate_multiple())\n</code></pre>"},{"location":"features/models/vllm/#structured-generation","title":"Structured Generation","text":"<p>vLLM supports all output types available in Outlines. Simply provide an <code>output_type</code> after the prompt when calling the model. All structured generation features work with both synchronous and asynchronous models.</p>"},{"location":"features/models/vllm/#simple-type","title":"Simple Type","text":"<pre><code>import openai\nimport outlines\n\noutput_type = int\n\nopenai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/vllm/#json-schema","title":"JSON Schema","text":"<pre><code>import openai\nimport outlines\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nopenai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\nresult = model(\"Create a character.\", output_type=Character, frequency_penalty=1.5)\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/vllm/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport openai\nimport outlines\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nopenai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\nresult = model(\"What is the capital of France?\", output_type, temperature=0)\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/vllm/#regex","title":"Regex","text":"<pre><code>import openai\nimport outlines\nfrom outlines.types import Regex\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nopenai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\nresult = model(\"Generate a fake social security number.\", output_type, top_p=0.1)\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/vllm/#context-free-grammar","title":"Context-Free Grammar","text":"<pre><code>import openai\nimport outlines\nfrom outlines.types import CFG\n\narithmetic_grammar = \"\"\"\n?start: sum\n\n?sum: product\n| sum \"+\" product   -&gt; add\n| sum \"-\" product   -&gt; sub\n\n?product: atom\n| product \"*\" atom  -&gt; mul\n| product \"/\" atom  -&gt; div\n\n?atom: NUMBER           -&gt; number\n| \"-\" atom         -&gt; neg\n| \"(\" sum \")\"\n\n%import common.NUMBER\n%import common.WS_INLINE\n\n%ignore WS_INLINE\n\"\"\"\noutput_type = CFG(arithmetic_grammar)\n\nopenai_client = openai.OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\nresult = model(\"Write an addition.\", output_type, extra_body={\"guided_decoding_backend\": \"outlines\"})\nprint(result) # '23 + 48'\n</code></pre>"},{"location":"features/models/vllm/#async-structured-generation","title":"Async Structured Generation","text":"<p>All structured generation features work seamlessly with async models:</p> <pre><code>import asyncio\nimport openai\nimport outlines\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\nasync def generate_user():\n    async_client = openai.AsyncOpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"token-abc123\")\n    async_model = outlines.from_vllm(async_client, \"microsoft/Phi-3-mini-4k-instruct\")\n\n    result = await async_model(\"Generate a random user profile.\", output_type=User)\n    user = User.model_validate_json(result)\n    print(f\"Name: {user.name}, Email: {user.email}, Age: {user.age}\")\n\nasyncio.run(generate_user())\n</code></pre>"},{"location":"features/models/vllm/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional parameters on top of the prompt and the output type. Those will be passed on to the <code>chat.completions.create</code> method of the OpenAI client.</p> <p>An optional parameter of particular interest is <code>extra_body</code>, which is a dictionary containing arguments that are specific to vLLM and are not part of the standard <code>openai</code> interface. Among those, <code>guided_decoding_backend</code> allows you to select the library used by the vLLM server to control structured generation. You can use the value <code>outlines</code> to generated structured text with Outlines.</p> <p>See the vLLM documentation on extra parameters for the OpenAI-compatible server for more information on inference parameters.</p>"},{"location":"features/models/vllm_offline/","title":"vLLM Offline","text":"<p>Outlines provides an integration with vLLM using the vllm library. This model allows you to use vLLM in the \"Offline Inference\" mode, meaning that text generation happens within the model, there is no separate server. If you want to use vLLM with a server, see the VLLM model documentation.</p> <p>Installation</p> <p>You need to install the <code>vllm</code> library to be able to use the <code>VLLMOffline</code> model: <code>pip install vllm</code>. Due to a library version conflict between outlines and vllm, you MUST install <code>vllm</code> before installing <code>outlines</code>.</p> <p>When installing <code>outlines</code> (after having first installed <code>vllm</code>), you may encounter the following error: <code>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed</code>. You can safely ignore it.</p> <p>See the vLLM documentation for instructions on how to install vLLM for CPU, ROCm...</p>"},{"location":"features/models/vllm_offline/#model-initialization","title":"Model Initialization","text":"<p>To load the model, you can use the <code>from_vllm_offline</code> function. The single argument of the function is a <code>LLM</code> model instance from the <code>vllm</code> library. You will then receive a <code>VLLMOffline</code> model instance you can use to generate text.</p> <p>Consult the LLM class API reference for detailed information on how to create an <code>LLM</code> instance and on the various available parameters.</p> <p>For instance:</p> <pre><code>import outlines\nfrom vllm import LLM\n\n# Create the model\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n</code></pre> <p>Note</p> <p>When initializing the <code>vllm.LLM</code> object, you can specify a <code>guided_decoding_backend</code> to choose what library will be used by vLLM to constrain the generation. Consult the vLLM documentation on structured output for the list of possible values.</p>"},{"location":"features/models/vllm_offline/#text-generation","title":"Text Generation","text":"<p>Once you've created your Outlines <code>VLLMOffline</code> model instance, you're all set to generate text with this provider. You can simply call the model with a prompt.</p> <p>For instance:</p> <pre><code>import outlines\nfrom vllm import LLM, SamplingParams\n\n# Create the model\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Call it to generate text\nresponse = model(\"What's the capital of Latvia?\", sampling_params=SamplingParams(max_tokens=20))\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/vllm_offline/#chat","title":"Chat","text":"<p>You can also use chat inputs with the <code>VLLMOffline</code> model. To do so, call the model with a <code>Chat</code> instance. The content of messsage within the chat can be vision inputs as described above.</p> <p>For instance:</p> <pre><code>import outlines\nfrom vllm import LLM, SamplingParams\nfrom outlines.inputs import Chat\n\n# Create the model\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create the chat prompt\nprompt = Chat([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the capital of Latvia?\"},\n])\n\n# Call the model to generate a response\nresponse = model(prompt, sampling_params=SamplingParams(max_tokens=50))\nprint(response) # 'Riga'\n</code></pre>"},{"location":"features/models/vllm_offline/#streaming","title":"Streaming","text":"<p>The <code>VLLMOffline</code> model supports streaming through the <code>stream</code> method.</p> <p>For instance:</p> <pre><code>import outlines\nfrom vllm import LLM, SamplingParams\n\n# Create the model\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Stream the response\nfor chunk in model.stream(\"Tell me a short story about a cat.\", sampling_params=SamplingParams(max_tokens=50)):\n    print(chunk) # 'Once...'\n</code></pre>"},{"location":"features/models/vllm_offline/#batching","title":"Batching","text":"<p>Finally, the <code>VLLMOffline</code> model also supports batching through the <code>batch</code> method. To use it, provide a list of prompts (using the formats described above) to the <code>batch</code> method. You will receive as a result a list of completions.</p> <p>For instance:</p> <pre><code>import outlines\nfrom vllm import LLM\n\n# Create the model\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create a list of prompts that will be used in a single batch\nprompts = [\n    \"What's the capital of Lithuania?\",\n    \"What's the capital of Latvia?\",\n    \"What's the capital of Estonia?\"\n]\n\n# Call it to generate text\nresult = model.batch(prompts, max_new_tokens=20)\nprint(result) # ['Vilnius', 'Riga', 'Tallinn']\n</code></pre>"},{"location":"features/models/vllm_offline/#structured-generation","title":"Structured Generation","text":"<p>The <code>VLLMOffline</code> model supports all output types available in Outlines. Simply provide an <code>output_type</code> after the prompt when calling the model.</p>"},{"location":"features/models/vllm_offline/#simple-type","title":"Simple Type","text":"<pre><code>import outlines\nfrom vllm import LLM\n\noutput_type = int\n\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"How many countries are there in the world?\", output_type)\nprint(result) # '200'\n</code></pre>"},{"location":"features/models/vllm_offline/#json-schema","title":"JSON Schema","text":"<pre><code>import outlines\nfrom vllm import LLM, SamplingParams\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    skills: List[str]\n\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Create a character.\", output_type=Character, sampling_params=SamplingParams(frequency_penalty=1.5, max_tokens=200))\nprint(result) # '{\"name\": \"Evelyn\", \"age\": 34, \"skills\": [\"archery\", \"stealth\", \"alchemy\"]}'\nprint(Character.model_validate_json(result)) # name=Evelyn, age=34, skills=['archery', 'stealth', 'alchemy']\n</code></pre>"},{"location":"features/models/vllm_offline/#multiple-choice","title":"Multiple Choice","text":"<pre><code>from typing import Literal\nimport outlines\nfrom vllm import LLM, SamplingParams\n\noutput_type = Literal[\"Paris\", \"London\", \"Rome\", \"Berlin\"]\n\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"What is the capital of France?\", output_type, sampling_params=SamplingParams(temperature=0))\nprint(result) # 'Paris'\n</code></pre>"},{"location":"features/models/vllm_offline/#regex","title":"Regex","text":"<pre><code>import outlines\nfrom vllm import LLM, SamplingParams\nfrom outlines.types import Regex\n\noutput_type = Regex(r\"\\d{3}-\\d{2}-\\d{4}\")\n\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Generate a fake social security number.\", output_type, sampling_params=SamplingParams(top_p=0.1))\nprint(result) # '782-32-3789'\n</code></pre>"},{"location":"features/models/vllm_offline/#context-free-grammar","title":"Context-Free Grammar","text":"<pre><code>import outlines\nfrom vllm import LLM, SamplingParams\nfrom outlines.types import CFG\n\narithmetic_grammar = \"\"\"\n?start: sum\n\n?sum: product\n| sum \"+\" product   -&gt; add\n| sum \"-\" product   -&gt; sub\n\n?product: atom\n| product \"*\" atom  -&gt; mul\n| product \"/\" atom  -&gt; div\n\n?atom: NUMBER           -&gt; number\n| \"-\" atom         -&gt; neg\n| \"(\" sum \")\"\n\n%import common.NUMBER\n%import common.WS_INLINE\n\n%ignore WS_INLINE\n\"\"\"\noutput_type = CFG(arithmetic_grammar)\n\nmodel = outlines.from_vllm_offline(\n    LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\nresult = model(\"Write an addition.\", output_type)\nprint(result) # '23 + 48'\n</code></pre>"},{"location":"features/models/vllm_offline/#inference-arguments","title":"Inference Arguments","text":"<p>When calling the model, you can provide optional parameters on top of the prompt and the output type. Those will be passed on to the <code>generate</code> method of the <code>LLM</code> model instance. An argument of particular interest is <code>sampling_params</code>. It takes as a value a <code>vllm.SamplingParams</code> instance containing parameters such as max_tokens or temperature.</p> <p>See the vLLM documentation on sampling parameters for more information on inference parameters.</p>"},{"location":"features/utility/application/","title":"Application","text":"<p>The <code>Application</code> class enables you to encapsulate a prompt template and an output type into a reusable component.</p>"},{"location":"features/utility/application/#overview","title":"Overview","text":"<p>An <code>Application</code> combines a prompt template with an output type, creating a reusable component that can be applied to different models.</p> <p>Applications are useful for simplifying repeated tasks where you have a well-defined <code>Template</code> and a fixed output type, such as classification tasks or data extraction.</p> <p>To create an <code>Application</code> instance, initialize the class with a prompt template and an output type. You can then call the application with a model and the variables defined in your template in a dictionary.</p> <p>For instance:</p> <pre><code>from typing import Literal\nimport transformers\nfrom outlines import Application, Template, from_transformers\n\n# Create a template\ntemplate_str = \"Is {{ name }} a boy or a girl name?\"\ntemplate = Template.from_string(template_str)\n\n# Create a model\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create the application and call it to generate text\napplication = Application(template, Literal[\"boy\", \"girl\"])\nresponse = application(model, {\"name\": \"Alice\"}, max_new_tokens=10)\n\nprint(response) # \"girl\"\n</code></pre> <p>Instead of providing an Outlines <code>Template</code> instance, you can provide a <code>Callable</code> that returns a string. The parameters of the callable are used as the variables of the template such that you must provide values for them in the dictionary when calling the application.</p> <p>For instance, we can create the same example as above using a a function instead of a template:</p> <pre><code>from typing import Literal\nimport transformers\nfrom outlines import Application, from_transformers\n\n# Create a function that will be used as a template\ndef template_func(name: str) -&gt; str:\n    return f\"Is {name} a boy or a girl name?\"\n\n# Create a model\nmodel = from_transformers(\n    transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\"),\n    transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n)\n\n# Create the application with the function template and call it to generate text\napplication = Application(template_func, Literal[\"boy\", \"girl\"])\nresponse = application(model, {\"name\": \"Alice\"}, max_new_tokens=10)\n\nprint(response) # \"girl\"\n</code></pre>"},{"location":"features/utility/regex_dsl/","title":"Regex DSL","text":"<p>This library provides a Domain-Specific Language (DSL) to construct regular expressions in a more intuitive and modular way. It allows you to create complex regexes using simple building blocks that represent literal strings, patterns, and various quantifiers. Additionally, these custom regex types can be used directly as types in Pydantic schemas to enforce pattern constraints during text generation.</p>"},{"location":"features/utility/regex_dsl/#why-use-this-dsl","title":"Why Use This DSL?","text":"<ol> <li>Modularity &amp; Readability: Instead of writing cryptic regular expression strings, you compose a regex as a tree of objects.</li> <li>Enhanced Debugging: Each expression can be visualized as an ASCII tree, making it easier to understand and debug complex regexes.</li> <li>Pydantic Integration: Use your DSL-defined regex as types in Pydantic models. The DSL seamlessly converts to JSON Schema with proper pattern constraints.</li> <li>Extensibility: Easily add or modify quantifiers and other regex components by extending the provided classes.</li> </ol>"},{"location":"features/utility/regex_dsl/#building-blocks","title":"Building Blocks","text":"<p>Every regex component in this DSL is a Term. Here are two primary types:</p> <ul> <li><code>String</code>: Represents a literal string. It escapes the characters that have a special meaning in regular expressions.</li> <li><code>Regex</code>: Represents an existing regex pattern string.</li> </ul> <pre><code>from outlines.types import String, Regex\n\n# A literal string \"hello\"\nliteral = String(\"hello\")   # Internally represents \"hello\"\n\n# A regex pattern to match one or more digits\ndigit = Regex(r\"[0-9]+\")     # Internally represents the pattern [0-9]+\n\n# Converting to standard regex strings:\nfrom outlines.types.dsl import to_regex\n\nprint(to_regex(literal))  # Output: hello\nprint(to_regex(digit))    # Output: [0-9]+\n</code></pre>"},{"location":"features/utility/regex_dsl/#early-introduction-to-quantifiers-combining-terms","title":"Early Introduction to Quantifiers &amp; Combining Terms","text":"<p>The DSL supports common regex quantifiers as methods on every <code>Term</code>. These methods allow you to specify how many times a pattern should be matched. They include:</p> <ul> <li><code>exactly(count)</code>: Matches the term exactly <code>count</code> times.</li> <li><code>optional()</code>: Matches the term zero or one time.</li> <li><code>one_or_more()</code>: Matches the term one or more times (Kleene Plus).</li> <li><code>zero_or_more()</code>: Matches the term zero or more times (Kleene Star).</li> <li><code>between(min_count, max_count)</code>: Matches the term between <code>min_count</code> and <code>max_count</code> times (inclusive).</li> <li><code>at_least(count)</code>: Matches the term at least <code>count</code> times.</li> <li><code>at_most(count)</code>: Matches the term up to <code>count</code> times.</li> </ul> <p>These quantifiers can also be used as functions that take the <code>Term</code> as an argument. If the term is a plain string, it will be automatically converted to a <code>String</code> object. Thus <code>String(\"foo\").optional()</code> is equivalent to <code>optional(\"foo\")</code>.</p> <p>Let's see these quantifiers side by side with examples.</p>"},{"location":"features/utility/regex_dsl/#quantifiers-in-action","title":"Quantifiers in Action","text":""},{"location":"features/utility/regex_dsl/#exactlycount","title":"<code>exactly(count)</code>","text":"<p>This method restricts the term to appear exactly <code>count</code> times.</p> <pre><code># Example: exactly 5 digits\nfive_digits = Regex(r\"\\d\").exactly(5)\nprint(to_regex(five_digits))  # Output: (\\d){5}\n</code></pre> <p>You can also use the <code>exactly</code> function:</p> <pre><code>from outlines.types import exactly\n\n# Example: exactly 5 digits\nfive_digits = exactly(Regex(r\"\\d\"), 5)\nprint(to_regex(five_digits))  # Output: (\\d){5}\n</code></pre>"},{"location":"features/utility/regex_dsl/#optional","title":"<code>optional()</code>","text":"<p>This method makes a term optional, meaning it may occur zero or one time.</p> <pre><code># Example: an optional \"s\" at the end of a word\nmaybe_s = String(\"s\").optional()\nprint(to_regex(maybe_s))  # Output: (s)?\n</code></pre> <p>You can also use the <code>optional</code> function:</p> <pre><code>from outlines.types import optional\n\n# Example: an optional \"s\" at the end of a word\nmaybe_s = optional(\"s\")\nprint(to_regex(maybe_s))  # Output: (s)?\n</code></pre>"},{"location":"features/utility/regex_dsl/#one_or_more","title":"<code>one_or_more()</code>","text":"<p>This method indicates that the term must appear at least once.</p> <pre><code># Example: one or more alphabetic characters\nletters = Regex(r\"[A-Za-z]\").one_or_more()\nprint(to_regex(letters))  # Output: ([A-Za-z])+\n</code></pre> <p>You can also use the <code>one_or_more</code> function:</p> <pre><code>from outlines.types import one_or_more\n\n# Example: one or more alphabetic characters\nletters = one_or_more(Regex(r\"[A-Za-z]\"))\nprint(to_regex(letters))  # Output: ([A-Za-z])+\n</code></pre>"},{"location":"features/utility/regex_dsl/#zero_or_more","title":"<code>zero_or_more()</code>","text":"<p>This method indicates that the term can occur zero or more times.</p> <pre><code># Example: zero or more spaces\nspaces = String(\" \").zero_or_more()\nprint(to_regex(spaces))  # Output: ( )*\n</code></pre> <p>You can also use the <code>zero_or_more</code> function:</p> <pre><code>from outlines.types import zero_or_more\n\n# Example: zero or more spaces\nspaces = zero_or_more(\" \")\nprint(to_regex(spaces))  # Output: ( )*\n</code></pre>"},{"location":"features/utility/regex_dsl/#betweenmin_count-max_count","title":"<code>between(min_count, max_count)</code>","text":"<p>This method indicates that the term can appear any number of times between <code>min_count</code> and <code>max_count</code> (inclusive).</p> <pre><code># Example: Between 2 and 4 word characters\nword_chars = Regex(r\"\\w\").between(2, 4)\nprint(to_regex(word_chars))  # Output: (\\w){2,4}\n</code></pre> <p>You can also use the <code>between</code> function:</p> <pre><code>from outlines.types import between\n\n# Example: Between 2 and 4 word characters\nword_chars = between(Regex(r\"\\w\"), 2, 4)\nprint(to_regex(word_chars))  # Output: (\\w){2,4}\n</code></pre>"},{"location":"features/utility/regex_dsl/#at_leastcount","title":"<code>at_least(count)</code>","text":"<p>This method indicates that the term must appear at least <code>count</code> times.</p> <pre><code># Example: At least 3 digits\nat_least_three = Regex(r\"\\d\").at_least(3)\nprint(to_regex(at_least_three))  # Output: (\\d){3,}\n</code></pre> <p>You can also use the <code>at_least</code> function:</p> <pre><code>from outlines.types import at_least\n\n# Example: At least 3 digits\nat_least_three = at_least(Regex(r\"\\d\"), 3)\nprint(to_regex(at_least_three))  # Output: (\\d){3,}\n</code></pre>"},{"location":"features/utility/regex_dsl/#at_mostcount","title":"<code>at_most(count)</code>","text":"<p>This method indicates that the term can appear at most <code>count</code> times.</p> <pre><code># Example: At most 3 digits\nup_to_three = Regex(r\"\\d\").at_most(3)\nprint(to_regex(up_to_three))  # Output: (\\d){0,3}\n</code></pre> <p>You can also use the <code>at_most</code> function:</p> <pre><code>from outlines.types import at_most\n\n# Example: At most 3 digits\nup_to_three = at_most(Regex(r\"\\d\"), 3)\nprint(to_regex(up_to_three))  # Output: (\\d){0,3}\n</code></pre>"},{"location":"features/utility/regex_dsl/#combining-terms","title":"Combining Terms","text":"<p>The DSL allows you to combine basic terms into more complex patterns using concatenation and alternation.</p>"},{"location":"features/utility/regex_dsl/#concatenation","title":"Concatenation (<code>+</code>)","text":"<p>The <code>+</code> operator (and its reflected variant) concatenates terms, meaning that the terms are matched in sequence.</p> <pre><code># Example: Match \"hello world\"\npattern = String(\"hello\") + \" \" + Regex(r\"\\w+\")\nprint(to_regex(pattern))  # Output: hello\\ (\\w+)\n</code></pre>"},{"location":"features/utility/regex_dsl/#alternation-either","title":"Alternation (<code>either()</code>)","text":"<p>The <code>either()</code> function creates alternatives, allowing a match for one of several patterns. You can provide as many terms as you want.</p> <pre><code># Example: Match either \"cat\" or \"dog\" or \"mouse\"\nanimal = either(String(\"cat\"), \"dog\", \"mouse\")\nprint(to_regex(animal))  # Output: (cat|dog|mouse)\n</code></pre> <p>Note: When using <code>either()</code> with plain strings (such as <code>\"dog\"</code>), the DSL automatically wraps them in a <code>String</code> object that escapes the characters that have a special meaning in regular expressions, just like with quantifier functions.</p>"},{"location":"features/utility/regex_dsl/#custom-types","title":"Custom types","text":"<p>The DSL comes \"batteries included\" with types that represent common text constructs:</p> <ul> <li><code>integer</code> represents an integer number as recognized by <code>int</code></li> <li><code>boolean</code> represents a boolean, \"True\" or \"False\" as recognized by <code>bool</code></li> <li><code>number</code> represents a floating-point number recognize by Python's <code>float</code></li> <li><code>date</code> represents a date as understood by <code>datetime.date</code></li> <li><code>time</code> represents a time as understood by <code>datetime.time</code></li> <li><code>datetime</code> represents a time as understood by <code>datetime.datetime</code></li> <li><code>digit</code> represents a single digit</li> <li><code>char</code> represents a single character</li> <li><code>newline</code> represents a new line character</li> <li><code>whitespace</code> represents a white space</li> <li><code>hex_str</code> represents a hexadecimal string, optionally prefixed with \"0x\"</li> <li><code>uuid4</code> represents a UUID version 4 string in the format \"xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\"</li> <li><code>ipv4</code> represents an IPv4 address in the format \"xxx.xxx.xxx.xxx\" where each octet is between 0 and 255</li> <li><code>sentence</code> represents a sentence</li> <li><code>paragraph</code> represents a paragraph (one or more sentences separated by one or more line breaks)</li> </ul> <p>For instance you can describe the answers in the GSM8K dataset using the following pattern:</p> <pre><code>from outlines.types import sentence, digit\n\nanswer = \"A: \" + sentence.between(2,4) + \" So the answer is: \" + digit.between(1,4)\n</code></pre>"},{"location":"features/utility/regex_dsl/#practical-examples","title":"Practical Examples","text":""},{"location":"features/utility/regex_dsl/#example-1-matching-a-custom-id-format","title":"Example 1: Matching a Custom ID Format","text":"<p>Suppose you want to create a regex that matches an ID format like \"ID-12345\", where:</p> <ul> <li>The literal \"ID-\" must be at the start.</li> <li>Followed by exactly 5 digits.</li> </ul> <pre><code>id_pattern = \"ID-\" + Regex(r\"\\d\").exactly(5)\nprint(to_regex(id_pattern))  # Output: ID-(\\d){5}\n</code></pre>"},{"location":"features/utility/regex_dsl/#example-2-email-validation-with-pydantic","title":"Example 2: Email Validation with Pydantic","text":"<p>You can define a regex for email validation and use it as a type in a Pydantic model.</p> <pre><code>from pydantic import BaseModel, ValidationError\n\n# Define an email regex term (this is a simplified version)\nemail_regex = Regex(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n\nclass User(BaseModel):\n    name: str\n    email: email_regex  # Use our DSL regex as a field type\n\n# Valid input\nuser = User(name=\"Alice\", email=\"alice@example.com\")\nprint(user)\n\n# Invalid input (raises a ValidationError)\ntry:\n    User(name=\"Bob\", email=\"not-an-email\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>When used in a Pydantic model, the email field is automatically validated against the regex pattern and its JSON Schema includes the <code>pattern</code> constraint.</p>"},{"location":"features/utility/regex_dsl/#example-3-building-a-complex-pattern","title":"Example 3: Building a Complex Pattern","text":"<p>Consider a pattern to match a simple date format: <code>YYYY-MM-DD</code>.</p> <pre><code>year = Regex(r\"\\d\").exactly(4)         # Four digits for the year\nmonth = Regex(r\"\\d\").exactly(2)        # Two digits for the month\nday = Regex(r\"\\d\").exactly(2)          # Two digits for the day\n\n# Combine with literal hyphens\ndate_pattern = year + \"-\" + month + \"-\" + day\nprint(to_regex(date_pattern))\n# Output: (\\d){4}\\-(\\d){2}\\-(\\d){2}\n</code></pre>"},{"location":"features/utility/regex_dsl/#visualizing-your-pattern","title":"Visualizing Your Pattern","text":"<p>One of the unique features of this DSL is that each term can print its underlying structure as an ASCII tree. This visualization can be particularly helpful when dealing with complex expressions.</p> <pre><code># A composite pattern using concatenation and quantifiers\npattern = \"a\" + String(\"b\").one_or_more() + \"c\"\nprint(pattern)\n</code></pre> <p>Expected Output:</p> <pre><code>\u2514\u2500\u2500 Sequence\n    \u251c\u2500\u2500 String('a')\n    \u251c\u2500\u2500 KleenePlus(+)\n    \u2502   \u2514\u2500\u2500 String('b')\n    \u2514\u2500\u2500 String('c')\n</code></pre> <p>This tree representation makes it easy to see the hierarchy and order of operations in your regular expression.</p>"},{"location":"features/utility/regex_dsl/#final-words","title":"Final Words","text":"<p>This DSL is designed to simplify the creation and management of regular expressions\u2014whether you're validating inputs in a web API, constraining the output of an LLM, or just experimenting with regex patterns. With intuitive methods for common quantifiers and operators, clear visual feedback, and built-in integration with Pydantic, you can build robust and maintainable regex-based validations with ease.</p> <p>Feel free to explore the library further and adapt the examples to your use cases. Happy regexing!</p>"},{"location":"features/utility/template/","title":"Template","text":"<p>Outlines templates provide a way of creating reusable prompt structures with placeholders for dynamic content.</p>"},{"location":"features/utility/template/#overview","title":"Overview","text":"<p>To create a <code>Template</code> instance, you can use two class methods: - <code>from_string</code>: Creates a template from a string containing a Jinja2 template - <code>from_file</code>: Creates a template from a file containing a Jinja2 template</p> <p>After creating a template, you can call it with the variables required by the template as keyword arguments.</p> <p>For instance:</p> <pre><code>from outlines import Template\n\n# Create a template from a string\ntemplate_str = \"\"\"\nHello, {{ name }}!\nThe weather today is {{ weather }}.\n\"\"\"\ntemplate = Template.from_string(template_str)\n\n# Create a template from a file, assuming the content of template_str is put into a file\ntemplate = Template.from_file(\"path_to/my_file.txt\")\n\n# Call the template to render the prompt\nprompt: str = template(name=\"Alice\", weather=\"sunny\")\nprint(prompt)  # \"Hello, Alice!\\nThe weather today is sunny.\"\n</code></pre>"},{"location":"features/utility/template/#composite-templates","title":"Composite Templates","text":"<p>Templates can be nested and composed to create complex prompt structures:</p> <pre><code>from outlines import Template\n\n# Create component templates\nuser_template = Template.from_string(\"User: {{ query }}\")\nsystem_template = Template.from_string(\"System: {{ instruction }}\")\n\n# Create a composite template\nchat_template = Template.from_string(\"\"\"\n{{ system }}\n{{ user }}\n\"\"\")\n\n# Fill in nested templates\nprompt = chat_template(\n    system=system_template(instruction=\"You are a helpful assistant.\"),\n    user=user_template(query=\"What is machine learning?\")\n)\n\nprint(prompt)\n# System: You are a helpful assistant.\n#\n# User: What is machine learning?\n</code></pre>"},{"location":"features/utility/template/#custom-filters","title":"Custom Filters","text":"<p>You can add custom filters to your Outlines template to extend the templating functionality. To do so, provide as second argument a dictionary with filter names as keys and filter functions as values. The filter can then be used in your jinja2 template following the regular syntax. When rendering a prompt, the function will be applied to the associated variable.</p> <p>For instance:</p> <pre><code>from outlines import Template\n\ndef uppercase(text: str) -&gt; str:\n    return text.upper()\n\n# Add custom filter when creating template\ntemplate = Template.from_string(\n    \"Hello {{ name | uppercase }}!\",\n    filters={\"uppercase\": uppercase}\n)\nprompt = template(name=\"alice\")\nprint(prompt)  # \"Hello ALICE!\"\n</code></pre>"},{"location":"guide/chat_templating/","title":"Chat templating","text":"<p>Instruction-tuned language models use \"special tokens\" to indicate different parts of text, such as the system prompt, the user prompt, any images, and the assistant's response. A chat template is how different types of input are composited together into a single, machine-readable string.</p> <p>Outlines supports chat templating throught the <code>Chat</code> model input class. It contains a list of messages similar in format to the chat history you would use with API models such as OpenAI or Anthropic and to the expected arguments of the <code>apply_chat_template</code> method of transformers tokenizers. You can find detailed information on the interface of this object in the model inputs documentation.</p>"},{"location":"guide/core_concepts/","title":"Core concepts","text":"<p>Coming soon. This will document various concepts at a high level, so users can understand Outlines before diving into specific implementations.</p> <ol> <li>Constrained decoding, tokens, and the basics of logit biasing</li> <li>Different ways to define output structure (regex, JSON schema, Pydantic models, context-free grammars)</li> <li>How finite state machines are used to guarantee output structure</li> <li><code>Generator</code>, <code>Application</code>, <code>Template</code>,</li> <li>Prompt engineering vs. structured generation</li> </ol>"},{"location":"guide/fastapi_vllm_deployment/","title":"Deploying with FastAPI","text":"<p>This guide demonstrates how to build a FastAPI application that leverages Outlines' async integration with vLLM. We create a customer support API that can intelligently categorize tickets and generate structured responses.</p>"},{"location":"guide/fastapi_vllm_deployment/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have a vLLM server running (locally or remotely) and the following packages installed:</p> <pre><code>pip install fastapi uvicorn outlines openai pydantic\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#building-the-application","title":"Building the Application","text":""},{"location":"guide/fastapi_vllm_deployment/#step-1-define-data-models","title":"Step 1: Define Data Models","text":"<p>First, let's define our Pydantic models for structured outputs:</p> <pre><code># models.py\nfrom enum import Enum\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclass TicketCategory(str, Enum):\n    BILLING = \"billing\"\n    TECHNICAL = \"technical\"\n    ACCOUNT = \"account\"\n    PRODUCT = \"product\"\n    OTHER = \"other\"\n\nclass TicketPriority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    URGENT = \"urgent\"\n\nclass TicketAnalysis(BaseModel):\n    category: TicketCategory\n    priority: TicketPriority\n    summary: str = Field(description=\"Brief summary of the issue\")\n    customer_sentiment: str = Field(description=\"Customer emotional state\")\n    key_issues: List[str] = Field(description=\"List of main problems\")\n    requires_human: bool = Field(description=\"Whether this needs human intervention\")\n\nclass SupportResponse(BaseModel):\n    greeting: str\n    acknowledgment: str = Field(description=\"Acknowledge the customer's issue\")\n    solution_steps: List[str] = Field(description=\"Steps to resolve the issue\")\n    closing: str\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#step-2-define-the-prompts","title":"Step 2: Define the prompts","text":"<p>Let us now write the prompts that we will be using in our application, using Jinja 2's templating language. We separate them from the application implementation so they are easier to modify and version.</p> <pre><code>{# prompts/categorize.txt #}\nAnalyze this customer support ticket:\n\nCustomer ID: {{ customer_id }}\nMessage: {{ message }}\n\nExtract the category, priority, and other relevant information.\n</code></pre> <pre><code>{# prompts/respond.txt #}\nGenerate a professional customer support response.\n\nCustomer Message: {{ message }}\nCategory: {{ category }}\nPriority: {{  priority }}\nCustomer Sentiment: {{ customer_sentiment }}\n\nCreate a helpful, empathetic response that addresses their concerns.\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#step-3-create-the-fastapi-application","title":"Step 3: Create the FastAPI Application","text":"<p>Now let's create our FastAPI application with async vLLM integration:</p> <pre><code># main.py\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import Optional\n\nimport openai\nfrom outlines import models, Template\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\nfrom models import TicketAnalysis, SupportResponse\n\n# Request model\nclass TicketRequest(BaseModel):\n    customer_id: str\n    message: str\n\n# Global model instance\nasync_model = None\n\n# The lifespan function is a FastAPI construct\n# used to define startup and shutdown logic for the API.\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Initialize the async vLLM model on startup.\"\"\"\n    global async_model\n\n    client = openai.AsyncOpenAI(\n        base_url=\"http://localhost:8000/v1\",  # Adjust to your vLLM server URL\n        api_key=\"dummy\"  # vLLM doesn't require a real API key\n    )\n    async_model = models.from_vllm(client, \"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n    yield\n\n    async_model = None  # Cleanup\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Customer Support Assistant API\",\n    description=\"AI-powered customer support with structured outputs\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n\n@app.post(\"/analyze-ticket\", response_model=TicketAnalysis)\nasync def analyze_ticket(request: TicketRequest):\n    \"\"\"Analyze a customer support ticket and extract structured information.\"\"\"\n    if async_model is None:\n        raise HTTPException(status_code=503, detail=\"Model not initialized\")\n\n    template = Template.from_file(\"prompts/categorize.txt\")\n    prompt = template(\n        customer_id=request.customer_id,\n        message=request.message\n    )\n\n    try:\n        # Generate and parse a structured response\n        result = await async_model(prompt, TicketAnalysis, max_tokens=5000)\n        analysis = TicketAnalysis.model_validate_json(result)\n\n        return analysis\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Analysis failed: {str(e)}\")\n\n\n@app.post(\"/generate-response\", response_model=SupportResponse)\nasync def generate_response(\n    request: TicketRequest,\n    analysis: TicketAnalysis\n):\n    \"\"\"Generate a structured support response based on ticket analysis.\"\"\"\n    if async_model is None:\n        raise HTTPException(status_code=503, detail=\"Model not initialized\")\n\n    template = Template.from_file(\"prompts/respond.txt\")\n    prompt = template(\n        message=request.message,\n        category=analysis.category,\n        priority=analysis.priority,\n        customer_sentiment=analysis.customer_sentiment\n    )\n\n    try:\n        # Generate and parse a structured response\n        result = await async_model(prompt, SupportResponse, max_tokens=5000)\n        response = SupportResponse.model_validate_json(result)\n\n        return response\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Response generation failed: {str(e)}\")\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#running-the-application","title":"Running the Application","text":""},{"location":"guide/fastapi_vllm_deployment/#step-1-start-your-vllm-server","title":"Step 1: Start your vLLM server","text":"<pre><code>vllm serve Qwen/Qwen2.5-VL-7B-Instruct\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#step-2-run-the-fastapi-application","title":"Step 2: Run the FastAPI application","text":"<pre><code>uvicorn main:app --reload --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#testing-the-api","title":"Testing the API","text":""},{"location":"guide/fastapi_vllm_deployment/#example-1-analyze-a-support-ticket","title":"Example 1: Analyze a support ticket","text":"<pre><code>curl -X POST \"http://localhost:8080/analyze-ticket\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"customer_id\": \"CUST123\",\n    \"message\": \"I have been charged twice for my subscription this month. This is unacceptable and I want a refund immediately!\"\n  }'\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"category\": \"billing\",\n  \"priority\": \"high\",\n  \"summary\": \"Customer charged twice for subscription, requesting refund\",\n  \"customer_sentiment\": \"angry\",\n  \"key_issues\": [\"duplicate charge\", \"subscription billing\", \"refund request\"],\n  \"requires_human\": false\n}\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#example-2-generate-a-support-response","title":"Example 2: Generate a support response","text":"<pre><code># First, get the analysis\nANALYSIS=$(curl -s -X POST \"http://localhost:8080/analyze-ticket\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"customer_id\": \"CUST456\",\n    \"message\": \"My app keeps crashing when I try to upload photos.\"\n  }')\n\n# Then generate a response\ncurl -X POST \"http://localhost:8080/generate-response\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"request\\\": {\n      \\\"customer_id\\\": \\\"CUST456\\\",\n      \\\"message\\\": \\\"My app keeps crashing when I try to upload photos.\\\"\n    },\n    \\\"analysis\\\": $ANALYSIS\n  }\"\n</code></pre> <p>By combining FastAPI's async capabilities with Outlines' structured generation, you can build robust APIs that leverage large language models.</p>"},{"location":"guide/fastapi_vllm_deployment/#using-alternative-backends-sglang-and-tgi","title":"Using Alternative Backends: SGLang and TGI","text":"<p>One of the key advantages of Outlines is its unified API across different inference backends. You can easily switch from vLLM to SGLang or TGI with minimal code changes - just modify the model initialization in the <code>lifespan</code> function.</p>"},{"location":"guide/fastapi_vllm_deployment/#using-sglang-instead-of-vllm","title":"Using SGLang Instead of vLLM","text":"<p>To use SGLang, simply change the client initialization:</p> <pre><code>@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Initialize the async SGLang model on startup.\"\"\"\n    global async_model\n\n    client = openai.AsyncOpenAI(\n        base_url=\"http://localhost:30000/v1\",  # SGLang server URL\n        api_key=\"dummy\"\n    )\n    async_model = models.from_sglang(client)\n\n    yield\n\n    async_model = None\n</code></pre> <p>Start your SGLang server with:</p> <pre><code>python -m sglang.launch_server \\\n    --model-path meta-llama/Llama-2-7b-chat-hf \\\n    --port 30000\n</code></pre>"},{"location":"guide/fastapi_vllm_deployment/#using-tgi-instead-of-vllm","title":"Using TGI Instead of vLLM","text":"<p>For TGI (Text Generation Inference), use the Hugging Face client:</p> <pre><code>import huggingface_hub\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Initialize the async TGI model on startup.\"\"\"\n    global async_model\n\n    client = huggingface_hub.AsyncInferenceClient(\n        \"http://localhost:8080\"  # TGI server URL\n    )\n    async_model = models.from_tgi(client)\n\n    yield\n\n    async_model = None\n</code></pre> <p>Start your TGI server with:</p> <pre><code>docker run --gpus all -p 8080:80 \\\n    ghcr.io/huggingface/text-generation-inference:latest \\\n    --model-id meta-llama/Llama-2-7b-chat-hf\n</code></pre> <p>The rest of your FastAPI application - all the endpoints, error handling, and business logic - remains completely unchanged. This flexibility allows you to test different inference engines without rewriting your application.</p>"},{"location":"guide/getting_started/","title":"Getting Started","text":""},{"location":"guide/getting_started/#installation","title":"Installation","text":"<p>We recommend using <code>uv</code> to install Outlines. You can find <code>uv</code> installation instructions here.</p> <pre><code>uv pip install 'outlines[transformers]'\n</code></pre> <p>or the classic <code>pip</code>:</p> <pre><code>pip install 'outlines[transformers]'\n</code></pre> <p>For more information, see the installation guide.</p>"},{"location":"guide/getting_started/#creating-a-model","title":"Creating a Model","text":"<p>Outlines contains a variety of models that wrap LLM inference engines/clients. For each of them, you need to install the model's associated library as described in the installation guide.</p> <p>The full list of available models along with detailed explanation on how to use them can be found in the models page of the Features section of the documentation.</p> <p>For a quick start, you can find below an example of how to initialize all supported models in Outlines:</p> vLLMOllamaOpenAITransformersllama.cppGeminimlx-lmSgLangTGIvLLM (offline) <pre><code>import outlines\nfrom openai import OpenAI\n\n# You must have a separate vLLM server running\n# Create an OpenAI client with the base URL of the VLLM server\nopenai_client = OpenAI(base_url=\"http://localhost:11434/v1\")\n\n# Create an Outlines model\nmodel = outlines.from_vllm(openai_client, \"microsoft/Phi-3-mini-4k-instruct\")\n</code></pre> <pre><code>import outlines\nfrom ollama import Client\n\n# Create an Ollama client\nollama_client = Client()\n\n# Create an Outlines model, the model must be available on your system\nmodel = outlines.from_ollama(ollama_client, \"tinyllama\")\n</code></pre> <pre><code>import outlines\nfrom openai import OpenAI\n\n# Create an OpenAI client instance\nopenai_client = OpenAI()\n\n# Create an Outlines model\nmodel = outlines.from_openai(openai_client, \"gpt-4o\")\n</code></pre> <pre><code>import outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Define the model you want to use\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n\n# Create a HuggingFace model and tokenizer\nhf_model = AutoModelForCausalLM.from_pretrained(model_name)\nhf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create an Outlines model\nmodel = outlines.from_transformers(hf_model, hf_tokenizer)\n</code></pre> <pre><code>import outlines\nfrom llama_cpp import Llama\n\n# Model to use, it will be downloaded from the HuggingFace hub\nrepo_id = \"TheBloke/Llama-2-13B-chat-GGUF\"\nfile_name = \"llama-2-13b-chat.Q4_K_M.gguf\"\n\n# Create a Llama.cpp model\nllama_cpp_model = Llama.from_pretrained(repo_id, file_name)\n\n# Create an Outlines model\nmodel = outlines.from_llamacpp(llama_cpp_model)\n</code></pre> <pre><code>import outlines\nfrom google.generativeai import GenerativeModel\n\n# Create a Gemini client\ngemini_client = GenerativeModel()\n\n# Create an Outlines model\nmodel = outlines.from_gemini(gemini_client, \"gemini-1-5-flash\")\n</code></pre> <pre><code>import outlines\nimport mlx_lm\n\n# Create an MLXLM model with the output of mlx_lm.load\n# The model will be downloaded from the HuggingFace hub\nmodel = outlines.from_mlxlm(\n    *mlx_lm.load(\"mlx-community/SmolLM-135M-Instruct-4bit\")\n)\n</code></pre> <pre><code>import outlines\nfrom openai import OpenAI\n\n# You must have a separate SgLang server running\n# Create an OpenAI client with the base URL of the SgLang server\nopenai_client = OpenAI(base_url=\"http://localhost:11434/v1\")\n\n# Create an Outlines model\nmodel = outlines.from_sglang(openai_client)\n</code></pre> <pre><code># SgLang\n\nimport outlines\nfrom huggingface_hub import InferenceClient\n\n# You must have a separate TGI server running\n# Create an InferenceClient client with the base URL of the TGI server\ntgi_client = InferenceClient(\"http://localhost:8080\")\n\n# Create an Outlines model\nmodel = outlines.from_tgi(tgi_client)\n</code></pre> <pre><code>import outlines\nfrom vllm import LLM\n\n# Create a vLLM model\nvllm_model = LLM(\"microsoft/Phi-3-mini-4k-instruct\")\n\n# Create an Outlines model\nmodel = outlines.from_vllm_offline(vllm_model)\n</code></pre>"},{"location":"guide/getting_started/#generating-text","title":"Generating Text","text":"<p>Once you have created the Outlines model for your inference engine/client, you are already all set to generate text! Models are callable such that you can simply call them with a text prompt. For instance:</p> <pre><code>model = &lt;your_model_as_defined_above&gt;\n\n# Call the model to generate text\nresult = model(\"Write a short story about a cat.\")\nprint(result) # 'In a quiet village where the cobblestones hummed softly beneath the morning mist...'\n</code></pre> <p>Most models also support streaming through the use of a <code>streaming</code> method. You can directly use with a prompt just like regular text generation. For instance:</p> <pre><code>model = &lt;your_model_as_defined_above&gt;\n\n# Stream text\nfor chunk in model.streaming(\"Write a short story about a cat.\")\n    print(chunk) # 'In ...'\n</code></pre>"},{"location":"guide/getting_started/#structured-generation","title":"Structured Generation","text":"<p>Outlines follows a simple pattern that mirrors Python's own type system for structured outputs. Simply specify the desired output type as you would when using type hinting with a function, and Outlines will ensure your data matches that structure exactly.</p> <p>Supported output types can be organized in 5 categories:</p> <ul> <li>Basic Types: <code>int</code>, <code>float</code>, <code>bool</code>...</li> <li>Multiple Choices: using <code>Literal</code> or <code>Enum</code></li> <li>JSON Schemas: using a wide range of possible objects including Pydantic models and dataclasses</li> <li>Regex: through the Outlines's <code>Regex</code> object</li> <li>Context-free Grammars: through the Outlines's <code>CFG</code> object</li> </ul> <p>Consult the section on Output Types in the features documentation for more detailed information on all supported types for each output type category.</p> <p>In the meantime, you can find below examples of using each of the five output type categories:</p> Basic TypesMultiple ChoiceJSON SchemasRegexContext-free Grammars <pre><code>model = &lt;your_model_as_defined_above&gt;\n\n# Generate an integer\nresult = model(\"How many countries are there in the world?\", int)\nprint(result) # '200'\n</code></pre> <pre><code>from enum import Enum\n\n# Define our multiple choice output type\nclass PizzaOrBurger(Enum):\n    pizza = \"pizza\"\n    burger = \"burger\"\n\nmodel = &lt;your_model_as_defined_above&gt;\n\n# Generate text corresponding to either of the choices defined above\nresult = model(\"What do you want to eat, a pizza or a burger?\", PizzaOrBurger)\nprint(result) # 'pizza'\n</code></pre> <pre><code>from datetime import date\nfrom typing import Dict, List, Union\nfrom pydantic import BaseModel\n\nmodel = &lt;your_model_as_defined_above&gt;\n\n# Define the class we will use as an output type\nclass Character(BaseModel):\n    name: str\n    birth_date: date\n    skills: Union[Dict, List[str]]\n\n# Generate a character\nresult = model(\"Create a character\", Character)\nprint(result) # '{\"name\": \"Aurora\", \"birth_date\": \"1990-06-15\", \"skills\": [\"Stealth\", \"Diplomacy\"]}'\nprint(Character.model_validate_json(result)) # name=Aurora birth_date=datetime.date(1990, 6, 15) skills=['Stealth', 'Diplomacy']\n</code></pre> <pre><code>from outlines.types import Regex\n\nmodel = &lt;your_model_as_defined_above&gt;\n\n# Define our regex for a 3 digit number\noutput_type = Regex(r\"[0-9]{3}\")\n\n# Generate the number\nresult = model(\"Write a 3 digit number\", output_type)\nprint(result) # '236'\n</code></pre> <pre><code>from outlines.types import CFG\n\nmodel = &lt;your_model_as_defined_above&gt;\n\n# Define your Lark grammar as string\narithmetic_grammar = \"\"\"\n    ?start: sum\n\n    ?sum: product\n        | sum \"+\" product   -&gt; add\n        | sum \"-\" product   -&gt; sub\n\n    ?product: atom\n        | product \"*\" atom  -&gt; mul\n        | product \"/\" atom  -&gt; div\n\n    ?atom: NUMBER           -&gt; number\n        | \"-\" atom         -&gt; neg\n        | \"(\" sum \")\"\n\n    %import common.NUMBER\n    %import common.WS_INLINE\n\n    %ignore WS_INLINE\n\"\"\"\n\n# Generate an arithmetic operation\nresult = model(\"Write an arithmetic operation\", CFG(grammar_string))\nprint(result) # '2 + 3'\n</code></pre> <p>It's important to note that not all output types are available for all models due to limitations in the underlying inference engines. The Models section of the features documentation includes a features matrix that summarize the availability of output types.</p>"},{"location":"guide/getting_started/#generators","title":"Generators","text":"<p>Generators are an important type of objects in Outlines that are used to encapsulate a model and an output type. After having created a generator, you can call it using a similar interface to a model and it will generate text conforming to the output type you initially provided.</p> <p>This feature is useful if you want to generate text several times for given model and output type. Not only does it prevent having to include the same output type at each call, but it also allows us to compile the output type only once instead of doing it at each generation (which is important for local models as this operation can be expensive).</p> <p>For instance:</p> <pre><code>from typing import Literal\nfrom outlines import Generator\n\nmodel = &lt;your_model_as_defined_above&gt;\n\n# Create a generator\ngenerator = Generator(model, Literal[\"pizza\", \"burger\"])\n\n# Call it as you would call a model\nresult = generator(\"What do you want to eat, a pizza or a burger?\")\nprint(result) # pizza\n</code></pre> <p>You can find more information on generators in the dedicated page on Generators in the features documentation.</p>"},{"location":"guide/getting_started/#other-features","title":"Other features","text":"<p>On top of more detailed explanation on the concepts already discussed here, the Features section of the documentation contains information on additional Outlines features such as applications, prompt templates, the regex DSL...</p>"},{"location":"guide/installation/","title":"Installation","text":""},{"location":"guide/installation/#dependency-management","title":"Dependency Management","text":"<p>We recommend using modern Python packaging tools such as <code>uv</code> for managing python dependencies.</p>"},{"location":"guide/installation/#uv-recommended","title":"uv (Recommended)","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install Outlines\nuv venv\nsource .venv/bin/activate\nuv pip install outlines\n</code></pre> <p>or with pip:</p> <pre><code>pip install outlines\n</code></pre>"},{"location":"guide/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>To use Outlines models, you need to install the Python libraries for the associated inference engines/clients. Such libraries are not part of the general installation as you should only install the libraries needed for the specific models you want to use.</p> <p>Outlines models with the installation of their associated additional depencies:</p> <ul> <li>Anthropic: <code>pip install anthropic</code></li> <li>Dottxt: <code>pip install dottxt</code></li> <li>Gemini: <code>pip install google-generativeai</code></li> <li>Llamacpp: <code>pip install llama-cpp-python</code></li> <li>Mlx-lm: <code>pip install mlx mlx-lm</code></li> <li>Ollama: <code>pip install ollama</code> (after having downloaded Ollama in your system)</li> <li>OpenAI: <code>pip install openai</code></li> <li>SGLang: <code>pip install openai</code></li> <li>TGI: <code>pip install huggingface_hub</code></li> <li>Transformers: <code>pip install transformers</code></li> <li>TransformersMultiModal: <code>pip install transformers</code></li> <li>vLLM (online server): <code>pip install openai</code></li> <li>vLLM (offline): <code>pip install vllm</code></li> </ul> <p>If you encounter any problems using Outlines with these libraries, take a look at their installation instructions. The installation of <code>openai</code> and <code>transformers</code> should be straightforward, but other libraries have specific hardware requirements.</p> <p>Hardware Requirements</p> <p>If you are using a local model, your model may require specific hardware. Please check the documentation for these libraries.</p> <p>Some libraries like <code>vllm</code> and <code>llama-cpp-python</code> require specific hardware, such as a compatible GPU. <code>mlx-lm</code> on its side is designed for Apple Silicon, so it may not be appropriate for your use case if you are on a different platform.</p>"},{"location":"guide/installation/#bleeding-edge","title":"Bleeding Edge","text":"<p>You can install the latest version of Outlines from the repository's <code>main</code> branch:</p> <pre><code>pip install git+https://github.com/dottxt-ai/outlines.git@main\n</code></pre> <p>This can be useful, for instance, when a fix has been merged but not yet released.</p>"},{"location":"guide/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing documentation for instructions on how to install Outlines for development, including an example using the <code>dot-install</code> method for one of the backends.</p>"},{"location":"guide/migration/","title":"Outlines 1.0 migration guide","text":"<p>Outlines 1.0 introduces some breaking changes that affect the way you use the library. You are likely concerned by all of the following sections, so please read this document carefully until the end.</p> <p>This guide will help you migrate your code to the new version.</p> <p>All previous functionalities will be supported until Outlines version 1.1.0, but a warning message will be displayed to remind you to migrate your code and provide instructions to help you do so. Please migrate your code to the v1 as soon as possible.</p>"},{"location":"guide/migration/#removed-or-modified-features","title":"Removed or modified features","text":"<ul> <li>Generate functions</li> <li>Models</li> <li>Samplers</li> <li>Functions</li> <li>Text generation return types</li> <li>Inference arguments</li> </ul>"},{"location":"guide/migration/#generate-functions","title":"Generate functions","text":"<p>The whole <code>generate</code> module has been removed. That includes the functions <code>generate.cfg</code>, <code>generate.choice</code>, <code>generate.format</code>,<code>generate.fsm</code>, <code>generate.json</code>, <code>generate.regex</code> and <code>generate.text</code>.</p> <p>You should replace these functions by the <code>Generator</code> object along with the right output type as an argument (on top of the model). The output type can either be a python type or be an object from the <code>outlines.types</code> module. You can find more information about the output types in the Output Types section of the features documentation.</p> <p>Associated v1 output types for each deprecated function: - <code>generate.cfg</code> -&gt; <code>outlines.types.CFG</code> - <code>generate.choice</code> -&gt; <code>typing.Literal</code> or <code>typing.Union</code> - <code>generate.format</code> -&gt; native python types (<code>str</code>, <code>int</code> etc.) - <code>generate.fsm</code> -&gt; <code>outlines.types.FSM</code> - <code>generate.json</code> -&gt; <code>pydantic.BaseModel</code>, <code>typing.TypedDict</code>, <code>dataclasses.dataclass</code>, <code>genson.schema.SchemaBuilder</code> or <code>outlines.types.JsonSchema</code> - <code>generate.regex</code> -&gt; <code>outlines.types.Regex</code> - <code>generate.text</code> -&gt; no output type (<code>None</code>)</p> <p>For instance, instead of:</p> <pre><code>from outlines import generate\n\nmodel = ...\ngenerator = generate.choice(model, [\"foo\", \"bar\"])\n</code></pre> <p>You should now use:</p> <pre><code>from typing import Literal\nfrom outlines import Generator\n\nmodel = ...\ngenerator = Generator(model, Literal[\"foo\", \"bar\"])\n</code></pre>"},{"location":"guide/migration/#models","title":"Models","text":"<p>The model classes found in the <code>outlines.models</code> module are maintained but there are a few important changes to be aware of.</p> <p>The functions used to created a model have been replaced by equivalent functions named with a <code>from_</code> prefix. The function <code>outlines.models.transformers</code> has been replaced by <code>outlines.from_transformers</code> for instance. On top of this change of name, the arguments have been modified. You should refer to the models documentation for more details, but the overall idea is that you now need to provide a model/client instance from the inference library the Outlines model is wrapping.</p> <p>For instance, instead of:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\n    repo_id=\"M4-ai/TinyMistral-248M-v2-Instruct-GGUF\",\n    filename=\"TinyMistral-248M-v2-Instruct.Q4_K_M.gguf\",\n    chat_format=\"qwen\",\n)\n</code></pre> <p>You should now do:</p> <pre><code>from llama_cpp import Llama\nfrom outlines import from_llamacpp\n\nllamacpp_model = Llama.from_pretrained(\n    repo_id=\"M4-ai/TinyMistral-248M-v2-Instruct-GGUF\",\n    filename=\"TinyMistral-248M-v2-Instruct.Q4_K_M.gguf\",\n    chat_format=\"qwen\",\n)\nmodel = from_llamacpp(llamacpp_model)\n</code></pre> <p>The <code>load_lora</code> methods that are present on the <code>VLLM</code> and <code>LlamaCpp</code> models have been removed. You should now handle lora loading through the <code>Llama</code> instance in the case of the <code>LlamaCpp</code> model or provide it as a keyword argument when calling the model in the case of the <code>VLLM</code> model.</p> <p>For instance, instead of:</p> <pre><code>from outlines import from_vllm\nfrom vllm import LLM\n\nmodel = from_vllm(\n    LLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n)\nmodel.load_lora(\"path/to/lora/file\")\n\nresponse = model(\"foo\")\n</code></pre> <p>You should now do:</p> <pre><code>from outlines import from_vllm\nfrom vllm import LLM\nfrom vllm.lora.request import LoRARequest\n\nmodel = from_vllm(\n    LLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n)\nlora_request = LoRARequest(\"path/to/lora/file\", 1, \"path/to/lora/file\")\n\nresponse = model(\"foo\", lora_request=lora_request)\n</code></pre> <p>The <code>ExLlamav2</code> model has been removed without replacement. This inference library is not fully compatible with Outlines, so we decided to remove it. You can still use it until final deprecation, but we recommend you to migrate to a different inference library right now.</p>"},{"location":"guide/migration/#samplers","title":"Samplers","text":"<p>The <code>outlines.samplers</code> module has been removed without replacement. You should now use the arguments of the inference library model to control the sampling. Depending on the model you use, this could be done at initialization or when calling the model to generate text (so when calling the outlines model or a generator).</p> <p>For instance, instead of:</p> <pre><code>from outlines import generate\n\nmodel = &lt;transformers_model&gt;\n\ngenerator = generate.text(model, samplers.beam_search(2))\nresponse = generator(\"foo\")\n</code></pre> <p>You should now do:</p> <pre><code>from outlines import Generator\n\nmodel = &lt;transformers_model&gt;\n\ngenerator = Generator(model)\nresponse = generator(\"foo\", num_beams=2)\n</code></pre>"},{"location":"guide/migration/#functions","title":"Functions","text":"<p>The <code>outlines.function</code> module has been removed. It is replaced by the <code>outlines.applications</code> module. An <code>Application</code> serves a similar purpose as a <code>Function</code>: it encapsulates a prompt template and an output type. A difference is that can <code>Application</code> is not instantiated with a model name. Instead, you should provide a model instance along with the prompt when calling it.</p> <p>For instance, instead of:</p> <pre><code>from outlines import Function\n\nprompt_template = ...\noutput_type = ...\n\nfn = Function(\n    prompt_template,\n    output_type,\n    \"hf-internal-testing/tiny-random-GPTJForCausalLM\",\n)\n\nresult = fn(\"foo\")\n</code></pre> <p>You should now do:</p> <pre><code>from outlines import Application\n\nprompt_template = ...\noutput_type = ...\n\napplication = Application(\n    prompt_template,\n    output_type,\n)\n\nmodel = ...\n\nresult = application(model, \"foo\")\n</code></pre>"},{"location":"guide/migration/#text-generation-return-types","title":"Text generation return types","text":"<p>In the previous version of Outlines, the return type of the generators depended on the output type provided. For instance, if you passed a Pydantic model to the <code>generate.json</code> function, the return type was a Pydantic model instance. In the v1, the return type of a generator is always a <code>str</code>, the raw text generated by the model. You are responsible for parsing the text into the desired format.</p> <p>For instance, instead of:</p> <pre><code>from pydantic import BaseModel\nfrom outlines import generate\n\nclass Foo(BaseModel):\n    bar: str\n\nmodel = ...\n\ngenerator = generate.json(model, Foo)\n\nresult = generator(\"foo\")\nprint(result.bar)\n</code></pre> <p>You should now do:</p> <pre><code>from pydantic import BaseModel\nfrom outlines import Generator\n\nclass Foo(BaseModel):\n    bar: str\n\nmodel = ...\n\ngenerator = Generator(model, Foo)\n\nresult = generator(\"foo\")\nresult = Foo.model_validate_json(result) # parse the text into the Pydantic model instance\nprint(result.bar)\n</code></pre> <p>The Output Types section of the features documentation includes extensive details on available output types.</p>"},{"location":"guide/migration/#inference-arguments","title":"Inference arguments","text":"<p>In the previous version of Outlines, some of the inference arguments were standardized across the models and were provided as positional arguments to the generator or through the sampling params dictionary. Additionally, various default values were added by outlines to the inference library models. This is no longer the case. You should refer to the documentation of the inference library you use to find the right arguments for your use case and pass them as keyword arguments to the outlines generator when calling it.</p> <p>For instance, instead of:</p> <pre><code>from outlines import generate\n\nmodel = &lt;transformers_model&gt;\n\ngenerator = generate.text(model)\n\nresult = generator(\"foo\", 256, \".\", 10) # 256 tokens, stop at \".\" and seed 10\n</code></pre> <p>You should now do:</p> <pre><code>from outlines import Generator\n\nmodel = &lt;transformers_model&gt;\n\ngenerator = Generator(model)\n\nresult = generator(\"foo\", max_new_tokens=256, stop_strings=\".\", seed=10)\n</code></pre>"},{"location":"guide/selecting_an_inference_backend/","title":"Selecting an inference backend","text":"<p>This guide should provide a general overview of the available models in the API reference.</p>"},{"location":"guide/selecting_an_inference_backend/#models","title":"Models","text":"<ul> <li>Anthropic</li> </ul>"},{"location":"guide/vlm/","title":"Vision-Language Models with Outlines","text":"<p>This guide demonstrates how to use Outlines with vision-language models. Vision-language models can process both text and images, allowing for tasks like image captioning, visual question answering, and more.</p> <p>We will be using the Pixtral-12B model from Mistral to take advantage of some of its visual reasoning capabilities and a workflow to generate a multistage atomic caption.</p>"},{"location":"guide/vlm/#setup","title":"Setup","text":"<p>First, we need to install the necessary dependencies. In addition to Outlines, we\"ll need to install the transformers library and any specific requirements for the vision-language model we\"ll be using.</p> <pre><code>pip install outlines transformers torch pillow\n</code></pre>"},{"location":"guide/vlm/#initializing-the-model","title":"Initializing the Model","text":"<p>We\"ll use the <code>outlines.from_transformers</code> function to initialize our vision-language model. For this function to return a vision multi-modal model we need to pass in a transformers model and a transformers processor that can handle both text and image inputs. Today we\"ll be using the Pixtral model with the AutoProcessor.</p> <pre><code>import outlines\nimport torch\nfrom transformers import (\n    AutoProcessor,\n    LlavaForConditionalGeneration\n)\n\nmodel_name=\"mistral-community/pixtral-12b\" # original magnet model is able to be loaded without issue\nmodel_class=LlavaForConditionalGeneration\nprocessor_class=AutoProcessor\n\ndef get_vision_model(model_name: str, model_class, processor_class):\n    model_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"attn_implementation\": \"flash_attention_2\",\n        \"device_map\": \"auto\",\n    }\n    processor_kwargs = {\n        \"device\": \"cuda\",\n    }\n\n    model = outlines.from_transformers(\n        model_class.from_pretrained(model_name, **model_kwargs),\n        processor_class.from_pretrained(model_name, **processor_kwargs),\n    )\n    return model\nmodel = get_vision_model(model_name, model_class, processor_class)\n</code></pre>"},{"location":"guide/vlm/#defining-the-schema","title":"Defining the Schema","text":"<p>Next, we will define a schema for the output we expect from our vision multi-modal model. This schema will help structure the model's responses. We use the <code>outlines.Generator</code> object to create a generator for our schema that will then be called with our prompt and images.</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel, Field, confloat, constr\nfrom pydantic.types import StringConstraints, PositiveFloat\nfrom typing import List\nfrom typing_extensions import Annotated\n\nclass TagType(Enum):\n    ENTITY = \"Entity\"\n    RELATIONSHIP = \"Relationship\"\n    STYLE = \"Style\"\n    ATTRIBUTE = \"Attribute\"\n    COMPOSITION = \"Composition\"\n    CONTEXTUAL = \"Contextual\"\n    TECHNICAL = \"Technical\"\n    SEMANTIC = \"Semantic\"\n\nclass ImageTag(BaseModel):\n    tag: Annotated[\n        constr(min_length=1, max_length=30),\n        Field(\n            description=(\n                \"Descriptive keyword or phrase representing the tag.\"\n            )\n        )\n    ]\n    category: TagType\n    confidence: Annotated[\n        confloat(le=1.0),\n        Field(\n            description=(\n                \"Confidence score for the tag, between 0 (exclusive) and 1 (inclusive).\"\n            )\n        )\n    ]\n\nclass ImageData(BaseModel):\n    tags_list: List[ImageTag] = Field(..., min_items=8, max_items=20)\n    short_caption: Annotated[str, StringConstraints(min_length=10, max_length=150)]\n    dense_caption: Annotated[str, StringConstraints(min_length=100, max_length=2048)]\n\nimage_data_generator = outlines.Generator(model, ImageData)\n</code></pre> <p>This schema defines the structure for image tags, including categories like Entity, Relationship, Style, etc., as well as short and dense captions.</p>"},{"location":"guide/vlm/#preparing-the-prompt","title":"Preparing the Prompt","text":"<p>We'll create a prompt that instructs the model on how to analyze the image and generate the structured output:</p> <pre><code>pixtral_instruction = \"\"\"\n&lt;s&gt;[INST]\n&lt;Task&gt;You are a structured image analysis agent. Generate comprehensive tag list, caption, and dense caption for an image classification system.&lt;/Task&gt;\n&lt;TagCategories requirement=\"You should generate a minimum of 1 tag for each category.\" confidence=\"Confidence score for the tag, between 0 (exclusive) and 1 (inclusive).\"&gt;\n- Entity : The content of the image, including the objects, people, and other elements.\n- Relationship : The relationships between the entities in the image.\n- Style : The style of the image, including the color, lighting, and other stylistic elements.\n- Attribute : The most important attributes of the entities and relationships in the image.\n- Composition : The composition of the image, including the arrangement of elements.\n- Contextual : The contextual elements of the image, including the background, foreground, and other elements.\n- Technical : The technical elements of the image, including the camera angle, lighting, and other technical details.\n- Semantic : The semantic elements of the image, including the meaning of the image, the symbols, and other semantic details.\n&lt;Examples note=\"These show the expected format as an abstraction.\"&gt;\n{\n  \"tags_list\": [\n    {\n      \"tag\": \"subject 1\",\n      \"category\": \"Entity\",\n      \"confidence\": 0.98\n    },\n    {\n      \"tag\": \"subject 2\",\n      \"category\": \"Entity\",\n      \"confidence\": 0.95\n    },\n    {\n      \"tag\": \"subject 1 runs from subject 2\",\n      \"category\": \"Relationship\",\n      \"confidence\": 0.90\n    },\n   }\n&lt;/Examples&gt;\n&lt;/TagCategories&gt;\n&lt;ShortCaption note=\"The short caption should be a concise single sentence caption of the image content with a maximum length of 100 characters.\"&gt;\n&lt;DenseCaption note=\"The dense caption should be a descriptive but grounded narrative paragraph of the image content with high quality narrative prose. It should incorporate elements from each of the tag categories to provide a broad dense caption\"&gt;\n[IMG]&lt;image&gt;[/INST]\n\"\"\".strip()\n</code></pre> <p>This prompt provides detailed instructions to the model on how to generate comprehensive tag lists, captions, and dense captions for image analysis. Because of the ordering of the instructions the original tag generation serves as a sort of visual grounding for the captioning task, reducing the amount of manual post processing required. It is essential to include the  tag in the prompt at the location where the image will be inserted."},{"location":"guide/vlm/#generating-structured-output","title":"Generating Structured Output","text":"<p>Now we can use our model to generate structured output based on an input image:</p> <pre><code>from io import BytesIO\nfrom urllib.request import urlopen\nfrom PIL import Image\n\ndef img_from_url(url):\n    img_byte_stream = BytesIO(urlopen(url).read())\n    return Image.open(img_byte_stream).convert(\"RGB\")\n\nimage_url=\"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\"\nimage= img_from_url(image_url)\nresult = image_data_generator({\n    \"text\": pixtral_instruction,\n    \"images\": image\n})\nprint(result)\n</code></pre> <p>This code loads an image from a URL, passes it to our vision multi-modal model along with the instruction prompt, and generates a structured output based on the defined schema. We end up with an output like this, ready to be used for the next stage in your pipeline:</p> <pre><code>{\"tags_list\": [\n  {\n    \"tag\": \"astronaut\",\n    \"category\": &lt;TagType.ENTITY: \"Entity\"&gt;,\n    \"confidence\": 0.99\n  },\n  {\"tag\": \"moon\", \"category\": &lt;TagType.ENTITY: \"Entity\"&gt;, \"confidence\": 0.98},\n  {\n    \"tag\": \"space suit\",\n    \"category\": &lt;TagType.ATTRIBUTE: \"Attribute\"&gt;,\n    \"confidence\": 0.97\n  },\n  {\n    \"tag\": \"lunar module\",\n    \"category\": &lt;TagType.ENTITY: \"Entity\"&gt;,\n    \"confidence\": 0.95\n  },\n  {\n    \"tag\": \"shadow of astronaut\",\n    \"category\": &lt;TagType.COMPOSITION: \"Composition\"&gt;,\n    \"confidence\": 0.95\n  },\n  {\n    \"tag\": \"footprints in moon dust\",\n    \"category\": &lt;TagType.CONTEXTUAL: \"Contextual\"&gt;,\n    \"confidence\": 0.93\n  },\n  {\n    \"tag\": \"low angle shot\",\n    \"category\": &lt;TagType.TECHNICAL: \"Technical\"&gt;,\n    \"confidence\": 0.92\n  },\n  {\n    \"tag\": \"human first steps on the moon\",\n    \"category\": &lt;TagType.SEMANTIC: \"Semantic\"&gt;,\n    \"confidence\": 0.95\n  }],\n  \"short_caption\": \"First man on the Moon\",\n  \"dense_caption\": \"The figure clad in a pristine white space suit, emblazoned with the American flag, stands powerfully on the moon's desolate and rocky surface. The lunar module, a workhorse of space engineering, looms in the background, its metallic legs sinking slightly into the dust where footprints and tracks from the mission's journey are clearly visible. The photograph captures the astronaut from a low angle, emphasizing his imposing presence against the desolate lunar backdrop. The stark contrast between the blacks and whiteslicks of lost light and shadow adds dramatic depth to this seminal moment in human achievement.\"\n}\n</code></pre>"},{"location":"guide/vlm/#conclusion","title":"Conclusion","text":"<p>This guide demonstrated how Outlines enables structured output generation with vision-language models. With the techniques shown above, you can build:</p> <ul> <li>Content Management Systems: Automatically tag and categorize visual content with structured metadata that can be directly stored in databases, enabling powerful search and filtering capabilities</li> <li>Accessibility Tools: Generate rich, structured descriptions of images that can be adapted for different contexts - from brief alt-text to detailed scene descriptions for screen readers</li> <li>Quality Assurance Pipelines: Validate visual content against specific criteria by extracting structured attributes and checking them against business rules</li> </ul>"}]}