{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"installation/","title":"Installation","text":"<p>You can install Outlines with <code>pip</code>:</p> <pre><code>pip install outlines\n</code></pre> <p>Outlines supports OpenAI, Transformers, Mamba, llama.cpp, and ExLlamaV2, but you will need to install them manually:</p> <pre><code>pip install openai\npip install transformers datasets accelerate torch\npip install llama-cpp-python\npip install exllamav2 transformers torch\npip install mamba_ssm transformers torch\npip install vllm\n</code></pre> <p>If you encounter any problems using Outlines with these libraries, take a look at their installation instructions. The installation of <code>openai</code> and <code>transformers</code> should be straightforward, but other libraries have specific hardware requirements.</p>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Outlines provides multiple optional dependency sets to support different backends and use cases. You can install them as needed using:</p> <ul> <li><code>pip install \"outlines[vllm]\"</code> for vLLM, optimized for high-throughput inference.</li> <li><code>pip install \"outlines[transformers]\"</code> for Hugging Face Transformers.</li> <li><code>pip install \"outlines[mlx]\"</code> for MLX-LM, optimized for Apple silicon.</li> <li><code>pip install \"outlines[openai]\"</code> to use OpenAI\u2019s API.</li> <li><code>pip install \"outlines[llamacpp]\"</code> for llama.cpp, a lightweight LLM inference engine.</li> <li><code>pip install \"outlines[exllamav2]\"</code> for ExLlamaV2, optimized for NVIDIA GPUs.</li> </ul>"},{"location":"installation/#bleeding-edge","title":"Bleeding Edge","text":"<p>You can install the latest version of Outlines from the repository's <code>main</code> branch:</p> <pre><code>pip install git+https://github.com/dottxt-ai/outlines.git@main\n</code></pre> <p>This can be useful, for instance, when a fix has been merged but not yet released.</p>"},{"location":"installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing documentation for instructions on how to install Outlines for development, including an example using the <code>dot-install</code> method for one of the backends.</p>"},{"location":"licence/","title":"Licence and citations","text":"<p>Outlines is licenced under the Apache 2.0 licence. To comply with the licence you need to add the following notice at the top every file that uses part of Outlines' code:</p> <pre><code>Copyright 2023- The Outlines developers\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre> <p>If you use Outlines in your work you can use the following citation:</p> <pre><code>@article{willard2023efficient,\n  title={Efficient Guided Generation for LLMs},\n  author={Willard, Brandon T and Louf, R{\\'e}mi},\n  journal={arXiv preprint arXiv:2307.09702},\n  year={2023}\n}\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>After installing Outlines, the fastest way to get to up to speed with the library is to get acquainted with its few core elements. We advise you to take a quick look at this page to see everything Outlines has to offer before diving in the documentation.</p>"},{"location":"quickstart/#core-elements","title":"Core elements","text":""},{"location":"quickstart/#models","title":"Models","text":"<p>The first step when writing a program with Outlines is to initialize a model. Weights will be loaded on the device at this step:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    device=\"cuda\"  # optional device argument, default is cpu\n)\n</code></pre> <p>Outlines supports a wide variety of inference engines and model weight types. More details on different models can be found in the Outlines Models documentation page.</p>"},{"location":"quickstart/#generation","title":"Generation","text":"<p>Once the model is initialized you can build an <code>outlines.generate</code> generator. This generator can be called with a prompt directly.</p> <p>(Outlines Structured Generation Full Documentation)</p> TextStructured <pre><code>generator = outlines.generate.text(model)\n\nresult = generator(\"Question: What's 2+2? Answer:\", max_tokens=100)\nprint(result)\n# The answer is 4\n\n# Outlines also supports streaming output\nstream = generator.stream(\"What's 2+2?\", max_tokens=4)\nfor i in range(5):\n    token = next(stream)\n    print(repr(token))\n# '2'\n# '+'\n# '2'\n# ' equals'\n# '4'\n</code></pre> <p>Along with typical language model generation behavior via, <code>outlines.generate.text</code>, Outlines supports structured generation, which guarantees the tokens generated by the model will follow a predefined structure. Structures can be defined by a regex pattern, JSON schema, python object type, or a Lark grammar defining a parsable language such as SQL or Python.</p> <p>Example: using pydantic to enforce a JSON schema</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr, conint\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: conint(gt=18, lt=99)\n    armor: (Enum('Armor', {'leather': 'leather', 'chainmail': 'chainmail', 'plate': 'plate'}))\n    strength: conint(gt=1, lt=100)\n\ngenerator = outlines.generate.json(model, Character)\n\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# Character(name='Zara', age=25, armor=&lt;Armor.leather: 'leather'&gt;, strength=85)\n</code></pre>"},{"location":"quickstart/#deploy-using-vllm-and-fastapi","title":"Deploy using vLLM and FastAPI","text":"<p>Outlines can be deployed as a LLM service using vLLM and FastAPI. The server supports asynchronous processing of incoming requests, and benefits from the performance of vLLM.</p> <p>First start the server:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>Or you can start the server with Outlines' official Docker image:</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used.</p> <p>You can then query the model in shell by passing a prompt and a JSON Schema specification for the structure of the output:</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"Question: What is a language model? Answer:\",\n        \"schema\": {\"type\": \"string\"}\n        }'\n</code></pre> <p>Or use the requests library from another python program. You can read the vLLM documentation for more details.</p>"},{"location":"quickstart/#utilities","title":"Utilities","text":""},{"location":"quickstart/#prompt-templates","title":"Prompt templates","text":"<p>Prompting can lead to messy code. Outlines' prompt functions are python functions that contain a template for the prompt in their docstring. We use a powerful templating language to allow you to loop over lists, dictionaries, add conditionals, etc. directly from the prompt. When called, a prompt function returns the rendered template:</p> <pre><code>from outlines import Template\n\nfew_shots = Template.from_string(\n    \"\"\"{{ instructions }}\n\n    Examples\n    --------\n\n    {% for example in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n\n    {% endfor %}\n    Question\n    --------\n\n    Q: {{ question }}\n    A:\n    \"\"\"\n)\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n# Please answer the following question following the examples\n\n# Examples\n# --------\n\n# Q: 2+2=?\n# A: 4\n\n# Q: 3+3=?\n# A: 6\n\n# Question\n# --------\n\n# Q: 4+4 = ?\n# A:\n</code></pre>"},{"location":"quickstart/#outlines-functions","title":"Outlines functions","text":"<p>Once you are done experimenting with a prompt and an output structure, it is useful to be able to encapsulate all of these in a single function that can be called from other parts of the program. This is what <code>outlines.Function</code> allows you to do:</p> function.pyCall a functionCall a function stored on GitHub <pre><code>from pydantic import BaseModel\n\nimport outlines\n\n\ntell_a_joke = outlines.Template.from_string(\n    \"\"\"Tell me a joke about {{ topic }}.\"\"\"\n)\n\nclass Joke(BaseModel):\n    setup: str\n    punchline: str\n\ngenerate_joke = outlines.Function(\n    tell_a_joke,\n    Joke,\n    \"microsoft/Phi-3-mini-4k-instruct\"\n)\n</code></pre> <pre><code>from .function import generate_joke\n\nresponse = generate_joke(\"baseball\")\n\n# haha\n# Joke(setup='Why was the baseball in a bad mood?', punchline='Because it got hit around a lot.')\n</code></pre> <p>You can load a function that is stored on a repository on GitHub directly from Outlines. Say <code>Someone</code> stores a function in <code>joke.py</code> at the root of the <code>TheirRepo</code> repository:</p> <p><pre><code>import outlines\n\njoke = outlines.Function.from_github(\"Someone/TheirRepo/joke\")\nresponse = joke(\"baseball\")\n</code></pre> It make it easier for the community to collaborate on the infinite number of use cases enabled by these models!</p>"},{"location":"quickstart/#going-further","title":"Going further","text":"<p>If you need more inspiration you can take a look at the cookbook or watch Remi Louf's AI Engineer World\u2019s Fair Presentation on Outlines. If you have any question, or requests for documentation please reach out to us on GitHub, Twitter or Discord.</p>"},{"location":"welcome/","title":"Welcome to Outlines!","text":"<p>Outlines is a Python library that allows you to use Large Language Model in a simple and robust way (with structured generation). It is built by .txt, and is already used in production by many companies.</p>"},{"location":"welcome/#what-models-do-you-support","title":"What models do you support?","text":"<p>We support Openai, but the true power of Outlines is unleashed with Open Source models available via the transformers, llama.cpp, exllama2, mlx-lm and vllm models. If you want to build and maintain an integration with another library, get in touch.</p>"},{"location":"welcome/#what-are-the-main-features","title":"What are the main features?","text":"<ul> <li> <p> Make LLMs generate valid JSON</p> <p>No more invalid JSON outputs, 100% guaranteed</p> <p> Generate JSON</p> </li> <li> <p> JSON mode for vLLM</p> <p>Deploy a LLM service using Outlines' JSON structured generation and vLLM</p> <p> Deploy outlines</p> </li> <li> <p> Make LLMs follow a Regex</p> <p>Generate text that parses correctly 100% of the time</p> <p> Guide LLMs</p> </li> <li> <p> Powerful Prompt Templating</p> <p>Better manage your prompts' complexity with prompt templating</p> <p> Learn more</p> </li> </ul>"},{"location":"welcome/#why-use-outlines","title":"Why use Outlines?","text":"<p>Outlines is built at .txt by engineers with decades of experience in software engineering, machine learning (Bayesian Statistics and NLP), and compilers. .txt is a VC-backed company fully focused on the topic of structured generation and is committed to make the community benefit from its experience.</p> <p>We are also open source veterans and have authored/maintained many libraries over the years: the Aesara and Pythological ecosystems, Blackjax and Hy among many others. .</p> <p>Outlines does not use unnecessary abstractions that tend to get in your way. We have a laser focus on reliable text generation with LLMs, a clear roadmap to push the state of the art in this area and a commitment to clean and robust code.</p> <p>And last but not least, unlike alternatives, Outlines' structured generation introduces no overhead during inference.</p>"},{"location":"welcome/#who-is-using-outlines","title":"Who is using Outlines?","text":"<p>Hundreds of organisations and the main LLM serving frameworks (vLLM, TGI, LoRAX, xinference, SGLang) are using Outlines. Some of the prominent companies and organizations that are using Outlines include:</p> <p> <p></p> <p>Organizations are included either because they use Outlines as a dependency in a public repository, or because of direct communication between members of the Outlines team and employees at these organizations.</p> <p>Still not convinced, read what people say about us. And make sure to take a look at what the community is building!</p>"},{"location":"welcome/#philosophy","title":"Philosophy","text":"<p>Outlines  is a library for neural text generation. You can think of it as a more flexible replacement for the <code>generate</code> method in the transformers library.</p> <p>Outlines  helps developers structure text generation to build robust interfaces with external systems. It provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.</p> <p>Outlines  provides robust prompting primitives that separate the prompting from the execution logic and lead to simple implementations of few-shot generations, ReAct, meta-prompting, agents, etc.</p> <p>Outlines  is designed as a library that is meant to be compatible the broader ecosystem, not to replace it. We use as few abstractions as possible, and generation can be interleaved with control flow, conditionals, custom Python functions and calls to other libraries.</p> <p>Outlines  is compatible with every auto-regressive model. It only interfaces with models via the next-token logits distribution.</p>"},{"location":"welcome/#outlines-people","title":"Outlines people","text":"<p>Outlines would not be what it is today without a community of dedicated developers:</p> <p> </p>"},{"location":"welcome/#acknowledgements","title":"Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard. It is now maintained by .txt.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/guide/","title":"Guide","text":""},{"location":"api/guide/#outlines.fsm.guide.CFGGuide","title":"<code>CFGGuide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Guide to generate text that is in the language of a context-free Lark grammar.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class CFGGuide(Guide):\n    \"\"\"Guide to generate text that is in the language of a context-free Lark grammar.\"\"\"\n\n    def __init__(self, cfg_string: str, tokenizer):\n        \"\"\"\n        Construct the PartialLark parser and set the empty initial_state (PartialParserState)\n        \"\"\"\n        warnings.warn(\n            \"Outlines' public *community-contributed* CFG structured generation is experimental. \"\n            \"Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\"\n        )\n\n        self.cfg_string = cfg_string\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.parser = PartialLark(\n            cfg_string,\n            parser=\"lalr\",\n            import_paths=[grammars.GRAMMAR_PATH],\n        )\n        self.initial_state = CFGState(\n            parser_state=self.parser.parse(\"\"), prev_token=None\n        )\n\n    def get_next_instruction(self, state: CFGState) -&gt; Instruction:\n        \"\"\"Return the next instruction for guided generation.\n\n        Current lazy approach:\n        - For each token in the vocabulary\n          - create a copy of the parsers state\n          - add the tokens to the parsers input text\n          - if valid, add token to returned tokens\n\n        Further refinements are necessary for performant text processing.\n\n        Parameters\n        ----------\n        state\n            The guides current PartialParserState, or None if complete\n\n        Returns\n        -------\n        A `Generate` instance that contains the model and the allowed token ids.\n\n        \"\"\"\n\n        if state.parser_state is None:\n            return Write(torch.tensor([self.eos_token_id]))\n\n        valid_tokens = list(\n            self.iter_valid_token_ids(state, self.tokenizer.vocabulary.values())\n        )\n        if len(valid_tokens) == 1:\n            return Write(torch.tensor(valid_tokens))\n        return Generate(torch.tensor(valid_tokens))\n\n    def iter_valid_token_ids(\n        self, state: CFGState, candidate_token_ids: list\n    ) -&gt; Generator[int, None, None]:\n        \"\"\"\n        Iterate over the given token_ids and yield those that are valid for the current parser state.\n\n        Parameters\n        ----------\n        parser_state\n            The current state of the parser, or None if complete.\n        token_ids\n            The list of token ids to check for validity.\n\n        Yields\n        ------\n        int\n            Valid token ids.\n        \"\"\"\n        if state.parser_state is None:\n            yield self.eos_token_id\n            return\n\n        for token_id in candidate_token_ids:\n            if token_id == self.eos_token_id:\n                if self.can_terminate_state(state):\n                    yield token_id\n            else:\n                try:\n                    self._get_parser_state_token_applied(state, int(token_id))\n                    yield token_id\n                except (\n                    ValueError,\n                    EOFError,\n                    UnexpectedToken,\n                    UnexpectedCharacters,\n                    DedentError,\n                ):\n                    pass\n\n    def get_next_state(self, state: CFGState, token_id: int) -&gt; CFGState:\n        \"\"\"\n        Update the state of the guide.\n        Decode the token_id, and calculate the new parser_state with the token applied.\n\n        Parameters\n        ----------\n        state\n            The guides current PartialParserState, or None if complete\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The guides new PartialParserState\n\n        \"\"\"\n        if state.parser_state is None or token_id == self.eos_token_id:\n            parser_state = None\n        else:\n            parser_state = self._get_parser_state_token_applied(state, int(token_id))\n        return CFGState(parser_state=parser_state, prev_token=token_id)\n\n    def _get_parser_state_token_applied(\n        self, state: CFGState, token_id: int\n    ) -&gt; PartialParserState:\n        \"\"\"\n        Don't mutate `parser_state`, copy to protect\n\n        Get the token string\n          - if first token in generation: tokenizer.decode (no leading whitespace)\n          - else: normalized (with possibly leading whitespace)\n\n        Don't allow empty (\"\") tokens, raise ValueError\n        \"\"\"\n        parser_state = copy.copy(state.parser_state)  # prevent side effects\n\n        # normalize\n        if state.prev_token is None:\n            new_token_str = self.tokenizer.decode([token_id])[0]\n        else:\n            prev_token_str = self.tokenizer.decode([[state.prev_token]])[0]\n            combined_token_str = self.tokenizer.decode([[state.prev_token, token_id]])[\n                0\n            ]\n            new_token_str = combined_token_str[len(prev_token_str) :]\n\n        if new_token_str == \"\":\n            raise ValueError(\"empty next token\")\n\n        # update parser with new token\n        parser_state.lexer.state.text += new_token_str\n        self.parser.parse_from_state(parser_state, is_end=False)\n\n        return parser_state\n\n    def is_final_state(self, state: CFGState) -&gt; bool:\n        # TODO: remove this method, use can_terminate_state and must_terminate_state\n        # here and in RegexGuide per https://github.com/dottxt-ai/outlines/issues/885\n        return self.can_terminate_state(state)\n\n    def can_terminate_state(self, state: CFGState) -&gt; bool:\n        \"\"\"Generation is allowed to terminate\"\"\"\n        if state.parser_state is not None:\n            try:\n                copy.copy(state.parser_state).feed_eof()\n            except UnexpectedToken:\n                return False\n        return True\n\n    def must_terminate_state(self, state: CFGState) -&gt; bool:\n        \"\"\"Generation must terminate, no legal continuations\"\"\"\n        return state.parser_state is None or set(state.parser_state.accepts()).issubset(\n            {\"$END\"}\n        )\n\n    def copy(self) -&gt; \"CFGGuide\":\n        \"\"\"Create a copy of the Guide.\"\"\"\n        return CFGGuide(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.__init__","title":"<code>__init__(cfg_string, tokenizer)</code>","text":"<p>Construct the PartialLark parser and set the empty initial_state (PartialParserState)</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def __init__(self, cfg_string: str, tokenizer):\n    \"\"\"\n    Construct the PartialLark parser and set the empty initial_state (PartialParserState)\n    \"\"\"\n    warnings.warn(\n        \"Outlines' public *community-contributed* CFG structured generation is experimental. \"\n        \"Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\"\n    )\n\n    self.cfg_string = cfg_string\n    self.tokenizer = tokenizer\n    self.eos_token_id = self.tokenizer.eos_token_id\n    self.parser = PartialLark(\n        cfg_string,\n        parser=\"lalr\",\n        import_paths=[grammars.GRAMMAR_PATH],\n    )\n    self.initial_state = CFGState(\n        parser_state=self.parser.parse(\"\"), prev_token=None\n    )\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.can_terminate_state","title":"<code>can_terminate_state(state)</code>","text":"<p>Generation is allowed to terminate</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def can_terminate_state(self, state: CFGState) -&gt; bool:\n    \"\"\"Generation is allowed to terminate\"\"\"\n    if state.parser_state is not None:\n        try:\n            copy.copy(state.parser_state).feed_eof()\n        except UnexpectedToken:\n            return False\n    return True\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the Guide.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def copy(self) -&gt; \"CFGGuide\":\n    \"\"\"Create a copy of the Guide.\"\"\"\n    return CFGGuide(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_instruction","title":"<code>get_next_instruction(state)</code>","text":"<p>Return the next instruction for guided generation.</p> <p>Current lazy approach: - For each token in the vocabulary   - create a copy of the parsers state   - add the tokens to the parsers input text   - if valid, add token to returned tokens</p> <p>Further refinements are necessary for performant text processing.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>CFGState</code> <p>The guides current PartialParserState, or None if complete</p> required <p>Returns:</p> Type Description <code>A `Generate` instance that contains the model and the allowed token ids.</code> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def get_next_instruction(self, state: CFGState) -&gt; Instruction:\n    \"\"\"Return the next instruction for guided generation.\n\n    Current lazy approach:\n    - For each token in the vocabulary\n      - create a copy of the parsers state\n      - add the tokens to the parsers input text\n      - if valid, add token to returned tokens\n\n    Further refinements are necessary for performant text processing.\n\n    Parameters\n    ----------\n    state\n        The guides current PartialParserState, or None if complete\n\n    Returns\n    -------\n    A `Generate` instance that contains the model and the allowed token ids.\n\n    \"\"\"\n\n    if state.parser_state is None:\n        return Write(torch.tensor([self.eos_token_id]))\n\n    valid_tokens = list(\n        self.iter_valid_token_ids(state, self.tokenizer.vocabulary.values())\n    )\n    if len(valid_tokens) == 1:\n        return Write(torch.tensor(valid_tokens))\n    return Generate(torch.tensor(valid_tokens))\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_state","title":"<code>get_next_state(state, token_id)</code>","text":"<p>Update the state of the guide. Decode the token_id, and calculate the new parser_state with the token applied.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>CFGState</code> <p>The guides current PartialParserState, or None if complete</p> required <code>token_id</code> <code>int</code> <p>The id of the token that was just generated.</p> required <p>Returns:</p> Type Description <code>The guides new PartialParserState</code> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def get_next_state(self, state: CFGState, token_id: int) -&gt; CFGState:\n    \"\"\"\n    Update the state of the guide.\n    Decode the token_id, and calculate the new parser_state with the token applied.\n\n    Parameters\n    ----------\n    state\n        The guides current PartialParserState, or None if complete\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The guides new PartialParserState\n\n    \"\"\"\n    if state.parser_state is None or token_id == self.eos_token_id:\n        parser_state = None\n    else:\n        parser_state = self._get_parser_state_token_applied(state, int(token_id))\n    return CFGState(parser_state=parser_state, prev_token=token_id)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.iter_valid_token_ids","title":"<code>iter_valid_token_ids(state, candidate_token_ids)</code>","text":"<p>Iterate over the given token_ids and yield those that are valid for the current parser state.</p> <p>Parameters:</p> Name Type Description Default <code>parser_state</code> <p>The current state of the parser, or None if complete.</p> required <code>token_ids</code> <p>The list of token ids to check for validity.</p> required <p>Yields:</p> Type Description <code>int</code> <p>Valid token ids.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def iter_valid_token_ids(\n    self, state: CFGState, candidate_token_ids: list\n) -&gt; Generator[int, None, None]:\n    \"\"\"\n    Iterate over the given token_ids and yield those that are valid for the current parser state.\n\n    Parameters\n    ----------\n    parser_state\n        The current state of the parser, or None if complete.\n    token_ids\n        The list of token ids to check for validity.\n\n    Yields\n    ------\n    int\n        Valid token ids.\n    \"\"\"\n    if state.parser_state is None:\n        yield self.eos_token_id\n        return\n\n    for token_id in candidate_token_ids:\n        if token_id == self.eos_token_id:\n            if self.can_terminate_state(state):\n                yield token_id\n        else:\n            try:\n                self._get_parser_state_token_applied(state, int(token_id))\n                yield token_id\n            except (\n                ValueError,\n                EOFError,\n                UnexpectedToken,\n                UnexpectedCharacters,\n                DedentError,\n            ):\n                pass\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.must_terminate_state","title":"<code>must_terminate_state(state)</code>","text":"<p>Generation must terminate, no legal continuations</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def must_terminate_state(self, state: CFGState) -&gt; bool:\n    \"\"\"Generation must terminate, no legal continuations\"\"\"\n    return state.parser_state is None or set(state.parser_state.accepts()).issubset(\n        {\"$END\"}\n    )\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.Guide","title":"<code>Guide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Base definition of a generation guide.</p> <p>A generation guide defines the behavior of a finite-state machine that guides a text generation procedure. Unlike the DFAs built from regular expressions guides can also emit a <code>Write</code> instructions which tells the model that it can append a sequence of tokens (or token word) instead of generating it.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class Guide(CoreGuide):\n    \"\"\"Base definition of a generation guide.\n\n    A generation guide defines the behavior of a finite-state machine that guides\n    a text generation procedure. Unlike the DFAs built from regular expressions\n    guides can also emit a `Write` instructions which tells the model that it can\n    append a sequence of tokens (or token word) instead of generating it.\n\n    \"\"\"\n\n    initial_state: Any\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.RegexGuide","title":"<code>RegexGuide</code>","text":"<p>               Bases: <code>RegexGuide</code></p> <p>Guide to generate text in the language of a regular expression. CoreRegexGuide with outlines cache</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class RegexGuide(CoreRegexGuide):\n    \"\"\"\n    Guide to generate text in the language of a regular expression.\n    CoreRegexGuide with outlines cache\n    \"\"\"\n\n    @classmethod\n    def from_regex(\n        cls,\n        regex_string: str,\n        tokenizer,\n        **kwargs,\n    ):\n        return super().from_regex(\n            regex_string,\n            tokenizer,\n            _create_states_mapping=cached_create_states_mapping,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.StopAtEOSGuide","title":"<code>StopAtEOSGuide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Guide to generate tokens until the EOS token has been generated.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class StopAtEOSGuide(Guide):\n    \"\"\"Guide to generate tokens until the EOS token has been generated.\"\"\"\n\n    final_state = 1\n    start_state = 0  # TODO: remove start_state, use only initial_state\n    initial_state = 0\n\n    def __init__(self, tokenizer: \"Tokenizer\"):\n        \"\"\"Initialize the generation guide.\n\n        model\n            The logit generator used to generate the next token.\n\n        \"\"\"\n        self.eos_token_id = tokenizer.eos_token_id\n        self.vocabulary = tokenizer.vocabulary.values()\n\n    def get_next_instruction(self, state: int) -&gt; Instruction:\n        if self.is_final_state(state):\n            return Write([self.eos_token_id])\n        return Generate(None)\n\n    def get_next_state(self, state: int, token_id: int) -&gt; int:\n        if token_id == self.eos_token_id or state == self.final_state:\n            return self.final_state\n\n        return self.initial_state\n\n    def is_final_state(self, state: int):\n        return state == self.final_state\n\n    def copy(self):\n        return self\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.StopAtEOSGuide.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize the generation guide.</p> <p>model     The logit generator used to generate the next token.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def __init__(self, tokenizer: \"Tokenizer\"):\n    \"\"\"Initialize the generation guide.\n\n    model\n        The logit generator used to generate the next token.\n\n    \"\"\"\n    self.eos_token_id = tokenizer.eos_token_id\n    self.vocabulary = tokenizer.vocabulary.values()\n</code></pre>"},{"location":"api/json_schema/","title":"Json schema","text":""},{"location":"api/json_schema/#outlines.fsm.json_schema.convert_json_schema_to_str","title":"<code>convert_json_schema_to_str(json_schema)</code>","text":"<p>Convert a JSON schema to a string.</p> <p>Parameters:</p> Name Type Description Default <code>json_schema</code> <code>Union[dict, str, Type[BaseModel]]</code> <p>The JSON schema.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The JSON schema converted to a string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the schema is not a dictionary, a string or a Pydantic class.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def convert_json_schema_to_str(json_schema: Union[dict, str, Type[BaseModel]]) -&gt; str:\n    \"\"\"Convert a JSON schema to a string.\n\n    Parameters\n    ----------\n    json_schema\n        The JSON schema.\n\n    Returns\n    -------\n    str\n        The JSON schema converted to a string.\n\n    Raises\n    ------\n    ValueError\n        If the schema is not a dictionary, a string or a Pydantic class.\n    \"\"\"\n    if isinstance(json_schema, dict):\n        schema_str = json.dumps(json_schema)\n    elif isinstance(json_schema, str):\n        schema_str = json_schema\n    elif issubclass(json_schema, BaseModel):\n        schema_str = json.dumps(json_schema.model_json_schema())\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {json_schema}. The schema must be either \"\n            + \"a Pydantic class, a dictionary or a string that contains the JSON \"\n            + \"schema specification\"\n        )\n    return schema_str\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; dict:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    try:\n        fn_name = fn.__name__\n    except Exception as e:\n        fn_name = \"Arguments\"\n        warnings.warn(\n            f\"The function name could not be determined. Using default name 'Arguments' instead. For debugging, here is exact error:\\n{e}\",\n            category=UserWarning,\n        )\n    model = create_model(fn_name, **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Module that contains all the models integrated in outlines.</p> <p>We group the models in submodules by provider instead of theme (completion, chat completion, diffusers, etc.) and use routing functions everywhere else in the codebase.</p>"},{"location":"api/models/#outlines.models.exllamav2","title":"<code>exllamav2</code>","text":""},{"location":"api/models/#outlines.models.exllamav2.ExLlamaV2Model","title":"<code>ExLlamaV2Model</code>","text":"<p>Represents a <code>exl2</code> model.</p> Source code in <code>outlines/models/exllamav2.py</code> <pre><code>class ExLlamaV2Model:\n    \"\"\"Represents a `exl2` model.\"\"\"\n\n    def __init__(\n        self,\n        generator: \"ExLlamaV2DynamicGenerator\",\n        tokenizer: \"OutlinesExLlamaV2Tokenizer\",\n        max_seq_len: int,\n    ):\n        self.generator = generator\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def prepare_generation_parameters(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        sampling_parameters: SamplingParameters,\n        structure_logits_processor,\n        **exllamav2_params: Unpack[ExllamaV2Params],\n    ) -&gt; Tuple[ExllamaV2Params, Union[str, List[str]]]:\n        \"\"\"Prepare the generation parameters.\n\n        `exllamav2` uses different default values\n\n        \"\"\"\n        from exllamav2.generator import ExLlamaV2Sampler\n\n        if isinstance(prompts, str):\n            prompts = [prompts]\n        max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n        if max_tokens is None:\n            max_tokens = []\n            for prompt in prompts:\n                ids = self.generator.tokenizer.encode(\n                    prompt, encode_special_tokens=True\n                )\n                prompt_tokens = ids.shape[-1]\n                max_tokens.append(self.max_seq_len - prompt_tokens)\n            exllamav2_params[\"max_new_tokens\"] = max_tokens\n        else:\n            exllamav2_params[\"max_new_tokens\"] = [\n                max_tokens for _ in range(len(prompts))\n            ]\n\n        stop_conditions = [self.generator.tokenizer.eos_token_id]\n        if isinstance(generation_parameters.stop_at, str):\n            stop_conditions.append(generation_parameters.stop_at)\n        elif isinstance(generation_parameters.stop_at, list):\n            for stop_at in generation_parameters.stop_at:\n                stop_conditions.append(stop_at)\n        exllamav2_params[\"stop_conditions\"] = stop_conditions\n        exllamav2_params[\"seed\"] = seed\n\n        gen_settings = ExLlamaV2Sampler.Settings()\n        if sampling_parameters.temperature is not None:\n            gen_settings.temperature = sampling_parameters.temperature\n        if sampling_parameters.top_p is not None:\n            gen_settings.top_p = sampling_parameters.top_p\n        if sampling_parameters.top_k is not None:\n            gen_settings.top_k = sampling_parameters.top_k\n        gen_settings.logits_processor = structure_logits_processor\n        exllamav2_params[\"gen_settings\"] = gen_settings\n        if sampling_parameters.num_samples &gt; 1:\n            prompts = prompts * sampling_parameters.num_samples\n            exllamav2_params[\"max_new_tokens\"] = (\n                exllamav2_params[\"max_new_tokens\"] * sampling_parameters.num_samples\n            )\n\n        if len(prompts) == 1:\n            prompts = prompts[0]\n\n        return exllamav2_params, prompts\n\n    def reformat_output(\n        self, output: Union[str, List[str]], sampling_parameters: SamplingParameters\n    ):\n        \"\"\"\n        The purpose of this function is to reformat the output from exllamav2's output format to outline's output format.\n\n        For exllamav2, it mainly accepts only a list or a string(they also do cfg sampling with tuples but we will ignore this for now).\n        The exllamav2's logic is:\n\n        1. If the prompt is a string, return a string. This is the same as outlines\n        2. If a prompt is a list, return a list. This is not the same as outlines output in that if the list is only one element, the string is expected to be outputted.\n        3. There is no such thing as num_samples, so the prompts had to be duplicated by num_samples times. Then, we had the function output a list of lists\n        \"\"\"\n        if isinstance(output, str):\n            return output\n        if len(output) == 1:\n            return output[0]\n        if sampling_parameters.num_samples &gt; 1:\n            if len(output) == sampling_parameters.num_samples:\n                return output\n            assert len(output) % sampling_parameters.num_samples == 0\n            num_items_per_sample = len(output) // sampling_parameters.num_samples\n            new_output = []\n            for i in range(sampling_parameters.num_samples):\n                curr_sample = []\n                for j in range(num_items_per_sample):\n                    curr_sample.append(output[i * num_items_per_sample + j])\n                new_output.append(curr_sample)\n            return new_output\n        return output\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        structure_logits_processor,\n        sampling_parameters: SamplingParameters,\n        **exllamav2_params: Unpack[ExllamaV2Params],\n    ) -&gt; Union[str, List[str]]:\n        exllamav2_params, prompts = self.prepare_generation_parameters(\n            prompts,\n            generation_parameters,\n            sampling_parameters,\n            structure_logits_processor,\n        )\n        \"\"\"\n        In exllamav2, it needs the max amount of new tokens generated.\n        The reason exllamav2_params[\"max_new_tokens\"] is a list is because in prepare_generation_parameters\n        the max amount of tokens that can be generated by the model for each prompt(by encoding with tokenizer) is calculated.\n        The minimum is picked because otherwise it might be possible for one of the\n        prompts to exceed the max sequence length.\n        \"\"\"\n        output = self.generator.generate(\n            prompt=prompts,\n            gen_settings=exllamav2_params[\"gen_settings\"],\n            max_new_tokens=min(exllamav2_params[\"max_new_tokens\"]),\n            completion_only=True,\n            encode_special_tokens=True,\n            stop_conditions=exllamav2_params[\"stop_conditions\"],\n            add_bos=False,\n            seed=exllamav2_params[\"seed\"],\n        )\n\n        return self.reformat_output(output, sampling_parameters)\n\n    def stream(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        structure_logits_processor,\n        sampling_parameters: SamplingParameters,\n        **exllamav2_params: Unpack[ExllamaV2Params],\n    ) -&gt; Iterator[Union[str, List[str]]]:\n        from exllamav2.generator import ExLlamaV2DynamicJob\n\n        exllamav2_params, prompts = self.prepare_generation_parameters(\n            prompts,\n            generation_parameters,\n            sampling_parameters,\n            structure_logits_processor,\n        )\n\n        order = {}\n        if isinstance(prompts, str):\n            prompts = [prompts]\n        batch_size = len(prompts)\n        seed = exllamav2_params[\"seed\"]\n        for idx, p in enumerate(prompts):\n            input_ids = self.generator.tokenizer.encode(\n                p, encode_special_tokens=True, add_bos=False\n            )\n\n            job = ExLlamaV2DynamicJob(\n                input_ids=input_ids,\n                max_new_tokens=exllamav2_params[\"max_new_tokens\"][idx],\n                min_new_tokens=0,\n                seed=seed,\n                stop_conditions=exllamav2_params[\"stop_conditions\"],\n                gen_settings=exllamav2_params[\"gen_settings\"],\n                token_healing=False,\n                decode_special_tokens=False,\n            )\n\n            if seed is not None:\n                seed += 1\n\n            serial = self.generator.enqueue(job)\n            order[serial] = idx\n\n        # Collect outputs until all jobs finish\n\n        next_text = [\"\"] * batch_size\n\n        def token_generator() -&gt; Iterator[str]:\n            while self.generator.num_remaining_jobs():\n                results = self.generator.iterate()\n                for r in results:\n                    idx = order[r[\"serial\"]]\n                    if r[\"stage\"] == \"streaming\":\n                        text = r.get(\"text\", \"\")\n                        next_text[idx] = text\n                    if r[\"eos\"]:\n                        next_text[idx] = \"\"\n                yield self.reformat_output(next_text, sampling_parameters)\n            return\n\n        return token_generator()\n</code></pre>"},{"location":"api/models/#outlines.models.exllamav2.ExLlamaV2Model.prepare_generation_parameters","title":"<code>prepare_generation_parameters(prompts, generation_parameters, sampling_parameters, structure_logits_processor, **exllamav2_params)</code>","text":"<p>Prepare the generation parameters.</p> <p><code>exllamav2</code> uses different default values</p> Source code in <code>outlines/models/exllamav2.py</code> <pre><code>def prepare_generation_parameters(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    sampling_parameters: SamplingParameters,\n    structure_logits_processor,\n    **exllamav2_params: Unpack[ExllamaV2Params],\n) -&gt; Tuple[ExllamaV2Params, Union[str, List[str]]]:\n    \"\"\"Prepare the generation parameters.\n\n    `exllamav2` uses different default values\n\n    \"\"\"\n    from exllamav2.generator import ExLlamaV2Sampler\n\n    if isinstance(prompts, str):\n        prompts = [prompts]\n    max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n    if max_tokens is None:\n        max_tokens = []\n        for prompt in prompts:\n            ids = self.generator.tokenizer.encode(\n                prompt, encode_special_tokens=True\n            )\n            prompt_tokens = ids.shape[-1]\n            max_tokens.append(self.max_seq_len - prompt_tokens)\n        exllamav2_params[\"max_new_tokens\"] = max_tokens\n    else:\n        exllamav2_params[\"max_new_tokens\"] = [\n            max_tokens for _ in range(len(prompts))\n        ]\n\n    stop_conditions = [self.generator.tokenizer.eos_token_id]\n    if isinstance(generation_parameters.stop_at, str):\n        stop_conditions.append(generation_parameters.stop_at)\n    elif isinstance(generation_parameters.stop_at, list):\n        for stop_at in generation_parameters.stop_at:\n            stop_conditions.append(stop_at)\n    exllamav2_params[\"stop_conditions\"] = stop_conditions\n    exllamav2_params[\"seed\"] = seed\n\n    gen_settings = ExLlamaV2Sampler.Settings()\n    if sampling_parameters.temperature is not None:\n        gen_settings.temperature = sampling_parameters.temperature\n    if sampling_parameters.top_p is not None:\n        gen_settings.top_p = sampling_parameters.top_p\n    if sampling_parameters.top_k is not None:\n        gen_settings.top_k = sampling_parameters.top_k\n    gen_settings.logits_processor = structure_logits_processor\n    exllamav2_params[\"gen_settings\"] = gen_settings\n    if sampling_parameters.num_samples &gt; 1:\n        prompts = prompts * sampling_parameters.num_samples\n        exllamav2_params[\"max_new_tokens\"] = (\n            exllamav2_params[\"max_new_tokens\"] * sampling_parameters.num_samples\n        )\n\n    if len(prompts) == 1:\n        prompts = prompts[0]\n\n    return exllamav2_params, prompts\n</code></pre>"},{"location":"api/models/#outlines.models.exllamav2.ExLlamaV2Model.reformat_output","title":"<code>reformat_output(output, sampling_parameters)</code>","text":"<p>The purpose of this function is to reformat the output from exllamav2's output format to outline's output format.</p> <p>For exllamav2, it mainly accepts only a list or a string(they also do cfg sampling with tuples but we will ignore this for now). The exllamav2's logic is:</p> <ol> <li>If the prompt is a string, return a string. This is the same as outlines</li> <li>If a prompt is a list, return a list. This is not the same as outlines output in that if the list is only one element, the string is expected to be outputted.</li> <li>There is no such thing as num_samples, so the prompts had to be duplicated by num_samples times. Then, we had the function output a list of lists</li> </ol> Source code in <code>outlines/models/exllamav2.py</code> <pre><code>def reformat_output(\n    self, output: Union[str, List[str]], sampling_parameters: SamplingParameters\n):\n    \"\"\"\n    The purpose of this function is to reformat the output from exllamav2's output format to outline's output format.\n\n    For exllamav2, it mainly accepts only a list or a string(they also do cfg sampling with tuples but we will ignore this for now).\n    The exllamav2's logic is:\n\n    1. If the prompt is a string, return a string. This is the same as outlines\n    2. If a prompt is a list, return a list. This is not the same as outlines output in that if the list is only one element, the string is expected to be outputted.\n    3. There is no such thing as num_samples, so the prompts had to be duplicated by num_samples times. Then, we had the function output a list of lists\n    \"\"\"\n    if isinstance(output, str):\n        return output\n    if len(output) == 1:\n        return output[0]\n    if sampling_parameters.num_samples &gt; 1:\n        if len(output) == sampling_parameters.num_samples:\n            return output\n        assert len(output) % sampling_parameters.num_samples == 0\n        num_items_per_sample = len(output) // sampling_parameters.num_samples\n        new_output = []\n        for i in range(sampling_parameters.num_samples):\n            curr_sample = []\n            for j in range(num_items_per_sample):\n                curr_sample.append(output[i * num_items_per_sample + j])\n            new_output.append(curr_sample)\n        return new_output\n    return output\n</code></pre>"},{"location":"api/models/#outlines.models.exllamav2.exl2","title":"<code>exl2(model_path, draft_model_path=None, max_seq_len=None, cache_q4=False, paged=True, max_chunk_size=None)</code>","text":"<p>Load an ExLlamaV2 model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model directory.</p> required <code>device</code> <p>Device to load the model on. Pass in 'cuda' for GPU or 'cpu' for CPU</p> required <code>max_seq_len</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <code>scale_pos_emb</code> <p>Scale factor for positional embeddings. Defaults to None.</p> required <code>scale_alpha_value</code> <p>Scale alpha value. Defaults to None.</p> required <code>no_flash_attn</code> <p>Disable flash attention. Defaults to None.</p> required <code>num_experts_per_token</code> <p>Number of experts per token. Defaults to None.</p> required <code>cache_q4</code> <code>bool</code> <p>Use Q4 cache. Defaults to False.</p> <code>False</code> <code>tokenizer_kwargs</code> <p>Additional keyword arguments for the tokenizer. Defaults to {}.</p> required <code>gpu_split</code> <p>\"auto\", or VRAM allocation per GPU in GB. Auto will use exllama's autosplit feature</p> required <code>low_mem</code> <p>Enable VRAM optimizations, potentially trading off speed</p> required <code>verbose</code> <p>Enable if you want debugging statements</p> required <p>Returns:</p> Type Description <code>An `ExLlamaV2Model` instance.</code> <p>Raises:</p> Type Description <code>`ImportError` if the `exllamav2` library is not installed.</code> Source code in <code>outlines/models/exllamav2.py</code> <pre><code>def exl2(\n    model_path: str,\n    draft_model_path: Optional[str] = None,\n    max_seq_len: Optional[int] = None,\n    cache_q4: bool = False,\n    paged: bool = True,\n    max_chunk_size: Optional[int] = None,\n) -&gt; ExLlamaV2Model:\n    \"\"\"\n    Load an ExLlamaV2 model.\n\n    Parameters\n    ----------\n    model_path (str)\n        Path to the model directory.\n    device (str)\n        Device to load the model on. Pass in 'cuda' for GPU or 'cpu' for CPU\n    max_seq_len (Optional[int], optional)\n        Maximum sequence length. Defaults to None.\n    scale_pos_emb (Optional[float], optional)\n        Scale factor for positional embeddings. Defaults to None.\n    scale_alpha_value (Optional[float], optional)\n        Scale alpha value. Defaults to None.\n    no_flash_attn (Optional[bool], optional)\n        Disable flash attention. Defaults to None.\n    num_experts_per_token (Optional[int], optional)\n        Number of experts per token. Defaults to None.\n    cache_q4 (bool, optional)\n        Use Q4 cache. Defaults to False.\n    tokenizer_kwargs (dict, optional)\n        Additional keyword arguments for the tokenizer. Defaults to {}.\n    gpu_split (str)\n        \\\"auto\\\", or VRAM allocation per GPU in GB. Auto will use exllama's autosplit feature\n    low_mem (bool, optional)\n        Enable VRAM optimizations, potentially trading off speed\n    verbose (bool, optional)\n        Enable if you want debugging statements\n\n    Returns\n    -------\n    An `ExLlamaV2Model` instance.\n\n    Raises\n    ------\n    `ImportError` if the `exllamav2` library is not installed.\n\n    \"\"\"\n    try:\n        from exllamav2 import (\n            ExLlamaV2,\n            ExLlamaV2Cache,\n            ExLlamaV2Cache_Q4,\n            ExLlamaV2Config,\n            ExLlamaV2Tokenizer,\n        )\n        from exllamav2.generator import ExLlamaV2DynamicGenerator\n\n    except ImportError:\n        raise ImportError(\n            \"The `exllamav2`, `transformers` and `torch` libraries needs to be installed in order to use `exllamav2` models. \"\n            \"Please run `pip install transformers torch git+https://github.com/lapp0/exllamav2@sampler-logits-processor` \"\n            \"Documentation: https://dottxt-ai.github.io/outlines/latest/reference/models/exllamav2/\"\n        )\n    config = ExLlamaV2Config(model_path)\n    if max_chunk_size is not None:\n        config.max_input_len = max_chunk_size\n        config.max_attention_size = max_chunk_size**2\n\n    config.arch_compat_overrides()\n    model = ExLlamaV2(config)\n    if max_seq_len is None:\n        max_seq_len = -1\n    if cache_q4:\n        cache = ExLlamaV2Cache_Q4(model, max_seq_len=max_seq_len, lazy=True)\n    else:\n        cache = ExLlamaV2Cache(model, max_seq_len=max_seq_len, lazy=True)\n    model.load_autosplit(cache, progress=True)\n\n    print(\"Loading tokenizer...\")\n    tokenizer = ExLlamaV2Tokenizer(config)\n    max_batch_size = 4 if paged else 1\n\n    draft_model = None\n    draft_cache = None\n    if draft_model_path is not None:\n        draft_config = ExLlamaV2Config(draft_model_path)\n        draft_model = ExLlamaV2(draft_config)\n\n        if cache_q4:\n            draft_cache = ExLlamaV2Cache_Q4(\n                draft_model, max_seq_len=max_seq_len, lazy=True\n            )\n        else:\n            draft_cache = ExLlamaV2Cache(\n                draft_model, max_seq_len=max_seq_len, lazy=True\n            )\n\n    # Initialize the generator with all default parameters\n    generator = ExLlamaV2DynamicGenerator(\n        model=model,\n        cache=cache,\n        draft_model=draft_model,\n        draft_cache=draft_cache,\n        tokenizer=tokenizer,\n        max_batch_size=max_batch_size,\n        use_ngram_draft=False,\n        max_chunk_size=max_chunk_size,\n        paged=paged,\n    )\n    max_seq_len = cache.max_seq_len\n\n    outlines_tokenizer = OutlinesExLlamaV2Tokenizer(tokenizer)\n    outlines_exl2_model = ExLlamaV2Model(generator, outlines_tokenizer, max_seq_len)\n    return outlines_exl2_model\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp","title":"<code>llamacpp</code>","text":""},{"location":"api/models/#outlines.models.llamacpp.LlamaCpp","title":"<code>LlamaCpp</code>","text":"<p>Represents a model provided by the <code>llama-cpp-python</code> library.</p> <p>We wrap models from model providing libraries in order to give all of them the same interface in Outlines and allow users to easily switch between providers. This class wraps the <code>llama_cpp.Llama</code> class from the <code>llama-cpp-python</code> library.</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCpp:\n    \"\"\"Represents a model provided by the `llama-cpp-python` library.\n\n    We wrap models from model providing libraries in order to give all of\n    them the same interface in Outlines and allow users to easily switch\n    between providers. This class wraps the `llama_cpp.Llama` class from the\n    `llama-cpp-python` library.\n\n    \"\"\"\n\n    def __init__(self, model: \"Llama\"):\n        self.model = model\n\n    @property\n    def tokenizer(self):\n        return LlamaCppTokenizer(self.model)\n\n    def prepare_generation_parameters(\n        self,\n        generation_parameters: GenerationParameters,\n        sampling_parameters: SamplingParameters,\n        structure_logits_processor,\n        **llama_cpp_params: Unpack[LlamaCppParams],\n    ):\n        \"\"\"Prepare the generation parameters.\n\n        `llama-cpp-python` uses different default values\n\n        \"\"\"\n        from llama_cpp import LogitsProcessorList\n\n        max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n        # We update `llama_cpp_params` with the values the user passed to the\n        # generator.\n        if \"stop\" not in llama_cpp_params:\n            llama_cpp_params[\"stop\"] = stop_at\n        if \"seed\" not in llama_cpp_params:\n            llama_cpp_params[\"seed\"] = seed\n\n        # Somehow `llama-cpp-python` generates `max_tokens + 1`  tokens\n        if \"max_tokens\" not in llama_cpp_params:\n            if max_tokens is None:\n                llama_cpp_params[\"max_tokens\"] = -1  # indicates unlimited tokens\n            else:\n                llama_cpp_params[\"max_tokens\"] = max_tokens - 1\n        else:\n            llama_cpp_params[\"max_tokens\"] = llama_cpp_params[\"max_tokens\"] - 1\n\n        sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n            sampling_parameters\n        )\n\n        # We update the `llama_cpp_params` with the sampling values that\n        # were specified by the user via the `Sampler` class, unless they\n        # are also specified in `llama_cpp_params`. We also disable other\n        # sampling methods that are enabled by default and reset the temperature\n        # value.\n        #\n        # See https://github.com/ggerganov/llama.cpp/blob/e11a8999b5690f810c2c99c14347f0834e68c524/common/sampling.h#L22\n        # for the default values in `llama.cpp` and indications to disable the sampling modes.\n        # Mirostat sampling, tail-free sampling and all penalties are disabled by default.\n        #\n        # See https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n        # for default values in `llama-cpp-python`\n        if sampler == \"beam_search\":\n            raise NotImplementedError(\n                \"The `llama_cpp_python` library does not support Beam Search.\"\n            )\n        if num_samples != 1:\n            raise NotImplementedError(\n                \"The `llama_cpp_python` library does not allow to take several samples.\"\n            )\n        if \"top_p\" not in llama_cpp_params:\n            if top_p is not None:\n                llama_cpp_params[\"top_p\"] = top_p\n            else:\n                llama_cpp_params[\"top_p\"] = 1.0\n\n        if \"min_p\" not in llama_cpp_params:\n            llama_cpp_params[\"min_p\"] = 0.0\n\n        if \"top_k\" not in llama_cpp_params:\n            if top_k is not None:\n                llama_cpp_params[\"top_k\"] = top_k\n            else:\n                llama_cpp_params[\"top_k\"] = -1\n\n        if \"temperature\" not in llama_cpp_params:\n            if temperature is not None:\n                llama_cpp_params[\"temperature\"] = temperature\n            else:\n                llama_cpp_params[\"temperature\"] = 1.0\n\n        if \"repeat_penalty\" not in llama_cpp_params:\n            llama_cpp_params[\"repeat_penalty\"] = 1.0\n\n        # The choice to stream or not should happen via the high-level API\n        llama_cpp_params[\"stream\"] = False\n\n        if structure_logits_processor is not None:\n            if \"logits_processor\" in llama_cpp_params:\n                llama_cpp_params[\"logits_processor\"].append(structure_logits_processor)\n            else:\n                llama_cpp_params[\"logits_processor\"] = LogitsProcessorList(\n                    [structure_logits_processor]\n                )\n\n        return llama_cpp_params\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        structure_logits_processor,\n        sampling_parameters: SamplingParameters,\n        **llama_cpp_params: Unpack[LlamaCppParams],\n    ) -&gt; str:\n        \"\"\"Generate text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n        llama_cpp_params\n            Keyword arguments that can be passed to\n            `llama_cpp_python.Llama.__call__`.  The values in `llama_cpp_params`\n            supersede the values of the parameters in `generation_parameters` and\n            `sampling_parameters`.  See the `llama_cpp_python` documentation for\n            a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n\n        Returns\n        -------\n        The generated text.\n\n        \"\"\"\n        if not isinstance(prompts, str):\n            raise NotImplementedError(\n                \"The `llama-cpp-python` library does not support batch inference.\"\n            )\n\n        llama_cpp_params = self.prepare_generation_parameters(\n            generation_parameters,\n            sampling_parameters,\n            structure_logits_processor,\n            **llama_cpp_params,\n        )\n        completion = self.model(prompts, **llama_cpp_params)\n        result = completion[\"choices\"][0][\"text\"]\n\n        self.model.reset()\n\n        return result\n\n    def stream(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        structure_logits_processor,\n        sampling_parameters: SamplingParameters,\n        **llama_cpp_params: Unpack[LlamaCppParams],\n    ) -&gt; Iterator[str]:\n        \"\"\"Stream text using `llama-cpp-python`.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n        llama_cpp_params\n            Keyword arguments that can be passed to\n            `llama_cpp_python.Llama.__call__`.  The values in `llama_cpp_params`\n            supersede the values of the parameters in `generation_parameters` and\n            `sampling_parameters`.  See the `llama_cpp_python` documentation for\n            a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n\n        Returns\n        -------\n        A generator that return strings.\n\n        \"\"\"\n\n        if not isinstance(prompts, str):\n            raise NotImplementedError(\n                \"The `llama-cpp-python` library does not support batch inference.\"\n            )\n\n        llama_cpp_params = self.prepare_generation_parameters(\n            generation_parameters,\n            sampling_parameters,\n            structure_logits_processor,\n            **llama_cpp_params,\n        )\n        llama_cpp_params[\"stream\"] = True\n        generator = self.model(prompts, **llama_cpp_params)\n\n        def token_generator() -&gt; Iterator[str]:\n            while True:\n                try:\n                    result = next(generator)\n                    yield result[\"choices\"][0][\"text\"]\n                except StopIteration:\n                    self.model.reset()\n                    return\n\n        return token_generator()\n\n    def load_lora(self, adapter_path: str):\n        if self.model._model.apply_lora_from_file(\n            adapter_path,\n            1.0,\n        ):\n            raise RuntimeError(f\"Failed to apply LoRA from lora path: {adapter_path}\")\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.LlamaCpp.generate","title":"<code>generate(prompts, generation_parameters, structure_logits_processor, sampling_parameters, **llama_cpp_params)</code>","text":"<p>Generate text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <code>llama_cpp_params</code> <code>Unpack[LlamaCppParams]</code> <p>Keyword arguments that can be passed to <code>llama_cpp_python.Llama.__call__</code>.  The values in <code>llama_cpp_params</code> supersede the values of the parameters in <code>generation_parameters</code> and <code>sampling_parameters</code>.  See the <code>llama_cpp_python</code> documentation for a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.call</p> <code>{}</code> <p>Returns:</p> Type Description <code>The generated text.</code> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def generate(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    structure_logits_processor,\n    sampling_parameters: SamplingParameters,\n    **llama_cpp_params: Unpack[LlamaCppParams],\n) -&gt; str:\n    \"\"\"Generate text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n    llama_cpp_params\n        Keyword arguments that can be passed to\n        `llama_cpp_python.Llama.__call__`.  The values in `llama_cpp_params`\n        supersede the values of the parameters in `generation_parameters` and\n        `sampling_parameters`.  See the `llama_cpp_python` documentation for\n        a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n\n    Returns\n    -------\n    The generated text.\n\n    \"\"\"\n    if not isinstance(prompts, str):\n        raise NotImplementedError(\n            \"The `llama-cpp-python` library does not support batch inference.\"\n        )\n\n    llama_cpp_params = self.prepare_generation_parameters(\n        generation_parameters,\n        sampling_parameters,\n        structure_logits_processor,\n        **llama_cpp_params,\n    )\n    completion = self.model(prompts, **llama_cpp_params)\n    result = completion[\"choices\"][0][\"text\"]\n\n    self.model.reset()\n\n    return result\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.LlamaCpp.prepare_generation_parameters","title":"<code>prepare_generation_parameters(generation_parameters, sampling_parameters, structure_logits_processor, **llama_cpp_params)</code>","text":"<p>Prepare the generation parameters.</p> <p><code>llama-cpp-python</code> uses different default values</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def prepare_generation_parameters(\n    self,\n    generation_parameters: GenerationParameters,\n    sampling_parameters: SamplingParameters,\n    structure_logits_processor,\n    **llama_cpp_params: Unpack[LlamaCppParams],\n):\n    \"\"\"Prepare the generation parameters.\n\n    `llama-cpp-python` uses different default values\n\n    \"\"\"\n    from llama_cpp import LogitsProcessorList\n\n    max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n    # We update `llama_cpp_params` with the values the user passed to the\n    # generator.\n    if \"stop\" not in llama_cpp_params:\n        llama_cpp_params[\"stop\"] = stop_at\n    if \"seed\" not in llama_cpp_params:\n        llama_cpp_params[\"seed\"] = seed\n\n    # Somehow `llama-cpp-python` generates `max_tokens + 1`  tokens\n    if \"max_tokens\" not in llama_cpp_params:\n        if max_tokens is None:\n            llama_cpp_params[\"max_tokens\"] = -1  # indicates unlimited tokens\n        else:\n            llama_cpp_params[\"max_tokens\"] = max_tokens - 1\n    else:\n        llama_cpp_params[\"max_tokens\"] = llama_cpp_params[\"max_tokens\"] - 1\n\n    sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n        sampling_parameters\n    )\n\n    # We update the `llama_cpp_params` with the sampling values that\n    # were specified by the user via the `Sampler` class, unless they\n    # are also specified in `llama_cpp_params`. We also disable other\n    # sampling methods that are enabled by default and reset the temperature\n    # value.\n    #\n    # See https://github.com/ggerganov/llama.cpp/blob/e11a8999b5690f810c2c99c14347f0834e68c524/common/sampling.h#L22\n    # for the default values in `llama.cpp` and indications to disable the sampling modes.\n    # Mirostat sampling, tail-free sampling and all penalties are disabled by default.\n    #\n    # See https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n    # for default values in `llama-cpp-python`\n    if sampler == \"beam_search\":\n        raise NotImplementedError(\n            \"The `llama_cpp_python` library does not support Beam Search.\"\n        )\n    if num_samples != 1:\n        raise NotImplementedError(\n            \"The `llama_cpp_python` library does not allow to take several samples.\"\n        )\n    if \"top_p\" not in llama_cpp_params:\n        if top_p is not None:\n            llama_cpp_params[\"top_p\"] = top_p\n        else:\n            llama_cpp_params[\"top_p\"] = 1.0\n\n    if \"min_p\" not in llama_cpp_params:\n        llama_cpp_params[\"min_p\"] = 0.0\n\n    if \"top_k\" not in llama_cpp_params:\n        if top_k is not None:\n            llama_cpp_params[\"top_k\"] = top_k\n        else:\n            llama_cpp_params[\"top_k\"] = -1\n\n    if \"temperature\" not in llama_cpp_params:\n        if temperature is not None:\n            llama_cpp_params[\"temperature\"] = temperature\n        else:\n            llama_cpp_params[\"temperature\"] = 1.0\n\n    if \"repeat_penalty\" not in llama_cpp_params:\n        llama_cpp_params[\"repeat_penalty\"] = 1.0\n\n    # The choice to stream or not should happen via the high-level API\n    llama_cpp_params[\"stream\"] = False\n\n    if structure_logits_processor is not None:\n        if \"logits_processor\" in llama_cpp_params:\n            llama_cpp_params[\"logits_processor\"].append(structure_logits_processor)\n        else:\n            llama_cpp_params[\"logits_processor\"] = LogitsProcessorList(\n                [structure_logits_processor]\n            )\n\n    return llama_cpp_params\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.LlamaCpp.stream","title":"<code>stream(prompts, generation_parameters, structure_logits_processor, sampling_parameters, **llama_cpp_params)</code>","text":"<p>Stream text using <code>llama-cpp-python</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <code>llama_cpp_params</code> <code>Unpack[LlamaCppParams]</code> <p>Keyword arguments that can be passed to <code>llama_cpp_python.Llama.__call__</code>.  The values in <code>llama_cpp_params</code> supersede the values of the parameters in <code>generation_parameters</code> and <code>sampling_parameters</code>.  See the <code>llama_cpp_python</code> documentation for a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.call</p> <code>{}</code> <p>Returns:</p> Type Description <code>A generator that return strings.</code> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def stream(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    structure_logits_processor,\n    sampling_parameters: SamplingParameters,\n    **llama_cpp_params: Unpack[LlamaCppParams],\n) -&gt; Iterator[str]:\n    \"\"\"Stream text using `llama-cpp-python`.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n    llama_cpp_params\n        Keyword arguments that can be passed to\n        `llama_cpp_python.Llama.__call__`.  The values in `llama_cpp_params`\n        supersede the values of the parameters in `generation_parameters` and\n        `sampling_parameters`.  See the `llama_cpp_python` documentation for\n        a list of possible values: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__\n\n    Returns\n    -------\n    A generator that return strings.\n\n    \"\"\"\n\n    if not isinstance(prompts, str):\n        raise NotImplementedError(\n            \"The `llama-cpp-python` library does not support batch inference.\"\n        )\n\n    llama_cpp_params = self.prepare_generation_parameters(\n        generation_parameters,\n        sampling_parameters,\n        structure_logits_processor,\n        **llama_cpp_params,\n    )\n    llama_cpp_params[\"stream\"] = True\n    generator = self.model(prompts, **llama_cpp_params)\n\n    def token_generator() -&gt; Iterator[str]:\n        while True:\n            try:\n                result = next(generator)\n                yield result[\"choices\"][0][\"text\"]\n            except StopIteration:\n                self.model.reset()\n                return\n\n    return token_generator()\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.LlamaCppTokenizer","title":"<code>LlamaCppTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>class LlamaCppTokenizer(Tokenizer):\n    def __init__(self, model: \"Llama\"):\n        self.eos_token_id = model.token_eos()\n        self.eos_token = model.tokenizer().decode([self.eos_token_id])\n        self.pad_token_id = self.eos_token_id\n        self.special_tokens: Set[str] = set()\n\n        self.vocabulary: Dict[str, int] = dict()\n\n        self.tokenizer = model.tokenizer()\n\n        # TODO: Remove when https://github.com/ggerganov/llama.cpp/pull/5613 is resolved\n        self._hf_tokenizer = None\n        try:\n            self.vocabulary = model.tokenizer_.hf_tokenizer.get_vocab()\n            self._hf_tokenizer = model.tokenizer_.hf_tokenizer\n        except AttributeError:\n            # ###\n            for t in range(model.n_vocab()):\n                token_piece = model.tokenizer().decode([t])\n                self.vocabulary[token_piece] = t\n\n        # ensure stable ordering of vocabulary\n        self.vocabulary = {\n            tok: tok_id\n            for tok, tok_id in sorted(self.vocabulary.items(), key=lambda x: x[1])\n        }\n\n        self._hash = None\n\n    def decode(self, token_ids: List[int]) -&gt; List[str]:\n        decoded_bytes = self.tokenizer.detokenize(token_ids)\n        return [decoded_bytes.decode(\"utf-8\", errors=\"ignore\")]\n\n    def encode(\n        self, prompt: Union[str, List[str]], add_bos: bool = True, special: bool = True\n    ) -&gt; Tuple[List[int], List[int]]:\n        if isinstance(prompt, list):\n            raise NotImplementedError(\n                \"llama-cpp-python tokenizer doesn't support batch tokenization\"\n            )\n        token_ids = self.tokenizer.tokenize(\n            prompt.encode(\"utf-8\", errors=\"ignore\"), add_bos=add_bos, special=special\n        )\n        # generate attention mask, missing from llama-cpp-python\n        attention_mask = [\n            1 if token_id != self.pad_token_id else 0 for token_id in token_ids\n        ]\n        return token_ids, attention_mask\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        if self._hf_tokenizer is not None:\n            from transformers.file_utils import SPIECE_UNDERLINE\n\n            token_str = self._hf_tokenizer.convert_tokens_to_string([token])\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                token_str = \" \" + token_str\n            return token_str\n        else:\n            return token\n\n    def __eq__(self, other):\n        if not isinstance(other, LlamaCppTokenizer):\n            return False\n        return self.__getstate__() == other.__getstate__()\n\n    def __hash__(self):\n        if self._hash is None:\n            self._hash = hash(pickle.dumps(self))\n        return self._hash\n\n    def __getstate__(self):\n        \"\"\"Create a stable representation for outlines.caching\"\"\"\n        return (\n            self.vocabulary,\n            self.eos_token_id,\n            self.eos_token,\n            self.pad_token_id,\n            sorted(self.special_tokens),\n        )\n\n    def __setstate__(self, state):\n        raise NotImplementedError(\"Cannot load a pickled llamacpp tokenizer\")\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.LlamaCppTokenizer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a stable representation for outlines.caching</p> Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Create a stable representation for outlines.caching\"\"\"\n    return (\n        self.vocabulary,\n        self.eos_token_id,\n        self.eos_token,\n        self.pad_token_id,\n        sorted(self.special_tokens),\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.llamacpp.llamacpp","title":"<code>llamacpp(repo_id, filename=None, **llamacpp_model_params)</code>","text":"<p>Load a model from the <code>llama-cpp-python</code> library.</p> <p>We use the <code>Llama.from_pretrained</code> classmethod that downloads models directly from the HuggingFace hub, instead of asking users to specify a path to the downloaded model. One can still load a local model by initializing <code>llama_cpp.Llama</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The name of the model repository.</p> required <code>filename</code> <code>Optional[str]</code> <p>A filename of glob pattern to match the model file in the repo.</p> <code>None</code> <code>llama_cpp_model_params</code> <p>Llama-specific model parameters. See the <code>llama-cpp-python</code> documentation for the full list: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.init</p> required Source code in <code>outlines/models/llamacpp.py</code> <pre><code>def llamacpp(\n    repo_id: str, filename: Optional[str] = None, **llamacpp_model_params\n) -&gt; LlamaCpp:\n    \"\"\"Load a model from the `llama-cpp-python` library.\n\n    We use the `Llama.from_pretrained` classmethod that downloads models\n    directly from the HuggingFace hub, instead of asking users to specify\n    a path to the downloaded model. One can still load a local model\n    by initializing `llama_cpp.Llama` directly.\n\n    Parameters\n    ----------\n    repo_id\n        The name of the model repository.\n    filename:\n        A filename of glob pattern to match the model file in the repo.\n    llama_cpp_model_params\n        Llama-specific model parameters. See the `llama-cpp-python` documentation\n        for the full list: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__init__\n\n    \"\"\"\n    from llama_cpp import Llama\n\n    # Default to using the model's full context length\n    if \"n_ctx\" not in llamacpp_model_params:\n        llamacpp_model_params[\"n_ctx\"] = 0\n\n    if \"verbose\" not in llamacpp_model_params:\n        llamacpp_model_params[\"verbose\"] = False\n\n    # TODO: Remove when https://github.com/ggerganov/llama.cpp/pull/5613 is resolved\n    if \"tokenizer\" not in llamacpp_model_params:\n        warnings.warn(\n            \"The pre-tokenizer in `llama.cpp` handles unicode improperly \"\n            + \"(https://github.com/ggerganov/llama.cpp/pull/5613)\\n\"\n            + \"Outlines may raise a `RuntimeError` when building the regex index.\\n\"\n            + \"To circumvent this error when using `models.llamacpp()` you may pass the argument\"\n            + \"`tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(&lt;hf_repo_id&gt;)`\\n\"\n        )\n\n    model = Llama.from_pretrained(repo_id, filename, **llamacpp_model_params)\n\n    return LlamaCpp(model)\n</code></pre>"},{"location":"api/models/#outlines.models.mlxlm","title":"<code>mlxlm</code>","text":""},{"location":"api/models/#outlines.models.mlxlm.MLXLM","title":"<code>MLXLM</code>","text":"<p>Represents an <code>mlx_lm</code> model</p> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>class MLXLM:\n    \"\"\"\n    Represents an `mlx_lm` model\n    \"\"\"\n\n    def __init__(\n        self,\n        model: \"nn.Module\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.model = model\n        self.mlx_tokenizer = tokenizer  # returns mlx tensors, used for encode()\n        self.tokenizer = TransformerTokenizer(\n            tokenizer._tokenizer\n        )  # _tokenizer is HF Tokenizer\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: \"GenerationParameters\",\n        logits_processor,\n        sampling_parameters: \"SamplingParameters\",\n    ) -&gt; str:\n        streamer = self.stream(\n            prompts, generation_parameters, logits_processor, sampling_parameters\n        )\n        return \"\".join(list(streamer))\n\n    def stream(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: \"GenerationParameters\",\n        logits_processor,\n        sampling_parameters: \"SamplingParameters\",\n    ) -&gt; Iterator[str]:\n        \"\"\"Generate text using `mlx_lm`.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n\n        Returns\n        -------\n        The generated text.\n        \"\"\"\n        import mlx.core as mx\n\n        max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n        sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n            sampling_parameters\n        )\n        if max_tokens is None:\n            max_tokens = int(1e9)\n\n        if not isinstance(prompts, str):\n            raise NotImplementedError(\n                \"The `mlx-lm` library does not support batch inference.\"\n            )\n        if sampler == \"beam_search\":\n            raise NotImplementedError(\n                \"The `mlx-lm` library does not support Beam Search.\"\n            )\n        if num_samples != 1:\n            raise NotImplementedError(\n                \"The `mlx-lm` library does not allow to take several samples.\"\n            )\n        if top_k is not None:\n            raise NotImplementedError(\"The `mlx-lm` library does not support top_k.\")\n        if seed is not None:\n            raise NotImplementedError(\"The `mlx-lm` library does not support seed.\")\n        if stop_at is not None:\n            raise NotImplementedError(\"The `mlx-lm` library does not support stop_at.\")\n\n        generate_kwargs = {\n            \"temp\": temperature,\n            \"top_p\": top_p,\n            \"sampler\": sampler,\n            \"logits_processor\": logits_processor,\n        }\n\n        # Adapted from\n        # https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L267\n        prompt_tokens = mx.array(self.mlx_tokenizer.encode(prompts))\n\n        detokenizer = self.mlx_tokenizer.detokenizer\n        detokenizer.reset()\n\n        for (token, prob), n in zip(\n            self.generate_step(prompt_tokens, **generate_kwargs),\n            range(max_tokens),\n        ):\n            if token == self.tokenizer.eos_token_id:\n                break\n            detokenizer.add_token(token)\n            yield detokenizer.last_segment\n\n        detokenizer.finalize()\n        yield detokenizer.last_segment\n\n    def generate_step(\n        self,\n        prompt: \"mx.array\",\n        temp: Optional[float],\n        top_p: Optional[float],\n        sampler: str,\n        logits_processor: \"OutlinesLogitsProcessor\",\n    ) -&gt; Generator[Tuple[int, float], None, None]:\n        \"\"\"\n        Adapted from\n        https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L129\n\n        A generator producing token ids based on the given prompt from the model.\n\n        Parameters\n        ----------\n        prompt\n            The input prompt.\n        temp\n            The temperature for sampling, if 0 the argmax is used.\n        top_p\n            Nulceus sampling, higher means model considers more less likely words.\n        sampler\n            The sampler string defined by SequenceGeneratorAdapter\n        logits_processor\n            Augment logits before sampling.\n        \"\"\"\n        import mlx.core as mx\n        import mlx_lm\n\n        temperature: float = temp or 1.0\n\n        def sample(logits: \"mx.array\") -&gt; Tuple[\"mx.array\", float]:\n            softmax_logits = mx.softmax(logits)\n\n            if temperature == 0.0 or sampler == \"greedy\":\n                token = mx.argmax(logits, axis=-1)\n            elif sampler == \"multinomial\":\n                if top_p is not None and top_p &gt; 0 and top_p &lt; 1.0:\n                    token = mlx_lm.sample_utils.top_p_sampling(\n                        logits, top_p, temperature\n                    )\n                else:\n                    token = mx.random.categorical(logits * (1 / temperature))\n            else:\n                raise ValueError(f\"Invalid mlx-lm sampler: `{sampler}`\")\n\n            prob = softmax_logits[0, token]\n            return token, prob\n\n        cache = mlx_lm.models.cache.make_prompt_cache(self.model)\n\n        # kv cache contains processed input IDs, we pass the unprocessed inputs and cache to model()\n        unprocessed_input_ids = prompt\n        generated_ids: List[int] = []\n\n        while True:\n            logits = self.model(unprocessed_input_ids[None], cache=cache)\n            logits = logits[:, -1, :]\n\n            if logits_processor is not None:\n                # convert to logits_processor 1d expectation, apply, then convert back\n                logits_1d = logits.reshape(-1)\n                logits_1d = logits_processor(generated_ids, logits_1d)\n                logits = logits_1d.reshape(1, -1)\n\n            new_token_single, prob = sample(logits)\n            new_token = new_token_single.item()\n            yield new_token, prob\n\n            generated_ids.append(new_token)\n            unprocessed_input_ids = new_token_single\n</code></pre>"},{"location":"api/models/#outlines.models.mlxlm.MLXLM.generate_step","title":"<code>generate_step(prompt, temp, top_p, sampler, logits_processor)</code>","text":"<p>Adapted from https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L129</p> <p>A generator producing token ids based on the given prompt from the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>array</code> <p>The input prompt.</p> required <code>temp</code> <code>Optional[float]</code> <p>The temperature for sampling, if 0 the argmax is used.</p> required <code>top_p</code> <code>Optional[float]</code> <p>Nulceus sampling, higher means model considers more less likely words.</p> required <code>sampler</code> <code>str</code> <p>The sampler string defined by SequenceGeneratorAdapter</p> required <code>logits_processor</code> <code>OutlinesLogitsProcessor</code> <p>Augment logits before sampling.</p> required Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def generate_step(\n    self,\n    prompt: \"mx.array\",\n    temp: Optional[float],\n    top_p: Optional[float],\n    sampler: str,\n    logits_processor: \"OutlinesLogitsProcessor\",\n) -&gt; Generator[Tuple[int, float], None, None]:\n    \"\"\"\n    Adapted from\n    https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L129\n\n    A generator producing token ids based on the given prompt from the model.\n\n    Parameters\n    ----------\n    prompt\n        The input prompt.\n    temp\n        The temperature for sampling, if 0 the argmax is used.\n    top_p\n        Nulceus sampling, higher means model considers more less likely words.\n    sampler\n        The sampler string defined by SequenceGeneratorAdapter\n    logits_processor\n        Augment logits before sampling.\n    \"\"\"\n    import mlx.core as mx\n    import mlx_lm\n\n    temperature: float = temp or 1.0\n\n    def sample(logits: \"mx.array\") -&gt; Tuple[\"mx.array\", float]:\n        softmax_logits = mx.softmax(logits)\n\n        if temperature == 0.0 or sampler == \"greedy\":\n            token = mx.argmax(logits, axis=-1)\n        elif sampler == \"multinomial\":\n            if top_p is not None and top_p &gt; 0 and top_p &lt; 1.0:\n                token = mlx_lm.sample_utils.top_p_sampling(\n                    logits, top_p, temperature\n                )\n            else:\n                token = mx.random.categorical(logits * (1 / temperature))\n        else:\n            raise ValueError(f\"Invalid mlx-lm sampler: `{sampler}`\")\n\n        prob = softmax_logits[0, token]\n        return token, prob\n\n    cache = mlx_lm.models.cache.make_prompt_cache(self.model)\n\n    # kv cache contains processed input IDs, we pass the unprocessed inputs and cache to model()\n    unprocessed_input_ids = prompt\n    generated_ids: List[int] = []\n\n    while True:\n        logits = self.model(unprocessed_input_ids[None], cache=cache)\n        logits = logits[:, -1, :]\n\n        if logits_processor is not None:\n            # convert to logits_processor 1d expectation, apply, then convert back\n            logits_1d = logits.reshape(-1)\n            logits_1d = logits_processor(generated_ids, logits_1d)\n            logits = logits_1d.reshape(1, -1)\n\n        new_token_single, prob = sample(logits)\n        new_token = new_token_single.item()\n        yield new_token, prob\n\n        generated_ids.append(new_token)\n        unprocessed_input_ids = new_token_single\n</code></pre>"},{"location":"api/models/#outlines.models.mlxlm.MLXLM.stream","title":"<code>stream(prompts, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Generate text using <code>mlx_lm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <p>Returns:</p> Type Description <code>The generated text.</code> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def stream(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: \"GenerationParameters\",\n    logits_processor,\n    sampling_parameters: \"SamplingParameters\",\n) -&gt; Iterator[str]:\n    \"\"\"Generate text using `mlx_lm`.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n\n    Returns\n    -------\n    The generated text.\n    \"\"\"\n    import mlx.core as mx\n\n    max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n    sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n        sampling_parameters\n    )\n    if max_tokens is None:\n        max_tokens = int(1e9)\n\n    if not isinstance(prompts, str):\n        raise NotImplementedError(\n            \"The `mlx-lm` library does not support batch inference.\"\n        )\n    if sampler == \"beam_search\":\n        raise NotImplementedError(\n            \"The `mlx-lm` library does not support Beam Search.\"\n        )\n    if num_samples != 1:\n        raise NotImplementedError(\n            \"The `mlx-lm` library does not allow to take several samples.\"\n        )\n    if top_k is not None:\n        raise NotImplementedError(\"The `mlx-lm` library does not support top_k.\")\n    if seed is not None:\n        raise NotImplementedError(\"The `mlx-lm` library does not support seed.\")\n    if stop_at is not None:\n        raise NotImplementedError(\"The `mlx-lm` library does not support stop_at.\")\n\n    generate_kwargs = {\n        \"temp\": temperature,\n        \"top_p\": top_p,\n        \"sampler\": sampler,\n        \"logits_processor\": logits_processor,\n    }\n\n    # Adapted from\n    # https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L267\n    prompt_tokens = mx.array(self.mlx_tokenizer.encode(prompts))\n\n    detokenizer = self.mlx_tokenizer.detokenizer\n    detokenizer.reset()\n\n    for (token, prob), n in zip(\n        self.generate_step(prompt_tokens, **generate_kwargs),\n        range(max_tokens),\n    ):\n        if token == self.tokenizer.eos_token_id:\n            break\n        detokenizer.add_token(token)\n        yield detokenizer.last_segment\n\n    detokenizer.finalize()\n    yield detokenizer.last_segment\n</code></pre>"},{"location":"api/models/#outlines.models.mlxlm.mlxlm","title":"<code>mlxlm(model_name, tokenizer_config={}, model_config={}, adapter_path=None, lazy=False)</code>","text":"<p>Instantiate a model from the <code>mlx_lm</code> library and its tokenizer.</p> <p>Signature adapted from https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L422</p> <p>Parameters:</p> Name Type Description Default <code>Args</code> <p>path_or_hf_repo (Path): The path or the huggingface repository to load the model from. tokenizer_config (dict, optional): Configuration parameters specifically for the tokenizer.     Defaults to an empty dictionary. model_config(dict, optional): Configuration parameters specifically for the model.     Defaults to an empty dictionary. adapter_path (str, optional): Path to the LoRA adapters. If provided, applies LoRA layers     to the model. Default: <code>None</code>. lazy (bool): If False eval the model parameters to make sure they are     loaded in memory before returning, otherwise they will be loaded     when needed. Default: <code>False</code></p> required <p>Returns:</p> Type Description <code>A `MLXLM` model instance.</code> Source code in <code>outlines/models/mlxlm.py</code> <pre><code>def mlxlm(\n    model_name: str,\n    tokenizer_config: dict = {},\n    model_config: dict = {},\n    adapter_path: Optional[str] = None,\n    lazy: bool = False,\n):\n    \"\"\"Instantiate a model from the `mlx_lm` library and its tokenizer.\n\n    Signature adapted from\n    https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L422\n\n    Parameters\n    ----------\n    Args:\n        path_or_hf_repo (Path): The path or the huggingface repository to load the model from.\n        tokenizer_config (dict, optional): Configuration parameters specifically for the tokenizer.\n            Defaults to an empty dictionary.\n        model_config(dict, optional): Configuration parameters specifically for the model.\n            Defaults to an empty dictionary.\n        adapter_path (str, optional): Path to the LoRA adapters. If provided, applies LoRA layers\n            to the model. Default: ``None``.\n        lazy (bool): If False eval the model parameters to make sure they are\n            loaded in memory before returning, otherwise they will be loaded\n            when needed. Default: ``False``\n\n    Returns\n    -------\n    A `MLXLM` model instance.\n\n    \"\"\"\n    try:\n        import mlx.core as mx\n        import mlx_lm\n    except ImportError:\n        raise ImportError(\n            \"The `mlx_lm` library needs to be installed in order to use `mlx_lm` models.\"\n        )\n    if not mx.metal.is_available():\n        raise RuntimeError(\"You cannot use `mlx_lm` without Apple Silicon (Metal)\")\n\n    model, tokenizer = mlx_lm.load(\n        model_name,\n        tokenizer_config=tokenizer_config,\n        model_config=model_config,\n        adapter_path=adapter_path,\n        lazy=lazy,\n    )\n    return MLXLM(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.openai","title":"<code>openai</code>","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>An object that represents the OpenAI API.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI:\n    \"\"\"An object that represents the OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        client,\n        config,\n        system_prompt: Optional[str] = None,\n    ):\n        \"\"\"Create an `OpenAI` instance.\n\n        This class supports the standard OpenAI API, the Azure OpeanAI API as\n        well as compatible APIs that rely on the OpenAI client.\n\n        Parameters\n        ----------\n        client\n            An instance of the API's async client.\n        config\n            An instance of `OpenAIConfig`. Can be useful to specify some\n            parameters that cannot be set by calling this class' methods.\n        \"\"\"\n\n        self.client = client\n        self.config = config\n\n        # We count the total number of prompt and generated tokens as returned\n        # by the OpenAI API, summed over all the requests performed with this\n        # model instance.\n        self.prompt_tokens = 0\n        self.completion_tokens = 0\n\n        self.format_sequence = lambda seq: seq\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        max_tokens: Optional[int] = None,\n        stop_at: Optional[Union[List[str], str]] = None,\n        *,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        samples: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Call the OpenAI API to generate text.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        max_tokens\n            The maximum number of tokens to generate\n        stop_at\n            A string or array of strings which, such that the generation stops\n            when they are generated.\n        system_prompt\n            The content of the system message that precedes the user's prompt.\n        temperature\n            The value of the temperature used to sample tokens\n        samples\n            The number of completions to generate for each prompt\n        stop_at\n            Up to 4 words where the API will stop the completion.\n\n        \"\"\"\n        if max_tokens is None:\n            max_tokens = self.config.max_tokens\n        if stop_at is None:\n            stop_at = self.config.stop\n        if temperature is None:\n            temperature = self.config.temperature\n        if samples is None:\n            samples = self.config.n\n\n        config = replace(\n            self.config,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            n=samples,\n            stop=stop_at,\n        )  # type: ignore\n\n        response, prompt_tokens, completion_tokens = generate_chat(\n            prompt, system_prompt, self.client, config\n        )\n        self.prompt_tokens += prompt_tokens\n        self.completion_tokens += completion_tokens\n\n        return self.format_sequence(response)\n\n    def stream(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"Streaming is currently not supported for the OpenAI API\"\n        )\n\n    def new_with_replacements(self, **kwargs):\n        new_instance = copy.copy(self)\n        new_instance.config = replace(new_instance.config, **kwargs)\n        return new_instance\n\n    def __str__(self):\n        return self.__class__.__name__ + \" API\"\n\n    def __repr__(self):\n        return str(self.config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__","title":"<code>__call__(prompt, max_tokens=None, stop_at=None, *, system_prompt=None, temperature=None, samples=None)</code>","text":"<p>Call the OpenAI API to generate text.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Union[str, List[str]]</code> <p>A string or list of strings that will be used to prompt the model</p> required <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate</p> <code>None</code> <code>stop_at</code> <code>Optional[Union[List[str], str]]</code> <p>A string or array of strings which, such that the generation stops when they are generated.</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>The content of the system message that precedes the user's prompt.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The value of the temperature used to sample tokens</p> <code>None</code> <code>samples</code> <code>Optional[int]</code> <p>The number of completions to generate for each prompt</p> <code>None</code> <code>stop_at</code> <code>Optional[Union[List[str], str]]</code> <p>Up to 4 words where the API will stop the completion.</p> <code>None</code> Source code in <code>outlines/models/openai.py</code> <pre><code>def __call__(\n    self,\n    prompt: Union[str, List[str]],\n    max_tokens: Optional[int] = None,\n    stop_at: Optional[Union[List[str], str]] = None,\n    *,\n    system_prompt: Optional[str] = None,\n    temperature: Optional[float] = None,\n    samples: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Call the OpenAI API to generate text.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    max_tokens\n        The maximum number of tokens to generate\n    stop_at\n        A string or array of strings which, such that the generation stops\n        when they are generated.\n    system_prompt\n        The content of the system message that precedes the user's prompt.\n    temperature\n        The value of the temperature used to sample tokens\n    samples\n        The number of completions to generate for each prompt\n    stop_at\n        Up to 4 words where the API will stop the completion.\n\n    \"\"\"\n    if max_tokens is None:\n        max_tokens = self.config.max_tokens\n    if stop_at is None:\n        stop_at = self.config.stop\n    if temperature is None:\n        temperature = self.config.temperature\n    if samples is None:\n        samples = self.config.n\n\n    config = replace(\n        self.config,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        n=samples,\n        stop=stop_at,\n    )  # type: ignore\n\n    response, prompt_tokens, completion_tokens = generate_chat(\n        prompt, system_prompt, self.client, config\n    )\n    self.prompt_tokens += prompt_tokens\n    self.completion_tokens += completion_tokens\n\n    return self.format_sequence(response)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(client, config, system_prompt=None)</code>","text":"<p>Create an <code>OpenAI</code> instance.</p> <p>This class supports the standard OpenAI API, the Azure OpeanAI API as well as compatible APIs that rely on the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An instance of the API's async client.</p> required <code>config</code> <p>An instance of <code>OpenAIConfig</code>. Can be useful to specify some parameters that cannot be set by calling this class' methods.</p> required Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client,\n    config,\n    system_prompt: Optional[str] = None,\n):\n    \"\"\"Create an `OpenAI` instance.\n\n    This class supports the standard OpenAI API, the Azure OpeanAI API as\n    well as compatible APIs that rely on the OpenAI client.\n\n    Parameters\n    ----------\n    client\n        An instance of the API's async client.\n    config\n        An instance of `OpenAIConfig`. Can be useful to specify some\n        parameters that cannot be set by calling this class' methods.\n    \"\"\"\n\n    self.client = client\n    self.config = config\n\n    # We count the total number of prompt and generated tokens as returned\n    # by the OpenAI API, summed over all the requests performed with this\n    # model instance.\n    self.prompt_tokens = 0\n    self.completion_tokens = 0\n\n    self.format_sequence = lambda seq: seq\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig","title":"<code>OpenAIConfig</code>  <code>dataclass</code>","text":"<p>Represents the parameters of the OpenAI API.</p> <p>The information was last fetched on 2023/11/20. We document below the properties that are specific to the OpenAI API. Not all these properties are supported by Outlines.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name of the model. Available models can be found on OpenAI's website.</p> <code>''</code> <code>frequency_penalty</code> <code>float</code> <p>Number between 2.0 and -2.0. Positive values penalize new tokens based on their existing frequency in the text,</p> <code>0</code> <code>logit_bias</code> <code>Dict[int, int]</code> <p>Modifies the likelihood of specified tokens to appear in the completion. Number between -100 (forbid) and +100 (only allows).</p> <code>dict()</code> <code>n</code> <code>int</code> <p>The number of completions to return for each prompt.</p> <code>1</code> <code>presence_penalty</code> <code>float</code> <p>Similar to frequency penalty.</p> <code>0</code> <code>response_format</code> <code>Optional[Dict[str, str]]</code> <p>Specifies the format the model must output. <code>{\"type\": \"json_object\"}</code> enables JSON mode.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Two completions with the same <code>seed</code> value should return the same completion. This is however not guaranteed.</p> <code>None</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>Up to 4 words where the API will stop the completion.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Number between 0 and 2. Higher values make the output more random, while lower values make it more deterministic.</p> <code>1.0</code> <code>top_p</code> <code>int</code> <p>Number between 0 and 1. Parameter for nucleus sampling.</p> <code>1</code> <code>user</code> <code>str</code> <p>A unique identifier for the end-user.</p> <code>str()</code> Source code in <code>outlines/models/openai.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Represents the parameters of the OpenAI API.\n\n    The information was last fetched on 2023/11/20. We document below the\n    properties that are specific to the OpenAI API. Not all these properties are\n    supported by Outlines.\n\n    Parameters\n    ----------\n    model\n        The name of the model. Available models can be found on OpenAI's website.\n    frequency_penalty\n        Number between 2.0 and -2.0. Positive values penalize new tokens based on\n        their existing frequency in the text,\n    logit_bias\n        Modifies the likelihood of specified tokens to appear in the completion.\n        Number between -100 (forbid) and +100 (only allows).\n    n\n        The number of completions to return for each prompt.\n    presence_penalty\n        Similar to frequency penalty.\n    response_format\n        Specifies the format the model must output. `{\"type\": \"json_object\"}`\n        enables JSON mode.\n    seed\n        Two completions with the same `seed` value should return the same\n        completion. This is however not guaranteed.\n    stop\n        Up to 4 words where the API will stop the completion.\n    temperature\n        Number between 0 and 2. Higher values make the output more random, while\n        lower values make it more deterministic.\n    top_p\n        Number between 0 and 1. Parameter for nucleus sampling.\n    user\n        A unique identifier for the end-user.\n    \"\"\"\n\n    model: str = \"\"\n    frequency_penalty: float = 0\n    logit_bias: Dict[int, int] = field(default_factory=dict)\n    max_tokens: Optional[int] = None\n    n: int = 1\n    presence_penalty: float = 0\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    temperature: float = 1.0\n    top_p: int = 1\n    user: str = field(default_factory=str)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.error_handler","title":"<code>error_handler(api_call_fn)</code>","text":"<p>Handle OpenAI API errors and missing API key.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def error_handler(api_call_fn: Callable) -&gt; Callable:\n    \"\"\"Handle OpenAI API errors and missing API key.\"\"\"\n\n    def call(*args, **kwargs):\n        import openai\n\n        try:\n            return api_call_fn(*args, **kwargs)\n        except (\n            openai.APITimeoutError,\n            openai.InternalServerError,\n            openai.RateLimitError,\n        ) as e:\n            raise OSError(f\"Could not connect to the OpenAI API: {e}\")\n        except (\n            openai.AuthenticationError,\n            openai.BadRequestError,\n            openai.ConflictError,\n            openai.PermissionDeniedError,\n            openai.NotFoundError,\n            openai.UnprocessableEntityError,\n        ) as e:\n            raise e\n\n    return call\n</code></pre>"},{"location":"api/models/#outlines.models.openai.generate_chat","title":"<code>generate_chat(prompt, system_prompt, client, config)</code>  <code>async</code>","text":"<p>Call OpenAI's Chat Completion API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt we use to start the generation. Passed to the model with the \"user\" role.</p> required <code>system_prompt</code> <code>Union[str, None]</code> <p>The system prompt, passed to the model with the \"system\" role before the prompt.</p> required <code>client</code> <p>The API client</p> required <code>config</code> <code>OpenAIConfig</code> <p>An <code>OpenAIConfig</code> instance.</p> required <p>Returns:</p> Type Description <code>A tuple that contains the model's response(s) and usage statistics.</code> Source code in <code>outlines/models/openai.py</code> <pre><code>@functools.partial(vectorize, signature=\"(),(),(),()-&gt;(s),(),()\")\nasync def generate_chat(\n    prompt: str,\n    system_prompt: Union[str, None],\n    client,\n    config: OpenAIConfig,\n) -&gt; Tuple[np.ndarray, int, int]:\n    \"\"\"Call OpenAI's Chat Completion API.\n\n    Parameters\n    ----------\n    prompt\n        The prompt we use to start the generation. Passed to the model\n        with the \"user\" role.\n    system_prompt\n        The system prompt, passed to the model with the \"system\" role\n        before the prompt.\n    client\n        The API client\n    config\n        An `OpenAIConfig` instance.\n\n    Returns\n    -------\n    A tuple that contains the model's response(s) and usage statistics.\n\n    \"\"\"\n\n    @error_handler\n    @cache()\n    async def call_api(prompt, system_prompt, config):\n        responses = await client.chat.completions.create(\n            messages=system_message + user_message,\n            **asdict(config),  # type: ignore\n        )\n        return responses.model_dump()\n\n    system_message = (\n        [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n    )\n    user_message = [{\"role\": \"user\", \"content\": prompt}]\n\n    responses = await call_api(prompt, system_prompt, config)\n\n    results = np.array(\n        [responses[\"choices\"][i][\"message\"][\"content\"] for i in range(config.n)]\n    )\n    usage = responses[\"usage\"]\n\n    return results, usage[\"prompt_tokens\"], usage[\"completion_tokens\"]\n</code></pre>"},{"location":"api/models/#outlines.models.tokenizer","title":"<code>tokenizer</code>","text":""},{"location":"api/models/#outlines.models.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Hashable</code>, <code>Protocol</code></p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>class Tokenizer(Hashable, Protocol):\n    eos_token: str\n    eos_token_id: int\n    pad_token_id: int\n    vocabulary: Dict[str, int]\n    special_tokens: Set[str]\n\n    def encode(\n        self, prompt: Union[str, List[str]]\n    ) -&gt; Tuple[NDArray[np.int64], NDArray[np.int64]]:\n        \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n        ...\n\n    def decode(self, token_ids: NDArray[np.int64]) -&gt; List[str]:\n        \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n        ...\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        \"\"\"Convert a token to its equivalent string.\n\n        This is for instance useful for BPE tokenizers where whitespaces are\n        represented by the special characted `\u0120`. This prevents matching a raw\n        token that includes `\u0120` with a string.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/models/#outlines.models.tokenizer.Tokenizer.convert_token_to_string","title":"<code>convert_token_to_string(token)</code>","text":"<p>Convert a token to its equivalent string.</p> <p>This is for instance useful for BPE tokenizers where whitespaces are represented by the special characted <code>\u0120</code>. This prevents matching a raw token that includes <code>\u0120</code> with a string.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def convert_token_to_string(self, token: str) -&gt; str:\n    \"\"\"Convert a token to its equivalent string.\n\n    This is for instance useful for BPE tokenizers where whitespaces are\n    represented by the special characted `\u0120`. This prevents matching a raw\n    token that includes `\u0120` with a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/models/#outlines.models.tokenizer.Tokenizer.decode","title":"<code>decode(token_ids)</code>","text":"<p>Translate an array of token ids to a string or list of strings.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def decode(self, token_ids: NDArray[np.int64]) -&gt; List[str]:\n    \"\"\"Translate an array of token ids to a string or list of strings.\"\"\"\n    ...\n</code></pre>"},{"location":"api/models/#outlines.models.tokenizer.Tokenizer.encode","title":"<code>encode(prompt)</code>","text":"<p>Translate the input prompts into arrays of token ids and attention mask.</p> Source code in <code>outlines/models/tokenizer.py</code> <pre><code>def encode(\n    self, prompt: Union[str, List[str]]\n) -&gt; Tuple[NDArray[np.int64], NDArray[np.int64]]:\n    \"\"\"Translate the input prompts into arrays of token ids and attention mask.\"\"\"\n    ...\n</code></pre>"},{"location":"api/models/#outlines.models.transformers","title":"<code>transformers</code>","text":""},{"location":"api/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers","title":"<code>Transformers</code>","text":"<p>Represents a <code>transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers:\n    \"\"\"Represents a `transformers` model.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.model = model\n        self.tokenizer = TransformerTokenizer(tokenizer)\n\n    def forward(\n        self,\n        input_ids: \"torch.LongTensor\",\n        attention_mask: \"torch.LongTensor\",\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; Tuple[\"torch.FloatTensor\", Optional[KVCacheType]]:\n        \"\"\"Compute a forward pass through the transformer model.\n\n        Parameters\n        ----------\n        input_ids\n            The input token ids.  Must be one or two dimensional.\n        attention_mask\n            The attention mask.  Must be one or two dimensional.\n        past_key_values\n            A tuple of tuples containing the cached key and value tensors for each\n            attention head.\n\n        Returns\n        -------\n        The computed logits and the new cached key and value tensors.\n\n        \"\"\"\n        try:\n            import torch\n        except ImportError:\n            ImportError(\n                \"The `torch` library needs to be installed to use `transformers` models.\"\n            )\n        assert 0 &lt; input_ids.ndim &lt; 3\n\n        if past_key_values:\n            input_ids = input_ids[..., -1].unsqueeze(-1)\n\n        with torch.inference_mode():\n            output = self.model(\n                input_ids,\n                attention_mask=attention_mask,\n                return_dict=True,\n                output_attentions=False,\n                output_hidden_states=False,\n                past_key_values=past_key_values,\n            )\n\n        return output.logits, output.past_key_values\n\n    def __call__(\n        self,\n        input_ids: \"torch.LongTensor\",\n        attention_mask: \"torch.LongTensor\",\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; \"torch.FloatTensor\":\n        logits, kv_cache = self.forward(input_ids, attention_mask, past_key_values)\n        next_token_logits = logits[..., -1, :]\n\n        return next_token_logits, kv_cache\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Union[str, List[str], List[List[str]]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n\n        Returns\n        -------\n        The generated text\n        \"\"\"\n        if isinstance(prompts, str):\n            # convert to 2d\n            input_ids, attention_mask = self.tokenizer.encode([prompts])\n        else:\n            input_ids, attention_mask = self.tokenizer.encode(prompts)\n\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": attention_mask.to(self.model.device),\n        }\n        if (\n            \"attention_mask\"\n            not in inspect.signature(self.model.forward).parameters.keys()\n        ):\n            del inputs[\"attention_mask\"]\n\n        generation_kwargs = self._get_generation_kwargs(\n            prompts,\n            generation_parameters,\n            logits_processor,\n            sampling_parameters,\n        )\n        generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n        # if single str input and single sample per input, convert to a 1D output\n        if isinstance(prompts, str):\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def stream(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Iterator[Union[str, List[str]]]:\n        \"\"\"\n        Temporary stream stand-in which implements stream() signature\n        and equivalent behaviour but isn't yielded until generation completes.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n        \"\"\"\n        if isinstance(prompts, str):\n            # convert to 2d\n            input_ids, attention_mask = self.tokenizer.encode([prompts])\n        else:\n            input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": attention_mask.to(self.model.device),\n        }\n        if (\n            \"attention_mask\"\n            not in inspect.signature(self.model.forward).parameters.keys()\n        ):\n            del inputs[\"attention_mask\"]\n\n        generation_kwargs = self._get_generation_kwargs(\n            prompts,\n            generation_parameters,\n            logits_processor,\n            sampling_parameters,\n        )\n        generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n        # if single str input and single sample per input, convert to a 1D output\n        if isinstance(prompts, str):\n            generated_ids = generated_ids.squeeze(0)\n\n        for i in range(generated_ids.size(-1)):\n            output_group_ids = generated_ids.select(-1, i).unsqueeze(-1)\n            yield self._decode_generation(output_group_ids)\n\n    def _get_generation_kwargs(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; dict:\n        \"\"\"\n        Convert outlines generation parameters into model.generate kwargs\n        \"\"\"\n        from transformers import GenerationConfig, LogitsProcessorList, set_seed\n\n        max_new_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n        sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n            sampling_parameters\n        )\n        if max_new_tokens is None:\n            max_new_tokens = int(2**30)\n\n        # global seed, not desirable\n        if seed is not None:\n            set_seed(seed)\n\n        if logits_processor is not None:\n            logits_processor_list = LogitsProcessorList([logits_processor])\n        else:\n            logits_processor_list = None\n\n        generation_config = GenerationConfig(\n            max_new_tokens=max_new_tokens,\n            stop_strings=stop_at,\n            num_return_sequences=(num_samples or 1),\n            top_p=top_p,\n            top_k=top_k,\n            temperature=temperature,\n            do_sample=(sampler == \"multinomial\"),\n            num_beams=(num_samples if sampler == \"beam_search\" else 1),\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n\n        return dict(\n            logits_processor=logits_processor_list,\n            generation_config=generation_config,\n            tokenizer=self.tokenizer.tokenizer,\n        )\n\n    def _generate_output_seq(\n        self, prompts, inputs, generation_config, **generation_kwargs\n    ):\n        input_ids = inputs[\"input_ids\"]\n        output_ids = self.model.generate(\n            **inputs, generation_config=generation_config, **generation_kwargs\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        # if batch list inputs AND multiple samples per input, convert generated_id to 3D view\n        num_samples = generation_config.num_return_sequences or 1\n\n        if num_samples &gt; 1 and isinstance(prompts, list):\n            batch_size = input_ids.size(0)\n            num_return_sequences = generation_config.num_return_sequences or 1\n            generated_ids = generated_ids.view(batch_size, num_return_sequences, -1)\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:\n            raise TypeError(\n                f\"Generated outputs aren't 1D, 2D or 3D, but instead are {generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.forward","title":"<code>forward(input_ids, attention_mask, past_key_values=None)</code>","text":"<p>Compute a forward pass through the transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>The input token ids.  Must be one or two dimensional.</p> required <code>attention_mask</code> <code>LongTensor</code> <p>The attention mask.  Must be one or two dimensional.</p> required <code>past_key_values</code> <code>Optional[Tuple]</code> <p>A tuple of tuples containing the cached key and value tensors for each attention head.</p> <code>None</code> <p>Returns:</p> Type Description <code>The computed logits and the new cached key and value tensors.</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def forward(\n    self,\n    input_ids: \"torch.LongTensor\",\n    attention_mask: \"torch.LongTensor\",\n    past_key_values: Optional[Tuple] = None,\n) -&gt; Tuple[\"torch.FloatTensor\", Optional[KVCacheType]]:\n    \"\"\"Compute a forward pass through the transformer model.\n\n    Parameters\n    ----------\n    input_ids\n        The input token ids.  Must be one or two dimensional.\n    attention_mask\n        The attention mask.  Must be one or two dimensional.\n    past_key_values\n        A tuple of tuples containing the cached key and value tensors for each\n        attention head.\n\n    Returns\n    -------\n    The computed logits and the new cached key and value tensors.\n\n    \"\"\"\n    try:\n        import torch\n    except ImportError:\n        ImportError(\n            \"The `torch` library needs to be installed to use `transformers` models.\"\n        )\n    assert 0 &lt; input_ids.ndim &lt; 3\n\n    if past_key_values:\n        input_ids = input_ids[..., -1].unsqueeze(-1)\n\n    with torch.inference_mode():\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            past_key_values=past_key_values,\n        )\n\n    return output.logits, output.past_key_values\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.generate","title":"<code>generate(prompts, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <p>Returns:</p> Type Description <code>The generated text</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n    sampling_parameters: SamplingParameters,\n) -&gt; Union[str, List[str], List[List[str]]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n\n    Returns\n    -------\n    The generated text\n    \"\"\"\n    if isinstance(prompts, str):\n        # convert to 2d\n        input_ids, attention_mask = self.tokenizer.encode([prompts])\n    else:\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n\n    inputs = {\n        \"input_ids\": input_ids.to(self.model.device),\n        \"attention_mask\": attention_mask.to(self.model.device),\n    }\n    if (\n        \"attention_mask\"\n        not in inspect.signature(self.model.forward).parameters.keys()\n    ):\n        del inputs[\"attention_mask\"]\n\n    generation_kwargs = self._get_generation_kwargs(\n        prompts,\n        generation_parameters,\n        logits_processor,\n        sampling_parameters,\n    )\n    generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n    # if single str input and single sample per input, convert to a 1D output\n    if isinstance(prompts, str):\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.stream","title":"<code>stream(prompts, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Temporary stream stand-in which implements stream() signature and equivalent behaviour but isn't yielded until generation completes.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def stream(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n    sampling_parameters: SamplingParameters,\n) -&gt; Iterator[Union[str, List[str]]]:\n    \"\"\"\n    Temporary stream stand-in which implements stream() signature\n    and equivalent behaviour but isn't yielded until generation completes.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n    \"\"\"\n    if isinstance(prompts, str):\n        # convert to 2d\n        input_ids, attention_mask = self.tokenizer.encode([prompts])\n    else:\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n    inputs = {\n        \"input_ids\": input_ids.to(self.model.device),\n        \"attention_mask\": attention_mask.to(self.model.device),\n    }\n    if (\n        \"attention_mask\"\n        not in inspect.signature(self.model.forward).parameters.keys()\n    ):\n        del inputs[\"attention_mask\"]\n\n    generation_kwargs = self._get_generation_kwargs(\n        prompts,\n        generation_parameters,\n        logits_processor,\n        sampling_parameters,\n    )\n    generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n    # if single str input and single sample per input, convert to a 1D output\n    if isinstance(prompts, str):\n        generated_ids = generated_ids.squeeze(0)\n\n    for i in range(generated_ids.size(-1)):\n        output_group_ids = generated_ids.select(-1, i).unsqueeze(-1)\n        yield self._decode_generation(output_group_ids)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.transformers","title":"<code>transformers(model_name, device=None, model_kwargs={}, tokenizer_kwargs={}, model_class=None, tokenizer_class=None)</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model as listed on Hugging Face's model page.</p> required <code>device</code> <code>Optional[str]</code> <p>The device(s) on which the model should be loaded. This overrides the <code>device_map</code> entry in <code>model_kwargs</code> when provided.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>A dictionary that contains the keyword arguments to pass to the <code>from_pretrained</code> method when loading the model.</p> <code>{}</code> <code>tokenizer_kwargs</code> <code>dict</code> <p>A dictionary that contains the keyword arguments to pass to the <code>from_pretrained</code> method when loading the tokenizer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>A `TransformersModel` model instance.</code> Source code in <code>outlines/models/transformers.py</code> <pre><code>def transformers(\n    model_name: str,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n    model_class=None,\n    tokenizer_class=None,\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    tokenizer_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the tokenizer.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    if model_class is None or tokenizer_class is None:\n        try:\n            from transformers import AutoModelForCausalLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n            )\n    if model_class is None:\n        model_class = AutoModelForCausalLM\n    if tokenizer_class is None:\n        tokenizer_class = AutoTokenizer\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = model_class.from_pretrained(model_name, **model_kwargs)\n\n    tokenizer_kwargs.setdefault(\"padding_side\", \"left\")\n    tokenizer = tokenizer_class.from_pretrained(model_name, **tokenizer_kwargs)\n\n    return Transformers(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers_vision","title":"<code>transformers_vision</code>","text":""},{"location":"api/models/#outlines.models.transformers_vision.TransformersVision","title":"<code>TransformersVision</code>","text":"<p>               Bases: <code>Transformers</code></p> Source code in <code>outlines/models/transformers_vision.py</code> <pre><code>class TransformersVision(Transformers):\n    def __init__(self, model, tokenizer, processor):\n        super().__init__(model, tokenizer)\n        self.processor = processor\n\n    def generate(  # type: ignore\n        self,\n        prompts: Union[str, List[str]],\n        media: Union[List[Any], List[List[Any]]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Union[str, List[str], List[List[str]]]:\n        \"\"\"Generate text using `transformers`.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        media\n            A List[PIL.Image] or List[List[PIL.Image]]\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n\n        Returns\n        -------\n        The generated text\n        \"\"\"\n        inputs = self.processor(\n            text=prompts, images=media, padding=True, return_tensors=\"pt\"\n        ).to(self.model.device)\n\n        generation_kwargs = self._get_generation_kwargs(\n            prompts,\n            generation_parameters,\n            logits_processor,\n            sampling_parameters,\n        )\n        generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n        # if single str input and single sample per input, convert to a 1D output\n        if isinstance(prompts, str):\n            # Should always be true until NotImplementedError above is fixed\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def stream(  # type: ignore\n        self,\n        prompts: Union[str, List[str]],\n        media: Union[Any, List[Any]],  # TODO: docstring\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Iterator[Union[str, List[str]]]:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/models/#outlines.models.transformers_vision.TransformersVision.generate","title":"<code>generate(prompts, media, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Generate text using <code>transformers</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>media</code> <code>Union[List[Any], List[List[Any]]]</code> <p>A List[PIL.Image] or List[List[PIL.Image]]</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <code>Optional[OutlinesLogitsProcessor]</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <p>Returns:</p> Type Description <code>The generated text</code> Source code in <code>outlines/models/transformers_vision.py</code> <pre><code>def generate(  # type: ignore\n    self,\n    prompts: Union[str, List[str]],\n    media: Union[List[Any], List[List[Any]]],\n    generation_parameters: GenerationParameters,\n    logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n    sampling_parameters: SamplingParameters,\n) -&gt; Union[str, List[str], List[List[str]]]:\n    \"\"\"Generate text using `transformers`.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    media\n        A List[PIL.Image] or List[List[PIL.Image]]\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n\n    Returns\n    -------\n    The generated text\n    \"\"\"\n    inputs = self.processor(\n        text=prompts, images=media, padding=True, return_tensors=\"pt\"\n    ).to(self.model.device)\n\n    generation_kwargs = self._get_generation_kwargs(\n        prompts,\n        generation_parameters,\n        logits_processor,\n        sampling_parameters,\n    )\n    generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n    # if single str input and single sample per input, convert to a 1D output\n    if isinstance(prompts, str):\n        # Should always be true until NotImplementedError above is fixed\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers_vision.transformers_vision","title":"<code>transformers_vision(model_name, model_class, device=None, model_kwargs={}, processor_kwargs={}, tokenizer_class=None, processor_class=None)</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model as listed on Hugging Face's model page.</p> required <code>model_class</code> <p>The <code>PreTrainedModel</code> class from transformers to use in initializing the vision model from <code>model_name</code>. https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel</p> required <code>device</code> <code>Optional[str]</code> <p>The device(s) on which the model should be loaded. This overrides the <code>device_map</code> entry in <code>model_kwargs</code> when provided.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>A dictionary that contains the keyword arguments to pass to the <code>from_pretrained</code> method when loading the model.</p> <code>{}</code> <code>processor_kwargs</code> <code>dict</code> <p>A dictionary that contains the keyword arguments to pass to the <code>from_pretrained</code> method when loading the processor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>A `TransformersModel` model instance.</code> Source code in <code>outlines/models/transformers_vision.py</code> <pre><code>def transformers_vision(\n    model_name: str,\n    model_class,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    processor_kwargs: dict = {},\n    tokenizer_class=None,\n    processor_class=None,\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    model_class\n        The `PreTrainedModel` class from transformers to use in initializing the vision model from `model_name`.\n        https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    processor_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the processor.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    if processor_class is None or tokenizer_class is None:\n        try:\n            from transformers import AutoProcessor, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n            )\n    if processor_class is None:\n        processor_class = AutoProcessor\n    if tokenizer_class is None:\n        tokenizer_class = AutoTokenizer\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = model_class.from_pretrained(model_name, **model_kwargs)\n\n    processor_kwargs.setdefault(\"padding_side\", \"left\")\n    processor_kwargs.setdefault(\"pad_token\", \"[PAD]\")\n    processor = processor_class.from_pretrained(model_name, **processor_kwargs)\n\n    if tokenizer_class is None:\n        if getattr(processor, \"tokenizer\", None):\n            tokenizer = processor.tokenizer\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(model_name, **processor_kwargs)\n    else:\n        tokenizer = tokenizer_class.from_pretrained(model_name, **processor_kwargs)\n\n    return TransformersVision(model, tokenizer, processor)\n</code></pre>"},{"location":"api/models/#outlines.models.vllm","title":"<code>vllm</code>","text":""},{"location":"api/models/#outlines.models.vllm.VLLM","title":"<code>VLLM</code>","text":"<p>Represents a vLLM model.</p> <p>We wrap models from model providing libraries in order to give all of them the same interface in Outlines and allow users to easily switch between providers. This class wraps the <code>vllm.LLM</code> class from the <code>vllm</code> library.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>class VLLM:\n    \"\"\"Represents a vLLM model.\n\n    We wrap models from model providing libraries in order to give all of\n    them the same interface in Outlines and allow users to easily switch\n    between providers. This class wraps the `vllm.LLM` class from the\n    `vllm` library.\n\n    \"\"\"\n\n    def __init__(self, model: \"LLM\"):\n        self.model = model\n        self.lora_request = None\n\n        self.tokenizer = self._get_tokenizer()\n\n    def _get_tokenizer(self):\n        if hasattr(self.model, \"get_tokenizer\"):\n            tokenizer = self.model.get_tokenizer()\n        elif hasattr(self.model, \"tokenizer\"):\n            if hasattr(self.model.tokenizer, \"tokenizer\"):\n                tokenizer = self.model.tokenizer.tokenizer\n            else:\n                tokenizer = self.model.tokenizer\n        else:\n            raise ValueError(\n                \"The provided LLM instance neither has a \"\n                \"`tokenizer` attribute or a `get_tokenizer` method.\"\n            )\n        return adapt_tokenizer(tokenizer=tokenizer)\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor,\n        sampling_parameters: SamplingParameters,\n        *,\n        sampling_params: Optional[\"SamplingParams\"] = None,\n        use_tqdm: bool = True,\n    ):\n        \"\"\"Generate text using vLLM.\n\n        Parameters\n        ----------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n        sampling_params\n            An instance of `vllm.sampling_params.SamplingParams`. The values\n            passed via this dataclass supersede the values of the parameters\n            in `generation_parameters` and `sampling_parameters`. See the\n            vLLM documentation for more details: https://docs.vllm.ai/en/latest/dev/sampling_params.html.\n        use_tqdm\n            A boolean in order to display progress bar while inferencing\n\n        Returns\n        -------\n        The generated text, of shape `(n_batch, n_samples)`. If there are only\n        one batch and several samples, the list is of shape `(n_samples)`. If\n        this is a batch with several sequences but only one sample the list is\n        of shape `(n_batch)`. If there is only one sequence and one sample, a\n        string is returned.\n\n        \"\"\"\n        from vllm.sampling_params import SamplingParams\n\n        if sampling_params is None:\n            sampling_params = SamplingParams()\n\n        max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n        # We only update the values in `sampling_params` if they\n        # are specified by the user when calling the generator.\n        if max_tokens is not None:\n            sampling_params.max_tokens = max_tokens\n        if stop_at is not None:\n            if isinstance(stop_at, str):\n                stop_at = [stop_at]\n            sampling_params.stop = stop_at\n        if seed is not None:\n            sampling_params.seed = seed\n\n        sampling_params.logits_processors = (\n            [logits_processor] if logits_processor is not None else []\n        )\n\n        sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n            sampling_parameters\n        )\n\n        # We only update the values in `sampling_params` that\n        # were not specified by the user.\n        if sampling_params.n == 1:\n            sampling_params.n = num_samples\n            sampling_params.best_of = num_samples\n        if top_p is not None and sampling_params.top_p == 1.0:\n            sampling_params.top_p = top_p\n        if top_k is not None and sampling_params.top_k == -1:\n            sampling_params.top_k = top_k\n            # TODO: remove this if statement once fixed\n            # https://github.com/vllm-project/vllm/issues/5404#issuecomment-2175972897\n            if top_k == 1:\n                sampling_params.repetition_penalty = 0\n        if temperature is not None and sampling_params.temperature == 1.0:\n            sampling_params.temperature = temperature\n        if sampler == \"beam_search\":\n            sampling_params.use_beam_search = True\n\n        results = self.model.generate(\n            prompts,\n            sampling_params=sampling_params,\n            lora_request=self.lora_request,\n            use_tqdm=use_tqdm,\n        )\n        results = [[sample.text for sample in batch.outputs] for batch in results]\n\n        batch_size = len(results)\n        sample_size = len(results[0])\n\n        if batch_size == 1 and sample_size == 1:\n            return results[0][0]\n        elif batch_size == 1:\n            return results[0]\n        elif sample_size == 1:\n            return [batch[0] for batch in results]\n\n        return results\n\n    def stream(self, *args, **kwargs):\n        \"\"\"Return a text generator.\n\n        Streaming is not yet available for `vllm.LLM`.\n\n        TODO: Implement the streaming functionality ourselves.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Streaming is not available for the vLLM integration.\"\n        )\n\n    def load_lora(self, adapter_path: Optional[str]):\n        from vllm.lora.request import LoRARequest\n\n        if adapter_path is None:\n            self.lora_request = None\n        else:\n            self.lora_request = LoRARequest(adapter_path, 1, adapter_path)\n</code></pre>"},{"location":"api/models/#outlines.models.vllm.VLLM.generate","title":"<code>generate(prompts, generation_parameters, logits_processor, sampling_parameters, *, sampling_params=None, use_tqdm=True)</code>","text":"<p>Generate text using vLLM.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>Union[str, List[str]]</code> <p>A prompt or list of prompts.</p> required <code>generation_parameters</code> <code>GenerationParameters</code> <p>An instance of <code>GenerationParameters</code> that contains the prompt, the maximum number of tokens, stop sequences and seed. All the arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method.</p> required <code>logits_processor</code> <p>The logits processor to use when generating text.</p> required <code>sampling_parameters</code> <code>SamplingParameters</code> <p>An instance of <code>SamplingParameters</code>, a dataclass that contains the name of the sampler to use and related parameters as available in Outlines.</p> required <code>sampling_params</code> <code>Optional[SamplingParams]</code> <p>An instance of <code>vllm.sampling_params.SamplingParams</code>. The values passed via this dataclass supersede the values of the parameters in <code>generation_parameters</code> and <code>sampling_parameters</code>. See the vLLM documentation for more details: https://docs.vllm.ai/en/latest/dev/sampling_params.html.</p> <code>None</code> <code>use_tqdm</code> <code>bool</code> <p>A boolean in order to display progress bar while inferencing</p> <code>True</code> <p>Returns:</p> Type Description <code>The generated text, of shape `(n_batch, n_samples)`. If there are only</code> <code>one batch and several samples, the list is of shape `(n_samples)`. If</code> <code>this is a batch with several sequences but only one sample the list is</code> <code>of shape `(n_batch)`. If there is only one sequence and one sample, a</code> <code>string is returned.</code> Source code in <code>outlines/models/vllm.py</code> <pre><code>def generate(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    logits_processor,\n    sampling_parameters: SamplingParameters,\n    *,\n    sampling_params: Optional[\"SamplingParams\"] = None,\n    use_tqdm: bool = True,\n):\n    \"\"\"Generate text using vLLM.\n\n    Parameters\n    ----------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n    sampling_params\n        An instance of `vllm.sampling_params.SamplingParams`. The values\n        passed via this dataclass supersede the values of the parameters\n        in `generation_parameters` and `sampling_parameters`. See the\n        vLLM documentation for more details: https://docs.vllm.ai/en/latest/dev/sampling_params.html.\n    use_tqdm\n        A boolean in order to display progress bar while inferencing\n\n    Returns\n    -------\n    The generated text, of shape `(n_batch, n_samples)`. If there are only\n    one batch and several samples, the list is of shape `(n_samples)`. If\n    this is a batch with several sequences but only one sample the list is\n    of shape `(n_batch)`. If there is only one sequence and one sample, a\n    string is returned.\n\n    \"\"\"\n    from vllm.sampling_params import SamplingParams\n\n    if sampling_params is None:\n        sampling_params = SamplingParams()\n\n    max_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n\n    # We only update the values in `sampling_params` if they\n    # are specified by the user when calling the generator.\n    if max_tokens is not None:\n        sampling_params.max_tokens = max_tokens\n    if stop_at is not None:\n        if isinstance(stop_at, str):\n            stop_at = [stop_at]\n        sampling_params.stop = stop_at\n    if seed is not None:\n        sampling_params.seed = seed\n\n    sampling_params.logits_processors = (\n        [logits_processor] if logits_processor is not None else []\n    )\n\n    sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n        sampling_parameters\n    )\n\n    # We only update the values in `sampling_params` that\n    # were not specified by the user.\n    if sampling_params.n == 1:\n        sampling_params.n = num_samples\n        sampling_params.best_of = num_samples\n    if top_p is not None and sampling_params.top_p == 1.0:\n        sampling_params.top_p = top_p\n    if top_k is not None and sampling_params.top_k == -1:\n        sampling_params.top_k = top_k\n        # TODO: remove this if statement once fixed\n        # https://github.com/vllm-project/vllm/issues/5404#issuecomment-2175972897\n        if top_k == 1:\n            sampling_params.repetition_penalty = 0\n    if temperature is not None and sampling_params.temperature == 1.0:\n        sampling_params.temperature = temperature\n    if sampler == \"beam_search\":\n        sampling_params.use_beam_search = True\n\n    results = self.model.generate(\n        prompts,\n        sampling_params=sampling_params,\n        lora_request=self.lora_request,\n        use_tqdm=use_tqdm,\n    )\n    results = [[sample.text for sample in batch.outputs] for batch in results]\n\n    batch_size = len(results)\n    sample_size = len(results[0])\n\n    if batch_size == 1 and sample_size == 1:\n        return results[0][0]\n    elif batch_size == 1:\n        return results[0]\n    elif sample_size == 1:\n        return [batch[0] for batch in results]\n\n    return results\n</code></pre>"},{"location":"api/models/#outlines.models.vllm.VLLM.stream","title":"<code>stream(*args, **kwargs)</code>","text":"<p>Return a text generator.</p> <p>Streaming is not yet available for <code>vllm.LLM</code>.</p> <p>TODO: Implement the streaming functionality ourselves.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def stream(self, *args, **kwargs):\n    \"\"\"Return a text generator.\n\n    Streaming is not yet available for `vllm.LLM`.\n\n    TODO: Implement the streaming functionality ourselves.\n\n    \"\"\"\n    raise NotImplementedError(\n        \"Streaming is not available for the vLLM integration.\"\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.vllm.adapt_tokenizer","title":"<code>adapt_tokenizer(tokenizer)</code>","text":"<p>Adapt a tokenizer to use to compile the FSM.</p> <p>The API of Outlines tokenizers is slightly different to that of <code>transformers</code>. In addition we need to handle the missing spaces to Llama's tokenizer to be able to compile FSMs for this model.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>The tokenizer of the model.</p> required <p>Returns:</p> Type Description <code>PreTrainedTokenizerBase</code> <p>The adapted tokenizer.</p> Source code in <code>outlines/models/vllm.py</code> <pre><code>def adapt_tokenizer(tokenizer: \"PreTrainedTokenizerBase\") -&gt; \"PreTrainedTokenizerBase\":\n    \"\"\"Adapt a tokenizer to use to compile the FSM.\n\n    The API of Outlines tokenizers is slightly different to that of `transformers`. In\n    addition we need to handle the missing spaces to Llama's tokenizer to be able to\n    compile FSMs for this model.\n\n    Parameters\n    ----------\n    tokenizer\n        The tokenizer of the model.\n\n    Returns\n    -------\n    PreTrainedTokenizerBase\n        The adapted tokenizer.\n    \"\"\"\n    from transformers import SPIECE_UNDERLINE\n\n    tokenizer.vocabulary = tokenizer.get_vocab()\n    tokenizer.special_tokens = set(tokenizer.all_special_tokens)\n\n    def convert_token_to_string(token: Union[str, bytes]) -&gt; str:\n        string = tokenizer.convert_tokens_to_string([token])\n\n        # A hack to handle missing spaces to HF's Llama tokenizers\n        if (\n            type(token) is str\n            and token.startswith(SPIECE_UNDERLINE)\n            or token == \"&lt;0x20&gt;\"\n        ):\n            return \" \" + string\n\n        return string\n\n    tokenizer.convert_token_to_string = convert_token_to_string\n\n    return tokenizer\n</code></pre>"},{"location":"api/models/#outlines.models.vllm.vllm","title":"<code>vllm(model_name, **vllm_model_params)</code>","text":"<p>Load a vLLM model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to load from the HuggingFace hub.</p> required <code>vllm_model_params</code> <p>vLLM-specific model parameters. See the vLLM code for the full list: https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py</p> <code>{}</code> Source code in <code>outlines/models/vllm.py</code> <pre><code>def vllm(model_name: str, **vllm_model_params):\n    \"\"\"Load a vLLM model.\n\n    Parameters\n    ---------\n    model_name\n        The name of the model to load from the HuggingFace hub.\n    vllm_model_params\n        vLLM-specific model parameters. See the vLLM code for the full list:\n        https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n\n    \"\"\"\n    from vllm import LLM\n\n    model = LLM(model_name, **vllm_model_params)\n\n    return VLLM(model)\n</code></pre>"},{"location":"api/parsing/","title":"Parsing","text":""},{"location":"api/parsing/#outlines.fsm.parsing.PartialIndenter","title":"<code>PartialIndenter</code>","text":"<p>               Bases: <code>Indenter</code></p> <p>An <code>Indenter</code> that doesn't reset its state every time <code>process</code> is called.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialIndenter(Indenter):\n    \"\"\"An `Indenter` that doesn't reset its state every time `process` is called.\"\"\"\n\n    def process(self, stream):\n        return self._process(stream)\n\n    def _process(self, stream):\n        for token in stream:\n            # These were previously *after* the `yield`, but that makes the\n            # state tracking unnecessarily convoluted.\n            if token.type in self.OPEN_PAREN_types:\n                self.paren_level += 1\n            elif token.type in self.CLOSE_PAREN_types:\n                self.paren_level -= 1\n                if self.paren_level &lt; 0:\n                    raise UnexpectedToken(token, [])\n\n            if token.type == self.NL_type:\n                yield from self.handle_NL(token)\n            else:\n                yield token\n\n        # TODO: What do we want to do here?\n        # while len(self.indent_level) &gt; 1:\n        #     self.indent_level.pop()\n        #     yield Token(self.DEDENT_type, \"\")\n\n    def accepts_token_type(self, token_type):\n        if token_type in self.CLOSE_PAREN_types and self.paren_level - 1 &lt; 0:\n            return False\n\n        # TODO:\n        # if token_type == self.NL_type and self.paren_level == 0:\n        #     ...\n        #     return False\n\n        return True\n\n    def __copy__(self):\n        res = type(self)()\n        res.paren_level = self.paren_level\n        res.indent_level = copy(self.indent_level)\n        return res\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(paren_level={self.paren_level!r}, indent_level={self.indent_level!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState","title":"<code>PartialParserState</code>","text":"<p>               Bases: <code>ParserState</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParserState(ParserState):\n    __slots__ = \"use_value_stack\"\n\n    def __init__(\n        self,\n        parse_conf,\n        lexer,\n        state_stack=None,\n        value_stack=None,\n        use_value_stack=False,\n    ):\n        super().__init__(\n            parse_conf, lexer, state_stack=state_stack, value_stack=value_stack\n        )\n        self.use_value_stack = use_value_stack\n\n    def feed_token(self, token, is_end=False):\n        if token.type == \"partial\":\n            # If none of the potential terminals can transition, we need to know now\n            current_state = self.state_stack[-1]\n            current_lexer = get_contextual_lexer(self.lexer).lexers[current_state]\n\n            # We have to feed the token and determine whether or not at least\n            # one terminal is consistent with the stack; otherwise, we'll miss\n            # invalid REDUCE cases.\n            # TODO: We should track separate parses conditional on possible\n            # token/symbol types, then we can coherently reuse the following\n            # results instead of recomputing it later.\n            can_transition = False\n            for terminal_info in token.value.terminals_and_info:\n                if terminal_info.terminal_name not in current_lexer.ignore_types:\n                    test_token = Token.new_borrow_pos(\n                        terminal_info.terminal_name, \"\", token\n                    )\n\n                    stack = copy(self.state_stack)\n                    try:\n                        self.feed_token_no_stack(test_token, is_end=is_end)\n                        can_transition = True\n                        break\n                    except UnexpectedToken:\n                        continue\n                    finally:\n                        self.state_stack = stack\n                else:\n                    can_transition = True\n\n            if not can_transition:\n                expected = {\n                    s\n                    for s in self.parse_conf.states[current_state].keys()\n                    if s.isupper()\n                }\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n        elif self.use_value_stack:\n            super().feed_token(token, is_end=is_end)\n        else:\n            self.feed_token_no_stack(token, is_end=is_end)\n\n    def feed_token_no_stack(self, token, is_end=False):\n        \"\"\"\n        This is a copy of `ParserState.feed_token` with all the value stack\n        steps removed.  Since we're not exactly parsing in order to obtain a\n        CST or anything similar, we can avoid the growing expense of tracking\n        the parse tree.\n        \"\"\"\n        state_stack = self.state_stack\n        states = self.parse_conf.states\n        end_state = self.parse_conf.end_state\n\n        while True:\n            state = state_stack[-1]\n            try:\n                action, arg = states[state][token.type]\n            except KeyError:\n                expected = {s for s in states[state].keys() if s.isupper()}\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n            assert arg != end_state\n\n            if action is Shift:\n                # shift once and return\n                assert not is_end\n                state_stack.append(arg)\n                return\n            else:\n                # reduce+shift as many times as necessary\n                rule = arg\n                size = len(rule.expansion)\n                if size:\n                    del state_stack[-size:]\n\n                _action, new_state = states[state_stack[-1]][rule.origin.name]\n                assert _action is Shift\n                state_stack.append(new_state)\n\n                if is_end and state_stack[-1] == end_state:\n                    return\n\n    def feed_eof(self):\n        last_token = self.lexer.state.last_token\n\n        if last_token is None:\n            eof_token = self.lexer._Token(\"$END\", \"\", 0, 1, 1)\n        else:\n            eof_token = Token.new_borrow_pos(\"$END\", \"\", last_token)\n\n        new_token_is_legal = (\n            last_token is None\n            or last_token.type != \"partial\"\n            or any(ti.is_final for ti in last_token.value.terminals_and_info)\n        )\n        if new_token_is_legal:\n            self.feed_token(eof_token, is_end=True)\n        else:\n            raise UnexpectedToken(eof_token, [], state=self, interactive_parser=None)\n\n    def choices(self):\n        return self.parse_conf.parse_table.states[self.position]\n\n    def accepts(self):\n        \"\"\"\n        Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95\n        Returns the set of possible tokens that will advance the parser into a new valid state.\n        \"\"\"\n        accepts = set()\n        conf_no_callbacks = copy(self.parse_conf)\n        # We don't want to call callbacks here since those might have arbitrary side effects\n        # and are unnecessarily slow.\n        conf_no_callbacks.callbacks = {}\n        for t in self.choices():\n            if t.isupper():  # is terminal?\n                new_state = copy(self)\n                new_state.parse_conf = conf_no_callbacks\n                try:\n                    new_state.feed_token(new_state.lexer._Token(t, \"\"))\n                except UnexpectedToken:\n                    pass\n                else:\n                    accepts.add(t)\n        return accepts\n\n    def __copy__(self):\n        return type(self)(\n            self.parse_conf,\n            copy(self.lexer),\n            copy(self.state_stack),\n            deepcopy(self.value_stack),\n            use_value_stack=self.use_value_stack,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(lexer={self.lexer!r}, state_stack={self.state_stack!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.accepts","title":"<code>accepts()</code>","text":"<p>Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95 Returns the set of possible tokens that will advance the parser into a new valid state.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def accepts(self):\n    \"\"\"\n    Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95\n    Returns the set of possible tokens that will advance the parser into a new valid state.\n    \"\"\"\n    accepts = set()\n    conf_no_callbacks = copy(self.parse_conf)\n    # We don't want to call callbacks here since those might have arbitrary side effects\n    # and are unnecessarily slow.\n    conf_no_callbacks.callbacks = {}\n    for t in self.choices():\n        if t.isupper():  # is terminal?\n            new_state = copy(self)\n            new_state.parse_conf = conf_no_callbacks\n            try:\n                new_state.feed_token(new_state.lexer._Token(t, \"\"))\n            except UnexpectedToken:\n                pass\n            else:\n                accepts.add(t)\n    return accepts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.feed_token_no_stack","title":"<code>feed_token_no_stack(token, is_end=False)</code>","text":"<p>This is a copy of <code>ParserState.feed_token</code> with all the value stack steps removed.  Since we're not exactly parsing in order to obtain a CST or anything similar, we can avoid the growing expense of tracking the parse tree.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def feed_token_no_stack(self, token, is_end=False):\n    \"\"\"\n    This is a copy of `ParserState.feed_token` with all the value stack\n    steps removed.  Since we're not exactly parsing in order to obtain a\n    CST or anything similar, we can avoid the growing expense of tracking\n    the parse tree.\n    \"\"\"\n    state_stack = self.state_stack\n    states = self.parse_conf.states\n    end_state = self.parse_conf.end_state\n\n    while True:\n        state = state_stack[-1]\n        try:\n            action, arg = states[state][token.type]\n        except KeyError:\n            expected = {s for s in states[state].keys() if s.isupper()}\n            raise UnexpectedToken(\n                token, expected, state=self, interactive_parser=None\n            )\n\n        assert arg != end_state\n\n        if action is Shift:\n            # shift once and return\n            assert not is_end\n            state_stack.append(arg)\n            return\n        else:\n            # reduce+shift as many times as necessary\n            rule = arg\n            size = len(rule.expansion)\n            if size:\n                del state_stack[-size:]\n\n            _action, new_state = states[state_stack[-1]][rule.origin.name]\n            assert _action is Shift\n            state_stack.append(new_state)\n\n            if is_end and state_stack[-1] == end_state:\n                return\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParsingFrontend","title":"<code>PartialParsingFrontend</code>","text":"<p>               Bases: <code>ParsingFrontend</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParsingFrontend(ParsingFrontend):\n    def __init__(self, lexer_conf, parser_conf, options, parser=None):\n        assert parser_conf.parser_type == \"lalr\"\n\n        options._plugins[\"LALR_Parser\"] = PartialLALRParser\n        options._plugins[\"BasicLexer\"] = PartialBasicLexer\n        options._plugins[\"ContextualLexer\"] = PartialContextualLexer\n        options._plugins[\"LexerThread\"] = PartialLexerThread\n\n        super().__init__(lexer_conf, parser_conf, options, parser=parser)\n\n        if lexer_conf.postlex:\n            self.lexer = PartialPostLexConnector(self.lexer.lexer, lexer_conf.postlex)\n\n        self._termset_fsm_info = None\n        self._symbols_to_states: Optional[\n            Dict[str, Set[Tuple[ParseStateType, Action]]]\n        ] = None\n        self._reverse_shifts: Optional[\n            Dict[ParseStateType, Dict[str, Set[ParseStateType]]]\n        ] = None\n        # self._state_transition_map: Optional[\n        #     Dict[Tuple[ParseStateType, str], Set[ParseStateType]]\n        # ] = None\n\n    def _compute_maps(\n        self,\n    ):\n        \"\"\"Compute state transition and symbols-to-states maps.\"\"\"\n        self._reverse_shifts = {}\n        self._symbols_to_states = {}\n\n        parse_table = self.parser.parser.parse_table\n\n        for from_state, symbols_to_ops in parse_table.states.items():\n            for symbol, op in symbols_to_ops.items():\n                if op[0] == Shift:\n                    symbols_to_from_states = self._reverse_shifts.setdefault(op[1], {})\n                    symbols_to_from_states.setdefault(symbol, set()).add(from_state)\n                self._symbols_to_states.setdefault(symbol, set()).add((from_state, op))\n\n        # # TODO: This approach is very wasteful.\n        # context_lexer = get_contextual_lexer(self)\n        # self._state_transition_map = {}\n        #\n        # for from_state, transitions in parse_table.states.items():\n        #     for symbol, action in transitions.items():\n        #         # TODO: Filter non-terminals\n        #         if symbol not in context_lexer.root_lexer.terminals_by_name:\n        #             continue\n        #\n        #         if action[0] is Shift:\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(action[1])\n        #             continue\n        #\n        #         antecedent_state_seqs = parse_to_terminal(self, [(from_state,)], symbol)\n        #\n        #         for antecedent_state_seq in antecedent_state_seqs:\n        #             antecedent_state = antecedent_state_seq[-1]\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(antecedent_state)\n\n    def _compute_termset_fsm_info(self):\n        \"\"\"Collect and return information about terminal symbol sets and their FSMs.\n\n        Terminal symbol sets (or \"termsets\") are ordered sequences of terminal\n        symbols that are used by each parser state.  Associated with each is a\n        collection of FSMs for each terminal and a single parse state FSM that is\n        the union of each terminal's FSM.\n\n        This constructs a list of tuples containing the termset, the set of\n        parse states that use the termsets, parse state FSMs, and information\n        mapping the components of the parse state FSMs to their terminal symbol\n        FSMs.\n\n        \"\"\"\n        context_lexer = get_contextual_lexer(self)\n        termsets_to_fsms = {}\n        termsets_to_parse_states: Dict[Tuple[str, ...], Set[ParseStateType]] = {}\n        for parse_state, lexer in context_lexer.lexers.items():\n            scanner = lexer.scanner\n            key = tuple(term.name for term in scanner.terminals)\n            termsets_to_fsms[key] = (scanner.fsm, scanner.fsms_to_trans_finals)\n            termsets_to_parse_states.setdefault(key, set()).add(parse_state)\n\n        self._termset_fsm_info = [\n            (\n                termset,\n                frozenset(termsets_to_parse_states[termset]),\n                fsm,\n                fsms_to_trans_finals,\n            )\n            for termset, (fsm, fsms_to_trans_finals) in termsets_to_fsms.items()\n        ]\n\n    @property\n    def termset_fsm_info(self):\n        if self._termset_fsm_info is None:\n            self._compute_termset_fsm_info()\n        return self._termset_fsm_info\n\n    @property\n    def symbols_to_states(self):\n        if self._symbols_to_states is None:\n            self._compute_maps()\n        return self._symbols_to_states\n\n    @property\n    def reverse_shifts(self):\n        if self._reverse_shifts is None:\n            self._compute_maps()\n        return self._reverse_shifts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner","title":"<code>PartialScanner</code>","text":"<p>               Bases: <code>Scanner</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialScanner(Scanner):\n    @classmethod\n    @lru_cache\n    def construct_terminal_fsm(cls, terminal):\n        # TODO: This should really be done at the lexer/parser level so that\n        # the lifetime of these objects is tied to the parser itself.\n        regex_str = terminal.pattern.to_regexp()\n        pattern = interegular.parse_pattern(regex_str)\n        fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        return fsm, pattern.prefix_postfix\n\n    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):\n        self.terminals = terminals\n        self.g_regex_flags = g_regex_flags\n        self.use_bytes = use_bytes\n        self.match_whole = match_whole\n        self.allowed_types = {t.name for t in self.terminals}\n        self._mres = None\n\n        fsms = []\n        for t in self.terminals:\n            fsm, prefix_postfix = self.construct_terminal_fsm(t)\n\n            # TODO FIXME: We don't support this right now.\n            assert prefix_postfix == (0, 0)\n\n            fsms.append(fsm)\n\n        self.fsm, self.fsms_to_trans_finals = fsm_union(fsms)\n\n    def get_terminals_info(\n        self, fsm_state_seq\n    ) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n        \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n        terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n            get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n        ):\n            terminal_name = self.terminals[fsm_id].name\n            info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n            terminals_and_info += (info,)\n            if in_final:\n                final_terminals_and_info += (info,)\n\n        return terminals_and_info, final_terminals_and_info\n\n    def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n        \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n        start_pos = pos\n\n        if last_fsm_state_seq:\n            assert len(last_fsm_state_seq) &gt; 1\n            start_pos += len(last_fsm_state_seq) - 1\n            start_state = last_fsm_state_seq[-1]\n        else:\n            start_state = self.fsm.initial\n\n        text_part = text[start_pos:]\n\n        text_transitions = get_token_transition_keys(\n            self.fsm.fsm_info.alphabet_symbol_mapping,\n            self.fsm.fsm_info.alphabet_anything_value,\n            text_part,\n        )\n\n        state_seq = walk_fsm(\n            self.fsm,\n            text_transitions,\n            start_state,\n            full_match=self.match_whole,\n        )\n\n        if not state_seq:\n            return None\n\n        if last_fsm_state_seq:\n            res = last_fsm_state_seq + tuple(state_seq)\n        else:\n            res = (start_state,) + tuple(state_seq)\n\n        return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.get_terminals_info","title":"<code>get_terminals_info(fsm_state_seq)</code>","text":"<p>Get the possible terminal symbols for an FSM state sequence.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def get_terminals_info(\n    self, fsm_state_seq\n) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n    \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n    terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n        get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n    ):\n        terminal_name = self.terminals[fsm_id].name\n        info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n        terminals_and_info += (info,)\n        if in_final:\n            final_terminals_and_info += (info,)\n\n    return terminals_and_info, final_terminals_and_info\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.match","title":"<code>match(text, pos, last_fsm_state_seq=None)</code>","text":"<p>Determine an FSM match over <code>text</code> starting at <code>pos</code> and continuing <code>last_fsm_state_seq</code>.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n    \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n    start_pos = pos\n\n    if last_fsm_state_seq:\n        assert len(last_fsm_state_seq) &gt; 1\n        start_pos += len(last_fsm_state_seq) - 1\n        start_state = last_fsm_state_seq[-1]\n    else:\n        start_state = self.fsm.initial\n\n    text_part = text[start_pos:]\n\n    text_transitions = get_token_transition_keys(\n        self.fsm.fsm_info.alphabet_symbol_mapping,\n        self.fsm.fsm_info.alphabet_anything_value,\n        text_part,\n    )\n\n    state_seq = walk_fsm(\n        self.fsm,\n        text_transitions,\n        start_state,\n        full_match=self.match_whole,\n    )\n\n    if not state_seq:\n        return None\n\n    if last_fsm_state_seq:\n        res = last_fsm_state_seq + tuple(state_seq)\n    else:\n        res = (start_state,) + tuple(state_seq)\n\n    return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.fsm_union","title":"<code>fsm_union(fsms)</code>","text":"<p>Construct an FSM representing the union of the FSMs in <code>fsms</code>.</p> <p>This is an updated version of <code>interegular.fsm.FSM.union</code> made to return an extra map of component FSMs to the sets of state transitions that correspond to them in the new FSM.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def fsm_union(\n    fsms: Sequence[FSM],\n) -&gt; Tuple[FSM, Dict[int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]]]:\n    \"\"\"Construct an FSM representing the union of the FSMs in `fsms`.\n\n    This is an updated version of `interegular.fsm.FSM.union` made to return an\n    extra map of component FSMs to the sets of state transitions that\n    correspond to them in the new FSM.\n\n    \"\"\"\n\n    alphabet, new_to_old = Alphabet.union(*[fsm.alphabet for fsm in fsms])\n\n    indexed_fsms = tuple(enumerate(fsms))\n\n    initial = {i: fsm.initial for (i, fsm) in indexed_fsms}\n\n    # Dedicated function accepting a \"superset\" and returning the next\n    # \"superset\" obtained by following this transition in the new FSM\n    def follow(current_state, new_transition: int):\n        next = {}\n        for i, f in indexed_fsms:\n            old_transition = new_to_old[i][new_transition]\n            if (\n                i in current_state\n                and current_state[i] in f.map\n                and old_transition in f.map[current_state[i]]\n            ):\n                next[i] = f.map[current_state[i]][old_transition]\n        if not next:\n            raise OblivionError\n        return next\n\n    states = [initial]\n    finals: Set[int] = set()\n    map: Dict[int, Dict[int, int]] = {}\n\n    # Map component FSMs to their new state-to-state transitions, finals, and a\n    # map translating component FSM states to aggregate FSM states\n    fsms_to_trans_finals: Dict[\n        int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]\n    ] = {}\n\n    i = 0\n    while i &lt; len(states):\n        state = states[i]\n\n        # Add to the finals of the aggregate FSM whenever we hit a final in a\n        # component FSM\n        if any(state.get(j, -1) in fsm.finals for (j, fsm) in indexed_fsms):\n            finals.add(i)\n\n        # Compute the map for this state\n        map[i] = {}\n        for transition in alphabet.by_transition:\n            try:\n                next = follow(state, transition)\n            except OblivionError:\n                # Reached an oblivion state; don't list it\n                continue\n            else:\n                try:\n                    # TODO: Seems like this could--and should--be avoided\n                    j = states.index(next)\n                except ValueError:\n                    j = len(states)\n                    states.append(next)\n\n                map[i][transition] = j\n\n                for fsm_id, fsm_state in next.items():\n                    (\n                        fsm_transitions,\n                        fsm_finals,\n                        fsm_old_to_new,\n                    ) = fsms_to_trans_finals.setdefault(fsm_id, (set(), set(), {}))\n                    old_from = state[fsm_id]\n                    old_to = fsm_state\n                    fsm_old_to_new.setdefault(old_from, set()).add(i)\n                    fsm_old_to_new.setdefault(old_to, set()).add(j)\n                    fsm_transitions.add((i, j))\n                    if fsm_state in fsms[fsm_id].finals:\n                        fsm_finals.add(j)\n\n        i += 1\n\n    fsm = FSM(\n        alphabet=alphabet,\n        states=range(len(states)),\n        initial=0,\n        finals=finals,\n        map=map,\n        __no_validation__=True,\n    )\n\n    fsm, old_to_new_states = make_deterministic_fsm(fsm)\n    _fsms_to_trans_finals = {\n        fsm_id: (\n            {(old_to_new_states[s1], old_to_new_states[s2]) for s1, s2 in transitions},\n            {old_to_new_states[s] for s in finals},\n            {\n                old_state: {old_to_new_states[new_state] for new_state in new_states}\n                for old_state, new_states in old_to_new.items()\n            },\n        )\n        for fsm_id, (transitions, finals, old_to_new) in sorted(\n            fsms_to_trans_finals.items(), key=lambda x: x[0]\n        )\n    }\n\n    return (\n        fsm,\n        _fsms_to_trans_finals,\n    )\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.get_sub_fsms_from_seq","title":"<code>get_sub_fsms_from_seq(state_seq, fsms_to_trans_finals)</code>","text":"<p>Get the indices of the sub-FSMs in <code>fsm</code> that could have matched the state sequence <code>state_seq</code>.</p> <p>Parameters:</p> Name Type Description Default <code>state_seq</code> <code>Sequence[int]</code> <p>A state sequence.</p> required <code>fsms_to_trans_finals</code> <code>Dict[int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]]</code> <p>A map from FSM indices to tuples containing sets of their state transitions and sets of the final/accept states.</p> required <p>Returns:</p> Type Description <code>A generator returning tuples containing each sub-FSM index (in the order</code> <code>they were union-ed to construct `fsm`) and booleans indicating whether or</code> <code>not there is another valid transition from the last state in the sequence</code> <code>for the associated sub-FSM (i.e. if the FSM can continue</code> <code>accepting/matching) and whether or not the sequence ends in a final state</code> <code>of the sub-FSM.</code> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def get_sub_fsms_from_seq(\n    state_seq: Sequence[int],\n    fsms_to_trans_finals: Dict[\n        int, Tuple[Set[Tuple[int, int]], Set[int], Dict[int, Set[int]]]\n    ],\n) -&gt; Generator[Tuple[int, bool, bool], None, None]:\n    \"\"\"Get the indices of the sub-FSMs in `fsm` that could have matched the state sequence `state_seq`.\n\n    Parameters\n    ----------\n    state_seq\n        A state sequence.\n    fsms_to_trans_finals\n        A map from FSM indices to tuples containing sets of their state transitions\n        and sets of the final/accept states.\n\n    Returns\n    -------\n    A generator returning tuples containing each sub-FSM index (in the order\n    they were union-ed to construct `fsm`) and booleans indicating whether or\n    not there is another valid transition from the last state in the sequence\n    for the associated sub-FSM (i.e. if the FSM can continue\n    accepting/matching) and whether or not the sequence ends in a final state\n    of the sub-FSM.\n    \"\"\"\n    state_seq_transitions = set(zip(state_seq[:-1], state_seq[1:]))\n    last_fsm_state = state_seq[-1]\n    yield from (\n        (\n            # The sub-FMS index\n            fsm_idx,\n            # Is there another possible transition in this sub-FSM?\n            any(last_fsm_state == from_s for (from_s, to_s) in transitions),\n            # Is this sub-FSM in a final state?\n            state_seq[-1] in finals,\n        )\n        for fsm_idx, (transitions, finals, _) in fsms_to_trans_finals.items()\n        if state_seq_transitions.issubset(transitions)\n    )\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.terminals_to_fsms","title":"<code>terminals_to_fsms(lp)</code>","text":"<p>Construct a <code>dict</code> mapping terminal symbol names to their finite state machines.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def terminals_to_fsms(lp: PartialLark) -&gt; Dict[str, FSM]:\n    \"\"\"Construct a ``dict`` mapping terminal symbol names to their finite state machines.\"\"\"\n\n    symbol_names_and_fsms = {}\n    for terminal in lp.terminals:\n        pattern = interegular.parse_pattern(terminal.pattern.to_regexp())\n        # TODO: Use `pyparser.terminals[0].pattern.flags`?\n        try:\n            fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        except Unsupported:\n            fsm = None\n\n        symbol_names_and_fsms[terminal.name] = fsm\n\n    return symbol_names_and_fsms\n</code></pre>"},{"location":"api/regex/","title":"Regex","text":""},{"location":"api/regex/#outlines.generate.regex.regex","title":"<code>regex(model, regex_str, sampler=multinomial())</code>","text":"<p>Generate structured text in the language of a regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>An instance of <code>Transformer</code> that represents a model from the <code>transformers</code> library.</p> required <code>regex_str</code> <code>str | Regex</code> <p>The regular expression that the output must follow.</p> required <code>sampler</code> <code>Sampler</code> <p>The sampling algorithm to use to generate token ids from the logits distribution.</p> <code>multinomial()</code> <p>Returns:</p> Type Description <code>A `SequenceGeneratorAdapter` instance that generates text constrained by the</code> <code>regular expression.</code> Source code in <code>outlines/generate/regex.py</code> <pre><code>@singledispatch\ndef regex(model, regex_str: str | Regex, sampler: Sampler = multinomial()):\n    \"\"\"Generate structured text in the language of a regular expression.\n\n    Parameters\n    ----------\n    model:\n        An instance of `Transformer` that represents a model from the\n        `transformers` library.\n    regex_str:\n        The regular expression that the output must follow.\n    sampler:\n        The sampling algorithm to use to generate token ids from the logits\n        distribution.\n\n    Returns\n    -------\n    A `SequenceGeneratorAdapter` instance that generates text constrained by the\n    regular expression.\n\n    \"\"\"\n    from outlines.processors import RegexLogitsProcessor\n\n    if isinstance(regex_str, Regex):\n        regex_str = regex_str.pattern\n\n    logits_processor = RegexLogitsProcessor(regex_str, tokenizer=model.tokenizer)\n    return SequenceGeneratorAdapter(model, logits_processor, sampler)\n</code></pre>"},{"location":"api/samplers/","title":"Samplers","text":""},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler","title":"<code>BeamSearchSampler</code>","text":"<p>Beam Search sampling algorithm.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <p>The number of samples taken for each input sequence. Equivalent to the number of beams.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class BeamSearchSampler:\n    \"\"\"Beam Search sampling algorithm.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence. Equivalent to the\n        number of beams.\n    \"\"\"\n\n    def __init__(self, beams: int = 1):\n        self.samples = beams\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        _,\n    ) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n        \"\"\"Call the beam search sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n        # Flatten scores to (n_batch, n_samples * vocab_size)\n        # and find the top-k weights for each batch.\n        batch_size = next_token_logits.shape[0] // self.samples\n        vocab_size = next_token_logits.shape[-1]\n        weights = weights.view(batch_size, self.samples * vocab_size)\n\n        # If the weights are all equal to 0 we are at the beginning of the search\n        # and thus only need to sample from one set of token logits for each\n        # batch.\n        if torch.all(sequence_weights == 0):\n            weights = weights[:, :vocab_size]\n\n        weights, indices = torch.topk(\n            weights, self.samples, dim=1, largest=True, sorted=True\n        )\n\n        ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n        next_token_ids = indices % vocab_size\n\n        # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n        first_batch_idx = torch.arange(\n            0, batch_size * self.samples, self.samples, device=next_token_logits.device\n        ).unsqueeze(1)\n        ancestors = ancestors + first_batch_idx\n\n        ancestors = ancestors.view(self.samples * batch_size)\n        weights = weights.view(self.samples * batch_size)\n        next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n        return next_token_ids, ancestors, weights\n\n    @property\n    def sampling_params(self):\n        return SamplingParameters(\"beam_search\", self.samples, None, None, 1.0)\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the beam search sampler.</p> <p>Parameters:</p> Name Type Description Default <code>next_token_logits</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the probability distribution of the next token over the vocabulary.</p> required <code>sequence_weights</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs,)</code> that represents the cumulative weight of each sequence.</p> required <code>rng</code> <p>A random number generator.</p> required <p>Returns:</p> Type Description <code>A tuple with an array that contains the ids of the sampled tokens of</code> <code>shape ``(n_seqs, 1)``, an array that contains the ancestors of each</code> <code>sampled id of shape ``(n_seqs,)`` and an array that contains the updated</code> <code>cumulative weights of each sequence of shape ``(n_seqs,)``.</code> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    _,\n) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n    \"\"\"Call the beam search sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n    # Flatten scores to (n_batch, n_samples * vocab_size)\n    # and find the top-k weights for each batch.\n    batch_size = next_token_logits.shape[0] // self.samples\n    vocab_size = next_token_logits.shape[-1]\n    weights = weights.view(batch_size, self.samples * vocab_size)\n\n    # If the weights are all equal to 0 we are at the beginning of the search\n    # and thus only need to sample from one set of token logits for each\n    # batch.\n    if torch.all(sequence_weights == 0):\n        weights = weights[:, :vocab_size]\n\n    weights, indices = torch.topk(\n        weights, self.samples, dim=1, largest=True, sorted=True\n    )\n\n    ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n    next_token_ids = indices % vocab_size\n\n    # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n    first_batch_idx = torch.arange(\n        0, batch_size * self.samples, self.samples, device=next_token_logits.device\n    ).unsqueeze(1)\n    ancestors = ancestors + first_batch_idx\n\n    ancestors = ancestors.view(self.samples * batch_size)\n    weights = weights.view(self.samples * batch_size)\n    next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler","title":"<code>GreedySampler</code>","text":"<p>Greedy Sampling algorithm.</p> <p>Greedy sampling consists in choosing the token with the largest likelihood at every step.</p> <p>We don't allow more than one sample. We could attribute this a meaning, for instance the k-th sample represents the k-th most likely token. In which case it would be equivalent to beam search without the sequence weights.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <p>The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class GreedySampler:\n    \"\"\"Greedy Sampling algorithm.\n\n    Greedy sampling consists in choosing the token with the largest\n    likelihood at every step.\n\n    We don't allow more than one sample. We could attribute this a meaning, for\n    instance the k-th sample represents the k-th most likely token. In which\n    case it would be equivalent to beam search without the sequence weights.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(self):\n        self.samples = 1\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        _,\n    ) -&gt; \"torch.DoubleTensor\":\n        \"\"\"Call the greedy sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n        ancestors = torch.arange(\n            next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n\n    @property\n    def sampling_params(self):\n        return SamplingParameters(\"greedy\", self.samples, None, None, 0.0)\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the greedy sampler.</p> <p>Parameters:</p> Name Type Description Default <code>next_token_logits</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the probability distribution of the next token over the vocabulary.</p> required <code>sequence_weights</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs,)</code> that represents the cumulative weight of each sequence.</p> required <code>rng</code> <p>A random number generator.</p> required <p>Returns:</p> Type Description <code>A tuple with an array that contains the ids of the sampled tokens of</code> <code>shape ``(n_seqs, 1)``, an array that contains the ancestors of each</code> <code>sampled id of shape ``(n_seqs,)`` and an array that contains the updated</code> <code>cumulative weights of each sequence of shape ``(n_seqs,)``.</code> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    _,\n) -&gt; \"torch.DoubleTensor\":\n    \"\"\"Call the greedy sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n    ancestors = torch.arange(\n        next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler","title":"<code>MultinomialSampler</code>","text":"<p>Multinomial sampling algorithm.</p> <p>Multinomial sampling consists in randomly sampling the next token assuming its distribution is a Categorical distribution parametrized by the next-token logits.</p> <p>Attributes:</p> Name Type Description <code>samples</code> <p>The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class MultinomialSampler:\n    \"\"\"Multinomial sampling algorithm.\n\n    Multinomial sampling consists in randomly sampling the next token assuming\n    its distribution is a Categorical distribution parametrized by the\n    next-token logits.\n\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        samples: int = 1,\n        *,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        temperature: Optional[float] = None,\n    ):\n        self.samples = samples\n        self.top_k = top_k\n        self.top_p = top_p\n        self.temperature = temperature\n\n        self.logits_processors = []\n        if top_k is not None:\n            self.logits_processors.append(keep_top_k_logits(top_k))\n        elif top_p is not None:\n            self.logits_processors.append(keep_top_p_logits(top_p))\n\n        if temperature is not None:\n            self.logits_processors.append(rescale_logits(temperature))\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        rng: \"torch.Generator\",\n    ) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n        \"\"\"Call the multinomial sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        altered_next_token_logits = next_token_logits\n        for logit_processor in self.logits_processors:\n            altered_next_token_logits = logit_processor(next_token_logits)\n\n        probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n        next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n        logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n        ancestors = torch.arange(\n            altered_next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n\n    @property\n    def sampling_params(self):\n        return SamplingParameters(\n            \"multinomial\",\n            self.samples,\n            self.top_p,\n            self.top_k,\n            self.temperature,\n        )\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, rng)</code>","text":"<p>Call the multinomial sampler.</p> <p>Parameters:</p> Name Type Description Default <code>next_token_logits</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the probability distribution of the next token over the vocabulary.</p> required <code>sequence_weights</code> <code>DoubleTensor</code> <p>A tensor of shape <code>(n_seqs,)</code> that represents the cumulative weight of each sequence.</p> required <code>rng</code> <code>Generator</code> <p>A random number generator.</p> required <p>Returns:</p> Type Description <code>A tuple with an array that contains the ids of the sampled tokens of</code> <code>shape ``(n_seqs, 1)``, an array that contains the ancestors of each</code> <code>sampled id of shape ``(n_seqs,)`` and an array that contains the updated</code> <code>cumulative weights of each sequence of shape ``(n_seqs,)``.</code> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    rng: \"torch.Generator\",\n) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n    \"\"\"Call the multinomial sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    altered_next_token_logits = next_token_logits\n    for logit_processor in self.logits_processors:\n        altered_next_token_logits = logit_processor(next_token_logits)\n\n    probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n    next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n    logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n    ancestors = torch.arange(\n        altered_next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.SamplingParameters","title":"<code>SamplingParameters</code>  <code>dataclass</code>","text":"<p>Sampling parameters available in Outlines.</p> Source code in <code>outlines/samplers.py</code> <pre><code>@dataclass(frozen=True)\nclass SamplingParameters:\n    \"\"\"Sampling parameters available in Outlines.\"\"\"\n\n    sampler: str\n    num_samples: int = 1\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n    temperature: Optional[float] = None\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_k_logits","title":"<code>keep_top_k_logits(k)</code>","text":"<p>Build a function that masks logits values smaller than the top <code>k</code> ones.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The ranking below which logit values are replaced by <code>-math.inf</code>.</p> required Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_k_logits(k: int) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that masks logits values smaller than the top `k` ones.\n\n    Parameters\n    ----------\n    k\n        The ranking below which logit values are replaced by `-math.inf`.\n\n    \"\"\"\n    import torch\n\n    if not isinstance(k, int) or k &lt; 1:\n        raise ValueError(f\"`k` must be a strictly positive integers, got {k} instead.\")\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        num_to_keep = min(k, logits.size(-1))\n        mask_idx = logits &lt; torch.topk(logits, num_to_keep)[0][..., -1, None]\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_p_logits","title":"<code>keep_top_p_logits(p)</code>","text":"<p>Build a function that masks the lowest probability tokens whose cumulative probability is below a certain threshold.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>The value of the threshold. We keep the highest probability tokens whose cumulative distribution is greater than or equal to <code>p</code> and mask the others. Its value must be between 0 (excluded) and 1 (included).</p> required Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_p_logits(p: float) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that masks the lowest probability tokens whose\n    cumulative probability is below a certain threshold.\n\n    Parameters\n    ----------\n    p\n        The value of the threshold. We keep the highest probability tokens whose\n        cumulative distribution is greater than or equal to `p` and mask the\n        others. Its value must be between 0 (excluded) and 1 (included).\n\n    \"\"\"\n    import torch\n\n    if p &lt;= 0.0 or p &gt; 1.0:\n        raise ValueError(\n            f\"`p` must be a floating point number between 0 (excluded) and 1 (included), got {p} instead.\"\n        )\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        sorted_logits, sorted_idx = torch.sort(logits, descending=False)\n        cumulative_probabilties = torch.nn.functional.softmax(\n            sorted_logits, dim=-1\n        ).cumsum(dim=-1)\n\n        sorted_masked_idx = cumulative_probabilties &lt;= (1 - p)\n        mask_idx = torch.scatter(sorted_masked_idx, 1, sorted_idx, sorted_masked_idx)\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.rescale_logits","title":"<code>rescale_logits(temperature)</code>","text":"<p>Build a function that rescales the token probabilities exponentially.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>The value by which we rescale the logits.</p> required Source code in <code>outlines/samplers.py</code> <pre><code>def rescale_logits(temperature: float) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that rescales the token probabilities exponentially.\n\n    Parameters\n    ----------\n    temperature\n        The value by which we rescale the logits.\n\n    \"\"\"\n\n    if not isinstance(temperature, float) or temperature &lt; 0.0:\n        raise ValueError(\n            f\"`temperature` must be a strictly positive floating point number, got {temperature} instead.\"\n        )\n    elif temperature == 0.0:\n        raise ValueError(\n            \"Please use the greedy sampler instead of setting the temperature to 0.\"\n        )\n\n    def logits_processor(logits: \"torch.Tensor\") -&gt; \"torch.Tensor\":\n        return logits / temperature\n\n    return logits_processor\n</code></pre>"},{"location":"api/templates/","title":"Templates","text":""},{"location":"api/templates/#outlines.templates.Template","title":"<code>Template</code>  <code>dataclass</code>","text":"<p>Represents a prompt template.</p> <p>We return a <code>Template</code> class instead of a simple function so the template can be accessed by callers.</p> Source code in <code>outlines/templates.py</code> <pre><code>@dataclass\nclass Template:\n    \"\"\"Represents a prompt template.\n\n    We return a `Template` class instead of a simple function so the\n    template can be accessed by callers.\n\n    \"\"\"\n\n    template: jinja2.Template\n    signature: Optional[inspect.Signature]\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        The rendered template as a Python ``str``.\n\n        \"\"\"\n        if self.signature is not None:\n            bound_arguments = self.signature.bind(*args, **kwargs)\n            bound_arguments.apply_defaults()\n            return self.template.render(**bound_arguments.arguments)\n        else:\n            return self.template.render(**kwargs)\n\n    @classmethod\n    def from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a string containing a Jinja template.\n\n        Parameters\n        ----------\n        content : str\n            The string content to be converted into a template.\n\n        Returns\n        -------\n        An instance of the class with the provided content as a template.\n        \"\"\"\n        return cls(build_template_from_string(content, filters), None)\n\n    @classmethod\n    def from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n        \"\"\"Create a `Template` instance from a file containing a Jinja template.\n\n        Note: This method does not allow to include and inheritance to reference files\n        that are outside the folder or subfolders of the file given to `from_file`.\n\n        Parameters\n        ----------\n        path : Path\n            The path to the file containing the Jinja template.\n\n        Returns\n        -------\n        Template\n            An instance of the Template class with the template loaded from the file.\n        \"\"\"\n        # We don't use a `Signature` here because it seems not feasible to infer one from a Jinja2 environment that is\n        # split across multiple files (since e.g. we support features like Jinja2 includes and template inheritance)\n        return cls(build_template_from_file(path, filters), None)\n</code></pre>"},{"location":"api/templates/#outlines.templates.Template.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p> <p>Returns:</p> Type Description <code>The rendered template as a Python ``str``.</code> Source code in <code>outlines/templates.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    The rendered template as a Python ``str``.\n\n    \"\"\"\n    if self.signature is not None:\n        bound_arguments = self.signature.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        return self.template.render(**bound_arguments.arguments)\n    else:\n        return self.template.render(**kwargs)\n</code></pre>"},{"location":"api/templates/#outlines.templates.Template.from_file","title":"<code>from_file(path, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a file containing a Jinja template.</p> <p>Note: This method does not allow to include and inheritance to reference files that are outside the folder or subfolders of the file given to <code>from_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file containing the Jinja template.</p> required <p>Returns:</p> Type Description <code>Template</code> <p>An instance of the Template class with the template loaded from the file.</p> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a file containing a Jinja template.\n\n    Note: This method does not allow to include and inheritance to reference files\n    that are outside the folder or subfolders of the file given to `from_file`.\n\n    Parameters\n    ----------\n    path : Path\n        The path to the file containing the Jinja template.\n\n    Returns\n    -------\n    Template\n        An instance of the Template class with the template loaded from the file.\n    \"\"\"\n    # We don't use a `Signature` here because it seems not feasible to infer one from a Jinja2 environment that is\n    # split across multiple files (since e.g. we support features like Jinja2 includes and template inheritance)\n    return cls(build_template_from_file(path, filters), None)\n</code></pre>"},{"location":"api/templates/#outlines.templates.Template.from_string","title":"<code>from_string(content, filters={})</code>  <code>classmethod</code>","text":"<p>Create a <code>Template</code> instance from a string containing a Jinja template.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The string content to be converted into a template.</p> required <p>Returns:</p> Type Description <code>An instance of the class with the provided content as a template.</code> Source code in <code>outlines/templates.py</code> <pre><code>@classmethod\ndef from_string(cls, content: str, filters: Dict[str, Callable] = {}):\n    \"\"\"Create a `Template` instance from a string containing a Jinja template.\n\n    Parameters\n    ----------\n    content : str\n        The string content to be converted into a template.\n\n    Returns\n    -------\n    An instance of the class with the provided content as a template.\n    \"\"\"\n    return cls(build_template_from_string(content, filters), None)\n</code></pre>"},{"location":"api/templates/#outlines.templates.create_jinja_env","title":"<code>create_jinja_env(loader, filters)</code>","text":"<p>Create a new Jinja environment.</p> <p>The Jinja environment is loaded with a set of pre-defined filters: - <code>name</code>: get the name of a function - <code>description</code>: get a function's docstring - <code>source</code>: get a function's source code - <code>signature</code>: get a function's signature - <code>args</code>: get a function's arguments - <code>schema</code>: isplay a JSON Schema</p> <p>Users may pass additional filters, and/or override existing ones.</p> Arguments <p>loader    An optional <code>BaseLoader</code> instance filters    A dictionary of filters, map between the filter's name and the    corresponding function.</p> Source code in <code>outlines/templates.py</code> <pre><code>def create_jinja_env(\n    loader: Optional[jinja2.BaseLoader], filters: Dict[str, Callable]\n) -&gt; jinja2.Environment:\n    \"\"\"Create a new Jinja environment.\n\n    The Jinja environment is loaded with a set of pre-defined filters:\n    - `name`: get the name of a function\n    - `description`: get a function's docstring\n    - `source`: get a function's source code\n    - `signature`: get a function's signature\n    - `args`: get a function's arguments\n    - `schema`: isplay a JSON Schema\n\n    Users may pass additional filters, and/or override existing ones.\n\n    Arguments\n    ---------\n    loader\n       An optional `BaseLoader` instance\n    filters\n       A dictionary of filters, map between the filter's name and the\n       corresponding function.\n\n    \"\"\"\n    env = jinja2.Environment(\n        loader=loader,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=jinja2.StrictUndefined,\n    )\n\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n    env.filters[\"args\"] = get_fn_args\n\n    # The filters passed by the user may override the\n    # pre-defined filters.\n    for name, filter_fn in filters.items():\n        env.filters[name] = filter_fn\n\n    return env\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_fn_args","title":"<code>get_fn_args(fn)</code>","text":"<p>Returns the arguments of a function with annotations and default values if provided.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_args(fn: Callable):\n    \"\"\"Returns the arguments of a function with annotations and default values if provided.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `args` filter only applies to callables.\")\n\n    arg_str_list = []\n    signature = inspect.signature(fn)\n    arg_str_list = [str(param) for param in signature.parameters.values()]\n    arg_str = \", \".join(arg_str_list)\n    return arg_str\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/templates.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api/templates/#outlines.templates.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/templates.py</code> <pre><code>@get_schema.register(type(pydantic.BaseModel))\ndef get_schema_pydantic(model: Type[pydantic.BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if not isinstance(model, type(pydantic.BaseModel)):\n        raise TypeError(\"The `schema` filter only applies to Pydantic models.\")\n\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api/templates/#outlines.templates.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/templates.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value:\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api/templates/#outlines.templates.prompt","title":"<code>prompt(fn=None, filters={})</code>","text":"<p>Decorate a function that contains a prompt template.</p> <p>This allows to define prompts in the docstring of a function and simplify their manipulation by providing some degree of encapsulation. It uses the <code>render</code> function internally to render templates.</p> <pre><code>&gt;&gt;&gt; import outlines\n&gt;&gt;&gt;\n&gt;&gt;&gt; @outlines.prompt\n&gt;&gt;&gt; def build_prompt(question):\n...    \"I have a ${question}\"\n...\n&gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n</code></pre> <p>This API can also be helpful in an \"agent\" context where parts of the prompt are set when the agent is initialized and never modified later. In this situation we can partially apply the prompt function at initialization.</p> <pre><code>&gt;&gt;&gt; import outlines\n&gt;&gt;&gt; import functools as ft\n...\n&gt;&gt;&gt; @outlines.prompt\n... def solve_task(name: str, objective: str, task: str):\n...     \"\"\"Your name is {{name}}.\n...     Your overall objective is to {{objective}}.\n...     Please solve the following task: {{task}}\n...     \"\"\"\n...\n&gt;&gt;&gt; hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")\n</code></pre> <p>Additional Jinja2 filters can be provided as keyword arguments to the decorator.</p> <pre><code>&gt;&gt;&gt; def reverse(s: str) -&gt; str:\n...     return s[::-1]\n...\n&gt;&gt;&gt; @outlines.prompt(filters={ 'reverse': reverse })\n... def reverse_prompt(text):\n...     \"\"\"{{ text | reverse }}\"\"\"\n...\n&gt;&gt;&gt; prompt = reverse_prompt(\"Hello\")\n&gt;&gt;&gt; print(prompt)\n... \"olleH\"\n</code></pre> <p>Returns:</p> Type Description <code>A `Template` callable class which will render the template when called.</code> Source code in <code>outlines/templates.py</code> <pre><code>def prompt(\n    fn: Optional[Callable] = None,\n    filters: Dict[str, Callable] = {},\n) -&gt; Callable:\n    \"\"\"Decorate a function that contains a prompt template.\n\n    This allows to define prompts in the docstring of a function and simplify their\n    manipulation by providing some degree of encapsulation. It uses the `render`\n    function internally to render templates.\n\n    ```pycon\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @outlines.prompt\n    &gt;&gt;&gt; def build_prompt(question):\n    ...    \"I have a ${question}\"\n    ...\n    &gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n    ```\n\n    This API can also be helpful in an \"agent\" context where parts of the prompt\n    are set when the agent is initialized and never modified later. In this situation\n    we can partially apply the prompt function at initialization.\n\n    ```pycon\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; import functools as ft\n    ...\n    &gt;&gt;&gt; @outlines.prompt\n    ... def solve_task(name: str, objective: str, task: str):\n    ...     \\\"\"\"Your name is {{name}}.\n    ...     Your overall objective is to {{objective}}.\n    ...     Please solve the following task: {{task}}\n    ...     \\\"\"\"\n    ...\n    &gt;&gt;&gt; hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")\n    ```\n\n    Additional Jinja2 filters can be provided as keyword arguments to the decorator.\n\n    ```pycon\n    &gt;&gt;&gt; def reverse(s: str) -&gt; str:\n    ...     return s[::-1]\n    ...\n    &gt;&gt;&gt; @outlines.prompt(filters={ 'reverse': reverse })\n    ... def reverse_prompt(text):\n    ...     \\\"\"\"{{ text | reverse }}\\\"\"\"\n    ...\n    &gt;&gt;&gt; prompt = reverse_prompt(\"Hello\")\n    &gt;&gt;&gt; print(prompt)\n    ... \"olleH\"\n    ```\n\n    Returns\n    -------\n    A `Template` callable class which will render the template when called.\n\n    \"\"\"\n    warnings.warn(\n        \"The @prompt decorator is deprecated and will be removed in outlines 1.1.0. \"\n        \"Instead of using docstring templates, please use Template.from_file() to \"\n        \"load your prompts from separate template files, or a simple Python function \"\n        \"that returns text. This helps keep prompt content separate from code and is \"\n        \"more maintainable.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if fn is None:\n        return lambda fn: prompt(fn, cast(Dict[str, Callable], filters))\n\n    signature = inspect.signature(fn)\n\n    # The docstring contains the template that will be rendered to be used\n    # as a prompt to the language model.\n    docstring = fn.__doc__\n    if docstring is None:\n        raise TypeError(\"Could not find a template in the function's docstring.\")\n\n    template = build_template_from_string(cast(str, docstring), filters)\n\n    return Template(template, signature)\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/01/10/roadmap-for-2024/","title":"Roadmap for 2024","text":"<p>Outlines is not even one year old and it's already gone a long way! As we just reached 4000 stars, and before laying out the roadmap for the following year, we would like to pause and thank all of you for supporting us, using and contributing to the library!</p> <p></p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#thoughts","title":"Thoughts","text":"<p>Before delving into the detailed roadmap, let me share a few thoughts and explain the general direction of the library. These thoughts are informed with my multiple interactions with users, either on Twitter or in our Discord server.</p> <p>Outlines currently differentiates itself from other libraries with its efficient JSON- and regex- constrained generation. A user-facing interface for grammar-structured generation (it had been hidden in the repository) was also recently added. But there is much more we can do along these lines. In 2024 will we will keep pushing in the direction of more accurate, faster constrained generation.</p> <p>Outlines also supports many models providers: <code>transformers</code>, <code>mamba</code>, <code>llama.cpp</code> and <code>exllama2</code>. Those integrations represent a lot of maintenance, and we will need to simplify them. For instance, <code>transformers</code> now supports quantized models, and we will soon deprecate the support for <code>autoawq</code> and <code>autogptq</code>. Thanks to a refactor of the library, it is now possible to use our constrained generation method by using logits processor with all other libraries, except <code>mamba</code>. We will look for libraries that provide state-space models and allow to pass a logits processor during inference. We will interface with <code>llama.cpp</code> and <code>exllama2</code> using logits processors.</p> <p>We would like expand our work to the whole sampling layer, and add new sampling methods that should make structured generation more accurate. This means we will keep the <code>transformers</code> integration as it is today and will expand our text generation logic around this library.</p> <p>Making workflows re-usable and easy to share is difficult today. That is why we are big believers in outlines functions. We will keep improving the interface and adding examples.</p> <p>Finally, we want to add a CLI tool, <code>outlines serve</code>. This will allows you to either serve an API that does general constrained generation, or to serve Outlines function.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#detailed-roadmap","title":"Detailed roadmap","text":"<p>Here is a more detailed roadmap for the next 12 months. Outlines is a community effort, and we invite you to pick either topic and contribute to the library. I will progressively add related issues in the repository.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#many-more-examples-and-tutorials","title":"Many more examples and tutorials","text":"<p>Let's be honest, Outlines is lacking clear and thorough examples. We want to change this!</p> <ul> <li>How does Outlines work? What can you do with it?</li> <li>What can you do with Outlines that is harder or impossible to do with other libraries?</li> <li>How you can perform standard LLM workflows, for instance Chain of Thoughts, Tree of Thoughts, etc?</li> <li>How does Oultines integrates with the larger ecosystem, for instance other libraries like LangChain and LlamaIndex?</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#simplify-the-integrations","title":"Simplify the integrations","text":"<p>We want to keep the current integrations but lower the maintenance cost so we can focus on what we bring to the table.</p> <ul> <li>Deprecate every obsolete integration: <code>transformers</code> has recently integrated <code>autoawq</code> and <code>autogptq</code> for instance. (PR)</li> <li>See if we can integrate to a library that provides state-space models via a logit processing function;</li> <li>Integrate with llama.cpp via a logits processor;</li> <li>Integrate with exllamav2 via a logits processor;</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#push-structured-generation-further","title":"Push structured generation further","text":"<p>We're just getting started!</p> <ul> <li>Improve the performance of existing structured generation algorithms;</li> <li>Improve the correctness of structured generation algorithms;</li> <li>Add ready-to-use grammars in the grammars repository or in a submodule in Outlines.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#keep-developing-outlines-functions","title":"Keep developing Outlines functions","text":"<p>Functions are awesome, use them!</p> <ul> <li>Implement a CLI <code>outlines serve</code> that allows to serve Outlines functions locally;</li> <li>Add more functions to the functions repository.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#serve-structured-generation","title":"Serve structured generation","text":"<p>We want to make it easier to serve structured generation and outlines functions.</p> <ul> <li>Implement the outlines serve CLI <code>outlines serve</code></li> <li>Serve local APIs that perform structured generation;</li> <li>Serve Outlines functions.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#improve-the-generation-layer","title":"Improve the generation layer","text":"<ul> <li>Use <code>transformers</code>'s private API to prepare inputs for generation inside the <code>Transformers</code> class;</li> <li>Support successions of model generation and text infilling for methods like Beam Search and SMC;</li> <li>Differentiate by adding new caching methods: attention sink, trie-based caching, etc;</li> <li>Differentiate by implementing SMC;</li> <li>Implement Beam Search;</li> <li>Add token healing.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#a-more-seamless-integration-with-openai","title":"A more seamless integration with OpenAI","text":"<ul> <li>Provide the same user interface for OpenAI and open source models so they are easily interchangeable;</li> <li>Integrate the function calling API.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#last-word","title":"Last word","text":"<p>This roadmap was influenced by the expressed interests of the community. If it doesn't reflect your needs please come and share your experience with us.</p>"},{"location":"community/","title":"Community","text":"<p>Outlines exists for a community of users who believe software doesn't need to be complicated. Who share the same passion for Large Language Models but don't want to compromise on robustness. Together, we are bringing these powerful models back to the world of software.</p>"},{"location":"community/#connect-on-discord","title":"Connect on Discord","text":"<p>The Outlines community lives on our Discord server. There you can ask questions, share ideas or just chat with people like you. Don't be a stranger and join us.</p>"},{"location":"community/contribute/","title":"Contribute","text":""},{"location":"community/contribute/#what-contributions","title":"What contributions?","text":"<ul> <li>Documentation contributions are very valuable to us!</li> <li>Examples. Show us what you did with Outlines :)</li> <li>Bug reports with a minimum working examples in the issue tracker</li> <li>Bug fixes are always a pleasure to review.</li> <li>New features. Please start a new discussion, or come chat with us beforehand!</li> </ul> <p>Note that the issue tracker is only intended for actionable items. In doubt, open a discussion or come talk to us.</p>"},{"location":"community/contribute/#how-to-contribute","title":"How to contribute?","text":""},{"location":"community/contribute/#setup","title":"Setup","text":"<p>First, fork the repository on GitHub and clone the fork locally:</p> <pre><code>git clone git@github.com/YourUserName/outlines.git\ncd outlines\n</code></pre> <p>Create a new virtual environment:</p> <p>If you are using <code>uv</code>:</p> <pre><code>uv venv\nsource .venv/bin/activate\nalias pip=\"uv pip\" # ... or just remember to prepend any pip command with uv in the rest of this guide\n</code></pre> <p>If you are using <code>venv</code>:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>If you are using <code>conda</code>:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Then install the dependencies in editable mode, and install the <code>pre-commit</code> hooks:</p> <p><pre><code>pip install -e \".[test]\"\npre-commit install\n</code></pre> If you own a GPU and want to run the vLLM tests you will have to run:</p> <pre><code>pip install -e \".[test-gpu]\"\n</code></pre> <p>instead.</p> <p>Outlines provides optional dependencies for different supported backends, which you can install with</p> <pre><code>pip install \".[vllm]\"\n</code></pre> <p>A list of supported optional dependencies can be found in the installation guide.</p>"},{"location":"community/contribute/#using-vscode-devcontainer-github-codespaces","title":"Using VSCode DevContainer / GitHub Codespaces","text":"<p>If you want a fully pre-configured development environment, you can use VSCode DevContainers or GitHub Codespaces.</p>"},{"location":"community/contribute/#vscode-devcontainer","title":"VSCode DevContainer","text":"<ol> <li>Ensure that the Docker daemon is running on your machine.</li> <li>Install the Dev Containers extension in VSCode.</li> <li>Open the Outlines repository in VSCode. When prompted, Reopen in Container (or press <code>F1</code> and select \"Remote-Containers: Reopen in Container\").</li> <li>Run the normal setup steps. Your environment will not complain about missing system dependencies!</li> </ol>"},{"location":"community/contribute/#github-codespaces","title":"GitHub Codespaces","text":"<ol> <li>Navigate to the Outlines repository on GitHub.</li> <li>Click on the Code button and select the Codespaces tab.</li> <li>Click Create codespace on main (or another branch you are working on).</li> <li>GitHub will launch a pre-configured cloud development environment.</li> </ol> <p>You will not have access to a GPU, but you'll be able to make basic contributions to the project on the go while using a fully featured web-based IDE.</p>"},{"location":"community/contribute/#before-pushing-your-code","title":"Before pushing your code","text":"<p>Run the tests:</p> <pre><code>pytest\n</code></pre> <p>And run the code style checks:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"community/contribute/#benchmarking","title":"Benchmarking","text":"<p>Outlines uses asv for automated benchmark testing. Benchmarks are run automatically before pull requests are merged to prevent performance degradation.</p> <p>You can run the benchmark test suite locally with the following command:</p> <pre><code>asv run --config benchmarks/asv.conf.json\n</code></pre> <p>Caveats:</p> <ul> <li>If you're on a device with CUDA, you must add the argument <code>--launch-method spawn</code></li> <li>Uncommitted code will not be benchmarked, you must first commit your changes.</li> </ul>"},{"location":"community/contribute/#run-a-specific-test","title":"Run a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#profile-a-specific-test","title":"Profile a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json --profile -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#compare-to-originmain","title":"Compare to <code>origin/main</code>","text":"<pre><code>get fetch origin\nasv continuous origin/main HEAD --config benchmarks/asv.conf.json\n</code></pre>"},{"location":"community/contribute/#asv-pr-behavior","title":"ASV PR Behavior","text":"<ul> <li>View ASV Benchmark Results: Open the workflow, view <code>BENCHMARK RESULTS</code> section.</li> <li>Merging is blocked unless benchmarks are run for the latest commit.</li> <li>Benchmarks fail if performance degrades by more than 10% for any individual benchmark.</li> <li>The \"Benchmark PR\" workflow runs when it is manually dispatched, or if the <code>run_benchmarks</code> label is added to the PR they run for every commit.</li> </ul>"},{"location":"community/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>To work on the documentation you will need to install the related dependencies:</p> <pre><code>pip install -r requirements-doc.txt\n</code></pre> <p>To build the documentation and serve it locally, run the following command in the repository's root folder:</p> <pre><code>mkdocs serve\n</code></pre> <p>By following the instruction you will be able to view the documentation locally. It will be updated every time you make a change.</p>"},{"location":"community/contribute/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a new branch on your fork, commit and push the changes:</p> <pre><code>git checkout -b new-branch\ngit add .\ngit commit -m \"Changes I made\"\ngit push origin new-branch\n</code></pre> <p>Then you can open a pull request on GitHub. It should prompt you to do so. Every subsequent change that you make on your branch will update the pull request.</p> <p>Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback. If you need help, come tell us on Discord.</p>"},{"location":"community/examples/","title":"Community projects and articles","text":"<p>Publishing examples and articles about Outlines are a meaningful way to contribute to the community. Here is a list of projects we are aware of. Drop us a line if we forgot yours!</p> <p>MMSG is a Python library for generating interleaved text and image content in a structured format you can directly pass to downstream APIs.</p> <p>Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report shows that Structured Generation can outperform finetuning, and maybe even multimodality, in document-image understanding tasks as part of CVPR's 2nd MMFM Challenge.</p> <p>Chess LLM Arena is a HuggingFace Space where you can make LLMs compete in a chess match.</p> <p>LLM Data Gen is a HuggingFace Space that generates synthetic dataset files in JSONLines format.</p> <p>Fast, High-Fidelity LLM Decoding with Regex Constraints  presents an efficient alternative to Outlines's structured generation.</p> <p>gigax is an Open-Source library that allows to create real-time LLM-powered NPCs for video games.</p> <p>Improving Prompt Consistency with Structured Generations shows how structured generation can improve consistency of evaluation runs by reducing sensitivity to changes in prompt format.</p> <p>AskNews is a news curation service processing 300k news articles per day in a structured way, with Outlines.</p>"},{"location":"community/feedback/","title":"Feedback","text":"<p>If Outlines has been helpful to you, let us know on Discord or give us a shoutout on Twitter! It's always heartwarming \u2764\ufe0f</p> <p> <p></p> <p>I am once again reminding you that structured extraction using LLMs is going to transform every single industry in the next 10 years https://t.co/xQ3tcWnrZ8</p>\u2014 Sam Hogan (@0xSamHogan) April 17, 2024 <p>outline's growth is insane, using is an understatement! https://t.co/rHCNWhZdCs</p>\u2014 jason liu (@jxnlco) April 17, 2024 <p>Outlines is an amazing lib and more popular than @remilouf\u2019s modesty will admit. https://t.co/DfHbMPIlX1 https://t.co/mDHIWJrD0C</p>\u2014 Delip Rao e/\u03c3 (@deliprao) April 18, 2024 <p>Impressive implementation of a true regex / json / grammar guided text generation pic.twitter.com/RX5RVYaVIx</p>\u2014 Rohan Paul (@rohanpaul_ai) December 30, 2023 <p>Most underrated Github Repo in AI + LLM JSON guided Generation: https://t.co/lSB8KIet1H</p>\u2014 \ud83c\udf99Jean-Louis Queguiner (@JiliJeanlouis) December 18, 2023 <p>Nice and useful. https://t.co/LX72AE0lgt</p>\u2014 Dan Roy (@roydanroy) August 15, 2023 <p>HUGE dub for open source AI https://t.co/bYKuiEUZ1j</p>\u2014 kenneth \ud83d\udd87 (@k3nnethfrancis) August 15, 2023 <p>This is amazing - glad to see more outp guidance modules! Will try this out soon I'm wondering how they translate from regex automatons to token boundariesAlso why Open Source will succeed. Even today I don't see any guided output functionality from the big providers. https://t.co/Ity2H25Klf</p>\u2014 Hrishi (@hrishioa) August 14, 2023 <p>Outlines - a library to help LLM developers guide text generation in a fast and reliable way.\"Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.\"Need to check this out. Reliable JSON output is a common use\u2026 pic.twitter.com/Bkbh8vKogN</p>\u2014 elvis (@omarsar0) August 14, 2023 <p>Woah this is cool! Makes open source models more usable.Give any LLM Function Call capability (and more) with Outlines: https://t.co/PtPykR5ZGR https://t.co/RRQjWHnIxv pic.twitter.com/BwNnH8SMwv</p>\u2014 Yohei (@yoheinakajima) August 14, 2023 <p>This is awesome! Being able to guarantee the output's structure unblocks so many applications. This is a great milestone and a fundamental building block for more advanced AI apps. https://t.co/WdwMOc7hE8</p>\u2014 Guilherme Castro (@skastr052) August 15, 2023 <p>Juggling with the unpredictable outputs of ChatGPT API lately while building my product. \ud83d\ude13 Tried prompt engineering to channel its wisdom into a neat JSON, but it's like asking a cat to fetch. \ud83d\udc31Luckily, stumbled upon \"Outlines\" \u2013 looks like a promising way to tame the LLM\u2026 pic.twitter.com/oYQ6q8exAS</p>\u2014 Charlie (@14435635Sun) August 15, 2023 <p>A complex system of LLM input-outputs interacting with non-LLM agents and models benefits immeasurably from structured outputs. The outlines package saves so much time, https://t.co/NhVQ6NpKDR</p>\u2014 Amir Sani (@amirsani) November 26, 2023"},{"location":"community/feedback/#let-us-know","title":"Let us know!","text":"<p>We highly value the insights of our users, and we would love to hear from you. If you are using Outlines for your projects and would like to share your experience with us, let's connect:</p> <ul> <li>What are you building with it?</li> <li>What do you like about it?</li> <li>What challenges are you facing?</li> <li>What do you think could be improved?</li> </ul> <p>To schedule an appointment follow this link. This is exclusively intended to share your experience, please go on Discord or GitHub for support.</p>"},{"location":"community/versioning/","title":"Versioning Guide","text":"<p>The Outlines project follows a structured versioning scheme designed to provide clarity and minimize risk for downstream dependents.</p> <p>Each part of the version number (<code>major.minor.patch</code>) conveys information about the nature and impact of the changes included in the release.</p> <ul> <li>Major Releases includes compatibility-breaking changes to core interfaces, such as <code>LogitsProcessor</code>s and <code>Guides</code>.</li> <li>Minor Releases introduce changes of substance to internal or unexposed functionality. These changes are well tested and intended to maintain compatability with existing use of core interfaces.</li> <li>Patch Releases address bug fixes and incorporate low-risk changes to improve stability and performance.</li> </ul>"},{"location":"community/versioning/#releases","title":"Releases","text":"<p>Releases along with release notes can be found on the Outlines Releases GitHub Page.</p>"},{"location":"community/versioning/#version-pinning-recommendations","title":"Version Pinning Recommendations","text":"<p>Here are our recommendations for managing dependencies on the Outlines package:</p> <p>Small, Risk-Tolerant Projects: Pin to a specific major version.</p> <p>Large, Conservative Projects: Pin to a specific minor version.</p>"},{"location":"cookbook/","title":"Examples","text":"<p>This part of the documentation provides a few cookbooks that you can browse to get acquainted with the library and get some inspiration about what you could do with structured generation. Remember that you can easily change the model that is being used!</p> <ul> <li>Classification: Classify customer requests.</li> <li>Named Entity Extraction: Extract information from pizza orders.</li> <li>Dating Profile: Build dating profiles from descriptions using prompt templating and JSON-structured generation.</li> <li>Chain Of Density: Summarize documents using chain of density prompting and JSON-structured generation.</li> <li>Playing Chess: Make Phi-3 Mini play chess against itself using regex-structured generation.</li> <li>SimToM: Improve LLMs' Theory of Mind capabilities with perspective-taking prompting and JSON-structured generation.</li> <li>Q&amp;A with Citations: Answer questions and provide citations using JSON-structured generation.</li> <li>Knowledge Graph Generation: Generate a Knowledge Graph from unstructured text using JSON-structured generation.</li> <li>Chain Of Thought (CoT): Generate a series of intermediate reasoning steps using regex-structured generation.</li> <li>ReAct Agent: Build an agent with open weights models using regex-structured generation.</li> <li>Earnings reports to CSV: Extract data from earnings reports to CSV using regex-structured generation.</li> <li>Vision-Language Models: Use Outlines with vision-language models for tasks like image captioning and visual reasoning.</li> <li>Receipt Digitization: Extract information from a picture of a receipt using structured generation.</li> <li>Structured Generation from PDFs: Use Outlines with vision-language models to read PDFs and produce structured output.</li> </ul>"},{"location":"cookbook/atomic_caption/","title":"Vision-Language Models with Outlines","text":"<p>This guide demonstrates how to use Outlines with vision-language models, leveraging the new transformers_vision module. Vision-language models can process both text and images, allowing for tasks like image captioning, visual question answering, and more.</p> <p>We will be using the Pixtral-12B model from Mistral to take advantage of some of its visual reasoning capabilities and a workflow to generate a multistage atomic caption.</p>"},{"location":"cookbook/atomic_caption/#setup","title":"Setup","text":"<p>First, we need to install the necessary dependencies. In addition to Outlines, we'll need to install the transformers library and any specific requirements for the vision-language model we'll be using.</p> <pre><code>pip install outlines transformers torch\n</code></pre>"},{"location":"cookbook/atomic_caption/#initializing-the-model","title":"Initializing the Model","text":"<p>We'll use the transformers_vision function to initialize our vision-language model. This function is specifically designed to handle models that can process both text and image inputs. Today we'll be using the Pixtral model with the llama tokenizer. (Currently the mistral tokenizer is pending support).</p> <pre><code>import torch\nfrom transformers import (\n    LlavaForConditionalGeneration,\n)\nmodel_name=\"mistral-community/pixtral-12b\" # original magnet model is able to be loaded without issue\nmodel_class=LlavaForConditionalGeneration\n\ndef get_vision_model(model_name: str, model_class: VisionModel):\n    model_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"attn_implementation\": \"flash_attention_2\",\n        \"device_map\": \"auto\",\n    }\n    processor_kwargs = {\n        \"device\": \"cuda\",\n    }\n\n    model = outlines.models.transformers_vision(\n        model.model_name,\n        model_class=model.model_class,\n        model_kwargs=model_kwargs,\n        processor_kwargs=processor_kwargs,\n    )\n    return model\nmodel = get_vision_model(model_name, model_class)\n</code></pre>"},{"location":"cookbook/atomic_caption/#defining-the-schema","title":"Defining the Schema","text":"<p>Next, we'll define a schema for the output we expect from our vision-language model. This schema will help structure the model's responses.</p> <pre><code>from pydantic import BaseModel, Field, confloat, constr\nfrom pydantic.types import StringConstraints, PositiveFloat\nfrom typing import List\nfrom typing_extensions import Annotated\n\nfrom enum import StrEnum\nclass TagType(StrEnum):\n    ENTITY = \"Entity\"\n    RELATIONSHIP = \"Relationship\"\n    STYLE = \"Style\"\n    ATTRIBUTE = \"Attribute\"\n    COMPOSITION = \"Composition\"\n    CONTEXTUAL = \"Contextual\"\n    TECHNICAL = \"Technical\"\n    SEMANTIC = \"Semantic\"\n\nclass ImageTag(BaseModel):\n    tag: Annotated[\n        constr(min_length=1, max_length=30),\n        Field(\n            description=(\n                \"Descriptive keyword or phrase representing the tag.\"\n            )\n        )\n    ]\n    category: TagType\n    confidence: Annotated[\n        confloat(le=1.0),\n        Field(\n            description=(\n                \"Confidence score for the tag, between 0 (exclusive) and 1 (inclusive).\"\n            )\n        )\n    ]\n\nclass ImageData(BaseModel):\n    tags_list: List[ImageTag] = Field(..., min_items=8, max_items=20)\n    short_caption: Annotated[str, StringConstraints(min_length=10, max_length=150)]\n    dense_caption: Annotated[str, StringConstraints(min_length=100, max_length=2048)]\n\nimage_data_generator = outlines.generate.json(model, ImageData)\n</code></pre> <p>This schema defines the structure for image tags, including categories like Entity, Relationship, Style, etc., as well as short and dense captions.</p>"},{"location":"cookbook/atomic_caption/#preparing-the-prompt","title":"Preparing the Prompt","text":"<p>We'll create a prompt that instructs the model on how to analyze the image and generate the structured output:</p> <pre><code>pixtral_instruction = \"\"\"\n&lt;s&gt;[INST]\n&lt;Task&gt;You are a structured image analysis agent. Generate comprehensive tag list, caption, and dense caption for an image classification system.&lt;/Task&gt;\n&lt;TagCategories requirement=\"You should generate a minimum of 1 tag for each category.\" confidence=\"Confidence score for the tag, between 0 (exclusive) and 1 (inclusive).\"&gt;\n- Entity : The content of the image, including the objects, people, and other elements.\n- Relationship : The relationships between the entities in the image.\n- Style : The style of the image, including the color, lighting, and other stylistic elements.\n- Attribute : The most important attributes of the entities and relationships in the image.\n- Composition : The composition of the image, including the arrangement of elements.\n- Contextual : The contextual elements of the image, including the background, foreground, and other elements.\n- Technical : The technical elements of the image, including the camera angle, lighting, and other technical details.\n- Semantic : The semantic elements of the image, including the meaning of the image, the symbols, and other semantic details.\n&lt;Examples note=\"These show the expected format as an abstraction.\"&gt;\n{\n  \"tags_list\": [\n    {\n      \"tag\": \"subject 1\",\n      \"category\": \"Entity\",\n      \"confidence\": 0.98\n    },\n    {\n      \"tag\": \"subject 2\",\n      \"category\": \"Entity\",\n      \"confidence\": 0.95\n    },\n    {\n      \"tag\": \"subject 1 runs from subject 2\",\n      \"category\": \"Relationship\",\n      \"confidence\": 0.90\n    },\n   }\n&lt;/Examples&gt;\n&lt;/TagCategories&gt;\n&lt;ShortCaption note=\"The short caption should be a concise single sentence caption of the image content with a maximum length of 100 characters.\"&gt;\n&lt;DenseCaption note=\"The dense caption should be a descriptive but grounded narrative paragraph of the image content with high quality narrative prose. It should incorporate elements from each of the tag categories to provide a broad dense caption\"&gt;\\n[IMG][/INST]\n\"\"\".strip()\n</code></pre> <p>This prompt provides detailed instructions to the model on how to generate comprehensive tag lists, captions, and dense captions for image analysis. Because of the ordering of the instructions the original tag generation serves as a sort of visual grounding for the captioning task, reducing the amount of manual post processing required.</p>"},{"location":"cookbook/atomic_caption/#generating-structured-output","title":"Generating Structured Output","text":"<p>Now we can use our model to generate structured output based on an input image:</p> <pre><code>def img_from_url(url):\n    img_byte_stream = BytesIO(urlopen(url).read())\n    return Image.open(img_byte_stream).convert(\"RGB\")\n\nimage_url=\"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\"\nimage= img_from_url(image_url)\nresult = image_data_generator(\n    pixtral_instruction,\n    [image]\n)\nprint(result)\n</code></pre> <p>This code loads an image from a URL, passes it to our vision-language model along with the instruction prompt, and generates a structured output based on the defined schema. We end up with an output like this, ready to be used for the next stage in your pipeline:</p> <pre><code>{'tags_list': [{'tag': 'astronaut',\n   'category': &lt;TagType.ENTITY: 'Entity'&gt;,\n   'confidence': 0.99},\n  {'tag': 'moon', 'category': &lt;TagType.ENTITY: 'Entity'&gt;, 'confidence': 0.98},\n  {'tag': 'space suit',\n   'category': &lt;TagType.ATTRIBUTE: 'Attribute'&gt;,\n   'confidence': 0.97},\n  {'tag': 'lunar module',\n   'category': &lt;TagType.ENTITY: 'Entity'&gt;,\n   'confidence': 0.95},\n  {'tag': 'shadow of astronaut',\n   'category': &lt;TagType.COMPOSITION: 'Composition'&gt;,\n   'confidence': 0.95},\n  {'tag': 'footprints in moon dust',\n   'category': &lt;TagType.CONTEXTUAL: 'Contextual'&gt;,\n   'confidence': 0.93},\n  {'tag': 'low angle shot',\n   'category': &lt;TagType.TECHNICAL: 'Technical'&gt;,\n   'confidence': 0.92},\n  {'tag': 'human first steps on the moon',\n   'category': &lt;TagType.SEMANTIC: 'Semantic'&gt;,\n   'confidence': 0.95}],\n 'short_caption': 'First man on the Moon',\n 'dense_caption': \"The figure clad in a pristine white space suit, emblazoned with the American flag, stands powerfully on the moon's desolate and rocky surface. The lunar module, a workhorse of space engineering, looms in the background, its metallic legs sinking slightly into the dust where footprints and tracks from the mission's journey are clearly visible. The photograph captures the astronaut from a low angle, emphasizing his imposing presence against the desolate lunar backdrop. The stark contrast between the blacks and whiteslicks of lost light and shadow adds dramatic depth to this seminal moment in human achievement.\"}\n</code></pre>"},{"location":"cookbook/atomic_caption/#conclusion","title":"Conclusion","text":"<p>The transformers_vision module in Outlines provides a powerful way to work with vision-language models. It allows for structured generation of outputs that combine image analysis with natural language processing, opening up possibilities for complex tasks like detailed image captioning, visual question answering, and more.</p> <p>By leveraging the capabilities of models like Pixtral-12B and the structured output generation of Outlines, you can create sophisticated applications that understand and describe visual content in a highly structured and customizable manner.</p>"},{"location":"cookbook/chain_of_density/","title":"Summarize documents using Chain of Density prompting","text":"<p>A good summary should be informative, concise and clear. While large language models are generally good at summarizing documents, their summaries tend to be long and contain redundant information; their information density tends to be on the lower end. This is where chain of Density, a new prompting technique, comes in. In this example we will show how one can implement chain of density with a few lines of code using Outlines, leveraging both Outline's prompt templating and its structured generation capabilities.</p> <p>The article we will try to summarize is the first three paragraphs of the Alan Turing page on Wikipedia:</p> <pre><code>article = \"\"\"\nAlan Mathison Turing OBE FRS (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] He is widely considered to be the father of theoretical computer science and artificial intelligence.[9]\n\nBorn in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes\u2013no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.[10][11]\n\nAfter the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.[13]\n\"\"\"\n</code></pre>"},{"location":"cookbook/chain_of_density/#how-chain-of-density-works","title":"How Chain Of Density works","text":"<p>Chain Of Density starts with asking the model to generate a first long and non-specific summary. Then it asks the model to generate 4 extra summaries by proceeding in the following way:</p> <ol> <li>Identify 1-3 entities missing in the previous summary;</li> <li>Add all entities marked as missing in the previous step, while not dropping entities;</li> <li>Make the summary more concise;</li> </ol> <p>The prompt also asks the model to return a list of JSON objects that contain the missing entities and the new summary. This is where structured generation will come in handy :) The paper provides the prompt and an example:</p> <p></p> <p>We can now implement the prompt provided in the paper:</p> <pre><code>from outlines import Template\n\n\nchain_of_density = Template.from_string(\n    \"\"\"Article: {{ article }}\n\n    You will generate increasingly concise, entity-dense summaries of the above Article.\n\n    Repeat the following 2 steps 5 times.\n\n    Step 1. Identify 1-3 informative Entities (\"; \" delimited) from the Article which are missing from the previously generated summary.\n    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n\n    A Missing Entity is:\n    - Relevant: to the main story.\n    - Specific: descriptive yet concise (5 words or fewer).\n    - Novel: not in the previous summary.\n    - Faithful: present in the Article.\n    - Anywhere: located anywhere in the Article.\n\n    Guidelines:\n    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n    - Missing entities can appear anywhere in the new summary.\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\n    Remember, use the exact same number of words for each summary.\n\n    Answer in JSON. The JSON should be a a dictionary with key \"summaries\" that contains a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n    \"\"\"\n)\n</code></pre> Note <p>Note that we modified the prompt slightly so it returns a JSON object that contains the summaries, instead of a list of summaries.</p>"},{"location":"cookbook/chain_of_density/#outlines-implementation","title":"Outlines implementation","text":"<p>We will use Outline's JSON-structured generation to ensure that the model's output is consistent with the format specified in the prompt. We start with defining the JSON objects that the model is asked to return using Pydantic. One JSON object that contains a list of <code>Summary</code> objects that contain the missing entities and new summary:</p> <pre><code>from pydantic import BaseModel, conlist\n\nclass Summary(BaseModel):\n    missing_entities: str\n    denser_summary: str\n\nclass Summaries(BaseModel):\n    summaries: conlist(Summary, max_length=5, min_length=5)\n</code></pre> <p>We now generate the prompt by passing the article we want to summarize to the template. We load a quantized version of Mistral-7B using the AutoAWQ library, and then use JSON-structured generation to generate the summaries:</p> <pre><code>model = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\")\n\nprompt = chain_of_density(article)\nresult = outlines.generate.json(model, Summaries)(prompt)\n</code></pre> <p>We can now check the results:</p> <pre><code>print(result.model_dump())\n# {'summaries': [\n#     {\n#       'missing_entities': 'English mathematician, cryptanalyst, philosopher',\n#       'denser_summary': 'Alan Mathison Turing was an English mathematician, cryptanalyst, philosopher.'\n#     },\n#     {\n#       'missing_entities': '',\n#       'denser_summary': \"Alan Mathison Turing was an English mathematician who was a crucial figure in WW2's Bletchley Park codebreaking centre and designed one of the first computers.\"\n#     },\n#     {\n#       'missing_entities': 'cryptanalyst, studied, biology, father',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and contributed to mathematical biology.'\n#     },\n#     {\n#       'missing_entities': 'biology, morphogenesis, chemical',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and predicted chemical reactions in morphogenesis.\n#     '},\n#     {\n#       'missing_entities': '',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, developed computer science, and made strides in mathematical biology research.'\n#       }\n# ]}\n</code></pre> <p>Not bad, considering we used a smallish model to generate the summary! Chain of Density seems to be a very effective prompting technique to generate dense summaries, even with small quantized models. Its implementation in Outlines is also very short.</p> <p>Note that this is the first article I tried and it worked out of the box. Try it out on other articles, and please share the results on Twitter, or by opening a new discussion on the Outlines repository!</p>"},{"location":"cookbook/chain_of_thought/","title":"Chain of thought","text":"<p>Chain of thought is a prompting technique introduced in the paper \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" where throught prompting the authors generate a series of intermediate reasoning steps which improves the ability of LLMs to perform complex reasoning.</p> <p>In this guide, we use outlines to apply chain of thought through structured output.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/chain_of_thought/#chain-of-thought_1","title":"Chain of thought","text":"<p>We first define our Pydantic class for a reasoning step:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reasoning_Step(BaseModel):\n    reasoning_step: str = Field(..., description=\"Reasoning step\")\n</code></pre> <p>We then define the Pydantic class for reasoning which will consist on a list of reasoning steps and a conclusion, and we get its JSON schema:</p> <pre><code>from typing import List\n\nclass Reasoning(BaseModel):\n    reasoning: List[Reasoning_Step] = Field(..., description=\"List of reasoning steps\")\n    conclusion: str = Field(..., description=\"Conclusion\")\n\njson_schema = Reasoning.model_json_schema()\n</code></pre> <p>We could generate a response using the json schema but for a change we will use the regex:</p> <pre><code>from outlines.fsm.json_schema import convert_json_schema_to_str\nfrom outlines_core.fsm.json_schema import build_regex_from_schema\n\nschema_str = convert_json_schema_to_str(json_schema=json_schema)\nregex_str = build_regex_from_schema(schema_str)\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(user_prompt):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{json_schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + user_prompt\n        + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>For a given user prompt:</p> <pre><code>user_prompt = \"9.11 and 9.9 -- which is bigger?\"\n</code></pre> <p>we can use <code>generate.regex</code> by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>generator = generate.regex(model, regex_str)\nprompt = generate_hermes_prompt(user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain a series of intermediate reasoning steps as well as the conclusion:</p> <pre><code>import json\n\njson_response = json.loads(response)\n\nprint(json_response[\"reasoning\"])\nprint(json_response[\"conclusion\"])\n# [{'reasoning_step': 'Both 9.11 and 9.9 are decimal numbers.'},\n#  {'reasoning_step': 'When comparing decimal numbers, we look at the numbers after the decimal point.'},\n#  {'reasoning_step': 'In this case, 9.11 has the number 1 after the decimal point, while 9.9 has the number 9.'},\n#  {'reasoning_step': 'Since 1 is greater than 9, 9.11 is greater than 9.9.'}]\n# '9.11 is bigger.'\n</code></pre> <p>We notice that the 4th reasoning step is wrong ``Since 1 is greater than 9, 9.11 is greater than 9.9.'', so we should probably give the model some examples for this particular task.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/classification/","title":"Classification","text":"<p>Classification is a classic problem in NLP and finds many applications: spam detection, sentiment analysis, triaging of incoming requests, etc. We will use the example of a company that wants to sort support requests between those that require immediate attention (<code>URGENT</code>), those that can wait a little (<code>STANDARD</code>). You could easily extend the example by adding new labels.</p> <p>This tutorial shows how one can implement multi-label classification using Outlines. We will use two functionalities of the library: <code>generate.choice</code> and <code>generate.json</code>.</p> <p>As always, we start with initializing the model. Since we are GPU poor we will be using a quantized version of Mistal-7B-v0.1:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>We will use the following prompt template:</p> <pre><code>from outlines import Template\n\n\ncustomer_support = Template.from_string(\n    \"\"\"You are an experienced customer success manager.\n\n    Given a request from a client, you need to determine when the\n    request is urgent using the label \"URGENT\" or when it can wait\n    a little with the label \"STANDARD\".\n\n    # Examples\n\n    Request: \"How are you?\"\n    Label: STANDARD\n\n    Request: \"I need this fixed immediately!\"\n    Label: URGENT\n\n    # TASK\n\n    Request: {{ request }}\n    Label: \"\"\"\n)\n</code></pre>"},{"location":"cookbook/classification/#choosing-between-multiple-choices","title":"Choosing between multiple choices","text":"<p>Outlines provides a shortcut to do multi-label classification, using the <code>outlines.generate.choice</code> function to initialize a generator. Outlines uses multinomial sampling by default, here we will use the greedy sampler to get the label with the highest probability:</p> <p><pre><code>from outlines.samplers import greedy\n\ngenerator = outlines.generate.choice(model, [\"URGENT\", \"STANDARD\"], sampler=greedy())\n</code></pre> Outlines supports batched requests, so we will pass two requests to the model:</p> <pre><code>requests = [\n    \"My hair is one fire! Please help me!!!\",\n    \"Just wanted to say hi\"\n]\n\nprompts = [customer_support(request) for request in requests]\n</code></pre> <p>We can now asks the model to classify the requests:</p> <pre><code>labels = generator(prompts)\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre> <p>Now, you might be in a hurry and don't want to wait until the model finishes completion. After all, you only need to see the first letter of the response to know whether the request is urgent or standard. You can instead stream the response:</p> <pre><code>tokens = generator.stream(prompts)\nlabels = [\"URGENT\" if \"U\" in token else \"STANDARD\" for token in next(tokens)]\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre>"},{"location":"cookbook/classification/#using-json-structured-generation","title":"Using JSON-structured generation","text":"<p>Another (convoluted) way to do multi-label classification is to JSON-structured generation in Outlines. We first need to define our Pydantic schema that contains the labels:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\n\nclass Label(str, Enum):\n    urgent = \"URGENT\"\n    standard = \"STANDARD\"\n\n\nclass Classification(BaseModel):\n    label: Label\n</code></pre> <p>and we can use <code>generate.json</code> by passing this Pydantic model we just defined, and call the generator:</p> <pre><code>generator = outlines.generate.json(model, Classification, sampler=greedy())\nlabels = generator(prompts)\nprint(labels)\n# [Classification(label=&lt;Label.urgent: 'URGENT'&gt;), Classification(label=&lt;Label.standard: 'STANDARD'&gt;)]\n</code></pre>"},{"location":"cookbook/dating_profiles/","title":"Generate a synthetic dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>from dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines\n</code></pre>"},{"location":"cookbook/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"cookbook/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>from outlines import Template\n\ndating_profile_prompt = Template.from_string(\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n)\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"cookbook/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>config = transformers.AutoConfig.from_pretrained(\n    \"mosaicml/mpt-7b-8k-instruct\", trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel = outlines.models.transformers(\n    model_name=\"mosaicml/mpt-7b-8k-instruct\",\n    device=\"cuda\",\n    model_kwargs={\n        \"config\": config,\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": {\"\": 0},\n    },\n)\n</code></pre>"},{"location":"cookbook/dating_profiles/#json-structured-generation-of-profiles","title":"JSON-structured generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(new_description, samples)\nprofile = outlines.generate.json(model, DatingProfile)(prompt)\nparsed_profile = DatingProfile.model_validate_json(profile)\n</code></pre>"},{"location":"cookbook/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/","title":"Run Outlines using BentoML","text":"<p>BentoML is an open-source model serving library for building performant and scalable AI applications with Python. It comes with tools that you need for serving optimization, model packaging, and production deployment.</p> <p>In this guide, we will show you how to use BentoML to run programs written with Outlines on GPU locally and in BentoCloud, an AI Inference Platform for enterprise AI teams. The example source code in this guide is also available in the examples/bentoml/ directory.</p>"},{"location":"cookbook/deploy-using-bentoml/#import-a-model","title":"Import a model","text":"<p>First we need to download an LLM (Mistral-7B-v0.1 in this example and you can use any other LLM) and import the model into BentoML's Model Store. Let's install BentoML and other dependencies from PyPi (preferably in a virtual environment):</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Then save the code snippet below as <code>import_model.py</code> and run <code>python import_model.py</code>.</p> <p>Note: You need to accept related conditions on Hugging Face first to gain access to Mistral-7B-v0.1.</p> <pre><code>import bentoml\n\nMODEL_ID = \"mistralai/Mistral-7B-v0.1\"\nBENTO_MODEL_TAG = MODEL_ID.lower().replace(\"/\", \"--\")\n\ndef import_model(model_id, bento_model_tag):\n\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n\n    with bentoml.models.create(bento_model_tag) as bento_model_ref:\n        tokenizer.save_pretrained(bento_model_ref.path)\n        model.save_pretrained(bento_model_ref.path)\n\n\nif __name__ == \"__main__\":\n    import_model(MODEL_ID, BENTO_MODEL_TAG)\n</code></pre> <p>You can verify the download is successful by running:</p> <pre><code>$ bentoml models list\n\nTag                                          Module  Size        Creation Time\nmistralai--mistral-7b-v0.1:m7lmf5ac2cmubnnz          13.49 GiB   2024-04-25 06:52:39\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/#define-a-bentoml-service","title":"Define a BentoML Service","text":"<p>As the model is ready, we can define a BentoML Service to wrap the capabilities of the model.</p> <p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code>DEFAULT_SCHEMA = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>First, we need to define a BentoML service by decorating an ordinary class (<code>Outlines</code> here) with <code>@bentoml.service</code> decorator. We pass to this decorator some configuration and GPU on which we want this service to run in BentoCloud (here an L4 with 24GB memory):</p> <pre><code>import typing as t\nimport bentoml\n\nfrom import_model import BENTO_MODEL_TAG\n\n@bentoml.service(\n    traffic={\n        \"timeout\": 300,\n    },\n    resources={\n        \"gpu\": 1,\n        \"gpu_type\": \"nvidia-l4\",\n    },\n)\nclass Outlines:\n\n    bento_model_ref = bentoml.models.get(BENTO_MODEL_TAG)\n\n    def __init__(self) -&gt; None:\n\n        import outlines\n        import torch\n        self.model = outlines.models.transformers(\n            self.bento_model_ref.path,\n            device=\"cuda\",\n            model_kwargs={\"torch_dtype\": torch.float16},\n        )\n\n    ...\n</code></pre> <p>We then need to define an HTTP endpoint using <code>@bentoml.api</code> to decorate the method <code>generate</code> of <code>Outlines</code> class:</p> <pre><code>    ...\n\n    @bentoml.api\n    async def generate(\n        self,\n        prompt: str = \"Give me a character description.\",\n        json_schema: t.Optional[str] = DEFAULT_SCHEMA,\n    ) -&gt; t.Dict[str, t.Any]:\n\n        import outlines\n\n        generator = outlines.generate.json(self.model, json_schema)\n        character = generator(prompt)\n\n        return character\n</code></pre> <p>Here <code>@bentoml.api</code> decorator defines <code>generate</code> as an HTTP endpoint that accepts a JSON request body with two fields: <code>prompt</code> and <code>json_schema</code> (optional, which allows HTTP clients to provide their own JSON schema). The type hints in the function signature will be used to validate incoming JSON requests. You can define as many HTTP endpoints as you want by using <code>@bentoml.api</code> to decorate other methods of <code>Outlines</code> class.</p> <p>Now you can save the above code to <code>service.py</code> (or use this implementation), and run the code using the BentoML CLI.</p>"},{"location":"cookbook/deploy-using-bentoml/#run-locally-for-testing-and-debugging","title":"Run locally for testing and debugging","text":"<p>Then you can run a server locally by:</p> <pre><code>bentoml serve .\n</code></pre> <p>The server is now active at http://localhost:3000. You can interact with it using the Swagger UI or in other different ways:</p> CURL <pre><code>curl -X 'POST' \\\n  'http://localhost:3000/generate' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"prompt\": \"Give me a character description.\"\n}'\n</code></pre> Python client <pre><code>import bentoml\n\nwith bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n    response = client.generate(\n        prompt=\"Give me a character description\"\n    )\n    print(response)\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"name\": \"Aura\",\n  \"age\": 15,\n  \"armor\": \"plate\",\n  \"weapon\": \"sword\",\n  \"strength\": 20\n}\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/#deploy-to-bentocloud","title":"Deploy to BentoCloud","text":"<p>After the Service is ready, you can deploy it to BentoCloud for better management and scalability. Sign up if you haven't got a BentoCloud account.</p> <p>Make sure you have logged in to BentoCloud, then run the following command to deploy it.</p> <pre><code>bentoml deploy .\n</code></pre> <p>Once the application is up and running on BentoCloud, you can access it via the exposed URL.</p> <p>Note: For custom deployment in your own infrastructure, use BentoML to generate an OCI-compliant image.</p>"},{"location":"cookbook/deploy-using-cerebrium/","title":"Run Outlines using Cerebrium","text":"<p>Cerebrium is a serverless AI infrastructure platform that makes it easier for companies to build and deploy AI based applications. They offer Serverless GPU's\u00a0with low cold start times with over 12 varieties of GPU chips that auto scale and you only pay for the compute you use.</p> <p>In this guide we will show you how you can use Cerebrium to run programs written with Outlines on GPUs in the cloud.</p>"},{"location":"cookbook/deploy-using-cerebrium/#setup-cerebrium","title":"Setup Cerebrium","text":"<p>First, we install Cerebrium and login to get authenticated.</p> <pre><code>pip install cerebrium\ncerebrium login\n</code></pre> <p>Then let us create our first project</p> <pre><code>cerebrium init outlines-project\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#setup-environment-and-hardware","title":"Setup Environment and Hardware","text":"<p>You set up your environment and hardware in the cerebrium.toml file that was created using the init function above.</p> <pre><code>[cerebrium.deployment]\ndocker_base_image_url = \"nvidia/cuda:12.1.1-runtime-ubuntu22.04\"\n\n[cerebrium.hardware]\ncpu = 2\nmemory = 14.0\ngpu = \"AMPERE A10\"\ngpu_count = 1\nprovider = \"aws\"\nregion = \"us-east-1\"\n\n[cerebrium.dependencies.pip]\noutline = \"==0.0.37\"\ntransformers = \"==4.38.2\"\ndatasets = \"==2.18.0\"\naccelerate = \"==0.27.2\"\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#setup-inference","title":"Setup inference","text":"<p>Running code in Cerebrium is like writing normal python with no special syntax. In a <code>main.py</code> file specify the following:</p> <pre><code>import outlines\n\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n\ngenerator = outlines.generate.json(model, schema)\n</code></pre> <p>On first deploy, it will download the model and store it on disk therefore for subsequent calls it will load the model from disk.</p> <p>Every function in Cerebrium is callable through an API endpoint. Code at the top most layer (ie: not in a function) is instantiated only when the container is spun up the first time so for subsequent calls, it will simply run the code defined in the function you call.</p> <p>To deploy an API that creates a new character when called with a prompt you can add the following code to <code>main.py</code>:</p> <pre><code>def generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    return character\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#run-on-the-cloud","title":"Run on the cloud","text":"<pre><code>cerebrium deploy\n</code></pre> <p>You will see your application deploy, install pip packages and download the model. Once completed it will output a CURL request you can use to call your endpoint. Just remember to end the url with the function you would like to call - in this case /generate. You should see your response returned!</p>"},{"location":"cookbook/deploy-using-modal/","title":"Run Outlines using Modal","text":"<p>Modal is a serverless platform that allows you to easily run code on the cloud, including GPUs. It can come very handy for those of us who don't have a monster GPU at home and want to be able to quickly and easily provision, configure and orchestrate cloud infrastructure.</p> <p>In this guide we will show you how you can use Modal to run programs written with Outlines on GPU in the cloud.</p>"},{"location":"cookbook/deploy-using-modal/#requirements","title":"Requirements","text":"<p>We recommend installing <code>modal</code> and <code>outlines</code> in a virtual environment. You can create one with:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Then install the required packages:</p> <pre><code>pip install modal outlines\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#build-the-image","title":"Build the image","text":"<p>First we need to define our container image. If you need to access a gated model, you will need to provide an access token. See the <code>.env</code> call below for how to provide a HuggingFace token.</p> <p>Setting a token is best done by setting an environment variable <code>HF_TOKEN</code> with your token. If you do not wish to do this, we provide a commented-out line in the code to set the token directly in the code.</p> <pre><code>from modal import Image, App, gpu\nimport os\n\n# This creates a modal App object. Here we set the name to \"outlines-app\".\n# There are other optional parameters like modal secrets, schedules, etc.\n# See the documentation here: https://modal.com/docs/reference/modal.App\napp = App(name=\"outlines-app\")\n\n# Specify a language model to use.\n# Another good model to use is \"NousResearch/Hermes-2-Pro-Mistral-7B\"\nlanguage_model = \"mistral-community/Mistral-7B-v0.2\"\n\n# Please set an environment variable HF_TOKEN with your Hugging Face API token.\n# The code below (the .env({...}) part) will copy the token from your local\n# environment to the container.\n# More info on Image here: https://modal.com/docs/reference/modal.Image\noutlines_image = Image.debian_slim(python_version=\"3.11\").pip_install(\n    \"outlines\",\n    \"transformers\",\n    \"datasets\",\n    \"accelerate\",\n    \"sentencepiece\",\n).env({\n    # This will pull in your HF_TOKEN environment variable if you have one.\n    'HF_TOKEN':os.environ['HF_TOKEN']\n\n    # To set the token directly in the code, uncomment the line below and replace\n    # 'YOUR_TOKEN' with the HuggingFace access token.\n    # 'HF_TOKEN':'YOUR_TOKEN'\n})\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#setting-the-container-up","title":"Setting the container up","text":"<p>When running longer Modal apps, it's recommended to download your language model when the container starts, rather than when the function is called. This will cache the model for future runs.</p> <pre><code># This function imports the model from Hugging Face. The modal container\n# will call this function when it starts up. This is useful for\n# downloading models, setting up environment variables, etc.\ndef import_model():\n    import outlines\n    outlines.models.transformers(language_model)\n\n# This line tells the container to run the import_model function when it starts.\noutlines_image = outlines_image.run_function(import_model)\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#define-a-schema","title":"Define a schema","text":"<p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code># Specify a schema for the character description. In this case,\n# we want to generate a character with a name, age, armor, weapon, and strength.\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>To make the inference work on Modal we need to wrap the corresponding function in a <code>@app.function</code> decorator. We pass to this decorator the image and GPU on which we want this function to run.</p> <p>Let's choose an A100 with 80GB memory. Valid GPUs can be found here.</p> <pre><code># Define a function that uses the image we chose, and specify the GPU\n# and memory we want to use.\n@app.function(image=outlines_image, gpu=gpu.A100(size='80GB'))\ndef generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # Remember, this function is being executed in the container,\n    # so we need to import the necessary libraries here. You should\n    # do this with any other libraries you might need.\n    import outlines\n\n    # Load the model into memory. The import_model function above\n    # should have already downloaded the model, so this call\n    # only loads the model into GPU memory.\n    model = outlines.models.transformers(\n        language_model, device=\"cuda\"\n    )\n\n    # Generate a character description based on the prompt.\n    # We use the .json generation method -- we provide the\n    # - model: the model we loaded above\n    # - schema: the JSON schema we defined above\n    generator = outlines.generate.json(model, schema)\n\n    # Make sure you wrap your prompt in instruction tags ([INST] and [/INST])\n    # to indicate that the prompt is an instruction. Instruction tags can vary\n    # by models, so make sure to check the model's documentation.\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    # Print out the generated character.\n    print(character)\n</code></pre> <p>We then need to define a <code>local_entrypoint</code> to call our function <code>generate</code> remotely.</p> <pre><code>@app.local_entrypoint()\ndef main(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # We use the \"generate\" function defined above -- note too that we are calling\n    # .remote() on the function. This tells modal to run the function in our cloud\n    # machine. If you want to run the function locally, you can call .local() instead,\n    # though this will require additional setup.\n    generate.remote(prompt)\n</code></pre> <p>Here <code>@app.local_entrypoint()</code> decorator defines <code>main</code> as the function to start from locally when using the Modal CLI. You can save above code to <code>example.py</code> (or use this implementation). Let's now see how to run the code on the cloud using the Modal CLI.</p>"},{"location":"cookbook/deploy-using-modal/#run-on-the-cloud","title":"Run on the cloud","text":"<p>First install the Modal client from PyPi, if you have not already:</p> <pre><code>pip install modal\n</code></pre> <p>You then need to obtain a token from Modal. Run the following command:</p> <pre><code>modal setup\n</code></pre> <p>Once that is set you can run inference on the cloud using:</p> <pre><code>modal run example.py\n</code></pre> <p>You should see the Modal app initialize, and soon after see the result of the <code>print</code> function in your terminal. That's it!</p>"},{"location":"cookbook/earnings-reports/","title":"Extracting financial data from earnings reports","text":"<p>A common task in finance is to extract financial data from earnings reports. Earnings reports are infamously poorly formatted, as the SEC does not have requirements for producing machine-readable documents.</p> <p>Earnings reports are often provided as HTML documents, which can be difficult to parse. Investors often use complicated parsing systems or manual review to extract data. Entire companies are built around automating this task.</p> <p>This cookbook is a proof of concept about how we can use LLMs to extract financial data directly into CSV. Comma-separated values are well-structured and can be defined by a regular expression, which Outlines can use to guide the LLM's output.</p> <p>The example is a smaller subset of a full demo found here. The demo contains the full set of pre-processing steps needed to convert raw HTML into a structured CSV file, and tests the results across three company's 10k reports.</p>"},{"location":"cookbook/earnings-reports/#setup","title":"Setup","text":"<p>Install outlines and required dependencies:</p> <pre><code># Later versions of torch can have difficulty with certain CUDA drivers.\n# We recommend using 2.4.0 for now, but you may wish to experiment with\n# other versions.\npip install outlines pandas transformers torch==2.4.0 accelerate\n</code></pre>"},{"location":"cookbook/earnings-reports/#load-the-model","title":"Load the model","text":"<p>Choose your language model. We'll use Phi-3 mini, which is small enough to run on reasonably small machines.</p> <pre><code>import outlines\nimport torch\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\nmodel = outlines.models.transformers(\n    model_name,\n    device='auto',\n    model_kwargs={\n        # To reduce memory usage, we'll use bfloat16\n        \"torch_dtype\": torch.bfloat16,\n    },\n)\n</code></pre>"},{"location":"cookbook/earnings-reports/#set-up-the-data","title":"Set up the data","text":"<p>For brevity, we've attached the markdown version of Nvidia's 10k report. The full demonstration processes the raw HTML version of the report to these markdown tables. Pages are filtered by whether they seem to contain income statements, and then compacted into the string you see below.</p> <pre><code>income_statement = \"\"\"\nTable of ContentsNVIDIA Corporation and SubsidiariesConsolidated Statements of Income(In millions, except per share data)\n\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | | | Year Ended | | | | | | | | | | | | | | |\n|  | | | Jan 28, 2024 | | |  | | | Jan 29, 2023 | | |  | | | Jan 30, 2022 | | |\n| Revenue | | | $ | 60,922 |  |  | | | $ | 26,974 |  |  | | | $ | 26,914 |  |\n| Cost of revenue | | | 16,621 | |  |  | | | 11,618 | |  |  | | | 9,439 | |  |\n| Gross profit | | | 44,301 | |  |  | | | 15,356 | |  |  | | | 17,475 | |  |\n| Operating expenses | | |  | | |  | | |  | | |  | | |  | | |\n| Research and development | | | 8,675 | |  |  | | | 7,339 | |  |  | | | 5,268 | |  |\n| Sales, general and administrative | | | 2,654 | |  |  | | | 2,440 | |  |  | | | 2,166 | |  |\n| Acquisition termination cost | | | \u0097 | |  |  | | | 1,353 | |  |  | | | \u0097 | |  |\n| Total operating expenses | | | 11,329 | |  |  | | | 11,132 | |  |  | | | 7,434 | |  |\n| Operating income | | | 32,972 | |  |  | | | 4,224 | |  |  | | | 10,041 | |  |\n| Interest income | | | 866 | |  |  | | | 267 | |  |  | | | 29 | |  |\n| Interest expense | | | (257) | |  |  | | | (262) | |  |  | | | (236) | |  |\n| Other, net | | | 237 | |  |  | | | (48) | |  |  | | | 107 | |  |\n| Other income (expense), net | | | 846 | |  |  | | | (43) | |  |  | | | (100) | |  |\n| Income before income tax | | | 33,818 | |  |  | | | 4,181 | |  |  | | | 9,941 | |  |\n| Income tax expense (benefit) | | | 4,058 | |  |  | | | (187) | |  |  | | | 189 | |  |\n| Net income | | | $ | 29,760 |  |  | | | $ | 4,368 |  |  | | | $ | 9,752 |  |\n|  | | |  | | |  | | |  | | |  | | |  | | |\n| Net income per share: | | |  | | |  | | |  | | |  | | |  | | |\n| Basic | | | $ | 12\\.05 |  |  | | | $ | 1\\.76 |  |  | | | $ | 3\\.91 |  |\n| Diluted | | | $ | 11\\.93 |  |  | | | $ | 1\\.74 |  |  | | | $ | 3\\.85 |  |\n|  | | |  | | |  | | |  | | |  | | |  | | |\n| Weighted average shares used in per share computation: | | |  | | |  | | |  | | |  | | |  | | |\n| Basic | | | 2,469 | |  |  | | | 2,487 | |  |  | | | 2,496 | |  |\n| Diluted | | | 2,494 | |  |  | | | 2,507 | |  |  | | | 2,535 | |  |\n\"\"\"\n</code></pre> <p>The markdown tables extracted from the earnings reports can vary widely in row names, column counts, data types, etc. The advantage of LLMs here is that we can define the data we want in terms of the data types, and the LLM will output the data in the desired format.</p> <p>For comparison, here is how the income statement looks in the original HTML:</p> <p></p>"},{"location":"cookbook/earnings-reports/#define-the-data-we-want","title":"Define the data we want","text":"<p>Outlines is often used for JSON output, but it can also be used for CSV. We know the columns we want to extract, and we know the data types of the columns. Year for example is always a four-digit number, revenue is a number with commas, and so on.</p> <p>We can define a regex pattern for each column type:</p> <pre><code># Define the column type regex patterns\ncolumn_types = {\n    # Year is always a four-digit number\n    \"year\": r\"\\d{4}\",\n\n    # Revenue, operating income, and net income are always numbers with commas.\n    # This regex permits integers that may begin with a minus sign, and may have\n    # commas separating the thousands, millions, etc.\n    \"integer_comma\": r\"((-?\\d+),?\\d+|(-?\\d+))\",\n    # Number is currently not used, but it represents a number with up to two decimal places.\n    \"number\": r\"(-?\\d+(?:\\.\\d{1,2})?)\",\n}\n</code></pre> <p>Next, let's choose the columns we want to extract. We want</p> <ul> <li>Year, always a four-digit number</li> <li>Revenue, a number with commas</li> <li>Operating income, a number with commas</li> <li>Net income, a number with commas</li> </ul> <pre><code># Define the columns to extract, and their data types.\ncolumns_to_extract = {\n    \"year\": \"year\",\n    \"revenue\": \"integer_comma\",\n    \"operating_income\": \"integer_comma\",\n    \"net_income\": \"integer_comma\",\n}\n</code></pre> <p>You can modify <code>column_type_regex</code> to match the data types of the columns you want to extract.  Adding a new financial metric to extract is as simple as adding a new key/value pair to <code>columns_to_extract</code>:</p> <pre><code>columns_to_extract[\"diluted_earnings_per_share\"] = \"number\"\n</code></pre> <p>Additional columns are not well tested for accuracy, so use with caution.</p>"},{"location":"cookbook/earnings-reports/#create-the-regex-describing-the-data-we-want","title":"Create the regex describing the data we want","text":"<pre><code># Create the header line. This is the requested column names\n# separated by commas, i.e. \"year,revenue,...\"\nheader = \",\".join(columns_to_extract.keys())\n\n# Create the data capture patterns. These are the regex patterns\n# that will be used to capture the data in each column\ndata_patterns = [column_types[dtype] for dtype in columns_to_extract.values()]\ndata_line = \",\".join(data_patterns)\n\n# Our final regex pattern.\nmax_rows = 3 # We expect 3 rows of data, firms usually report 3 years of income statements\ncsv_regex = f\"{header}(\\n{data_line}){{,{max_rows}}}\\n\\n\"\n\nprint(csv_regex)\n</code></pre> <p>which gives us</p> <pre><code>year,revenue,operating_income,net_income,basic_earnings_per_share(\n\\d{4},((-?\\d+),?\\d+|(-?\\d+)),((-?\\d+),?\\d+|(-?\\d+)),((-?\\d+),?\\d+|(-?\\d+)),(-?\\d+(?:\\.\\d{1,2})?)){,3}\n</code></pre> <p>Pretty hairy, right? Thankfully, we have a simple function to construct this regex for you. The regex defines a header line, followed by a data line that repeats for each row of data we want to extract. Passing the regex to <code>outlines.generate.regex</code> will produce a function that will always produce a CSV string that is consistent with the regex.</p>"},{"location":"cookbook/earnings-reports/#prompting-the-model","title":"Prompting the model","text":"<p>Outlines does not add system or instruction tokens by default, so we need to use <code>transformers.AutoTokenizer</code> to add them for whatever model we're using.</p> <p><pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef add_instruction(prompt):\n    return tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n\nprint(add_instruction(\"Howdy\"))\n</code></pre> <pre><code>&lt;|user|&gt;\nHowdy&lt;|end|&gt;\n&lt;|assistant|&gt;\n</code></pre></p> <p>Our prompt roughly describes the task we want the model to perform, and a few pieces of information it may need to know about income statements.</p> <pre><code>def extract_financial_data_prompt(columns_to_extract, income_statement):\n    user_prompt = f\"\"\"\n    Extract annual financial data from this set of pages. Pages\n    are from a 10k filing and were chosen because they may contain\n    a comprehensive income statement. Note that selected pages may\n    be incorrectly extracted, so you should verify that you are extracting\n    from the comprehensive income statement and not some other financial\n    statement.\n\n    Create a row for each year available in the income statement with the\n    following columns: {', '.join(columns_to_extract.keys())}. Firms typically report the\n    most recent 3 years of data, but this can vary.\n\n    Each column has types: {', '.join(columns_to_extract.values())}.\n\n    # Relevant pages:\n\n    {income_statement}\n\n    # Key instructions:\n\n    1. Look ONLY at the \"Consolidated Statements of Income\" table\n    2. For operating income, look for \"Income from operations\" or \"Operating income\"\n    3. For net income, use the TOTAL net income figure, not amounts allocated to specific share classes\n    4. Use NULL for missing values\n    5. Operating income must be less than revenue\n    6. Net income must be less than operating income\n    7. Ignore segment breakdowns, quarterly data, or per-share amounts\n\n    # Output format:\n\n    - CSV format with headers: {','.join(columns_to_extract.keys())}\n    - Use NULL for missing values\n    - If no data are found, do not create a row.\n    - Enter two newline characters to terminate the CSV when no more data are found.\n\n    # Definitions:\n    - Revenue: Total sales of goods and services. Usually this is at the top of the\n    income statement.\n    - Operating income: Revenue minus operating expenses for the entire company. This is revenue\n    minus costs. Operating income is also called operating profit, EBIT, or income from\n    operations.\n    - Net income: Operating income minus taxes. This is the bottom line of the\n    income statement.\n    \"\"\"\n\n    return add_instruction(user_prompt)\n</code></pre>"},{"location":"cookbook/earnings-reports/#running-the-model","title":"Running the model","text":"<p>Now that we have our prompt and regular expression, we can run the model.</p> <p>Construct our regex extractor function. We'll use a greedy sampler, which samples the most likely next token at each step. It's a simple sampler that is more reproducible than multinomial sampling.</p> <pre><code>csv_extractor = outlines.generate.regex(\n    model, csv_regex, sampler=outlines.samplers.greedy()\n)\n</code></pre> <p>Provide the prompt to the model and run it:</p> <p><pre><code>csv_data = csv_extractor(\n    extract_financial_data_prompt(columns_to_extract, income_statement),\n    max_tokens=1024,\n)\n\nprint(csv_data)\n</code></pre> <pre><code>year,revenue,operating_income,net_income\n2024,60922,32972,29760\n2023,26974,4224,4368\n2022,26914,10041,9752\n</code></pre></p> <p>Voila! We've extracted the financial data from the income statement, and it's correct upon inspection.</p> <p>You can even load this into a <code>pandas</code> DataFrame for further analysis:</p> <p><pre><code>import pandas as pd\nfrom io import StringIO\n\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n</code></pre> <pre><code>   year  revenue  operating_income  net_income\n0  2024    60922             32972       29760\n1  2023    26974              4224        4368\n2  2022    26914             10041        9752\n</code></pre></p>"},{"location":"cookbook/extract_event_details/","title":"Extract events details from text","text":"<p>This recipe demonstrates how to use the <code>outlines</code> library to extract structured event details from a text message. We will extract the title, location, and start date and time from messages like the following:</p> <pre><code>Hello Kitty, my grandmother will be here, I think it's better to postpone\nour appointment to review math lessons to next Monday at 2pm at the same\nplace, 3 avenue des tanneurs, one hour will be enough see you \ud83d\ude18\n</code></pre> <p>Let see how to extract the event details from the message with the MLX library dedicated to  Apple Silicon processor  (M series).</p> <pre><code>from datetime import datetime\n\nfrom pydantic import BaseModel, Field\n\nfrom outlines import generate, models\n\n# Load the model\nmodel = models.mlxlm(\"mlx-community/Hermes-3-Llama-3.1-8B-8bit\")\n\n\n# Define the event schema using Pydantic\nclass Event(BaseModel):\n    title: str = Field(description=\"title of the event\")\n    location: str\n    start: datetime = Field(\n        default=None, description=\"date of the event if available in iso format\"\n    )\n\n\n# Get the current date and time\nnow = datetime.now().strftime(\"%A %d %B %Y and it's %H:%M\")\n\n# Define the prompt\nprompt = f\"\"\"\nToday's date and time are {now}\nGiven a user message, extract information of the event like date and time in iso format, location and title.\nIf the given date is relative, think step by step to find the right date.\nHere is the message:\n\"\"\"\n\n# Sample message\nmessage = \"\"\"Hello Kitty, my grandmother will be here , I think it's better to postpone our\nappointment to review math lessons to next Friday at 2pm at the same place, 3 avenue des tanneurs, I think that one hour will be enough\nsee you \ud83d\ude18 \"\"\"\n\n# Create the generator\ngenerator = generate.json(model, Event)\n\n# Extract the event information\nevent = generator(prompt + message)\n\n# Print the current date and time\nprint(f\"Today: {now}\")\n\n# Print the extracted event information in JSON format\nprint(event.json())\n</code></pre> <p>The output will be:</p> <pre><code>Today: Saturday 16 November 2024 and it's 10:55\n</code></pre> <p>and the extracted event information will be:</p> <pre><code>{\n  \"title\":\"Math Review\",\n  \"location\":\"3 avenue des tanneurs\",\n  \"start\":\"2024-11-22T14:00:00Z\"\n}\n</code></pre> <p>To find out more about this use case, we recommend the project developped by Joseph Rudoler the ICS Generator</p>"},{"location":"cookbook/extraction/","title":"Named entity extraction","text":"<p>Named Entity Extraction is a fundamental problem in NLP. It involves identifying and categorizing named entities within a document: people, organization, dates, places, etc. It is usually the first step in a more complex NLP worklow. Here we will use the example of a pizza restaurant that receives orders via their website and need to identify the number and types of pizzas that are being ordered.</p> <p>Getting LLMs to output the extracted entities in a structured format can be challenging. In this tutorial we will see how we can use Outlines' JSON-structured generation to extract entities from a document and return them in a valid JSON data structure 100% of the time.</p> <p>As always, we start with initializing the model. We will be using a quantized version of Mistal-7B-v0.1 (we're GPU poor):</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>And we will be using the following prompt template:</p> <pre><code>from outlines import Template\n\n\ntake_order = Template.from_string(\n    \"\"\"You are the owner of a pizza parlor. Customers \\\n    send you orders from which you need to extract:\n\n    1. The pizza that is ordered\n    2. The number of pizzas\n\n    # EXAMPLE\n\n    ORDER: I would like one Margherita pizza\n    RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n\n    # OUTPUT INSTRUCTIONS\n\n    Answer in valid JSON. Here are the different objects relevant for the output:\n\n    Order:\n        pizza (str): name of the pizza\n        number (int): number of pizzas\n\n    Return a valid JSON of type \"Order\"\n\n    # OUTPUT\n\n    ORDER: {{ order }}\n    RESULT: \"\"\"\n)\n</code></pre> <p>We now define our data model using Pydantic:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Pizza(str, Enum):\n    margherita = \"Margherita\"\n    pepperonni = \"Pepperoni\"\n    calzone = \"Calzone\"\n\nclass Order(BaseModel):\n    pizza: Pizza\n    number: int\n</code></pre> <p>We can now define our generator and call it on several incoming orders:</p> <pre><code>orders = [\n    \"Hi! I would like to order two pepperonni pizzas and would like them in 30mins.\",\n    \"Is it possible to get 12 margheritas?\"\n]\nprompts = [take_order(order) for order in orders]\n\ngenerator = outlines.generate.json(model, Order)\n\nresults = generator(prompts)\nprint(results)\n# [Order(pizza=&lt;Pizza.pepperonni: 'Pepperoni'&gt;, number=2),\n#  Order(pizza=&lt;Pizza.margherita: 'Margherita'&gt;, number=12)]\n</code></pre> <p>There are several ways you could improve this example:</p> <ul> <li>Clients may order several types of pizzas.</li> <li>Clients may order drinks as well.</li> <li>If the pizza place has a delivery service we need to extract the client's address and phone number</li> <li>Clients may specify the time for which they want the pizza. We could then check against a queuing system and reply to them with the estimated delivery time.</li> </ul> <p>How would you change the Pydantic model to account for these use cases?</p>"},{"location":"cookbook/knowledge_graph_extraction/","title":"Knowledge Graph Extraction","text":"<p>In this guide, we use outlines to extract a knowledge graph from unstructured text.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/knowledge_graph_extraction/#knowledge-graph-extraction_1","title":"Knowledge Graph Extraction","text":"<p>We first need to define our Pydantic class for each node and each edge of the knowledge graph:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Node(BaseModel):\n    \"\"\"Node of the Knowledge Graph\"\"\"\n\n    id: int = Field(..., description=\"Unique identifier of the node\")\n    label: str = Field(..., description=\"Label of the node\")\n    property: str = Field(..., description=\"Property of the node\")\n\n\nclass Edge(BaseModel):\n    \"\"\"Edge of the Knowledge Graph\"\"\"\n\n    source: int = Field(..., description=\"Unique source of the edge\")\n    target: int = Field(..., description=\"Unique target of the edge\")\n    label: str = Field(..., description=\"Label of the edge\")\n    property: str = Field(..., description=\"Property of the edge\")\n</code></pre> <p>We then define the Pydantic class for the knowledge graph and get its JSON schema:</p> <pre><code>from typing import List\n\nclass KnowledgeGraph(BaseModel):\n    \"\"\"Generated Knowledge Graph\"\"\"\n\n    nodes: List[Node] = Field(..., description=\"List of nodes of the knowledge graph\")\n    edges: List[Edge] = Field(..., description=\"List of edges of the knowledge graph\")\n\nschema = KnowledgeGraph.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(user_prompt):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + user_prompt\n        + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>For a given user prompt, for example:</p> <pre><code>user_prompt = \"Alice loves Bob and she hates Charlie.\"\n</code></pre> <p>We can use <code>generate.json</code> by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>from outlines import generate, models\n\nmodel = models.LlamaCpp(llm)\ngenerator = generate.json(model, KnowledgeGraph)\nprompt = generate_hermes_prompt(user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain the nodes and edges of the knowledge graph:</p> <pre><code>print(response.nodes)\nprint(response.edges)\n# [Node(id=1, label='Alice', property='Person'),\n# Node(id=2, label='Bob', property='Person'),\n# Node(id=3, label='Charlie', property='Person')]\n# [Edge(source=1, target=2, label='love', property='Relationship'),\n# Edge(source=1, target=3, label='hate', property='Relationship')]\n</code></pre>"},{"location":"cookbook/knowledge_graph_extraction/#optional-visualizing-the-knowledge-graph","title":"(Optional) Visualizing the Knowledge Graph","text":"<p>We can use the Graphviz library to visualize the generated knowledge graph. For detailed installation instructions, see here.</p> <pre><code>from graphviz import Digraph\n\ndot = Digraph()\nfor node in response.nodes:\n    dot.node(str(node.id), node.label, shape='circle', width='1', height='1')\nfor edge in response.edges:\n    dot.edge(str(edge.source), str(edge.target), label=edge.label)\n\ndot.render('knowledge-graph.gv', view=True)\n</code></pre> <p></p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/models_playing_chess/","title":"Large language models playing chess","text":"<p>In this example we will make a Phi-2 model play chess against itself. On its own the model easily generates invalid moves, so we will give it a little help. At each step we will generate a regex that only matches valid move, and use it to help the model only generating valid moves.</p>"},{"location":"cookbook/models_playing_chess/#the-chessboard","title":"The chessboard","text":"<p>The game will be played on a standard checkboard. We will use the <code>chess</code> library to track the opponents' moves, and check that the moves are valid.</p> <pre><code>%pip install outlines -q\n%pip install chess -q\n%pip install transformers accelerate einops -q\n\nimport chess\n\nboard = chess.Board(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#the-opponents","title":"The opponents","text":"<p>Phi-2 will be playing against itself:</p> <pre><code>from outlines import models\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#a-little-help-for-the-language-model","title":"A little help for the language model","text":"<p>To make sure Phi-2 generates valid chess moves we will use Outline's regex-structured generation. We define a function that takes the current state of the board and returns a regex that matches all possible legal moves:</p> <pre><code>import re\n\ndef legal_moves_regex(board):\n    \"\"\"Build a regex that only matches valid moves.\"\"\"\n    legal_moves = list(board.legal_moves)\n    legal_modes_str = [board.san(move) for move in legal_moves]\n    legal_modes_str = [re.sub(r\"[+#]\", \"\", move) for move in legal_modes_str]\n    regex_pattern = \"|\".join(re.escape(move) for move in legal_modes_str)\n    regex_pattern = f\"{regex_pattern}\"\n    return regex_pattern\n</code></pre>"},{"location":"cookbook/models_playing_chess/#prompting-the-language-model","title":"Prompting the language model","text":"<p>The prompt corresponds to the current state of the board, so we start with:</p> <pre><code>prompt = \"Let's play Chess. Moves: \"\n</code></pre> <p>We update the prompt at each step so it reflects the state of the board after the previous move.</p>"},{"location":"cookbook/models_playing_chess/#lets-play","title":"Let's play","text":"<pre><code>from outlines import generate\n\nboard_state = \" \"\nturn_number = 0\nwhile not board.is_game_over():\n    regex_pattern = legal_moves_regex(board)\n    structured = generate.regex(model, regex_pattern)(prompt + board_state)\n    move = board.parse_san(structured)\n\n    if turn_number % 2 == 0 :  # It's White's turn\n        board_state += board.san(move) + \" \"\n    else:\n        board_state += board.san(move) + \" \" + str(turn_number) + \".\"\n\n    turn_number += 1\n\n    board.push(move)\n\n    print(board_state)\n</code></pre> <p>Interestingly enough, Phi-2 hates capturing.</p> <pre><code> e4 e5 1.Nf3 Ne7 3.b4 Nf5 5.Nc3 Ne7 7.Bb5 a6 9.Na4 b6 11.c3 Nec6 13.c4 a5 15.d4 Qg5 17.Nd2 Bb7 19.dxe5\n</code></pre> <p>This example was originally authored by @903124S in this gist.</p>"},{"location":"cookbook/qa-with-citations/","title":"Generate Synthetic Data and Q&amp;A with Citations","text":"<p>This tutorial is adapted from the instructor-ollama notebook. We start with a simple example to generate synthetic data and then we approach the problem of question answering by providing citations.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/qa-with-citations/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>We first need to define our Pydantic class for a user:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier\") # so the model keeps track of the number of users\n    first_name: str\n    last_name: str\n    age: int\n</code></pre> <p>We then define a Pydantic class for a list of users:</p> <pre><code>from typing import List\n\nclass Users(BaseModel):\n    users: List[UserDetail]\n</code></pre> <p>We can use a <code>generate.json</code> by passing this Pydantic class we just defined, and call the generator:</p> <pre><code>model = models.LlamaCpp(llm)\ngenerator = generate.json(model, Users)\nresponse = generator(\"Create 5 fake users\", max_tokens=1024, temperature=0, seed=42)\nprint(response.users)\n# [UserDetail(id=1, first_name='John', last_name='Doe', age=25),\n# UserDetail(id=2, first_name='Jane', last_name='Doe', age=30),\n# UserDetail(id=3, first_name='Bob', last_name='Smith', age=40),\n# UserDetail(id=4, first_name='Alice', last_name='Smith', age=35),\n# UserDetail(id=5, first_name='John', last_name='Smith', age=20)]\n</code></pre> <pre><code>for user in response.users:\n    print(user.first_name)\n    print(user.last_name)\n    print(user.age)\n    print(#####)\n# John\n# Doe\n# 25\n# #####\n# Jane\n# Doe\n# 30\n# #####\n# Bob\n# Smith\n# 40\n# #####\n# Alice\n# Smith\n# 35\n# #####\n# John\n# Smith\n# 20\n# #####\n</code></pre>"},{"location":"cookbook/qa-with-citations/#qa-with-citations","title":"QA with Citations","text":"<p>We first need to define our Pydantic class for QA with citations:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n    citations: List[str]\n\nschema = QuestionAnswer.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(question, context, schema=schema):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON with correct and exact citations \"\n        \"extracted from the `Context`. \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + \"`Context`: \"\n        + context\n        + \"\\n`Question`: \"\n        + question + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>We can use <code>generate.json</code> by passing the Pydantic class we previously defined, and call the generator with Hermes prompt:</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\ngenerator = generate.json(model, QuestionAnswer)\nprompt = generate_hermes_prompt(question, context)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\nprint(response)\n# QuestionAnswer(question='What did the author do during college?', answer='The author studied Computational Mathematics and physics in university and was also involved in starting the Data Science club, serving as its president for 2 years.', citations=['I went to an arts high school but in university I studied Computational Mathematics and physics.', 'I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.'])\n</code></pre> <p>We can do the same for a list of question-context pairs:</p> <pre><code>question1 = \"Where was John born?\"\ncontext1 = \"\"\"\nJohn Doe is a software engineer who was born in New York, USA.\nHe studied Computer Science at the Massachusetts Institute of Technology.\nDuring his studies, he interned at Google and Microsoft.\nHe also founded the Artificial Intelligence club at his university and served as its president for three years.\n\"\"\"\n\nquestion2 = \"What did Emily study in university?\"\ncontext2 = \"\"\"\nEmily Smith is a data scientist from London, England.\nShe attended the University of Cambridge where she studied Statistics and Machine Learning.\nShe interned at IBM and Amazon during her summer breaks.\nEmily was also the head of the Women in Tech society at her university.\n\"\"\"\n\nquestion3 = \"Which companies did Robert intern at?\"\ncontext3 = \"\"\"\nRobert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert.\nHe studied Information Systems at the University of Melbourne.\nRobert interned at several cybersecurity firms including NortonLifeLock and McAfee.\nHe was also the leader of the Cybersecurity club at his university.\n\"\"\"\n\nquestion4 = \"What club did Alice start at her university?\"\ncontext4 = \"\"\"\nAlice Williams, a native of Dublin, Ireland, is a successful web developer.\nShe studied Software Engineering at Trinity College Dublin.\nAlice interned at several tech companies including Shopify and Squarespace.\nShe started the Web Development club at her university and was its president for two years.\n\"\"\"\n\nquestion5 = \"What did Michael study in high school?\"\ncontext5 = \"\"\"\nMichael Brown is a game developer from Tokyo, Japan.\nHe attended a specialized high school where he studied Game Design.\nHe later attended the University of Tokyo where he studied Computer Science.\nMichael interned at Sony and Nintendo during his university years.\nHe also started the Game Developers club at his university.\n\"\"\"\n\nfor question, context in [\n    (question1, context1),\n    (question2, context2),\n    (question3, context3),\n    (question4, context4),\n    (question5, context5),\n]:\n    final_prompt = my_final_prompt(question, context)\n    generator = generate.json(model, QuestionAnswer)\n    response = generator(final_prompt, max_tokens=1024, temperature=0, seed=42)\n    display(question)\n    display(response.answer)\n    display(response.citations)\n    print(\"\\n\\n\")\n\n# 'Where was John born?'\n# 'John Doe was born in New York, USA.'\n# ['John Doe is a software engineer who was born in New York, USA.']\n#\n#\n# 'What did Emily study in university?'\n# 'Emily studied Statistics and Machine Learning in university.'\n# ['She attended the University of Cambridge where she studied Statistics and Machine Learning.']\n#\n#\n# 'Which companies did Robert intern at?'\n# 'Robert interned at NortonLifeLock and McAfee.'\n# ['Robert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert. He interned at several cybersecurity firms including NortonLifeLock and McAfee.']\n#\n#\n# 'What club did Alice start at her university?'\n# 'Alice started the Web Development club at her university.'\n# ['Alice Williams, a native of Dublin, Ireland, is a successful web developer. She started the Web Development club at her university and was its president for two years.']\n#\n#\n# 'What did Michael study in high school?'\n# 'Michael studied Game Design in high school.'\n# ['Michael Brown is a game developer from Tokyo, Japan. He attended a specialized high school where he studied Game Design.']\n</code></pre> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/react_agent/","title":"ReAct Agent","text":"<p>This example shows how to use outlines to build your own agent with open weights local models and structured outputs. It is inspired by the blog post A simple Python implementation of the ReAct pattern for LLMs by Simon Willison.</p> <p>The ReAct pattern (for Reason+Act) is described in the paper ReAct: Synergizing Reasoning and Acting in Language Models. It's a pattern where you implement additional actions that an LLM can take - searching Wikipedia or running calculations for example - and then teach it how to request the execution of those actions, and then feed their results back into the LLM.</p> <p>Additionally, we give the LLM the possibility of using a scratchpad described in the paper Show Your Work: Scratchpads for Intermediate Computation with Language Models which improves the ability of LLMs to perform multi-step computations.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/react_agent/#build-a-react-agent","title":"Build a ReAct agent","text":"<p>In this example, we use two tools:</p> <ul> <li>wikipedia: \\&lt;search term&gt; - search Wikipedia and returns the snippet of the first result</li> <li>calculate: \\&lt;expression&gt; - evaluate an expression using Python's eval() function</li> </ul> <pre><code>import httpx\n\ndef wikipedia(q):\n    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n        \"action\": \"query\",\n        \"list\": \"search\",\n        \"srsearch\": q,\n        \"format\": \"json\"\n    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n\n\ndef calculate(numexp):\n    return eval(numexp)\n</code></pre> <p>We define the logic of the agent through a Pydantic class. First, we want the LLM to decide only between the two previously defined tools:</p> <pre><code>from enum import Enum\n\nclass Action(str, Enum):\n    wikipedia = \"wikipedia\"\n    calculate = \"calculate\"\n</code></pre> <p>Our agent will loop through Thought and Action. We explicitly give the Action Input field so it doesn't forget to add the arguments of the Action. We also add a scratchpad (optional).</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reason_and_Act(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Thought: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n    Action: Action\n    Action_Input: str = Field(..., description=\"The arguments of the Action.\")\n</code></pre> <p>Our agent will reach a Final Answer. We also add a scratchpad (optional).</p> <pre><code>class Final_Answer(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Final_Answer: str = Field(..., description=\"Answer to the question grounded on the Observation\")\n</code></pre> <p>Our agent will decide when it has reached a Final Answer and therefore to stop the loop of Thought and Action.</p> <pre><code>from typing import Union\n\nclass Decision(BaseModel):\n    Decision: Union[Reason_and_Act, Final_Answer]\n</code></pre> <p>We could generate a response using the json schema but we will use the regex and check that everything is working as expected:</p> <pre><code>from outlines.fsm.json_schema import convert_json_schema_to_str\nfrom outlines_core.fsm.json_schema import build_regex_from_schema\n\njson_schema = Decision.model_json_schema()\nschema_str = convert_json_schema_to_str(json_schema=json_schema)\nregex_str = build_regex_from_schema(schema_str)\nprint(regex_str)\n# '\\\\{[ ]?\"Decision\"[ ]?:[ ]?(\\\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Thought\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Action\"[ ]?:[ ]?(\"wikipedia\"|\"calculate\")[ ]?,[ ]?\"Action_Input\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?\\\\}|\\\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Final_Answer\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?\\\\})[ ]?\\\\}'\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema and explain the agent logic:</p> <pre><code>import datetime\n\ndef generate_hermes_prompt(question, schema=\"\"):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON with correct Pydantic schema. \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;\\n\"\n        \"Today is \" + datetime.datetime.today().strftime('%Y-%m-%d') + \".\\n\" +\n        \"You run in a loop of Scratchpad, Thought, Action, Action Input, PAUSE, Observation. \"\n        \"At the end of the loop you output a Final Answer. \"\n        \"Use Scratchpad to store the information from the Observation useful to answer the question \"\n        \"Use Thought to describe your thoughts about the question you have been asked \"\n        \"and reflect carefully about the Observation if it exists. \"\n        \"Use Action to run one of the actions available to you. \"\n        \"Use Action Input to input the arguments of the selected action - then return PAUSE. \"\n        \"Observation will be the result of running those actions. \"\n        \"Your available actions are:\\n\"\n        \"calculate:\\n\"\n        \"e.g. calulate: 4**2 / 3\\n\"\n        \"Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\"\n        \"wikipedia:\\n\"\n        \"e.g. wikipedia: Django\\n\"\n        \"Returns a summary from searching Wikipedia\\n\"\n        \"DO NOT TRY TO GUESS THE ANSWER. Begin! &lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;user\\n\" + question + \"&lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;assistant\\n\"\n    )\n</code></pre> <p>We define a ChatBot class</p> <pre><code>class ChatBot:\n    def __init__(self, prompt=\"\"):\n        self.prompt = prompt\n\n    def __call__(self, user_prompt):\n        self.prompt += user_prompt\n        result = self.execute()\n        return result\n\n    def execute(self):\n        generator = generate.regex(model, regex_str)\n        result = generator(self.prompt, max_tokens=1024, temperature=0, seed=42)\n        return result\n</code></pre> <p>We define a query function:</p> <pre><code>import json\n\ndef query(question, max_turns=5):\n    i = 0\n    next_prompt = (\n        \"\\n&lt;|im_start|&gt;user\\n\" + question + \"&lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;assistant\\n\"\n    )\n    previous_actions = []\n    while i &lt; max_turns:\n        i += 1\n        prompt = generate_hermes_prompt(question=question, schema=Decision.model_json_schema())\n        bot = ChatBot(prompt=prompt)\n        result = bot(next_prompt)\n        json_result = json.loads(result)['Decision']\n        if \"Final_Answer\" not in list(json_result.keys()):\n            scratchpad = json_result['Scratchpad'] if i == 0 else \"\"\n            thought = json_result['Thought']\n            action = json_result['Action']\n            action_input = json_result['Action_Input']\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Thought: {thought} \\x1b[0m\")\n            print(f\"\\x1b[36m  -- running {action}: {str(action_input)}\\x1b[0m\")\n            if action + \": \" + str(action_input) in previous_actions:\n                observation = \"You already run that action. **TRY A DIFFERENT ACTION INPUT.**\"\n            else:\n                if action==\"calculate\":\n                    try:\n                        observation = eval(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n                elif action==\"wikipedia\":\n                    try:\n                        observation = wikipedia(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n            print()\n            print(f\"\\x1b[33m Observation: {observation} \\x1b[0m\")\n            print()\n            previous_actions.append(action + \": \" + str(action_input))\n            next_prompt += (\n                \"\\nScratchpad: \" + scratchpad +\n                \"\\nThought: \" + thought +\n                \"\\nAction: \" + action  +\n                \"\\nAction Input: \" + action_input +\n                \"\\nObservation: \" + str(observation)\n            )\n        else:\n            scratchpad = json_result[\"Scratchpad\"]\n            final_answer = json_result[\"Final_Answer\"]\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Final Answer: {final_answer} \\x1b[0m\")\n            return final_answer\n    print(f\"\\nFinal Answer: I am sorry, but I am unable to answer your question. Please provide more information or a different question.\")\n    return \"No answer found\"\n</code></pre> <p>We can now test our ReAct agent:</p> <pre><code>print(query(\"What's 2 to the power of 10?\"))\n# Scratchpad:\n# Thought: I need to perform a mathematical calculation to find the result of 2 to the power of 10.\n#  -- running calculate: 2**10\n#\n# Observation: 1024\n#\n# Scratchpad: 2 to the power of 10 is 1024.\n# Final Answer: 2 to the power of 10 is 1024.\n# 2 to the power of 10 is 1024.\n</code></pre> <pre><code>print(query(\"What does England share borders with?\"))\n# Scratchpad:\n# Thought: To answer this question, I will use the 'wikipedia' action to gather information about England's geographical location and its borders.\n#  -- running wikipedia: England borders\n#\n# Observation: Anglo-Scottish &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; of the United Kingdom separating Scotland and &lt;span class=\"searchmatch\"&gt;England&lt;/span&gt; which runs for\n#\n# Scratchpad: Anglo-Scottish border (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal border of the United Kingdom separating Scotland and England which runs for\n# Final Answer: England shares a border with Scotland.\n# England shares a border with Scotland.\n</code></pre> <p>As mentioned in Simon's blog post, this is not a very robust implementation at all and there's a ton of room for improvement. But it is lovely how simple it is with a few lines of Python to make these extra capabilities available to the LLM. And now you can run it locally with an open weights LLM.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/read-pdfs/","title":"PDF to structured output with vision language models","text":"<p>A common task with language models is to ask language models questions about a PDF file.</p> <p>Typically, the output is unstructured text, i.e. \"talking\" to your PDF.</p> <p>In some cases, you may wish to extract structured information from the PDF, like tables, lists, citations, etc.</p> <p>PDFs are difficult to machine read. However, you can simply convert the PDF to images, and then use a vision language model to extract structured information from the images.</p> <p>This cookbook demonstrates how to</p> <ol> <li>Convert a PDF to a list of images</li> <li>Use a vision language model to extract structured information from the images</li> </ol>"},{"location":"cookbook/read-pdfs/#dependencies","title":"Dependencies","text":"<p>You'll need to install these dependencies:</p> <pre><code>pip install outlines pillow transformers torch==2.4.0 pdf2image\n\n# Optional, but makes the output look nicer\npip install rich\n</code></pre>"},{"location":"cookbook/read-pdfs/#import-the-necessary-libraries","title":"Import the necessary libraries","text":"<pre><code>from PIL import Image\nimport outlines\nimport torch\nfrom transformers import AutoProcessor\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom pdf2image import convert_from_path\nimport os\nfrom rich import print\nimport requests\n</code></pre>"},{"location":"cookbook/read-pdfs/#choose-a-model","title":"Choose a model","text":"<p>We've tested this example with Pixtral 12b and Qwen2-VL-7B-Instruct.</p> <p>To use Pixtral:</p> <pre><code>from transformers import LlavaForConditionalGeneration\nmodel_name=\"mistral-community/pixtral-12b\"\nmodel_class=LlavaForConditionalGeneration\n</code></pre> <p>To use Qwen-2-VL:</p> <pre><code>from transformers import Qwen2VLForConditionalGeneration\nmodel_name = \"Qwen/Qwen2-VL-7B-Instruct\"\nmodel_class = Qwen2VLForConditionalGeneration\n</code></pre> <p>You can load your model into memory with:</p> <pre><code># This loads the model into memory. On your first run,\n# it will have to download the model, which might take a while.\nmodel = outlines.models.transformers_vision(\n    model_name,\n    model_class=model_class,\n    model_kwargs={\n        \"device_map\": \"auto\",\n        \"torch_dtype\": torch.bfloat16,\n    },\n    processor_kwargs={\n        \"device\": \"auto\",\n    },\n)\n</code></pre>"},{"location":"cookbook/read-pdfs/#convert-the-pdf-to-images","title":"Convert the PDF to images","text":"<p>We'll use the <code>pdf2image</code> library to convert each page of the PDF to an image.</p> <p><code>convert_pdf_to_images</code> is a convenience function that converts each page of the PDF to an image, and optionally saves the images to disk when <code>output_dir</code> is provided.</p> <p>Note: the <code>dpi</code> argument is important. It controls the resolution of the images. High DPI images are higher quality and may yield better results, but they are also larger, slower to process, and require more memory.</p> <pre><code>from pdf2image import convert_from_path\nfrom PIL import Image\nimport os\nfrom typing import List, Optional\n\ndef convert_pdf_to_images(\n    pdf_path: str,\n    output_dir: Optional[str] = None,\n    dpi: int = 120,\n    fmt: str = 'PNG'\n) -&gt; List[Image.Image]:\n    \"\"\"\n    Convert a PDF file to a list of PIL Image objects.\n\n    Args:\n        pdf_path: Path to the PDF file\n        output_dir: Optional directory to save the images\n        dpi: Resolution for the conversion. High DPI is high quality, but also slow and memory intensive.\n        fmt: Output format (PNG recommended for quality)\n\n    Returns:\n        List of PIL Image objects\n    \"\"\"\n    # Convert PDF to list of images\n    images = convert_from_path(\n        pdf_path,\n        dpi=dpi,\n        fmt=fmt\n    )\n\n    # Optionally save images\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n        for i, image in enumerate(images):\n            image.save(os.path.join(output_dir, f'page_{i+1}.{fmt.lower()}'))\n\n    return images\n</code></pre> <p>We're going to use the Louf &amp; Willard paper that described the method that Outlines uses for structured generation.</p> <p>To download the PDF, run:</p> <pre><code># Download the PDF file\npdf_url = \"https://arxiv.org/pdf/2307.09702\"\nresponse = requests.get(pdf_url)\n\n# Save the PDF locally\nwith open(\"louf-willard.pdf\", \"wb\") as f:\n    f.write(response.content)\n</code></pre> <p>Now, we can convert the PDF to a list of images:</p> <pre><code># Load the pdf\nimages = convert_pdf_to_images(\n    \"louf-willard.pdf\",\n    dpi=120,\n    output_dir=\"output_images\"\n)\n</code></pre>"},{"location":"cookbook/read-pdfs/#extract-structured-information-from-the-images","title":"Extract structured information from the images","text":"<p>The structured output you can extract is exactly the same as everywhere else in Outlines -- you can use regular expressions, JSON schemas, selecting from a list of options, etc.</p>"},{"location":"cookbook/read-pdfs/#extracting-data-into-json","title":"Extracting data into JSON","text":"<p>Suppose you wished to go through each page of the PDF, and extract the page description, key takeaways, and page number.</p> <p>You can do this by defining a JSON schema, and then using <code>outlines.generate.json</code> to extract the data.</p> <p>First, define the structure you want to extract:</p> <pre><code>class PageSummary(BaseModel):\n    description: str\n    key_takeaways: List[str]\n    page_number: int\n</code></pre> <p>Second, we need to set up the prompt. Adding special tokens can be tricky, so we use the transformers <code>AutoProcessor</code> to apply the special tokens for us. To do so, we specify a list of messages, where each message is a dictionary with a <code>role</code> and <code>content</code> key.</p> <p>Images are denoted with <code>type: \"image\"</code>, and text is denoted with <code>type: \"text\"</code>.</p> <pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            # The text you're passing to the model --\n            # this is where you do your standard prompting.\n            {\"type\": \"text\", \"text\": f\"\"\"\n                Describe the page in a way that is easy for a PhD student to understand.\n\n                Return the information in the following JSON schema:\n                {PageSummary.model_json_schema()}\n\n                Here is the page:\n                \"\"\"\n            },\n\n            # Don't need to pass in an image, since we do this\n            # when we call the generator function down below.\n            {\"type\": \"image\", \"image\": \"\"},\n        ],\n    }\n]\n\n# Convert the messages to the final prompt\nprocessor = AutoProcessor.from_pretrained(model_name)\ninstruction = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n</code></pre> <p>Now we iterate through each image, and extract the structured information:</p> <pre><code># Page summarizer function\npage_summary_generator = outlines.generate.json(model, PageSummary)\n\nfor image in images:\n    result = page_summary_generator(instruction, [image])\n    print(result)\n</code></pre>"},{"location":"cookbook/read-pdfs/#regular-expressions-to-extract-the-arxiv-paper-identifier","title":"Regular expressions to extract the arxiv paper identifier","text":"<p>The arXiv paper identifier is a unique identifier for each paper. These identifiers have the format <code>arXiv:YYMM.NNNNN</code> (five end digits) or <code>arXiv:YYMM.NNNN</code> (four end digits). arXiv identifiers are typically watermarked on papers uploaded to arXiv.</p> <p>arXiv identifiers are optionally followed by a version number, i.e. <code>arXiv:YYMM.NNNNNvX</code>.</p> <p>We can use a regular expression to define this patter:</p> <pre><code>paper_regex = r'arXiv:\\d{2}[01]\\d\\.\\d{4,5}(v\\d)?'\n</code></pre> <p>We can build an extractor function from the regex:</p> <pre><code>id_extractor = outlines.generate.regex(model, paper_regex)\n</code></pre> <p>Now, we can extract the arxiv paper identifier from the first image:</p> <pre><code>arxiv_instruction = processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"\"\"\n                Extract the arxiv paper identifier from the page.\n\n                Here is the page:\n                \"\"\"},\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Extract the arxiv paper identifier\npaper_id = id_extractor(arxiv_instruction, [images[0]])\n</code></pre> <p>As of the time of this writing, the arxiv paper identifier is</p> <pre><code>arXiv:2307.09702v4\n</code></pre> <p>Your version number may be different, but the part before <code>vX</code> should match.</p>"},{"location":"cookbook/read-pdfs/#categorize-the-paper-into-one-of-several-categories","title":"Categorize the paper into one of several categories","text":"<p><code>outlines.generate.choice</code> allows the model to select one of several options. Suppose we wanted to categorize the paper into being about \"language models\", \"economics\", \"structured generation\", or \"other\".</p> <p>Let's define a few categories we might be interested in:</p> <pre><code>categories = [\n    \"llms\",\n    \"cell biology\",\n    \"other\"\n]\n</code></pre> <p>Now we can construct the prompt:</p> <pre><code>categorization_instruction = processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"\"\"\n                Please choose one of the following categories\n                that best describes the paper.\n\n                {categories}\n\n                Here is the paper:\n                \"\"\"},\n\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n</code></pre> <p>Now we can show the model the first page and extract the category:</p> <pre><code># Build the choice extractor\ncategorizer = outlines.generate.choice(\n    model,\n    categories\n)\n\n# Categorize the paper\ncategory = categorizer(categorization_instruction, [images[0]])\nprint(category)\n</code></pre> <p>Which should return:</p> <pre><code>llms\n</code></pre>"},{"location":"cookbook/read-pdfs/#additional-notes","title":"Additional notes","text":"<p>You can provide multiple images to the model by</p> <ol> <li>Adding additional image messages</li> <li>Providing a list of images to the <code>generate</code> function</li> </ol> <p>For example, to have two images, you can do:</p> <pre><code>two_image_prompt = processor.apply_chat_template(\n    [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"are both of these images of hot dogs?\"},\n\n                # Tell the model there are two images\n                {\"type\": \"image\", \"image\": \"\"},\n                {\"type\": \"image\", \"image\": \"\"},\n            ],\n        }\n    ],\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n# Pass two images to the model\ngenerator = outlines.generate.choice(\n    model,\n    [\"hot dog\", \"not hot dog\"]\n)\n\nresult = generator(\n    two_image_prompt,\n\n    # Pass two images to the model\n    [images[0], images[1]]\n)\nprint(result)\n</code></pre> <p>Using the first to pages of the paper (they are not images of hot dogs), we should get</p> <pre><code>not hot dog\n</code></pre>"},{"location":"cookbook/receipt-digitization/","title":"Receipt Data Extraction with VLMs","text":""},{"location":"cookbook/receipt-digitization/#setup","title":"Setup","text":"<p>You'll need to install the dependencies:</p> <pre><code>pip install outlines torch==2.4.0 transformers accelerate pillow rich\n</code></pre>"},{"location":"cookbook/receipt-digitization/#import-libraries","title":"Import libraries","text":"<p>Load all the necessary libraries:</p> <pre><code># LLM stuff\nimport outlines\nimport torch\nfrom transformers import AutoProcessor\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional, List\n\n# Image stuff\nfrom PIL import Image\nimport requests\n\n# Rich for pretty printing\nfrom rich import print\n</code></pre>"},{"location":"cookbook/receipt-digitization/#choose-a-model","title":"Choose a model","text":"<p>This example has been tested with <code>mistral-community/pixtral-12b</code> (HF link) and <code>Qwen/Qwen2-VL-7B-Instruct</code> (HF link).</p> <p>We recommend Qwen-2-VL as we have found it to be more accurate than Pixtral.</p> <p>If you want to use Qwen-2-VL, you can do the following:</p> <pre><code># To use Qwen-2-VL:\nfrom transformers import Qwen2VLForConditionalGeneration\nmodel_name = \"Qwen/Qwen2-VL-7B-Instruct\"\nmodel_class = Qwen2VLForConditionalGeneration\n</code></pre> <p>If you want to use Pixtral, you can do the following:</p> <pre><code># To use Pixtral:\nfrom transformers import LlavaForConditionalGeneration\nmodel_name=\"mistral-community/pixtral-12b\"\nmodel_class=LlavaForConditionalGeneration\n</code></pre>"},{"location":"cookbook/receipt-digitization/#load-the-model","title":"Load the model","text":"<p>Load the model into memory:</p> <pre><code>model = outlines.models.transformers_vision(\n    model_name,\n    model_class=model_class,\n    model_kwargs={\n        \"device_map\": \"auto\",\n        \"torch_dtype\": torch.bfloat16,\n    },\n    processor_kwargs={\n        \"device\": \"cuda\", # set to \"cpu\" if you don't have a GPU\n    },\n)\n</code></pre>"},{"location":"cookbook/receipt-digitization/#image-processing","title":"Image processing","text":"<p>Images can be quite large. In GPU-poor environments, you may need to resize the image to a smaller size.</p> <p>Here's a helper function to do that:</p> <pre><code>def load_and_resize_image(image_path, max_size=1024):\n    \"\"\"\n    Load and resize an image while maintaining aspect ratio\n\n    Args:\n        image_path: Path to the image file\n        max_size: Maximum dimension (width or height) of the output image\n\n    Returns:\n        PIL Image: Resized image\n    \"\"\"\n    image = Image.open(image_path)\n\n    # Get current dimensions\n    width, height = image.size\n\n    # Calculate scaling factor\n    scale = min(max_size / width, max_size / height)\n\n    # Only resize if image is larger than max_size\n    if scale &lt; 1:\n        new_width = int(width * scale)\n        new_height = int(height * scale)\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n\n    return image\n</code></pre> <p>You can change the resolution of the image by changing the <code>max_size</code> argument. Small max sizes will make the image more blurry, but processing will be faster and require less memory.</p>"},{"location":"cookbook/receipt-digitization/#load-an-image","title":"Load an image","text":"<p>Load an image and resize it. We've provided a sample image of a Trader Joe's receipt, but you can use any image you'd like.</p> <p>Here's what the image looks like:</p> <p></p> <pre><code># Path to the image\nimage_path = \"https://raw.githubusercontent.com/dottxt-ai/outlines/refs/heads/main/docs/cookbook/images/trader-joes-receipt.jpg\"\n\n# Download the image\nresponse = requests.get(image_path)\nwith open(\"receipt.png\", \"wb\") as f:\n    f.write(response.content)\n\n# Load + resize the image\nimage = load_and_resize_image(\"receipt.png\")\n</code></pre>"},{"location":"cookbook/receipt-digitization/#define-the-output-structure","title":"Define the output structure","text":"<p>We'll define a Pydantic model to describe the data we want to extract from the image.</p> <p>In our case, we want to extract the following information:</p> <ul> <li>The store name</li> <li>The store address</li> <li>The store number</li> <li>A list of items, including the name, quantity, price per unit, and total price</li> <li>The tax</li> <li>The total</li> <li>The date</li> <li>The payment method</li> </ul> <p>Most fields are optional, as not all receipts contain all information.</p> <pre><code>class Item(BaseModel):\n    name: str\n    quantity: Optional[int]\n    price_per_unit: Optional[float]\n    total_price: Optional[float]\n\nclass ReceiptSummary(BaseModel):\n    store_name: str\n    store_address: str\n    store_number: Optional[int]\n    items: List[Item]\n    tax: Optional[float]\n    total: Optional[float]\n    # Date is in the format YYYY-MM-DD. We can apply a regex pattern to ensure it's formatted correctly.\n    date: Optional[str] = Field(pattern=r'\\d{4}-\\d{2}-\\d{2}', description=\"Date in the format YYYY-MM-DD\")\n    payment_method: Literal[\"cash\", \"credit\", \"debit\", \"check\", \"other\"]\n</code></pre>"},{"location":"cookbook/receipt-digitization/#prepare-the-prompt","title":"Prepare the prompt","text":"<p>We'll use the <code>AutoProcessor</code> to convert the image and the text prompt into a format that the model can understand. Practically, this is the code that adds user, system, assistant, and image tokens to the prompt.</p> <pre><code># Set up the content you want to send to the model\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                # The image is provided as a PIL Image object\n                \"type\": \"image\",\n                \"image\": image,\n            },\n            {\n                \"type\": \"text\",\n                \"text\": f\"\"\"You are an expert at extracting information from receipts.\n                Please extract the information from the receipt. Be as detailed as possible --\n                missing or misreporting information is a crime.\n\n                Return the information in the following JSON schema:\n                {ReceiptSummary.model_json_schema()}\n            \"\"\"},\n        ],\n    }\n]\n\n# Convert the messages to the final prompt\nprocessor = AutoProcessor.from_pretrained(model_name)\nprompt = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n</code></pre> <p>If you are curious, the final prompt that is sent to the model looks (roughly) like this:</p> <pre><code>&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;\nYou are an expert at extracting information from receipts.\nPlease extract the information from the receipt. Be as detailed as\npossible -- missing or misreporting information is a crime.\n\nReturn the information in the following JSON schema:\n\n&lt;JSON SCHEMA OMITTED&gt;\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>"},{"location":"cookbook/receipt-digitization/#run-the-model","title":"Run the model","text":"<pre><code># Prepare a function to process receipts\nreceipt_summary_generator = outlines.generate.json(\n    model,\n    ReceiptSummary,\n\n    # Greedy sampling is a good idea for numeric\n    # data extraction -- no randomness.\n    sampler=outlines.samplers.greedy()\n)\n\n# Generate the receipt summary\nresult = receipt_summary_generator(prompt, [image])\nprint(result)\n</code></pre>"},{"location":"cookbook/receipt-digitization/#output","title":"Output","text":"<p>The output should look like this:</p> <pre><code>ReceiptSummary(\n    store_name=\"Trader Joe's\",\n    store_address='401 Bay Street, San Francisco, CA 94133',\n    store_number=0,\n    items=[\n        Item(name='BANANA EACH', quantity=7, price_per_unit=0.23, total_price=1.61),\n        Item(name='BAREBELLS CHOCOLATE DOUG', quantity=1, price_per_unit=2.29, total_price=2.29),\n        Item(name='BAREBELLS CREAMY CRISP', quantity=1, price_per_unit=2.29, total_price=2.29),\n        Item(name='BAREBELLS CHOCOLATE DOUG', quantity=1, price_per_unit=2.29, total_price=2.29),\n        Item(name='BAREBELLS CARAMEL CASHEW', quantity=2, price_per_unit=2.29, total_price=4.58),\n        Item(name='BAREBELLS CREAMY CRISP', quantity=1, price_per_unit=2.29, total_price=2.29),\n        Item(name='SPINDRIFT ORANGE MANGO 8', quantity=1, price_per_unit=7.49, total_price=7.49),\n        Item(name='Bottle Deposit', quantity=8, price_per_unit=0.05, total_price=0.4),\n        Item(name='MILK ORGANIC GALLON WHOL', quantity=1, price_per_unit=6.79, total_price=6.79),\n        Item(name='CLASSIC GREEK SALAD', quantity=1, price_per_unit=3.49, total_price=3.49),\n        Item(name='COBB SALAD', quantity=1, price_per_unit=5.99, total_price=5.99),\n        Item(name='PEPPER BELL RED XL EACH', quantity=1, price_per_unit=1.29, total_price=1.29),\n        Item(name='BAG FEE.', quantity=1, price_per_unit=0.25, total_price=0.25),\n        Item(name='BAG FEE.', quantity=1, price_per_unit=0.25, total_price=0.25)\n    ],\n    tax=0.68,\n    total=41.98,\n    date='2023-11-04',\n    payment_method='debit',\n\n)\n</code></pre> <p>Voila! You've successfully extracted information from a receipt using an LLM.</p>"},{"location":"cookbook/receipt-digitization/#bonus-roasting-the-user-for-their-receipt","title":"Bonus: roasting the user for their receipt","text":"<p>You can roast the user for their receipt by adding a <code>roast</code> field to the end of the  <code>ReceiptSummary</code> model.</p> <pre><code>class ReceiptSummary(BaseModel):\n    ...\n    roast: str\n</code></pre> <p>which gives you a result like</p> <pre><code>ReceiptSummary(\n    ...\n    roast=\"You must be a fan of Trader Joe's because you bought enough\n    items to fill a small grocery bag and still had to pay for a bag fee.\n    Maybe you should start using reusable bags to save some money and the\n    environment.\"\n)\n</code></pre> <p>Qwen is not particularly funny, but worth a shot.</p>"},{"location":"cookbook/simtom/","title":"Build perspective-taking agents with SimToM","text":"<p>Prompting strategies like Chain-of-Thought (CoT) can improve LLMs' reasoning capabilities. However, they underwhelm in tasks that require keeping track of inconsistent world states. SimToM proposes a simple, two-stage prompting framework for LLMs inspired by Simulation Theory. The authors showed that this approach outperforms zero-shot prompting and CoT on ToMI and BigToM, two benchmarks with Theory of Mind questions.</p> <p>In this example, we will implement SimToM with a few lines of code using Outlines' prompt templating and structured generation capabilities.</p>"},{"location":"cookbook/simtom/#how-simtom-works","title":"How SimToM works","text":"<p>SimToM calls an LLM with two consecutive prompts:</p> <ol> <li>Perspective-taking: The first prompt receives a <code>story</code> and a <code>character</code>. The goal is to understand the situation based on the character's point of view and filter out the rest of the story.</li> <li>Question-Answering: The second prompt receives the character's point of view from the previous step and tasks the LLM to answer a question using that context.</li> </ol> <p></p>"},{"location":"cookbook/simtom/#outlines-implementation","title":"Outlines implementation","text":"<p>To implement SimToM with Outlines, we will need to:</p> <ol> <li>Write the prompts with prompt functions.</li> <li>Define the JSON object each prompt will return using Pydantic.</li> <li>Generate responses with a Mistral model using the transformers integration.</li> </ol> <p>Let's dive into it!</p>"},{"location":"cookbook/simtom/#using-prompt-templates","title":"Using Prompt Templates","text":"<p>The authors have shared their code, prompts and data in this GitHub repository. Below, we define in Outlines the prompts they used for the ToMI dataset:</p> <pre><code>from outlines import Template\n\n\nperspective_taking = Template.from_string(\n    \"\"\"&lt;s&gt;[INST] The following is a sequence of events about some characters, that takes place in multiple locations.\n    Your job is to output only the events that the specified character, {{character}}, knows about.\n\n    Here are a few rules:\n    1. A character knows about all events that they do.\n    2. If a character is in a certain room/location, that character knows about all other events that happens in the room. This includes other characters leaving or exiting the location, the locations of objects in that location, and whether somebody moves an object to another place.\n    3. If a character leaves a location, and is NOT in that location, they no longer know about any events that happen within that location. However, they can re-enter the location.\n\n    Story: {{story}}\n    What events does {{character}} know about? Only output the events according to the above rules, do not provide an explanation. [/INST]\"\"\" # noqa\n)\n\nsimulation = Template.from_string(\n    \"\"\"&lt;s&gt;[INST] {% for event in events %}\n    {{event}}\n    {% endfor %}\n    You are {{name}}.\n    Based on the above information, answer the following question:\n    {{question}}\n    You must choose one of the above choices, do not say there is not enough information. Answer with a single word, do not output anything else. [/INST]\"\"\" # noqa\n)\n</code></pre>"},{"location":"cookbook/simtom/#json-structured-generation","title":"JSON Structured Generation","text":"<p>Outlines guarantees that the LLM will return a valid JSON object, which we can specify as a Pydantic model.</p> <p>We will need two Pydantic models for SimToM, one for each prompt:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass PerspectiveTaking(BaseModel):\n    \"\"\"This is for the first prompt.\"\"\"\n    character: str = Field(description=\"The character we extract the events for.\")\n    events: List[str] = Field(description=\"All events that the character knows about.\")\n\n\nclass Simulation(BaseModel):\n    \"\"\"This is for the second prompt.\"\"\"\n    answer: str\n</code></pre>"},{"location":"cookbook/simtom/#calling-an-llm","title":"Calling an LLM","text":"<p>Let's try SimToM with an example from the ToMI dataset:</p> <pre><code>story = \"\"\"\n1 Aria entered the front_yard.\n2 Aiden entered the front_yard.\n3 The grapefruit is in the green_bucket.\n4 Aria moved the grapefruit to the blue_container.\n5 Aiden exited the front_yard.\n6 Noah entered the playroom.\n\"\"\"\nquestion = \"7 Where was the grapefruit at the beginning?\"\ncharacter = \"Aria\"\n</code></pre> <p>We load <code>Mistral-7B-Instruct-v0.3</code>, create the prompt using the template we defined earlier, and generate a structured response. As a reminder, the goal of the first call is to get all the events a character, <code>Aria</code>, knows about.</p> <pre><code># Load an LLM from Hugging Face\nMODEL_NAME = \"mistral-community/Mistral-7B-Instruct-v0.3\"\nmodel = outlines.models.transformers(MODEL_NAME, device=\"cuda\")\n\nperspective_prompt = perspective_taking(story=story, character=character)\n\n# Call Mistral 7B with the first prompt\ngenerator = outlines.generate.json(model, PerspectiveTaking)\nperspective = generator(perspective_prompt)\n\nprint(perspective.model_dump())\n# {'character': 'Aria', 'events': ['1 Aria entered the front_yard.', '3 The grapefruit is in the green_bucket.', '4 Aria moved the grapefruit to the blue_container.']}\n</code></pre> <p>Not bad! We will now generate the second prompt with those events.</p> <pre><code>sim_prompt = simulation(events=perspective.events, name=character, question=question)\n\n# Call Mistral 7B with the second prompt\ngenerator = outlines.generate.json(model, Simulation)\nresult = generator(sim_prompt)\n\nprint(result.model_dump())\n# {'answer': 'green_bucket'}\n</code></pre> <p>And this is it! SimToM could be useful in agentic workflows, where agents must act based on what they know, not all available information. One caveat of SimToM is that the perspective-taking step may remove important information, leading to wrong results. As the authors note in their paper, it can feature as a simple and effective baseline for evaluating LLMs on Theory of Mind reasoning tasks.</p>"},{"location":"cookbook/structured_generation_workflow/","title":"Structured Generation Workflow: Generating Synthetic Phone Numbers","text":"<p>This is a condensed version of Coding for Structured Generation with LLMs.</p> <p>For this example we're going to be building an LLM program to generate synthetic data in the form of realistic looking phone numbers for Washington State. Using an LLM for this task is a bit overkill since we could just as easily accomplish this with a tool like Faker, but this example still serves as a useful way to demonstrate a workflow for using structured generation.</p>"},{"location":"cookbook/structured_generation_workflow/#unstructured-approach","title":"Unstructured approach","text":"<p>Before diving into how to use structure generation for this task let's start with an unstructured example. We begin by loading our model:</p> <pre><code>import outlines\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\nmodel = outlines.models.transformers(model_name)\n</code></pre> <p>Next we need a prompt for this model. Since we're focusing on structured generation, we won't be engaging in any form of \"prompt hacking\" and will be leaving this prompt untouched for the rest of this example.</p> <pre><code>tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages_phone = [\n            {\"role\": \"user\", \"content\": \"\"\"\n            Please generate a realistic phone number for Washington State in the following format\n\n            (555) 555-5555\n\n            \"\"\"}\n]\n\n# This allows us to properly format our prompt for\n# Phi-3 Mini's 'Instruct' interface.\nprompt_phone = tokenizer.apply_chat_template(messages_phone, tokenize=False)\n</code></pre> <p>With our prompt ready we can now generate 10 example phone numbers</p> <pre><code>phone_generator_unstruct = outlines.generate.text(model)\nfor _ in range(10):\n    print(phone_generator_unstruct(prompt_phone,max_tokens=12))\n</code></pre> <p>I'd be happy to help you generate a realistic phone\\ I cannot generate a real phone number as I'm just\\ I'm an AI and don't have the ability\\ Sure! Here is a randomly generated phone number in the format\\ Here's a phone number that fits the format for a\\ In Washington State, phone numbers typically have a three-dig\\ Here are a few examples of phone numbers that could be considered\\ I'd be happy to help generate a realistic phone number\\ I'd be happy to help you generate a random phone\\ Based on the format you provided, a realistic phone number for\\</p> <p>As we can see, none of these outputs are even phone numbers!</p> <p>Let's see  if we can improve this using structured generation.</p>"},{"location":"cookbook/structured_generation_workflow/#the-structured-generation-workflow","title":"The Structured Generation Workflow","text":"<p>In order to solve this problem we're going to introduce a Structured Generation Workflow outlined in this image:</p> <p></p> <p>Let's step through this:</p>"},{"location":"cookbook/structured_generation_workflow/#real-example","title":"Real example","text":"<p>We start with a real example phone number, in this case for the Seattle Public Library, that we can use to verify the structure we are creating.</p> <pre><code>phone_number = \"(206) 386-4636\"\n</code></pre> <p>For a simple example like this, we'll just be using a single phone number, for more complex examples it can be helpful to have more examples.</p>"},{"location":"cookbook/structured_generation_workflow/#draft-structure","title":"Draft Structure","text":"<p>The next step in the process is for use to define a simple regex that we feel correctly models our real data.</p> <pre><code>phone_regex_1 = r'\\([0-9]{3}\\) [0-9]{3}-[0-9]{4}'\n</code></pre> <p>Next we need to validate this regex against our real data.</p>"},{"location":"cookbook/structured_generation_workflow/#validate-by-matching-examples","title":"Validate by matching examples","text":"<p>Whenever writing non-trivial code with structured generation it is essential that you first validate the code against your real data example(s).</p> <p>We'll start with a simple method of validation: just checking that our regex matches the data.</p> <pre><code>import re\nre.match(phone_regex_1, phone_number)\n\n# &lt;re.Match object; span=(0, 14), match='(206) 386-4636'&gt;\n</code></pre> <p>Now that we have a match, we can move on to generating structured output!</p>"},{"location":"cookbook/structured_generation_workflow/#generate-structure","title":"Generate Structure","text":"<p>We're ready to see if structured generation can make an improvement over our initial unstructured approach:</p> <pre><code>phone_generator_v1 = outlines.generate.regex(model, phone_regex_1)\nfor _ in range(10):\n    print(phone_generator_v1(prompt_phone))\n</code></pre> <p>(206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 123-4567\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234</p> <p>At least we have phone numbers! But I think we can do better!</p>"},{"location":"cookbook/structured_generation_workflow/#inspect-output","title":"Inspect output","text":"<p>In this case the model did create phone numbers and, impressively, got the area code correct. So using structured generation did improve things. However these numbers are pretty boring. Let's improve that structure!</p>"},{"location":"cookbook/structured_generation_workflow/#iteration","title":"Iteration","text":"<p>We've walked through the loop once, so we can go quickly now through each iteration.</p> <p>We start by improving our structure:</p> <pre><code>phone_regex_2 = r'\\([0-9]{3}\\) [2-46-9]{3}-[02-9]{4}'\n</code></pre> <p>Before rushing to another round of generation, let's validate this new regex. We'll add just a bit more sophistication over our last check:</p> <p><pre><code>re.match(phone_regex_2, phone_number)[0] == phone_number\n# True\n</code></pre> Now that we've validated, let's generate with this new regex!</p> <pre><code>phone_generator_v2 = outlines.generate.regex(model,\n                                             phone_regex_2)\nfor _ in range(10):\n    print(phone_generator_v2(prompt_phone))\n</code></pre> <p>(206) 867-5309\\ (206) 666-7777\\ (206) 444-3333\\ (206) 444-3333\\ (206) 943-2222\\ (206) 323-6789\\ (206) 444-3333\\ (206) 867-5309\\ (206) 466-2255\\ (206) 222-3333</p> <p>Better, but I don't like those repeated sequences. Like good software developers, let's iterate again!</p>"},{"location":"cookbook/structured_generation_workflow/#reiteration-with-debugging","title":"Reiteration - with debugging","text":"<p>Here's a fancier regex that should give us more interesting results:</p> <pre><code>phone_regex_3_error = r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4]'\n</code></pre> <p>This looks good to me, but there's a subtle bug, that's why we always need to validate our structure against real data. This time we'll make our validator do a bit more work to verify the correct string is matched:</p> <p><pre><code>if not re.match(phone_regex_3_error, phone_number):\n    print(\"Regex fails match\")\nelse:\n    matched_string = re.match(phone_regex_3_error, phone_number)[0]\n    if matched_string == phone_number:\n    print(\"Successful match\")\n    else:\n    print(f\"Error {matched_string} != {phone_number}\")\n</code></pre> This prints out:</p> <p>Error (206) 386-463 != (206) 386-4636</p> <p>Ah! We were missing the last digit, let's fix that and regenerate:</p> <pre><code>phone_regex_3_fixed = r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4][6-9]'\nphone_generator_v3 = outlines.generate.regex(model,\n                                             phone_regex_3_fixed)\nfor _ in range(10):\n    print(phone_generator_v3(prompt_phone))\n</code></pre> <p>(206) 494-3216\\ (206) 374-6218\\ (206) 494-3337\\ (206) 476-3216\\ (206) 484-3548\\ (206) 495-3218\\ (206) 494-5517\\ (206) 375-4636\\ (206) 384-6216\\ (206) 385-6218</p> <p>Much better!</p> <p>Now you've seen a quick example of the structured generation workflow that can be used at the basis for building and iteration on much larger structured generation tasks!</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#structured-generation","title":"Structured generation","text":"<p>While LLM capabilities are increasingly impressive, we can make their output more reliable by steering the generation. Outlines thus offers mechanisms to specify high level constraints on text completions by generative language models.</p> <p>Stopping sequence By default, language models stop generating tokens after and  token was generated, or after a set maximum number of tokens. Their output can be verbose, and for practical purposes it is often necessary to stop the generation after a given sequence has been found instead. You can use the stop_at keyword argument when calling the model with a prompt: <pre><code>import outlines.models as models\n\ncomplete = models.openai(\"gpt-4o-mini\")\nexpert = complete(\"Name an expert in quantum gravity.\", stop_at=[\"\\n\", \".\"])\n</code></pre>"},{"location":"reference/chat_templating/","title":"Chat templating","text":"<p>Instruction-tuned language models use \"special tokens\" to indicate different parts of text, such as the system prompt, the user prompt, any images, and the assistant's response. A chat template is how different types of input are composited together into a single, machine-readable string.</p> <p>Outlines does not manage chat templating tokens when using instruct models. You must apply the chat template tokens to the prompt yourself -- if you do not apply chat templating on instruction-tuned models, you will often get nonsensical output from the model.</p> <p>Chat template tokens are not needed for base models.</p> <p>You can find the chat template tokens in the model's HuggingFace repo or documentation. As an example, the <code>SmolLM2-360M-Instruct</code> special tokens can be found here.</p> <p>However, it can be slow to manually look up a model's special tokens, and special tokens vary by models. If you change the model, your prompts may break if you have hard-coded special tokens.</p> <p>If you need a convenient tool to apply chat templating for you, you should use the <code>tokenizer</code> from the <code>transformers</code> library:</p> <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\nprompt = tokenizer.apply_chat_template(\n    [\n        {\"role\": \"system\", \"content\": \"You extract information from text.\"},\n        {\"role\": \"user\", \"content\": \"What food does the following text describe?\"},\n    ],\n    tokenize=False,\n    add_bos=True,\n    add_generation_prompt=True,\n)\n</code></pre> <p>yields</p> <pre><code>&lt;|im_start|&gt;system\nYou extract information from text.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat food does the following text describe?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>"},{"location":"reference/prompting/","title":"Prompt templating","text":"<p>Outlines provides a powerful domain-specific language to write and manage prompts, via what we call prompt functions.  Prompt functions are Python functions that contain a template for the prompt in their docstring, and their arguments correspond to the variables used in the prompt. When called, a prompt function returns the template rendered with the values of the arguments.</p> <p>The aim of prompt functions is to solve several recurrent problems with prompting:</p> <ol> <li>Building complex prompts quickly leads to messy code. This problem has    already been solved in the web development community by using templating, so    why not use it here?</li> <li>Composing prompts is difficult. Why not just compose functions?</li> <li>Separating prompts from code. Encapsulation in functions allows a clean    separation between prompts and code. Moreover, like any function, prompt    functions can be imported from other modules.</li> </ol> <p>Outlines uses the Jinja templating engine to render prompts, which allows to easily compose complex prompts.</p> <p>Prompt rendering</p> <p>Prompt functions are opinionated when it comes to prompt rendering. These opinions are meant to avoid common prompting errors, but can have unintended consequences if you are doing something unusual. We advise to always print the prompt before using it. You can also read the reference section if you want to know more.</p>"},{"location":"reference/prompting/#your-first-prompt","title":"Your first prompt","text":"<p>The following snippet showcases a very simple prompt. The variables between curly brackets <code>{{  }}</code> are placeholders for the values of the arguments you will pass to the prompt function.</p> CodeOutput greetings.py<pre><code>from outlines import Template\n\nprompt = \"\"\"Hello, {{ name }}!\n{{ question }}\"\"\"\n\ngreetings = Template.from_string(prompt)\nprompt = greetings(\"user\", \"How are you?\")\nprint(prompt)\n</code></pre> <pre><code>Hello, user!\nHow are you?\n</code></pre> <p>If a variable is missing in the function's arguments, Jinja2 will throw an <code>UndefinedError</code> exception:</p> CodeOutput <pre><code>from outlines import Template\n\nprompt = \"\"\"Hello, {{ surname }}!\"\"\"\ngreetings = Template.from_string(prompt)\nprompt = greetings(\"user\")\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 9, in &lt;module&gt;\n  File \"/home/remi/projects/normal/outlines/outlines/templates.py\", line 38, in __call__\n      return render(self.template, **bound_arguments.arguments)\n  File \"/home/remi/projects/normal/outlines/outlines/templates.py\", line 213, in render\n      return jinja_template.render(**values)\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 1301, in render\n      self.environment.handle_exception()\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 936, in handle_exception\n      raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 1, in top-level template code\n  jinja2.exceptions.UndefinedError: 'surname' is undefined\n</code></pre>"},{"location":"reference/prompting/#importing-prompts-from-files","title":"Importing prompts from files","text":"<p>Outlines allows you to read a prompt template from a text file. This way you can build \"white space perfect\" prompts, and version them independently from your code. We have found ourselves gravitating around this pattern a lot since Outlines came out:</p> prompt.txtgenerate.pyOutput <pre><code>\"\"\"Hello, {{ name }}!\n{{ question }}\n\"\"\"\n</code></pre> <pre><code>from outlines import Template\n\ngreetings = Template.from_file(\"prompt.txt\")\nprompt = greetings(\"John Doe\", \"How are you today?\")\n</code></pre> <pre><code>Hello, John Doe!\nHow are you today?\n</code></pre>"},{"location":"reference/prompting/#few-shot-prompting","title":"Few-shot prompting","text":"<p>Few-shot prompting can lead to messy code. Prompt functions allow you to loop over lists or dictionaries from the template. In the following example we demonstrate how we can generate a prompt by passing a list of dictionaries with keys <code>question</code> and <code>answer</code> to the prompt function:</p> CodeOutput prompt.txt<pre><code>{{ instructions }}\n\nExamples\n--------\n\n{% for example in examples %}\nQ: {{ example.question }}\nA: {{ example.answer }}\n\n{% endfor %}\nQuestion\n--------\n\nQ: {{ question }}\nA:\n</code></pre> render.py<pre><code>from outlines import Template\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nfew_shots = Template.from_file(\"prompt.txt\")\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n</code></pre> <pre><code>Please answer the following question following the examples\n\nExamples\n--------\n\nQ: 2+2=?\nA: 4\n\nQ: 3+3=?\nA: 6\n\nQuestion\n--------\n\nQ: 4+4 = ?\nA:\n</code></pre>"},{"location":"reference/prompting/#conditionals-filters-etc","title":"Conditionals, filters, etc.","text":"<p>Jinja2 has many features beyond looping that are not described here: conditionals, filtering, formatting, etc. Please refer to the Jinja documentation for more information about the syntax of the templating language. The Jinja syntax is powerful, and we recommend you take some time to read their documentation if you are building complex prompts.</p>"},{"location":"reference/prompting/#tools","title":"Tools","text":"<p>Several projects (e.g.Toolformer, ViperGPT, AutoGPT, etc.) have shown that we can \"teach\" language models to use external functions by describing what these functions do in the prompt. In these projects the same information is often repeated twice: the function implementation, name, docstring, or arguments are copy-pasted in the prompt. This is cumbersome and error prone; you can directly pull this information from within an Outlines prompt function:</p> CodeOutput <pre><code>from outlines import Template\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n\nprompt = \"\"\"{{ question }}\n\nCOMMANDS\n1. {{ tool | name }}: {{ tool | description }}, args: {{ tool | args }}\n\n{{ tool | source }}\n\"\"\"\n\ntool_prompt = Template.from_string(prompt)\nprompt = tool_prompt(\"Can you do something?\", my_tool)\nprint(prompt)\n</code></pre> <pre><code>Can you do something?\n\nCOMMANDS\n1. my_tool: Tool description., args: arg1: str, arg2: int\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompting/#json-response-format","title":"JSON response format","text":"<p>To build reliable chains with language models we often need to instruct them the format in which we would like them to return their response.</p> <p>Without prompt templating, the information is repeated twice between creating the parsing function (e.g. a Pydantic model), and writing the desired schema in the prompt. This can lead to errors that are hard to debug.</p> <p>Outlines allows you to directly pull the JSON schema of a pydantic model, or pretty print a dictionary from within an Outlines prompt function</p> Code <p>```python from pydantic import BaseModel, Field</p> <p>from outlines import Template</p> <pre><code>class MyResponse(BaseModel):\n    field1: int = Field(description=\"an int\")\n    field2: str\n\n\nmy_prompt = Template.from_string(\"\"\"{{ response_model | schema }}\"\"\")\nprompt = my_prompt(MyResponse)\nprint(prompt)\n# {\n#   \"field1\": \"an int\",\n#   \"field2\": \"&lt;field2&gt;\"\n# }\n```\n</code></pre> Output <pre><code>response = {\n    \"field1\": \"&lt;field1&gt;\",\n    \"field2\": \"a string\"\n}\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"&lt;field1&gt;\",\n#   \"field2\": \"a string\"\n# }\n</code></pre>"},{"location":"reference/prompting/#formatting-conventions","title":"Formatting conventions","text":"<p>Prompt templates are opinionated when it comes to rendering a template read from a string, and these opinions are meant to avoid prompting mistakes and help with formatting.</p>"},{"location":"reference/prompting/#whitespaces","title":"Whitespaces","text":"<p>If you have experience working with strings between triple quotes you know that indenting has an influence on the string's formatting. Prompt functions adopt a few conventions so you don't have to think about indents when writing prompt.</p> <p>First, whether you start the prompt right after the triple quotes or on the line below does not matter for formatting:</p> CodeOutput <pre><code>from outlines import Template\n\n\nprompt1 = Template.from_string(\n\"\"\"My prompt\n\"\"\"\n)\n\nprompt2 = Template.from_string(\n\"\"\"\nMy prompt\n\"\"\"\n)\n\nprint(prompt1())\nprint(prompt2())\n</code></pre> <pre><code>My prompt\nMy prompt\n</code></pre> <p>Indentation is relative to the second line of the docstring, and leading spaces are removed:</p> CodeOutput <pre><code>from outlines import Template\n\nexample1 = Template.from_string(\n\"\"\"First line\nSecond line\n\"\"\"\n)\n\nexample2 = Template.from_string(\n\"\"\"\n    Second line\n    Third line\n\"\"\"\n)\n\nexample3 = Template.from_string(\n\"\"\"\n    Second line\n      Third line\n\"\"\"\n)\n\nprint(example1())\nprint(example2())\nprint(example3())\n</code></pre> <pre><code>First line\nSecond line\n\nSecond line\nThird line\n\nSecond line\n  Third line\n</code></pre> <p>Trailing whitespaces are not removed, unless they follow a linebreak symbol <code>\\</code> (see linebreaks).</p>"},{"location":"reference/prompting/#linebreaks","title":"Linebreaks","text":"<p>You can use the backslash <code>\\</code> to break a long line of text. It will render as a single line:</p> CodeOutput <pre><code>from outlines import Template\n\nexample = Template.from_string(\n\"\"\"\nBreak in \\\nseveral lines \\\nBut respect the indentation\n    on line breaks.\nAnd after everything \\\nGoes back to normal\n\"\"\"\n)\n\nprint(example())\n</code></pre> <pre><code>Break in several lines But respect the indentation\n    on line breaks.\nAnd after everything Goes back to normal\n</code></pre>"},{"location":"reference/regex_dsl/","title":"DSL to express constraints","text":"<p>This library provides a Domain-Specific Language (DSL) to construct regular expressions in a more intuitive and modular way. It allows you to create complex regexes using simple building blocks that represent literal strings, patterns, and various quantifiers. Additionally, these custom regex types can be used directly as types in Pydantic schemas to enforce pattern constraints during text generation.</p>"},{"location":"reference/regex_dsl/#why-use-this-dsl","title":"Why Use This DSL?","text":"<ol> <li>Modularity &amp; Readability: Instead of writing cryptic regular expression strings, you compose a regex as a tree of objects.</li> <li>Enhanced Debugging: Each expression can be visualized as an ASCII tree, making it easier to understand and debug complex regexes.</li> <li>Pydantic Integration: Use your DSL-defined regex as types in Pydantic models. The DSL seamlessly converts to JSON Schema with proper pattern constraints.</li> <li>Extensibility: Easily add or modify quantifiers and other regex components by extending the provided classes.</li> </ol>"},{"location":"reference/regex_dsl/#building-blocks","title":"Building Blocks","text":"<p>Every regex component in this DSL is a Term. Here are two primary types:</p> <ul> <li><code>String</code>: Represents a literal string. It escapes the characters that have a special meaning in regular expressions.</li> <li><code>Regex</code>: Represents an existing regex pattern string.</li> </ul> <pre><code>from outlines.types import String, Regex\n\n# A literal string \"hello\"\nliteral = String(\"hello\")   # Internally represents \"hello\"\n\n# A regex pattern to match one or more digits\ndigit = Regex(r\"[0-9]+\")     # Internally represents the pattern [0-9]+\n\n# Converting to standard regex strings:\nfrom outlines.types.dsl import to_regex\n\nprint(to_regex(literal))  # Output: hello\nprint(to_regex(digit))    # Output: [0-9]+\n</code></pre>"},{"location":"reference/regex_dsl/#early-introduction-to-quantifiers-combining-terms","title":"Early Introduction to Quantifiers &amp; Combining Terms","text":"<p>The DSL supports common regex quantifiers as methods on every <code>Term</code>. These methods allow you to specify how many times a pattern should be matched. They include:</p> <ul> <li><code>exactly(count)</code>: Matches the term exactly <code>count</code> times.</li> <li><code>optional()</code>: Matches the term zero or one time.</li> <li><code>one_or_more()</code>: Matches the term one or more times (Kleene Plus).</li> <li><code>zero_or_more()</code>: Matches the term zero or more times (Kleene Star).</li> <li><code>between(min_count, max_count)</code>: Matches the term between <code>min_count</code> and <code>max_count</code> times (inclusive).</li> <li><code>at_least(count)</code>: Matches the term at least <code>count</code> times.</li> <li><code>at_most(count)</code>: Matches the term up to <code>count</code> times.</li> </ul> <p>These quantifiers can also be used as functions that take the <code>Term</code> as an argument. If the term is a plain string, it will be automatically converted to a <code>String</code> object. Thus <code>String(\"foo\").optional()</code> is equivalent to <code>optional(\"foo\")</code>.</p> <p>Let's see these quantifiers side by side with examples.</p>"},{"location":"reference/regex_dsl/#quantifiers-in-action","title":"Quantifiers in Action","text":""},{"location":"reference/regex_dsl/#exactlycount","title":"<code>exactly(count)</code>","text":"<p>This method restricts the term to appear exactly <code>count</code> times.</p> <pre><code># Example: exactly 5 digits\nfive_digits = Regex(r\"\\d\").exactly(5)\nprint(to_regex(five_digits))  # Output: (\\d){5}\n</code></pre> <p>You can also use the <code>exactly</code> function:</p> <pre><code>from outlines.types import exactly\n\n# Example: exactly 5 digits\nfive_digits = exactly(Regex(r\"\\d\"), 5)\nprint(to_regex(five_digits))  # Output: (\\d){5}\n</code></pre>"},{"location":"reference/regex_dsl/#optional","title":"<code>optional()</code>","text":"<p>This method makes a term optional, meaning it may occur zero or one time.</p> <pre><code># Example: an optional \"s\" at the end of a word\nmaybe_s = String(\"s\").optional()\nprint(to_regex(maybe_s))  # Output: (s)?\n</code></pre> <p>You can also use the <code>optional</code> function:</p> <pre><code>from outlines.types import optional\n\n# Example: an optional \"s\" at the end of a word\nmaybe_s = optional(\"s\")\nprint(to_regex(maybe_s))  # Output: (s)?\n</code></pre>"},{"location":"reference/regex_dsl/#one_or_more","title":"<code>one_or_more()</code>","text":"<p>This method indicates that the term must appear at least once.</p> <pre><code># Example: one or more alphabetic characters\nletters = Regex(r\"[A-Za-z]\").one_or_more()\nprint(to_regex(letters))  # Output: ([A-Za-z])+\n</code></pre> <p>You can also use the <code>one_or_more</code> function:</p> <pre><code>from outlines.types import one_or_more\n\n# Example: one or more alphabetic characters\nletters = one_or_more(Regex(r\"[A-Za-z]\"))\nprint(to_regex(letters))  # Output: ([A-Za-z])+\n</code></pre>"},{"location":"reference/regex_dsl/#zero_or_more","title":"<code>zero_or_more()</code>","text":"<p>This method indicates that the term can occur zero or more times.</p> <pre><code># Example: zero or more spaces\nspaces = String(\" \").zero_or_more()\nprint(to_regex(spaces))  # Output: ( )*\n</code></pre> <p>You can also use the <code>zero_or_more</code> function:</p> <pre><code>from outlines.types import zero_or_more\n\n# Example: zero or more spaces\nspaces = zero_or_more(\" \")\nprint(to_regex(spaces))  # Output: ( )*\n</code></pre>"},{"location":"reference/regex_dsl/#betweenmin_count-max_count","title":"<code>between(min_count, max_count)</code>","text":"<p>This method indicates that the term can appear any number of times between <code>min_count</code> and <code>max_count</code> (inclusive).</p> <pre><code># Example: Between 2 and 4 word characters\nword_chars = Regex(r\"\\w\").between(2, 4)\nprint(to_regex(word_chars))  # Output: (\\w){2,4}\n</code></pre> <p>You can also use the <code>between</code> function:</p> <pre><code>from outlines.types import between\n\n# Example: Between 2 and 4 word characters\nword_chars = between(Regex(r\"\\w\"), 2, 4)\nprint(to_regex(word_chars))  # Output: (\\w){2,4}\n</code></pre>"},{"location":"reference/regex_dsl/#at_leastcount","title":"<code>at_least(count)</code>","text":"<p>This method indicates that the term must appear at least <code>count</code> times.</p> <pre><code># Example: At least 3 digits\nat_least_three = Regex(r\"\\d\").at_least(3)\nprint(to_regex(at_least_three))  # Output: (\\d){3,}\n</code></pre> <p>You can also use the <code>at_least</code> function:</p> <pre><code>from outlines.types import at_least\n\n# Example: At least 3 digits\nat_least_three = at_least(Regex(r\"\\d\"), 3)\nprint(to_regex(at_least_three))  # Output: (\\d){3,}\n</code></pre>"},{"location":"reference/regex_dsl/#at_mostcount","title":"<code>at_most(count)</code>","text":"<p>This method indicates that the term can appear at most <code>count</code> times.</p> <pre><code># Example: At most 3 digits\nup_to_three = Regex(r\"\\d\").at_most(3)\nprint(to_regex(up_to_three))  # Output: (\\d){0,3}\n</code></pre> <p>You can also use the <code>at_most</code> function:</p> <pre><code>from outlines.types import at_most\n\n# Example: At most 3 digits\nup_to_three = at_most(Regex(r\"\\d\"), 3)\nprint(to_regex(up_to_three))  # Output: (\\d){0,3}\n</code></pre>"},{"location":"reference/regex_dsl/#combining-terms","title":"Combining Terms","text":"<p>The DSL allows you to combine basic terms into more complex patterns using concatenation and alternation.</p>"},{"location":"reference/regex_dsl/#concatenation","title":"Concatenation (<code>+</code>)","text":"<p>The <code>+</code> operator (and its reflected variant) concatenates terms, meaning that the terms are matched in sequence.</p> <pre><code># Example: Match \"hello world\"\npattern = String(\"hello\") + \" \" + Regex(r\"\\w+\")\nprint(to_regex(pattern))  # Output: hello\\ (\\w+)\n</code></pre>"},{"location":"reference/regex_dsl/#alternation-either","title":"Alternation (<code>either()</code>)","text":"<p>The <code>either()</code> function creates alternatives, allowing a match for one of several patterns. You can provide as many terms as you want.</p> <pre><code># Example: Match either \"cat\" or \"dog\" or \"mouse\"\nanimal = either(String(\"cat\"), \"dog\", \"mouse\")\nprint(to_regex(animal))  # Output: (cat|dog|mouse)\n</code></pre> <p>Note: When using <code>either()</code> with plain strings (such as <code>\"dog\"</code>), the DSL automatically wraps them in a <code>String</code> object that escapes the characters that have a special meaning in regular expressions, just like with quantifier functions.</p>"},{"location":"reference/regex_dsl/#custom-types","title":"Custom types","text":"<p>The DSL comes \"batteries included\" with types that represent common text constructs:</p> <ul> <li><code>integer</code> represents an integer number as recognized by <code>int</code></li> <li><code>boolean</code> represents a boolean, \"True\" or \"False\" as recognized by <code>bool</code></li> <li><code>number</code> represents a floating-point number recognize by Python's <code>float</code></li> <li><code>date</code> represents a date as understood by <code>datetime.date</code></li> <li><code>time</code> represents a time as undestood by <code>datetime.time</code></li> <li><code>datetime</code> represents a time as understoof by <code>datetime.datetime</code></li> <li><code>digit</code> represents a single digit</li> <li><code>char</code> represents a single character</li> <li><code>newline</code> represents a new line character</li> <li><code>whitespace</code> represents a white space</li> <li><code>sentence</code> represents a sentence</li> <li><code>paragraph</code> reprensents a pagraph (one or more sentences separated by one or more ilne breaks)</li> </ul> <p>For instance you can describe the answers in the GSM8K dataset using the following pattern:</p> <pre><code>from outlines.types import sentence, digit\n\nanswer = \"A: \" + sentence.between(2,4) + \" So the answer is: \" + digit.between(1,4)\n</code></pre>"},{"location":"reference/regex_dsl/#practical-examples","title":"Practical Examples","text":""},{"location":"reference/regex_dsl/#example-1-matching-a-custom-id-format","title":"Example 1: Matching a Custom ID Format","text":"<p>Suppose you want to create a regex that matches an ID format like \"ID-12345\", where: - The literal \"ID-\" must be at the start. - Followed by exactly 5 digits.</p> <pre><code>id_pattern = \"ID-\" + Regex(r\"\\d\").exactly(5)\nprint(to_regex(id_pattern))  # Output: ID-(\\d){5}\n</code></pre>"},{"location":"reference/regex_dsl/#example-2-email-validation-with-pydantic","title":"Example 2: Email Validation with Pydantic","text":"<p>You can define a regex for email validation and use it as a type in a Pydantic model.</p> <pre><code>from pydantic import BaseModel, ValidationError\n\n# Define an email regex term (this is a simplified version)\nemail_regex = Regex(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n\nclass User(BaseModel):\n    name: str\n    email: email_regex  # Use our DSL regex as a field type\n\n# Valid input\nuser = User(name=\"Alice\", email=\"alice@example.com\")\nprint(user)\n\n# Invalid input (raises a ValidationError)\ntry:\n    User(name=\"Bob\", email=\"not-an-email\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>When used in a Pydantic model, the email field is automatically validated against the regex pattern and its JSON Schema includes the <code>pattern</code> constraint.</p>"},{"location":"reference/regex_dsl/#example-3-building-a-complex-pattern","title":"Example 3: Building a Complex Pattern","text":"<p>Consider a pattern to match a simple date format: <code>YYYY-MM-DD</code>.</p> <pre><code>year = Regex(r\"\\d\").exactly(4)         # Four digits for the year\nmonth = Regex(r\"\\d\").exactly(2)        # Two digits for the month\nday = Regex(r\"\\d\").exactly(2)          # Two digits for the day\n\n# Combine with literal hyphens\ndate_pattern = year + \"-\" + month + \"-\" + day\nprint(to_regex(date_pattern))\n# Output: (\\d){4}\\-(\\d){2}\\-(\\d){2}\n</code></pre>"},{"location":"reference/regex_dsl/#visualizing-your-pattern","title":"Visualizing Your Pattern","text":"<p>One of the unique features of this DSL is that each term can print its underlying structure as an ASCII tree. This visualization can be particularly helpful when dealing with complex expressions.</p> <pre><code># A composite pattern using concatenation and quantifiers\npattern = \"a\" + String(\"b\").one_or_more() + \"c\"\nprint(pattern)\n</code></pre> <p>Expected Output:</p> <pre><code>\u2514\u2500\u2500 Sequence\n    \u251c\u2500\u2500 String('a')\n    \u251c\u2500\u2500 KleenePlus(+)\n    \u2502   \u2514\u2500\u2500 String('b')\n    \u2514\u2500\u2500 String('c')\n</code></pre> <p>This tree representation makes it easy to see the hierarchy and order of operations in your regular expression.</p>"},{"location":"reference/regex_dsl/#final-words","title":"Final Words","text":"<p>This DSL is designed to simplify the creation and management of regular expressions\u2014whether you're validating inputs in a web API, constraining the output of an LLM, or just experimenting with regex patterns. With intuitive methods for common quantifiers and operators, clear visual feedback, and built-in integration with Pydantic, you can build robust and maintainable regex-based validations with ease.</p> <p>Feel free to explore the library further and adapt the examples to your use cases. Happy regexing!</p>"},{"location":"reference/samplers/","title":"Samplers","text":"<p>Outlines offers different sequence sampling algorithms, and we will integrate more in the future. You can read this blog post for an overview of the different sampling algorithm.</p> <p>Samplers provide control over the sampling process, allowing you to influence the output of the model. This can include controlling randomness (temperature), biasing towards certain tokens (top-k, top-p), or sequence generation (beam search).</p>"},{"location":"reference/samplers/#multinomial-sampling","title":"Multinomial sampling","text":"<p>Multinomial sampling is the default sampling algorithm in Outlines.</p> <p>As an example, suppose we have only two possible tokens: \"H\" and \"T\". For a fixed prompt such as \"Flip a coin, did you get heads or tails?\" The language model calculates probability for each token:</p> Token Probability \"H\" 0.5 \"T\" 0.5 <p>You'd expect to receive \"H\" 50% of the time and \"T\" 50% of the time.</p>"},{"location":"reference/samplers/#parameters","title":"Parameters","text":"<ul> <li><code>samples</code>: Number of samples to generate (default: 1)</li> <li><code>top_k</code>: Only consider the top k tokens (optional)</li> <li><code>top_p</code>: Only consider the top tokens with cumulative probability &gt;= p (optional)</li> <li><code>temperature</code>: Controls randomness of sampling (optional)</li> </ul>"},{"location":"reference/samplers/#default-behavior","title":"Default behavior","text":"<p>Outlines defaults to the multinomial sampler without top-p or top-k sampling, and temperature equal to 1.</p> <p>Not specifying a sampler is equivalent to:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre>"},{"location":"reference/samplers/#batching","title":"Batching","text":"<p>You can ask the generator to take multiple samples by passing the number of samples when initializing the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# [4, 4, 4]\n</code></pre> <p>If you ask multiple samples for a batch of prompts the returned array will be of shape <code>(num_samples, num_batches)</code>:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator([\"What is 2+2?\", \"What is 3+3?\"])\n\nprint(answer)\n# [[4, 4, 4], [6, 6, 6]]\n</code></pre>"},{"location":"reference/samplers/#temperature","title":"Temperature","text":"<p>You can control the temperature with</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3, temperature=0.5)\n\ngenerator = generate.text(model, sampler)\nanswer = generator([\"What is 2+2?\", \"What is 3+3?\"])\n\nprint(answer)\n</code></pre> <p>If you would like to use <code>temperature=0.0</code>, please use <code>sampler=samplers.greedy()</code> instead.</p>"},{"location":"reference/samplers/#top-k-sampling","title":"Top-k sampling","text":"<p>You can ask Outlines to only consider the top-k logits at each step by specifying the value of the <code>top-k</code> keyword argument when initializing the sampler.</p> <pre><code>sampler = samplers.multinomial(3, top_k=10)\n</code></pre>"},{"location":"reference/samplers/#top-p-sampling","title":"Top-p sampling","text":"<p>You can ask Outlines to only consider the highest probability tokens such that their cumulative probability is greater than a threshold <code>p</code>. Specify the <code>top_p</code> keyword argument when initializing the sampler:</p> <pre><code>sampler = samplers.multinomial(3, top_p=0.95)\n</code></pre>"},{"location":"reference/samplers/#greedy-sampler","title":"Greedy sampler","text":"<p>Greedy sampling selects the token with the highest probability at each step. It's deterministic and always produces the same output for a given input.</p> <p>To use the greedy sampler, initialize the generator with the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.greedy()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>You cannot ask for multiple samples with the greedy sampler since it does not clear what the result should be. Only the most likely token can be returned.</p>"},{"location":"reference/samplers/#beam-search","title":"Beam Search","text":"<p>Beam search maintains multiple candidate sequences at each step, potentially finding better overall sequences than greedy or multinomial sampling.</p> <p>To use Beam Search, initialize the generator with the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.beam_search(beams=5)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>Compatibility</p> <p>Only models from the <code>transformers</code>  and <code>exllamav2</code> libraries are compatible with Beam Search.</p>"},{"location":"reference/samplers/#parameters_1","title":"Parameters","text":"<ul> <li><code>beams</code>: Number of beams to use (default: 1)</li> </ul>"},{"location":"reference/samplers/#sampler-comparison","title":"Sampler Comparison","text":"<p>Here's a table comparing the different samplers:</p> Sampler Pros Cons Use Cases Greedy Deterministic, fast May produce repetitive text When you need consistent, predictable output Multinomial Balances exploration and exploitation Results may vary between runs General-purpose text generation, creative tasks Beam Search Can find globally better sequences More computationally expensive When sequence quality is critical, e.g., translation <p>For most use cases, we recommend using the default multinomial sampler.</p>"},{"location":"reference/text/","title":"Text generation","text":"<p>Outlines provides a unified interface to generate text with many language models, API-based and local. The same pattern is used throughout the library:</p> <ol> <li>Instantiate a generator by calling <code>outlines.generate.text</code> with the model to be used.</li> <li>Call the generator with the prompt and (optionally) some generation parameters.</li> </ol> <pre><code>from outlines import models, generate\n\nmodel = models.openai(\"gpt-4o-mini\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n</code></pre> <p>By default Outlines uses the multinomial sampler with <code>temperature=1</code>. See this section to learn how to use different samplers.</p>"},{"location":"reference/text/#streaming","title":"Streaming","text":"<p>Outlines allows you to stream the model's response by calling the <code>.stream</code> method of the generator with the prompt:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\ntokens = generator.stream(\"What is 2+2?\")\nfor token in tokens:\n    print(token)\n</code></pre>"},{"location":"reference/text/#parameters","title":"Parameters","text":""},{"location":"reference/text/#limit-the-number-of-tokens-generated","title":"Limit the number of tokens generated","text":"<p>To limit the number of tokens generated you can pass the <code>max_tokens</code> positional argument to the generator:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", 5)\nanswer = generator(\"What is 2+2?\", max_tokens=5)\n</code></pre>"},{"location":"reference/text/#stop-after-a-given-string-is-generated","title":"Stop after a given string is generated","text":"<p>You can also ask the model to stop generating text after a given string has been generated, for instance a period or a line break. You can pass a string or a line of string for the <code>stop_at</code> argument:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", stop_at=\".\")\nanswer = generator(\"What is 2+2?\", stop_at=[\".\", \"\\n\"])\n</code></pre> <p>The stopping string will be included in the response.</p>"},{"location":"reference/text/#seed-the-generation","title":"Seed the generation","text":"<p>It can be useful to seed the generation in order to get reproducible results:</p> <pre><code>import torch\nfrom outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nseed = 789001\n\nanswer = generator(\"What is 2+2?\", seed=seed)\n</code></pre>"},{"location":"reference/generation/cfg/","title":"Grammar-structured generation","text":"<p>You can pass any context-free grammar in the EBNF format and Outlines will generate an output that is valid to this grammar:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = \"\"\"\n    ?start: expression\n\n    ?expression: term ((\"+\" | \"-\") term)*\n\n    ?term: factor ((\"*\" | \"/\") factor)*\n\n    ?factor: NUMBER\n           | \"-\" factor\n           | \"(\" expression \")\"\n\n    %import common.NUMBER\n\"\"\"\n\nmodel = models.transformers(\"WizardLM/WizardMath-7B-V1.1\")\ngenerator = generate.cfg(model, arithmetic_grammar)\nsequence = generator(\n  \"Alice had 4 apples and Bob ate 2. \"\n  + \"Write an expression for Alice's apples:\"\n)\n\nprint(sequence)\n# (8-2)\n</code></pre>"},{"location":"reference/generation/cfg/#disclaimer","title":"Disclaimer","text":"<p>Experimental</p> <p>Outlines current community-contributed implementation of CFG-structured generation is experimental. This does not reflect the performance of .txt's product, where we have optimized grammar-structured generation to be as fast as regex-structured generation. Additionally, it does not fully align with the approach described in our technical report, aside from its use of incremental/partial parsing. This feature is still a work in progress, requiring performance enhancements and bug fixes for an ideal implementation. For more details, please see our grammar-related open issues on GitHub.</p> <p>Greedy</p> <p>To mitigate performance issues, CFG-structured generation will use rejection sampling and iterate over the candidate tokens highest logit first,, completing once a single valid token ID is selected. This is effectively greedy generation.</p>"},{"location":"reference/generation/cfg/#ready-to-use-grammars","title":"Ready-to-use grammars","text":"<p>Outlines contains a (small) library of grammars that can be imported and use directly. We can rewrite the previous example as:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = outlines.grammars.arithmetic\n\nmodel = models.transformers(\"WizardLM/WizardMath-7B-V1.1\")\ngenerator = generate.cfg(model, arithmetic_grammar)\nsequence = generator(\n  \"Alice had 4 apples and Bob ate 2. \"\n  + \"Write an expression for Alice's apples:\"\n)\n\nprint(sequence)\n# (8-2)\n</code></pre> <p>The following grammars are currently available:</p> <ul> <li>Arithmetic grammar via <code>outlines.grammars.arithmetic</code></li> <li>JSON grammar via <code>outlines.grammars.json</code></li> </ul> <p>If you would like more grammars to be added to the repository, please open an issue or a pull request.</p>"},{"location":"reference/generation/cfg/#grammar-guide","title":"Grammar guide","text":"<p>A grammar is a list of rules and terminals that define a language:</p> <ul> <li>Terminals define the vocabulary of the language; they may be a string, regular expression or combination of these and other terminals.</li> <li>Rules define the structure of that language; they are a list of terminals and rules.</li> </ul> <p>Outlines uses the Lark library to make Large Language Models generate text in a language of a grammar, it thus uses grammars defined in a format that Lark understands, based on the EBNF syntax. Read the Lark documentation for more details on grammar, the following is a small primer that should help get your started.</p> <p>In the following we will define a LOGO-like toy language for python's turtle library.</p>"},{"location":"reference/generation/cfg/#terminals","title":"Terminals","text":"<p>A turtle can take 4 different <code>MOVEMENT</code> move instructions: forward (<code>f</code>), backward (<code>b</code>), turn right (<code>r</code>) and turn left (<code>l</code>). It can take <code>NUMBER</code> number of steps in each direction, and draw lines in a specified <code>COLOR</code>. These define the vocabulary of our language:</p> <pre><code>MOVEMENT: \"f\"|\"b\"|\"r\"|\"l\"\nCOLOR: LETTER+\n\n%import common.LETTER\n%import common.INT -&gt; NUMBER\n%import common.WS\n%ignore WS\n</code></pre> <p>The lines that start with <code>%</code> are called \"directive\". They allow to import pre-defined terminals and rules, such as <code>LETTER</code> and <code>NUMBER</code>. <code>LETTER+</code> is a regular expressions, and indicates that a <code>COLOR</code> is made of at least one <code>LETTER</code>. The last two lines specify that we will ignore white spaces (<code>WS</code>) in the grammar.</p>"},{"location":"reference/generation/cfg/#rules","title":"Rules","text":"<p>We now need to define our rules, by decomposing instructions we can send to the turtle via our python program. At each line of the program, we can either choose a direction and execute a given number of steps, change the color used to draw the pattern. We can also choose to start filling, make a series of moves, and stop filling. We can also choose to repeat a series of move.</p> <p>We can easily write the first two rules:</p> <pre><code>instruction: MOVEMENT NUMBER   -&gt; movement\n           | \"c\" COLOR [COLOR] -&gt; change_color\n</code></pre> <p>where <code>movement</code> and <code>change_color</code> represent aliases for the rules. A whitespace implied concatenating the elements, and <code>|</code> choosing either of the elements. The <code>fill</code> and <code>repeat</code> rules are slightly more complex, since they apply to a code block, which is made of instructions. We thus define a new <code>code_block</code>  rule that refers to <code>instruction</code> and finish implementing our rules:</p> <pre><code>instruction: MOVEMENT NUMBER            -&gt; movement\n           | \"c\" COLOR [COLOR]          -&gt; change_color\n           | \"fill\" code_block          -&gt; fill\n           | \"repeat\" NUMBER code_block -&gt; repeat\n\ncode_block: \"{\" instruction \"}\"\n</code></pre> <p>We can now write the full grammar:</p> <pre><code>start: instruction+\n\ninstruction: MOVEMENT NUMBER            -&gt; movement\n            | \"c\" COLOR [COLOR]          -&gt; change_color\n            | \"fill\" code_block          -&gt; fill\n            | \"repeat\" NUMBER code_block -&gt; repeat\n\ncode_block: \"{\" instruction+ \"}\"\n\nMOVEMENT: \"f\"|\"b\"|\"l\"|\"r\"\nCOLOR: LETTER+\n\n%import common.LETTER\n%import common.INT -&gt; NUMBER\n%import common.WS\n%ignore WS\n</code></pre> <p>Notice the <code>start</code> rule, which defines the starting point of the grammar, i.e. the rule with which a program must start. This full grammars allows us to parse programs such as:</p> <pre><code>c red yellow\n    fill { repeat 36 {\n        f200 l170\n    }}\n</code></pre> <p>The result of the parse, the parse tree, can then easily be translated into a Python program that uses the <code>turtle</code> library to draw a pattern.</p>"},{"location":"reference/generation/cfg/#next-steps","title":"Next steps","text":"<p>This section provides a very brief overview of grammars and their possibilities. Check out the Lark documentation for more thorough explanations and more examples.</p>"},{"location":"reference/generation/choices/","title":"Multiple choices","text":"<p>Oultines allows you to make sure the generated text is chosen between different options:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.choice(model, [\"skirt\", \"dress\", \"pen\", \"jacket\"])\nanswer = generator(\"Pick the odd word out: skirt, dress, pen, jacket\")\n</code></pre> <p>Performance</p> <p><code>generation.choice</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate from the same list of choices several times make sure that you only call <code>generate.choice</code> once.</p>"},{"location":"reference/generation/creating_grammars/","title":"Overview","text":"<p>Outlines allows the use of Lark grammars to guide generation. These grammars are used to construct parsers that filter out incompatible tokens during the generation process The result is a generation that adheres to the grammar's production rules.</p>"},{"location":"reference/generation/creating_grammars/#primer-on-creating-grammars","title":"Primer on Creating Grammars","text":"<p>To create grammars for Outlines, a solid understanding of Lark grammars is necessary. Here's how you can get started:</p> <ul> <li>Read Lark's grammars documentations here.</li> <li>Review Outlines' existing grammars here.</li> </ul>"},{"location":"reference/generation/creating_grammars/#compatibility-with-outlines","title":"Compatibility With Outlines","text":"<p>It's important to note that not all Lark grammars work with Outlines. Changes may be necessary to ensure compatability.</p>"},{"location":"reference/generation/creating_grammars/#lalr1-parser","title":"LALR(1) Parser","text":"<p>Outlines utilizes Larks LALR(1) parser, meaning the grammar must be unambiguous at least up to the next token (one token lookahead). Read Lark's official LALR(1) parser documentation here.</p> <p>If your grammar is ambiguous, you will recieve the following error at runtime:</p> <pre><code>GrammarError: Reduce/Reduce collision in Terminal('B') between the following rules:\n</code></pre>"},{"location":"reference/generation/creating_grammars/#regex-terminal-restrictions","title":"Regex Terminal Restrictions","text":"<p>Outlines converts terminals to finite state machines using the Interegular library. Not all regular expressions work with Interegular, mitigation is described in the subsections which follow.</p>"},{"location":"reference/generation/creating_grammars/#avoid-lookarounds","title":"Avoid Lookarounds","text":"<p>Examples of removing lookaround while maintaining the same functionality</p>"},{"location":"reference/generation/creating_grammars/#example-escaped-string","title":"Example: Escaped String","text":"<p>From Outlines' modified <code>ESCAPED_STRING</code> in common.lark.</p> <p>Before: <pre><code>_STRING_INNER: /.*?/\n_STRING_ESC_INNER: _STRING_INNER /(?&lt;!\\\\)(\\\\\\\\)*?/\n\nESCAPED_STRING : \"\\\"\" _STRING_ESC_INNER \"\\\"\"\n</code></pre></p> <p>After: <pre><code>_NON_CONTROL_CHAR: /([^\"\\\\\\x00-\\x1F\\x7F-\\x9F])/\n_ESCAPED_CHAR: /\\\\/ (_NON_CONTROL_CHAR | /\\\\/ | /\"/)\nESCAPED_STRING_INNER: _NON_CONTROL_CHAR | _ESCAPED_CHAR\nESCAPED_STRING: /\"/ ESCAPED_STRING_INNER* /\"/\n</code></pre></p>"},{"location":"reference/generation/creating_grammars/#avoid-backreferences","title":"Avoid Backreferences","text":"<p>Backreferences, for example <code>([ab]^*)\\1</code>, cannot be simulated by a finite state machine, and will result in an error if used.</p>"},{"location":"reference/generation/creating_grammars/#creating-a-valid-grammar","title":"Creating a Valid Grammar","text":"<p>You can use Outlines' test suite to verify your grammar.</p>"},{"location":"reference/generation/creating_grammars/#1-create-your-grammar","title":"1) Create Your Grammar","text":"<p>Create your grammar file named <code>your_new_grammar.lark</code>, adhering to the guidelines provided above. Add it to <code>outlines/grammars/</code> (ensure attribution is included and license is compatible).</p> <p>Update <code>outlines/grammars.py</code> with a line including your grammar.</p>"},{"location":"reference/generation/creating_grammars/#2-test-your-grammar","title":"2) Test Your Grammar","text":"<p>Test grammar for false negatives, ensure sample grammars can be generated: - Add valid example outputs which are compliant with the grammar to <code>tests/benchmark/cfg_samples/your_new_grammar/</code> - Run the tests for your grammar via <code>pytest -s tests/fsm/test_cfg_guide.py::test_cfg_grammar_sample -k \"your_new_grammar\"</code></p> <p>Test grammar for false positives, ensure invalid outputs aren't generated.</p> <p>Currently there isn't a builtin false positive testing utility. It is recommended you smoke test via <pre><code>from outlines import models, generate, grammars\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.cfg(model, grammars.your_new_grammar)\nresult = generator(&lt;your prompt to generate output for your grammar&gt;)\nprint(result)\n</code></pre></p>"},{"location":"reference/generation/creating_grammars/#converting","title":"Converting","text":"<p>There are a few tools available for converting from other grammars to lark. These tools serve as a starting point. However, you will typically need to make additional adjustments to ensure full compatibility and proper functioning within Outlines.</p> <p>Tools: - Larks built in \"Nearley-to-Lark\" converter https://lark-parser.readthedocs.io/en/latest/tools.html - Convert ANTLR4 to Lark (Note, most antlr4 grammars are not LALR(1) compatible, so will require additional tweaking) https://github.com/kaby76/Domemtech.Trash/blob/main/src/trconvert/readme.md - Extract EBNF from Yacc files https://www.bottlecaps.de/rr/ui</p> <p>Reference Grammars: - Github Lark Grammars https://github.com/search?q=path%3A.lark&amp;type=code - Github Nearley Grammars https://github.com/search?q=path%3A.ne+%22-%3E%22&amp;type=code - Antlr4 grammars https://github.com/antlr/grammars-v4/ - Grammar zoo https://slebok.github.io/zoo/index.html#html</p>"},{"location":"reference/generation/custom_fsm_ops/","title":"Custom FSM Operations","text":"<p>Outlines is fast because it compiles regular expressions into an index ahead of inference. To do so we use the equivalence between regular expressions and Finite State Machines (FSMs), and the library interegular to perform the translation.</p> <p>Alternatively, one can pass a FSM built using <code>integular</code> directly to structure the generation.</p>"},{"location":"reference/generation/custom_fsm_ops/#example","title":"Example","text":""},{"location":"reference/generation/custom_fsm_ops/#using-the-difference-operation","title":"Using the <code>difference</code> operation","text":"<p>In the following example we build a fsm which recognizes only the strings valid to the first regular expression but not the second. In particular, it will prevent the words \"pink\" and \"elephant\" from being generated:</p> <pre><code>import interegular\nfrom outlines import models, generate\n\n\nlist_of_strings_pattern = \"\"\"\\[\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\]\"\"\"\npink_elephant_pattern = \"\"\".*(pink|elephant).*\"\"\"\n\nlist_of_strings_fsm = interegular.parse_pattern(list_of_strings_pattern).to_fsm()\npink_elephant_fsm = interegular.parse_pattern(pink_elephant_pattern).to_fsm()\n\ndifference_fsm = list_of_strings_fsm - pink_elephant_fsm\n\ndifference_fsm_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# False\ndifference_fsm_fsm.accepts('[\"a\",\"blue\",\"donkey\"]')\n# True\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.fsm(model, difference_fsm)\nresponse = generator(\"Don't talk about pink elephants\")\n</code></pre> <p>To see the other operations available, consult interegular's documentation.</p>"},{"location":"reference/generation/format/","title":"Type constraints","text":"<p>We can ask completions to be restricted to valid python types:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.format(model, int)\nanswer = generator(\"When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\")\nprint(answer)\n# 67\n</code></pre> <p>The following types are currently available:</p> <ul> <li>int</li> <li>float</li> <li>bool</li> <li>datetime.date</li> <li>datetime.time</li> <li>datetime.datetime</li> <li>We also provide custom types</li> </ul>"},{"location":"reference/generation/generation/","title":"Generation","text":"<p>Once an Outlines model is constructed you can use <code>outlines.generate</code> to generate text. Standard LLM generation is possible via <code>outlines.generate.text</code>, along with a variety of structured generation methods described below. (For a detailed technical explanation of how structured generation works, you may review the Structured Generation Explanation page)</p> <p>Before generating text, you must construct an <code>outlines.model</code>. Example:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\n</code></pre>"},{"location":"reference/generation/generation/#text-generator","title":"Text generator","text":"<pre><code>generator = outlines.generate.text(model)\n\nresult = generator(\"Question: What's 2+2? Answer:\", max_tokens=100)\nprint(result)\n# The answer is 4\n\n# Outlines also supports streaming output\nstream = generator.stream(\"What's 2+2?\", max_tokens=4)\nfor i in range(5):\n    token = next(stream)\n    print(repr(token))\n# '2'\n# '+'\n# '2'\n# ' equals'\n# '4'\n</code></pre>"},{"location":"reference/generation/generation/#multi-label-classification","title":"Multi-label classification","text":"<p>Outlines allows you to do multi-label classification by guiding the model so it can only output either of the specified choices:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.choice(model, [\"Blue\", \"Red\", \"Yellow\"])\n\ncolor = generator(\"What is the closest color to Indigo? \")\nprint(color)\n# Blue\n</code></pre>"},{"location":"reference/generation/generation/#json-structured-generation","title":"JSON-structured generation","text":"<p>Outlines can guide models so that they output valid JSON 100% of the time. You can either specify the structure using Pydantic or a string that contains a JSON Schema:</p> PydanticJSON Schema <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr, conint\n\nimport outlines\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: conint(gt=18, lt=99)\n    armor: Armor\n    strength: conint(gt=1, lt=100)\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.json(model, Character)\n\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# name='Orla' age=21 armor=&lt;Armor.plate: 'plate'&gt; strength=8\n</code></pre> <pre><code>import outlines\n\nschema = \"\"\"{\n    \"$defs\": {\n        \"Armor\": {\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"title\": \"Armor\",\n            \"type\": \"string\"\n        }\n    },\n    \"properties\": {\n        \"name\": {\"maxLength\": 10, \"title\": \"Name\", \"type\": \"string\"},\n        \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n        \"armor\": {\"$ref\": \"#/$defs/Armor\"},\n        \"strength\": {\"title\": \"Strength\", \"type\": \"integer\"}\\\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"strength\"],\n    \"title\": \"Character\",\n    \"type\": \"object\"\n}\"\"\"\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.json(model, schema)\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# {'name': 'Yuki', 'age': 24, 'armor': 'plate', 'strength': 3}\n</code></pre> <p>Note</p> <p>We advise you to constrain the length of the strings fields when first testing your schema, especially with small models.</p>"},{"location":"reference/generation/generation/#grammar-structured-generation","title":"Grammar-structured generation","text":"<p>Outlines also allows to generate text that is valid to any context-free grammar (CFG) in the EBNF format. Grammars can be intimidating, but they are a very powerful tool! Indeed, they determine the syntax of every programming language, valid chess moves, molecule structure, can help with procedural graphics generation, etc.</p> <p>Here we show a simple example of a grammar that defines arithmetic operations:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = \"\"\"\n    ?start: sum\n\n    ?sum: product\n        | sum \"+\" product   -&gt; add\n        | sum \"-\" product   -&gt; sub\n\n    ?product: atom\n        | product \"*\" atom  -&gt; mul\n        | product \"/\" atom  -&gt; div\n\n    ?atom: NUMBER           -&gt; number\n         | \"-\" atom         -&gt; neg\n         | \"(\" sum \")\"\n\n    %import common.NUMBER\n    %import common.WS_INLINE\n\n    %ignore WS_INLINE\n\"\"\"\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.cfg(model, arithmetic_grammar, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>EBNF grammars can be cumbersome to write. This is why Outlines provides grammar definitions in the <code>outlines.grammars.</code> module</p> <pre><code>from outlines import models, generate, grammars\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.cfg(model, grammars.arithmetic, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>The available grammars are listed here.</p>"},{"location":"reference/generation/generation/#regex-structured-generation","title":"Regex-structured generation","text":"<p>Slightly simpler, but no less useful, Outlines can generate text that is in the language of a regular expression. For instance to force the model to generate IP addresses:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\n\nregex_str = r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\"\ngenerator = generate.regex(model, regex_str)\n\nresult = generator(\"What is the IP address of localhost?\\nIP: \")\nprint(result)\n# 127.0.0.100\n</code></pre>"},{"location":"reference/generation/generation/#generate-a-given-python-type","title":"Generate a given Python type","text":"<p>We provide a shortcut to regex-structured generation for simple use cases. Pass a Python type to the <code>outlines.generate.format</code> function and the LLM will output text that matches this type:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.format(model, int)\n\nresult = generator(\"What is 2+2?\")\nprint(result)\n# 4\n</code></pre>"},{"location":"reference/generation/json/","title":"JSON structured generation","text":"<p>Outlines can make any open source model return a JSON object that follows a structure that is specified by the user. This is useful whenever we want the output of the model to be processed by code downstream: code does not understand natural language but rather the structured language it has been programmed to understand.</p> <p>There are mostly two reasons why someone would want to get an output formatted as JSON from a LLM:</p> <ol> <li>Parse the answer (e.g. with Pydantic), store it somewhere, return it to a user, etc.</li> <li>Call a function with the result</li> </ol> <p>Outlines has you covered in both cases! Indeed, to define the structure of the JSON you want the model to follow you can either provide a Pydantic model, or a function. No need to duplicate code!</p>"},{"location":"reference/generation/json/#using-pydantic","title":"Using Pydantic","text":"<p>Outlines can infer the structure of the output from a Pydantic model. The result is an instance of the model that contains the values returned by the LLM:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate\n\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, User)\nresult = generator(\n    \"Create a user profile with the fields name, last_name and id\"\n)\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre> <p>JSON and whitespaces</p> <p>By default Outlines prevents the model from generating json with syntactic newlines, tabs, or multiple spaces. The default <code>whitespace_pattern</code> is <code>r\"[ ]?\"</code>. Small models tend to enter an infinite repetition loop if the <code>whitespace_pattern</code> allows infinite spacing. If you would like to allow the model to generate multiple tabs, newlines, and spaces, you can set the whitespace pattern as follows:</p> <pre><code>generator = generate.json(model, User, whitespace_pattern=r\"[\\n\\t ]*\")\n</code></pre> <p>Performance</p> <p><code>generation.json</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate several times with the same schema make sure that you only call <code>generate.json</code> once.</p> <p>Custom types</p> <p>Outlines provides custom Pydantic types so you do not have to write regular expressions for common types, such as phone numbers or zip codes.</p>"},{"location":"reference/generation/json/#using-a-json-schema","title":"Using a JSON Schema","text":"<p>Instead of a Pydantic model you can pass a string that represents a JSON Schema specification to <code>generate.json</code>:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models\nfrom outlines import generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nschema = \"\"\"\n{\n  \"title\": \"User\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": {\"type\": \"string\"},\n    \"last_name\": {\"type\": \"string\"},\n    \"id\": {\"type\": \"integer\"}\n  },\n  \"required\": [\"name\", \"last_name\", \"id\"]\n}\n\"\"\"\n\ngenerator = generate.json(model, schema)\nresult = generator(\n    \"Create a user profile with the fields name, last_name and id\"\n)\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre>"},{"location":"reference/generation/json/#from-a-functions-signature","title":"From a function's signature","text":"<p>Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax <code>**</code>:</p> <pre><code>from outlines import models\nfrom outlines import generate\n\ndef add(a: int, b: int):\n    return a + b\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, add)\nresult = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n\nprint(add(**result))\n# 3\n</code></pre> <p>A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function's definition. No need to change the code at several places!</p>"},{"location":"reference/generation/json/#from-a-dynamic-json-schema-builder-genson","title":"From a dynamic JSON schema builder - GenSON","text":"<p>Outlines integrated GenSON builders to be able to dynamicly declare JSON schemas. It can be used as follow:</p> <pre><code>from genson import SchemaBuilder\n\nfrom outlines import models\nfrom outlines import generate\n\nbuilder = SchemaBuilder()\nbuilder.add_schema({\"type\": \"object\", \"properties\": {}})\nbuilder.add_object({\"name\": \"Toto\", \"age\": 5})\n\nmodel = models.transformers(\n    \"HuggingFaceTB/SmolLM2-135M\",\n    device=\"auto\",\n)\ngenerator = generate.json(model, builder)\n\nres = generator(\"Return a json of a young boy\")\nprint(res)\n# {\"name\": \"Ben\", \"age\": 10}\n</code></pre> <p>Anytime you are updating the schema through the builder, you need to redifine the outline generator to include these changes. From the the previous example:</p> <pre><code>from genson import SchemaBuilder\n\nfrom outlines import models\nfrom outlines import generate\n\nbuilder = SchemaBuilder()\nbuilder.add_schema({\"type\": \"object\", \"properties\": {}})\nbuilder.add_object({\"name\": \"Toto\", \"age\": 5})\n\nmodel = models.transformers(\n    \"HuggingFaceTB/SmolLM2-135M\",\n    device=\"auto\",\n)\ngenerator = generate.json(model, builder)\n\nres = generator(\"Return a json of a young boy\")\nprint(res)\n# {\"name\": \"Ben\", \"age\": 10}\n\nbuilder.add_object({\"hobby\": \"sports\"})\ngenerator = generate.json(model, builder)\n\nres = generator(\"Return a json of a youg boy whose hobby is coding\")\nprint(res)\n# {\"name\": \"Ben\", \"age\": 10, \"hobby\": \"coding\"}\n</code></pre> <p>Note</p> <p>Beware of GenSON's behavior regarding dynamic amending of schemas through their builder. Here is an example of how you could lose <code>required</code> informations and generate json with missing fields:</p> <pre><code>builder = SchemaBuilder()\nbuilder.add_schema({\"type\": \"object\", \"properties\": {}})\nbuilder.add_object({\"name\": \"Toto\", \"age\": 5})\n\nprint(builder.to_schema())\n# {'$schema': 'http://json-schema.org/schema#', 'type': 'object', 'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}}, 'required': ['age', 'name']}\n\nbuilder.add_object({\"hobby\": \"sport\"})\nprint(builder.to_schema())\n# {'name': {'type': 'string'}, 'age': {'type': 'integer'}, 'hobby': {'type': 'string'}}}\n</code></pre>"},{"location":"reference/generation/regex/","title":"Regular expressions","text":"<p>Outlines can guarantee that the text generated by the LLM will be valid to a regular expression:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\ngenerator = generate.regex(\n    model,\n    r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\",\n)\n\nprompt = \"What is the IP address of the Google DNS servers? \"\nanswer = generator(prompt, max_tokens=30)\n\nprint(answer)\n# What is the IP address of the Google DNS servers?\n# 2.2.6.1\n</code></pre> <p>If you find yourself using <code>generate.regex</code> to restrict the answers' type you can take a look at type-structured generation instead.</p> <p>Performance</p> <p><code>generate.regex</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate several times using the same regular expression make sure that you only call <code>generate.regex</code> once.</p>"},{"location":"reference/generation/structured_generation_explanation/","title":"How does Outlines work?","text":"<p>Language models generate text token by token, using the previous token sequence as input and sampled logits as output. This document explains the structured generation process, where only legal tokens are considered for the next step based on a predefined automata, e.g. a regex-defined finite-state machine (FSM) or Lark grammar.`</p>"},{"location":"reference/generation/structured_generation_explanation/#worked-example","title":"Worked Example","text":"<p>Let's consider a worked example with a pattern for whole and decimal numbers:</p> <p><code>^\\d*(\\.\\d+)?$</code>.</p>"},{"location":"reference/generation/structured_generation_explanation/#creating-automata","title":"Creating Automata","text":"<p>The pattern is first converted into an automata. Below is a brief explanation of the automata conversion and its representation.</p> <p>Automata Diagram:</p> <pre><code>graph LR\n    node0(\"1-9\") --&gt; node1(\"1-9\")\n    node1 --&gt; node1\n    node1 --&gt; nodeEND{{END}}\n    node1 --&gt; nodePeriod(\".\")\n    nodePeriod --&gt; node2(\"1-9\")\n    node2 --&gt; node2\n    node2 --&gt; nodeEND{{END}}</code></pre>"},{"location":"reference/generation/structured_generation_explanation/#generating-a-token","title":"Generating a Token","text":"<p>Let's assume that we're in the middle of generation, and so far \"748\" has been generated. Here is the automata with the current state highlighted in green, with the legal next characters being another number (1-9), a dot (.), or end of sequence.</p> <pre><code>graph LR\n    node0(\"1-9\") --&gt; node1(\"1-9\")\n    node1 --&gt; node1\n    node1 --&gt; nodeEND{{END}}\n    node1 --&gt; nodePeriod(\".\")\n    nodePeriod --&gt; node2(\"1-9\")\n    node2 --&gt; node2\n    node2 --&gt; nodeEND{{END}}\n\n    style node1 fill:#090</code></pre> <p>Generating a token requires the following steps:</p> <ul> <li>Feed the previous input sequence (\"748\") into the language model.</li> <li>Language model runs a forward pass and produces token logits.</li> <li>Outlines logits processor sets the probability of illegal tokens to 0%.</li> <li>A token is sampled from the set of legal tokens.</li> </ul> <p></p>"},{"location":"reference/generation/types/","title":"Custom types","text":"<p>Outlines provides custom Pydantic types so you can focus on your use case rather than on writing regular expressions:</p> Category Type Import Description ISBN 10 &amp; 13 <code>outlines.types.ISBN</code> There is no guarantee that the check digit will be correct Airport IATA <code>outlines.types.airports.IATA</code> Valid airport IATA codes Country alpha-2 code <code>outlines.types.airports.Alpha2</code> Valid country alpha-2 codes alpha-3 code <code>outlines.types.countries.Alpha3</code> Valid country alpha-3 codes numeric code <code>outlines.types.countries.Numeric</code> Valid country numeric codes name <code>outlines.types.countries.Name</code> Valid country names flag <code>outlines.types.countries.Flag</code> Valid flag emojis email <code>outlines.types.Email</code> Valid email address <p>Some types require localization. We currently only support US types, but please don't hesitate to create localized versions of the different types and open a Pull Request. Localized types are specified using <code>types.locale</code> in the following way:</p> <pre><code>from outlines import types\n\ntypes.locale(\"us\").ZipCode\ntypes.locale(\"us\").PhoneNumber\n</code></pre> <p>Here are the localized types that are currently available:</p> Category Locale Import Description Zip code US <code>ZipCode</code> Generate US Zip(+4) codes Phone number US <code>PhoneNumber</code> Generate valid US phone numbers <p>You can use these types in Pydantic schemas for JSON-structured generation:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate, types\n\n# Specify the locale for types\nlocale = types.locale(\"us\")\n\nclass Client(BaseModel):\n    name: str\n    phone_number: locale.PhoneNumber\n    zip_code: locale.ZipCode\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, Client)\nresult = generator(\n    \"Create a client profile with the fields name, phone_number and zip_code\"\n)\nprint(result)\n# name='Tommy' phone_number='129-896-5501' zip_code='50766'\n</code></pre> <p>Or simply with <code>outlines.generate.format</code>:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate, types\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.format(model, types.locale(\"us\").PhoneNumber)\nresult = generator(\n    \"Return a US Phone number: \"\n)\nprint(result)\n# 334-253-2630\n</code></pre> <p>We plan on adding many more custom types. If you have found yourself writing regular expressions to generate fields of a given type, or if you could benefit from more specific types don't hesite to submit a PR or open an issue.</p>"},{"location":"reference/models/exllamav2/","title":"ExllamaV2","text":"<p>The <code>outlines.models.exllamav2</code> model requires a Logits Processor component for compatibility with Outlines structured generation. While ExLlamaV2 doesn't natively support this feature, a third-party fork provides the necessary functionality. You can install it with the following command:</p> <pre><code>pip install git+https://github.com/lapp0/exllamav2@sampler-logits-processor\n</code></pre> <p>Install other requirements:</p> <pre><code>pip install transformers torch\n</code></pre> <p>Coming soon</p>"},{"location":"reference/models/llamacpp/","title":"Llama.cpp","text":"<p>Outlines provides an integration with Llama.cpp using the llama-cpp-python library. Llamacpp allows to run quantized models on machines with limited compute.</p> <p>Installation</p> <p>You need to install the <code>llama-cpp-python</code> library to use the llama.cpp integration. See the installation section for instructions to install <code>llama-cpp-python</code> with CUDA, Metal, ROCm and other backends. To get started quickly you can also run:</p> <pre><code>pip install \"outlines[llamacpp]\"\n</code></pre>"},{"location":"reference/models/llamacpp/#load-the-model","title":"Load the model","text":"<p>You can initialize the model by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern):</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\n</code></pre> <p>This will download the model files to the hub cache folder and load the weights in memory.</p> <p>You can also initialize the model by passing the path to the weights on your machine. Assuming Phi2's weights are in the current directory:</p> <pre><code>from outlines import models\nfrom llama_cpp import Llama\n\nllm = Llama(\"./phi-2.Q4_K_M.gguf\")\nmodel = models.LlamaCpp(llm)\n</code></pre> <p>If you need more control, you can pass the same keyword arguments to the model as you would pass in the llama-ccp-library:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\n    \"TheBloke/phi-2-GGUF\",\n    \"phi-2.Q4_K_M.gguf\"\n    n_ctx=512,  # to set the context length value\n)\n</code></pre> <p>Main parameters:</p> Parameters Type Description Default <code>n_gpu_layers</code> <code>int</code> Number of layers to offload to GPU. If -1, all layers are offloaded <code>0</code> <code>split_mode</code> <code>int</code> How to split the model across GPUs. <code>1</code> for layer-wise split, <code>2</code> for row-wise split <code>1</code> <code>main_gpu</code> <code>int</code> Main GPU <code>0</code> <code>tensor_split</code> <code>Optional[List[float]]</code> How split tensors should be distributed across GPUs. If <code>None</code> the model is not split. <code>None</code> <code>n_ctx</code> <code>int</code> Text context. Inference from the model if set to <code>0</code> <code>0</code> <code>n_threads</code> <code>Optional[int]</code> Number of threads to use for generation. All available threads if set to <code>None</code>. <code>None</code> <code>verbose</code> <code>bool</code> Print verbose outputs to <code>stderr</code> <code>False</code> <p>See the llama-cpp-python documentation for the full list of parameters.</p>"},{"location":"reference/models/llamacpp/#load-the-model-on-gpu","title":"Load the model on GPU","text":"<p>Note</p> <p>Make sure that you installed <code>llama-cpp-python</code> with GPU support.</p> <p>To load the model on GPU, pass <code>n_gpu_layers=-1</code>:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\n    \"TheBloke/phi-2-GGUF\",\n    \"phi-2.Q4_K_M.gguf\",\n    n_gpu_layers=-1,  # to use GPU acceleration\n)\n</code></pre> <p>This also works with generators built with <code>generate.regex</code>, <code>generate.json</code>, <code>generate.cfg</code>, <code>generate.format</code> and <code>generate.choice</code>.</p>"},{"location":"reference/models/llamacpp/#load-lora-adapters","title":"Load LoRA adapters","text":"<p>You can load LoRA adapters dynamically:</p> <pre><code>from outlines import models, generate\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\ngenerator = generate.text(model)\nanswer_1 = generator(\"prompt\")\n\nmodel.load_lora(\"./path/to/adapter.gguf\")\nanswer_2 = generator(\"prompt\")\n</code></pre> <p>To load another adapter you need to re-initialize the model. Otherwise the adapter will be added on top of the previous one:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\nmodel.load_lora(\"./path/to/adapter1.gguf\")  # Load first adapter\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\nmodel.load_lora(\"./path/to/adapter2.gguf\")  # Load second adapter\n</code></pre>"},{"location":"reference/models/llamacpp/#generate-text","title":"Generate text","text":"<p>In addition to the parameters described in the text generation section you can pass extra keyword arguments, for instance to set sampling parameters not exposed in Outlines' public API:</p> <pre><code>from outlines import models, generate\n\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\ngenerator = generate.text(model)\n\nanswer = generator(\"A prompt\", presence_penalty=0.8)\n</code></pre> <p>Extra keyword arguments:</p> <p>The value of the keyword arguments you pass to the generator suspersede the values set when initializing the sampler or generator. All extra sampling methods and repetition penalties are disabled by default.</p> Parameters Type Description Default <code>suffix</code> <code>Optional[str]</code> A suffix to append to the generated text. If <code>None</code> no suffix is added. <code>None</code> <code>echo</code> <code>bool</code> Whether to preprend the prompt to the completion. <code>False</code> <code>seed</code> <code>int</code> The random seed to use for sampling. <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> The maximum number of tokens to generate. If <code>None</code> the maximum number of tokens depends on <code>n_ctx</code>. <code>16</code> <code>frequence_penalty</code> <code>float</code> The penalty to apply to tokens based on their frequency in the past 64 tokens. <code>0.0</code> <code>presence_penalty</code> <code>float</code> The penalty to apply to tokens based on their presence in the past 64 tokens. <code>0.0</code> <code>repeat_penalty</code> <code>float</code> The penalty to apply to repeated tokens in the past 64 tokens. <code>1.</code> <code>stopping_criteria</code> <code>Optional[StoppingCriteriaList]</code> A list of stopping criteria to use. <code>None</code> <code>logits_processor</code> <code>Optional[LogitsProcessorList]</code> A list of logits processors to use. The logits processor used for structured generation will be added to this list. <code>None</code> <code>temperature</code> <code>float</code> The temperature to use for sampling <code>1.0</code> <code>top_p</code> <code>float</code> The top-p value to use for nucleus sampling. <code>1.</code> <code>min_p</code> <code>float</code> The min-p value to use for minimum-p sampling. <code>0.</code> <code>typical_p</code> <code>float</code> The p value to use for locally typical sampling. <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> A list of strings that stop generation when encountered. <code>[]</code> <code>top_k</code> <code>int</code> The top-k value used for top-k sampling. Negative value to consider all logit values. <code>-1.</code> <code>tfs_z</code> <code>float</code> The tail-free sampling parameter. <code>1.0</code> <code>mirostat_mode</code> <code>int</code> The mirostat sampling mode. <code>0</code> <code>mirostat_tau</code> <code>float</code> The target cross-entropy for mirostat sampling. <code>5.0</code> <code>mirostat_eta</code> <code>float</code> The learning rate used to update <code>mu</code> in mirostat sampling. <code>0.1</code> <p>See the llama-cpp-python documentation for the full and up-to-date list of parameters and the llama.cpp code for the default values of other sampling parameters.</p>"},{"location":"reference/models/llamacpp/#streaming","title":"Streaming","text":""},{"location":"reference/models/llamacpp/#installation","title":"Installation","text":"<p>You need to install the <code>llama-cpp-python</code> library to use the llama.cpp integration.</p>"},{"location":"reference/models/llamacpp/#cpu","title":"CPU","text":"<p>For a CPU-only installation run:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>Warning</p> <p>Do not run this command if you want support for BLAS, Metal or CUDA. Follow the instructions below instead.</p>"},{"location":"reference/models/llamacpp/#cuda","title":"CUDA","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n</code></pre> <p>It is also possible to install pre-built wheels with CUDA support (Python 3.10 and above):</p> <pre><code>pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/&lt;cuda-version&gt;\n</code></pre> <p>Where <code>&lt;cuda-version&gt;</code> is one of the following, depending on the version of CUDA installed on your system:</p> <ul> <li><code>cu121</code> for CUDA 12.1</li> <li><code>cu122</code> for CUDA 12.2</li> <li><code>cu123</code> CUDA 12.3</li> </ul>"},{"location":"reference/models/llamacpp/#metal","title":"Metal","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n</code></pre> <p>It is also possible to install pre-build wheels with Metal support (Python 3.10 or above, MacOS 11.0 and above):</p> <pre><code>pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal\n</code></pre>"},{"location":"reference/models/llamacpp/#openblas","title":"OpenBLAS","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n</code></pre>"},{"location":"reference/models/llamacpp/#other-backend","title":"Other backend","text":"<p><code>llama.cpp</code> supports many other backends. Refer to the llama.cpp documentation to use the following backends:</p> <ul> <li>CLBast (OpenCL)</li> <li>hipBLAS (ROCm)</li> <li>Vulkan</li> <li>Kompute</li> <li>SYCL</li> </ul>"},{"location":"reference/models/mlxlm/","title":"mlx-lm","text":"<p>Outlines provides an integration with mlx-lm, allowing models to be run quickly on Apple Silicon via the mlx library.</p> <p>Installation</p> <p>You need to install the <code>mlx</code> and <code>mlx-lm</code> libraries on a device which supports Metal to use the mlx-lm integration. To get started quickly you can also run:</p> <pre><code>pip install \"outlines[mlxlm]\"\n</code></pre>"},{"location":"reference/models/mlxlm/#load-the-model","title":"Load the model","text":"<p>You can initialize the model by passing the name of the repository on the HuggingFace Hub. The official repository for mlx-lm supported models is mlx-community.</p> <pre><code>from outlines import models\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\n</code></pre> <p>This will download the model files to the hub cache folder and load the weights in memory.</p> <p>The arguments <code>model_config</code> and <code>tokenizer_config</code> are available to modify loading behavior. For example, per the <code>mlx-lm</code> documentation, you must set an eos_token for <code>qwen/Qwen-7B</code>. In outlines you may do so via</p> <pre><code>model = models.mlxlm(\n    \"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\",\n    tokenizer_config={\"eos_token\": \"&lt;|endoftext|&gt;\", \"trust_remote_code\": True},\n)\n</code></pre> <p>Main parameters:</p> <p>(Subject to change. Table based on mlx-lm.load docstring)</p> Parameters Type Description Default <code>tokenizer_config</code> <code>dict</code> Configuration parameters specifically for the tokenizer. Defaults to an empty dictionary. <code>{}</code> <code>model_config</code> <code>dict</code> Configuration parameters specifically for the model. Defaults to an empty dictionary. <code>{}</code> <code>adapter_path</code> <code>str</code> Path to the LoRA adapters. If provided, applies LoRA layers to the model. <code>None</code> <code>lazy</code> <code>bool</code> If False, evaluate the model parameters to make sure they are loaded in memory before returning. <code>False</code>"},{"location":"reference/models/mlxlm/#generate-text","title":"Generate text","text":"<p>You may generate text using the parameters described in the text generation documentation.</p> <p>With the loaded model, you can generate text or perform structured generation, e.g.</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\ngenerator = generate.text(model)\n\nanswer = generator(\"A prompt\", temperature=2.0)\n</code></pre>"},{"location":"reference/models/mlxlm/#streaming","title":"Streaming","text":"<p>You may creating a streaming iterable with minimal changes</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\ngenerator = generate.text(model)\n\nfor token_str in generator.text(\"A prompt\", temperature=2.0):\n    print(token_str)\n</code></pre>"},{"location":"reference/models/mlxlm/#structured","title":"Structured","text":"<p>You may perform structured generation with mlxlm to guarantee your output will match a regex pattern, json schema, or lark grammar.</p> <p>Example: Phone number generation with pattern <code>\"\\\\+?[1-9][0-9]{7,14}\"</code>:</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\n\nphone_number_pattern = \"\\\\+?[1-9][0-9]{7,14}\"\ngenerator = generate.regex(model, phone_number_pattern)\n\nmodel_output = generator(\"What's Jennys Number?\\n\")\nprint(model_output)\n# '8675309'\n</code></pre>"},{"location":"reference/models/models/","title":"Models","text":"<p>Outlines supports generation using a number of inference engines (<code>outlines.models</code>). Loading a model using outlines follows a similar interface between inference engines:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\nmodel = outlines.models.transformers_vision(\"llava-hf/llava-v1.6-mistral-7b-hf\")\nmodel = outlines.models.vllm(\"microsoft/Phi-3-mini-128k-instruct\")\nmodel = outlines.models.llamacpp(\n    \"microsoft/Phi-3-mini-4k-instruct-gguf\", \"Phi-3-mini-4k-instruct-q4.gguf\"\n)\nmodel = outlines.models.exllamav2(\"bartowski/Phi-3-mini-128k-instruct-exl2\")\nmodel = outlines.models.mlxlm(\"mlx-community/Phi-3-mini-4k-instruct-4bit\")\n\nmodel = outlines.models.openai(\n    \"gpt-4o-mini\",\n    api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n</code></pre>"},{"location":"reference/models/models/#feature-matrix","title":"Feature Matrix","text":"Transformers Transformers Vision vLLM llama.cpp ExLlamaV2 MLXLM OpenAI* Device Cuda \u2705 \u2705 \u2705 \u2705 \u2705 \u274c N/A Apple Silicon \u2705 \u2705 \u274c \u2705 \u2705 \u2705 N/A x86 / AMD64 \u2705 \u2705 \u274c \u2705 \u2705 \u274c N/A Sampling Greedy \u2705 \u2705 \u2705 \u2705* \u2705 \u2705 \u274c Multinomial \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Multiple Samples \u2705 \u2705 \u274c \u274c \u2705 Beam Search \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u274c Generation Batch \u2705 \u2705 \u2705 \u274c ? \u274c \u274c Stream \u2705 \u274c \u274c \u2705 ? \u2705 \u274c Text \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Structured \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 JSON Schema \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Choice \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Regex \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Grammar \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c"},{"location":"reference/models/models/#caveats","title":"Caveats","text":"<ul> <li>OpenAI doesn't support structured generation due to limitations in their API and server implementation.</li> <li><code>outlines.generate</code> \"Structured\" includes methods such as <code>outlines.generate.regex</code>, <code>outlines.generate.json</code>, <code>outlines.generate.cfg</code>, etc.</li> <li>MLXLM only supports Apple Silicon.</li> <li>llama.cpp greedy sampling available via multinomial with <code>temperature = 0.0</code>.</li> </ul>"},{"location":"reference/models/openai/","title":"OpenAI and compatible APIs","text":"<p>Installation</p> <p>You need to install the <code>openai</code> library to be able to use the OpenAI API in Outlines. Or alternatively:</p> <pre><code>pip install \"outlines[openai]\"\n</code></pre>"},{"location":"reference/models/openai/#openai-models","title":"OpenAI models","text":"<p>Outlines supports models available via the OpenAI Chat API, e.g. GPT-4o, ChatGPT and GPT-4. You can initialize the model by passing the model name to <code>outlines.models.openai</code>:</p> <pre><code>from outlines import models\n\n\nmodel = models.openai(\"gpt-4o-mini\")\nmodel = models.openai(\"gpt-4o\")\n</code></pre> <p>Check the OpenAI documentation for an up-to-date list of available models. You can pass any parameter you would pass to <code>openai.AsyncOpenAI</code> as keyword arguments:</p> <pre><code>import os\nfrom outlines import models\n\n\nmodel = models.openai(\n    \"gpt-4o-mini\",\n    api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n</code></pre> <p>The following table enumerates the possible parameters. Refer to the OpenAI SDK's code for an up-to-date list.</p> <p>Parameters:</p> Parameters Type Description Default <code>api_key</code> <code>str</code> OpenAI API key. Infered from <code>OPENAI_API_KEY</code> if not specified <code>None</code> <code>organization</code> <code>str</code> OpenAI organization id. Infered from <code>OPENAI_ORG_ID</code> if not specified <code>None</code> <code>project</code> <code>str</code> OpenAI project id. Infered from <code>OPENAI_PROJECT_ID</code> if not specified. <code>None</code> <code>base_url</code> <code>str | https.URL</code> Base URL for the endpoint. Infered from <code>OPENAI_BASE_URL</code> if no specified. <code>None</code> <code>timeout</code> <code>float</code> Request timeout. <code>NOT_GIVEN</code> <code>max_retries</code> <code>int</code> Maximum number of retries for failing requests <code>2</code> <code>default_headers</code> <code>Mapping[str, str]</code> Default HTTP headers <code>None</code> <code>default_query</code> <code>Mapping[str, str]</code> Custom parameters added to the HTTP queries <code>None</code> <code>http_client</code> <code>https.AsyncClient</code> User-specified <code>httpx</code> client <code>None</code>"},{"location":"reference/models/openai/#azure-openai-models","title":"Azure OpenAI models","text":"<p>Outlines also supports Azure OpenAI models:</p> <pre><code>from outlines import models\n\n\nmodel = models.azure_openai(\n    \"azure-deployment-name\",\n    \"gpt-4o-mini\",\n    api_version=\"2024-07-18\",\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n</code></pre> <p>Why do I need to specify model and deployment name?</p> <p>The model name is needed to load the correct tokenizer for the model. The tokenizer is necessary for structured generation.</p> <p>You can pass any parameter you would pass to <code>openai.AsyncAzureOpenAI</code>. You can consult the OpenAI SDK's code for an up-to-date list.</p> <p>Parameters:</p> Parameters Type Description Default <code>azure_endpoint</code> <code>str</code> Azure endpoint, including the resource. Infered from <code>AZURE_OPENAI_ENDPOINT</code> if not specified <code>None</code> <code>api_version</code> <code>str</code> API version. Infered from <code>AZURE_OPENAI_API_KEY</code> if not specified <code>None</code> <code>api_key</code> <code>str</code> OpenAI API key. Infered from <code>OPENAI_API_KEY</code> if not specified <code>None</code> <code>azure_ad_token</code> <code>str</code> Azure active directory token. Inference from <code>AZURE_OPENAI_AD_TOKEN</code> if not specified <code>None</code> <code>azure_ad_token_provider</code> <code>AzureADTokenProvider</code> A function that returns an Azure Active Directory token <code>None</code> <code>organization</code> <code>str</code> OpenAI organization id. Infered from <code>OPENAI_ORG_ID</code> if not specified <code>None</code> <code>project</code> <code>str</code> OpenAI project id. Infered from <code>OPENAI_PROJECT_ID</code> if not specified. <code>None</code> <code>base_url</code> <code>str | https.URL</code> Base URL for the endpoint. Infered from <code>OPENAI_BASE_URL</code> if not specified. <code>None</code> <code>timeout</code> <code>float</code> Request timeout. <code>NOT_GIVEN</code> <code>max_retries</code> <code>int</code> Maximum number of retries for failing requests <code>2</code> <code>default_headers</code> <code>Mapping[str, str]</code> Default HTTP headers <code>None</code> <code>default_query</code> <code>Mapping[str, str]</code> Custom parameters added to the HTTP queries <code>None</code> <code>http_client</code> <code>https.AsyncClient</code> User-specified <code>httpx</code> client <code>None</code>"},{"location":"reference/models/openai/#models-that-follow-the-openai-standard","title":"Models that follow the OpenAI standard","text":"<p>Outlines supports models that follow the OpenAI standard. You will need to initialize the OpenAI client properly configured and pass it to <code>outlines.models.openai</code></p> <pre><code>import os\nfrom openai import AsyncOpenAI\nfrom outlines import models\nfrom outlines.models.openai import OpenAIConfig\n\n\nclient = AsyncOpenAI(\n    api_key=os.environ.get(\"PROVIDER_KEY\"),\n    base_url=\"http://other.provider.server.com\"\n)\nconfig = OpenAIConfig(\"model_name\")\nmodel = models.openai(client, config)\n</code></pre> <p>Warning</p> <p>You need to pass the async client to be able to do batch inference.</p>"},{"location":"reference/models/openai/#structured-generation-support","title":"Structured Generation Support","text":"<p>Outlines provides support for OpenAI Structured Outputs via <code>outlines.generate.json</code>, <code>outlines.generate.choice</code></p> <pre><code>from pydantic import BaseModel, ConfigDict\nimport outlines.models as models\nfrom outlines import generate\n\nmodel = models.openai(\"gpt-4o-mini\")\n\nclass Person(BaseModel):\n    model_config = ConfigDict(extra='forbid')  # required for openai\n    first_name: str\n    last_name: str\n    age: int\n\ngenerator = generate.json(model, Person)\ngenerator(\"current indian prime minister on january 1st 2023\")\n# Person(first_name='Narendra', last_name='Modi', age=72)\n\ngenerator = generate.choice(model, [\"Chicken\", \"Egg\"])\nprint(generator(\"Which came first?\"))\n# Chicken\n</code></pre> <p>Warning</p> <p>Structured generation support only provided to OpenAI-compatible endpoints which conform to OpenAI's standard. Additionally, <code>generate.regex</code> and <code>generate.cfg</code> are not supported.</p>"},{"location":"reference/models/openai/#advanced-configuration","title":"Advanced configuration","text":"<p>For more advanced configuration option, such as support proxy, please consult the OpenAI SDK's documentation:</p> <pre><code>from openai import AsyncOpenAI, DefaultHttpxClient\nfrom outlines import models\nfrom outlines.models.openai import OpenAIConfig\n\n\nclient = AsyncOpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=DefaultHttpxClient(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\nconfig = OpenAIConfig(\"model_name\")\nmodel = models.openai(client, config)\n</code></pre> <p>It is possible to specify the values for <code>seed</code>, <code>presence_penalty</code>, <code>frequence_penalty</code>, <code>top_p</code> by passing an instance of <code>OpenAIConfig</code> when initializing the model:</p> <pre><code>from outlines.models.openai import OpenAIConfig\nfrom outlines import models\n\n\nconfig = OpenAIConfig(\n    presence_penalty=1.,\n    frequency_penalty=1.,\n    top_p=.95,\n    seed=0,\n)\nmodel = models.openai(\"gpt-4o-mini\", config)\n</code></pre>"},{"location":"reference/models/openai/#monitoring-api-use","title":"Monitoring API use","text":"<p>It is important to be able to track your API usage when working with OpenAI's API. The number of prompt tokens and completion tokens is directly accessible via the model instance:</p> <pre><code>from openai import AsyncOpenAI\nimport outlines.models\n\n\nmodel = models.openai(\"gpt-4o\")\n\nprint(model.prompt_tokens)\n# 0\n\nprint(model.completion_tokens)\n# 0\n</code></pre> <p>These numbers are updated every time you call the model.</p>"},{"location":"reference/models/tgi/","title":"Text-generation-inference (TGI)","text":"<p>TGI uses Outlines to provide structured generation, see their documentation.</p>"},{"location":"reference/models/transformers/","title":"transformers","text":"<p>Installation</p> <p>You need to install the <code>transformer</code>, <code>datasets</code> and <code>torch</code> libraries to be able to use these models in Outlines, or alternatively:</p> <pre><code>pip install \"outlines[transformers]\"\n</code></pre> <p>Outlines provides an integration with the <code>torch</code> implementation of causal models in the transformers library. You can initialize the model by passing its name:</p> <pre><code>from outlines import models\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\n</code></pre> <p>If you need more fine-grained control you can also initialize the model and tokenizer separately:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines import models\n\nllm = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = models.Transformers(llm, tokenizer)\n</code></pre>"},{"location":"reference/models/transformers/#using-logits-processors","title":"Using Logits Processors","text":"<p>There are two ways to use Outlines Structured Generation with HuggingFace Transformers:</p> <ol> <li>Use Outlines generation wrapper, <code>outlines.models.transformers</code></li> <li>Use <code>OutlinesLogitsProcessor</code> with <code>transformers.AutoModelForCausalLM</code></li> </ol> <p>Outlines supports a myriad of logits processors for structured generation. In these example, we will use the <code>RegexLogitsProcessor</code> which guarantees generated text matches the specified pattern.</p>"},{"location":"reference/models/transformers/#using-outlinesmodelstransformers","title":"Using <code>outlines.models.transformers</code>","text":"<pre><code>import outlines\n\ntime_regex_pattern = r\"(0?[1-9]|1[0-2]):[0-5]\\d\\s?(am|pm)?\"\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\ngenerator = outlines.generate.regex(model, time_regex_pattern)\n\noutput = generator(\"The the best time to visit a dentist is at \")\nprint(output)\n# 2:30 pm\n</code></pre>"},{"location":"reference/models/transformers/#using-models-initialized-via-the-transformers-library","title":"Using models initialized via the <code>transformers</code>  library","text":"<pre><code>import outlines\nimport transformers\n\n\nmodel_uri = \"microsoft/Phi-3-mini-4k-instruct\"\n\noutlines_tokenizer = outlines.models.TransformerTokenizer(\n    transformers.AutoTokenizer.from_pretrained(model_uri)\n)\nphone_number_logits_processor = outlines.processors.RegexLogitsProcessor(\n    \"\\\\+?[1-9][0-9]{7,14}\",  # phone number pattern\n    outlines_tokenizer,\n)\n\ngenerator = transformers.pipeline('text-generation', model=model_uri)\n\noutput = generator(\n    \"Jenny gave me her number it's \",\n    logits_processor=transformers.LogitsProcessorList([phone_number_logits_processor])\n)\nprint(output)\n# [{'generated_text': \"Jenny gave me her number it's 2125550182\"}]\n# not quite 8675309 what we expected, but it is a valid phone number\n</code></pre>"},{"location":"reference/models/transformers/#alternative-model-classes","title":"Alternative Model Classes","text":"<p><code>outlines.models.transformers</code> defaults to <code>transformers.AutoModelForCausalLM</code>, which is the appropriate class for most standard large language models, including Llama 3, Mistral, Phi-3, etc.</p> <p>However other variants with unique behavior can be used as well by passing the appropriate class.</p>"},{"location":"reference/models/transformers/#mamba","title":"Mamba","text":"<p>Mamba is a transformers alternative which employs memory efficient, linear-time decoding.</p> <p>To use Mamba with outlines you must first install the necessary requirements: <pre><code>pip install causal-conv1d&gt;=1.2.0 mamba-ssm torch transformers\n</code></pre></p> <p>Then you can either create an Mamba-2 Outlines model via <pre><code>import outlines\n\nmodel = outlines.models.mamba(\"state-spaces/mamba-2.8b-hf\")\n</code></pre></p> <p>or explicitly with <pre><code>import outlines\nfrom transformers import MambaForCausalLM\n\nmodel = outlines.models.transformers(\n    \"state-spaces/mamba-2.8b-hf\",\n    model_class=MambaForCausalLM\n)\n</code></pre></p> <p>Read <code>transformers</code>'s documentation for more information.</p>"},{"location":"reference/models/transformers/#encoder-decoder-models","title":"Encoder-Decoder Models","text":"<p>You can use encoder-decoder (seq2seq) models like T5 and BART with Outlines.</p> <p>Be cautious with model selection though, some models such as <code>t5-base</code> don't include certain characters (<code>{</code>) and you may get an error when trying to perform structured generation.</p> <p>T5 Example: <pre><code>import outlines\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel_pile_t5 = outlines.models.transformers(\n    model_name=\"EleutherAI/pile-t5-large\",\n    model_class=AutoModelForSeq2SeqLM,\n)\n</code></pre></p> <p>Bart Example: <pre><code>model_bart = outlines.models.transformers(\n    model_name=\"facebook/bart-large\",\n    model_class=AutoModelForSeq2SeqLM,\n)\n</code></pre></p>"},{"location":"reference/models/transformers_vision/","title":"Transformers Vision","text":"<p>Outlines allows seamless use of vision models.</p> <p><code>outlines.models.transformers_vision</code> shares interfaces with, and is based on outlines.models.transformers.</p> <p>Tasks supported include</p> <ul> <li>image + text -&gt; text</li> <li>video + text -&gt; text</li> </ul>"},{"location":"reference/models/transformers_vision/#example-using-llava-next-vision-models","title":"Example: Using Llava-Next Vision Models","text":"<p>Install dependencies <code>pip install torchvision pillow flash-attn</code></p> <p>Create the model <pre><code>import outlines\nfrom transformers import LlavaNextForConditionalGeneration\n\nmodel = outlines.models.transformers_vision(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    model_class=LlavaNextForConditionalGeneration,\n    device=\"cuda\",\n)\n</code></pre></p> <p>Create convenience function to load a <code>PIL.Image</code> from URL <pre><code>from PIL import Image\nfrom io import BytesIO\nfrom urllib.request import urlopen\n\ndef img_from_url(url):\n    img_byte_stream = BytesIO(urlopen(url).read())\n    return Image.open(img_byte_stream).convert(\"RGB\")\n</code></pre></p>"},{"location":"reference/models/transformers_vision/#describing-an-image","title":"Describing an image","text":"<pre><code>description_generator = outlines.generate.text(model)\ndescription_generator(\n    \"&lt;image&gt; detailed description:\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\")]\n)\n</code></pre> <p>This is a color photograph featuring a Siamese cat with striking blue eyes. The cat has a creamy coat and a light eye color, which is typical for the Siamese breed. Its features include elongated ears, a long, thin tail, and a striking coat pattern. The cat is sitting in an indoor setting, possibly on a cat tower or a similar raised platform, which is covered with a beige fabric, providing a comfortable and soft surface for the cat to rest or perch. The surface of the wall behind the cat appears to be a light-colored stucco or plaster.</p>"},{"location":"reference/models/transformers_vision/#multiple-images","title":"Multiple Images","text":"<p>To include multiple images in your prompt you simply add more <code>&lt;image&gt;</code> tokens to the prompt</p> <pre><code>image_urls = [\n    \"https://cdn1.byjus.com/wp-content/uploads/2020/08/ShapeArtboard-1-copy-3.png\",  # triangle\n    \"https://cdn1.byjus.com/wp-content/uploads/2020/08/ShapeArtboard-1-copy-11.png\",  # hexagon\n]\ndescription_generator = outlines.generate.text(model)\ndescription_generator(\n    \"&lt;image&gt;&lt;image&gt;What shapes are present?\",\n    list(map(img_from_url, image_urls)),\n)\n</code></pre> <p>There are two shapes present. One shape is a hexagon and the other shape is an triangle. '</p>"},{"location":"reference/models/transformers_vision/#classifying-an-image","title":"Classifying an Image","text":"<pre><code>pattern = \"Mercury|Venus|Earth|Mars|Saturn|Jupiter|Neptune|Uranus|Pluto\"\nplanet_generator = outlines.generate.regex(model, pattern)\n\nplanet_generator(\n    \"What planet is this: &lt;image&gt;\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Saturn_from_Cassini_Orbiter_%282004-10-06%29.jpg\")]\n)\n</code></pre> <p>Saturn</p>"},{"location":"reference/models/transformers_vision/#extracting-structured-image-data","title":"Extracting Structured Image data","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\n\nclass ImageData(BaseModel):\n    caption: str\n    tags_list: List[str]\n    object_list: List[str]\n    is_photo: bool\n\nimage_data_generator = outlines.generate.json(model, ImageData)\n\nimage_data_generator(\n    \"&lt;image&gt; detailed JSON metadata:\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\")]\n)\n</code></pre> <p><code>ImageData(caption='An astronaut on the moon', tags_list=['moon', 'space', 'nasa', 'americanflag'], object_list=['moon', 'moon_surface', 'space_suit', 'americanflag'], is_photo=True)</code></p>"},{"location":"reference/models/transformers_vision/#resources","title":"Resources","text":""},{"location":"reference/models/transformers_vision/#choosing-a-model","title":"Choosing a model","text":"<ul> <li>https://mmbench.opencompass.org.cn/leaderboard</li> <li>https://huggingface.co/spaces/WildVision/vision-arena</li> </ul>"},{"location":"reference/models/vllm/","title":"vLLM","text":"<p>Installation</p> <p>You need to install the <code>vllm</code> library to use the vLLM integration. See the installation section for instructions to install vLLM for CPU or ROCm. To get started you can also run:</p> <pre><code>pip install \"outlines[vllm]\"\n</code></pre>"},{"location":"reference/models/vllm/#load-the-model","title":"Load the model","text":"<p>Outlines supports models available via vLLM's offline batched inference interface. You can load a model using:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"microsoft/Phi-3-mini-4k-instruct\")\n</code></pre> <p>Or alternatively:</p> <pre><code>import vllm\nfrom outlines import models\n\nllm = vllm.LLM(\"microsoft/Phi-3-mini-4k-instruct\")\nmodel = models.VLLM(llm)\n</code></pre> <p>Models are loaded from the HuggingFace hub.</p> <p>Device</p> <p>The default installation of vLLM only allows to load models on GPU. See the installation instructions to run models on CPU.</p> <p>You can pass any parameter that you would normally pass to <code>vllm.LLM</code>, as keyword arguments:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    trust_remote_code=True,\n    gpu_memory_utilization=0.7\n)\n</code></pre> <p>Main parameters:</p> Parameters Type Description Default <code>tokenizer_mode</code> <code>str</code> \"auto\" will use the fast tokenizer if available and \"slow\" will always use the slow tokenizer. <code>auto</code> <code>trust_remote_code</code> <code>bool</code> Trust remote code when downloading the model and tokenizer. <code>False</code> <code>tensor_parallel_size</code> <code>int</code> The number of GPUs to use for distributed execution with tensor parallelism. <code>1</code> <code>dtype</code> <code>str</code> The data type for the model weights and activations. Currently, we support <code>float32</code>, <code>float16</code>, and <code>bfloat16</code>. If <code>auto</code>, we use the <code>torch_dtype</code> attribute specified in the model config file. However, if the <code>torch_dtype</code> in the config is <code>float32</code>, we will use <code>float16</code> instead. <code>auto</code> <code>quantization</code> <code>Optional[str]</code> The method used to quantize the model weights. Currently, we support \"awq\", \"gptq\" and \"squeezellm\". If None, we first check the <code>quantization_config</code> attribute in the model config file. If that is None, we assume the model weights are not quantized and use <code>dtype</code> to determine the data type of the weights. <code>None</code> <code>revision</code> <code>Optional[str]</code> The specific model version to use. It can be a branch name, a tag name, or a commit id. <code>None</code> <code>tokenizer_revision</code> <code>Optional[str]</code> The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. <code>None</code> <code>gpu_memory_utilization</code> <code>float</code> The ratio (between 0 and 1) of GPU memory to reserve for the model weights, activations, and KV cache. Higher values will increase the KV cache size and thus improve the model's throughput. However, if the value is too high, it may cause out-of-memory (OOM) errors. <code>0.9</code> <code>swap_space</code> <code>int</code> The size (GiB) of CPU memory per GPU to use as swap space. This can be used for temporarily storing the states of the requests when their <code>best_of</code> sampling parameters are larger than 1. If all requests will have <code>best_of=1</code>, you can safely set this to 0. Otherwise, too small values may cause out-of-memory (OOM) errors. 4 <code>enforce_eager</code> <code>bool</code> Whether to enforce eager execution. If True, we will disable CUDA graph and always execute the model in eager mode. If False, we will use CUDA graph and eager execution in hybrid. <code>False</code> <code>enable_lora</code> <code>bool</code> Whether to enable loading LoRA adapters <code>False</code> <p>See the vLLM code for a list of all the available parameters.</p>"},{"location":"reference/models/vllm/#use-quantized-models","title":"Use quantized models","text":"<p>vLLM supports AWQ, GPTQ and SqueezeLLM quantized models:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"TheBloke/Llama-2-7B-Chat-AWQ\", quantization=\"awq\")\nmodel = models.vllm(\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\", quantization=\"gptq\")\nmodel = models.vllm(\"https://huggingface.co/squeeze-ai-lab/sq-llama-30b-w4-s5\", quantization=\"squeezellm\")\n</code></pre> <p>Dependencies</p> <p>To use AWQ model you need to install the autoawq library <code>pip install autoawq</code>.</p> <p>To use GPTQ models you need to install the autoGTPQ and optimum libraries <code>pip install auto-gptq optimum</code>.</p>"},{"location":"reference/models/vllm/#multi-gpu-usage","title":"Multi-GPU usage","text":"<p>To run multi-GPU inference with vLLM you need to set the <code>tensor_parallel_size</code> argument to the number of GPUs available when initializing the model. For instance to run inference on 2 GPUs:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\n    \"microsoft/Phi-3-mini-4k-instruct\"\n    tensor_parallel_size=2\n)\n</code></pre>"},{"location":"reference/models/vllm/#load-lora-adapters","title":"Load LoRA adapters","text":"<p>You can load LoRA adapters and alternate between them dynamically:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"facebook/opt-350m\", enable_lora=True)\nmodel.load_lora(\"ybelkaa/opt-350m-lora\")  # Load LoRA adapter\nmodel.load_lora(None)  # Unload LoRA adapter\n</code></pre>"},{"location":"reference/models/vllm/#generate-text","title":"Generate text","text":"<p>In addition to the parameters described in the text generation section you can pass an instance of <code>SamplingParams</code> directly to any generator via the <code>sampling_params</code> keyword argument:</p> <pre><code>from vllm.sampling_params import SamplingParams\nfrom outlines import models, generate\n\n\nmodel = models.vllm(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nparams = SamplingParams(n=2, frequency_penalty=1., min_tokens=2)\nanswer = generator(\"A prompt\", sampling_params=params)\n</code></pre> <p>This also works with generators built with <code>generate.regex</code>, <code>generate.json</code>, <code>generate.cfg</code>, <code>generate.format</code> and <code>generate.choice</code>.</p> <p>Note</p> <p>The values passed via the <code>SamplingParams</code> instance supersede the other arguments to the generator or the samplers.</p> <p><code>SamplingParams</code> attributes:</p> Parameters Type Description Default <code>n</code> <code>int</code> Number of output sequences to return for the given prompt. <code>1</code> <code>best_of</code> <code>Optional[int]</code> Number of output sequences that are generated from the prompt. From these <code>best_of</code> sequences, the top <code>n</code> sequences are returned. <code>best_of</code> must be greater than or equal to <code>n</code>. This is treated as the beam width when <code>use_beam_search</code> is True. By default, <code>best_of</code> is set to <code>n</code>. <code>None</code> <code>presence_penalty</code> <code>float</code> Float that penalizes new tokens based on whether they appear in the generated text so far. Values &gt; 0 encourage the model to use new tokens, while values &lt; 0 encourage the model to repeat tokens. <code>0.0</code> <code>frequency_penalty</code> <code>float</code> Float that penalizes new tokens based on their frequency in the generated text so far. Values &gt; 0 encourage the model to use new tokens, while values &lt; 0 encourage the model to repeat tokens. <code>0.0</code> <code>repetition_penalty</code> <code>float</code> Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values &gt; 1 encourage the model to use new tokens, while values &lt; 1 encourage the model to repeat tokens. <code>1.0</code> <code>temperature</code> <code>float</code> Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. <code>1.0</code> <code>top_p</code> <code>float</code> Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens. <code>1.0</code> <code>top_k</code> <code>int</code> Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens. <code>-1</code> <code>min_p</code> <code>float</code> Float that represents the minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1]. Set to 0 to disable this. <code>0.0</code> <code>seed</code> <code>Optional[int]</code> Random seed to use for the generation. <code>None</code> <code>use_beam_search</code> <code>bool</code> Whether to use beam search instead of sampling. <code>False</code> <code>length_penalty</code> <code>float</code> Float that penalizes sequences based on their length. Used in beam search. <code>1.0</code> <code>early_stopping</code> <code>Union[bool, str]</code> Controls the stopping condition for beam search. It accepts the following values: <code>True</code>, where the generation stops as soon as there are <code>best_of</code> complete candidates; <code>False</code>, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates; <code>\"never\"</code>, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm). <code>False</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> List of strings that stop the generation when they are generated. The returned output will not contain the stop strings. <code>None</code> <code>stop_token_ids</code> <code>Optional[List[int]]</code> List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens. <code>None</code> <code>include_stop_str_in_output</code> <code>bool</code> Whether to include the stop strings in output text. Defaults to False. <code>False</code> <code>ignore_eos</code> <code>bool</code> Whether to ignore the EOS token and continue generating tokens after the EOS token is generated. <code>False</code> <code>max_tokens</code> <code>int</code> Maximum number of tokens to generate per output sequence. <code>16</code> <code>min_tokens</code> <code>int</code> Minimum number of tokens to generate per output sequence before EOS or stop_token_ids can be generated <code>0</code> <code>skip_special_tokens</code> <code>bool</code> Whether to skip special tokens in the output. <code>True</code> <code>spaces_between_special_tokens</code> <code>bool</code> Whether to add spaces between special tokens in the output.  Defaults to True. <code>True</code>"},{"location":"reference/models/vllm/#streaming","title":"Streaming","text":"<p>Warning</p> <p>Streaming is not available for the offline vLLM integration.</p>"},{"location":"reference/models/vllm/#installation","title":"Installation","text":"<p>By default the vLLM library is installed with pre-commpiled C++ and CUDA binaries and will only run on GPU:</p> <pre><code>pip install vllm\n</code></pre>"},{"location":"reference/models/vllm/#cpu","title":"CPU","text":"<p>You need to have the <code>gcc</code> compiler installed on your system. Then you will need to install vLLM from source. First clone the repository:</p> <pre><code>git clone https://github.com/vllm-project/vllm.git\ncd vllm\n</code></pre> <p>Install the Python packages needed for the installation:</p> <pre><code>pip install --upgrade pip\npip install wheel packaging ninja setuptools&gt;=49.4.0 numpy\npip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n</code></pre> <p>and finally run:</p> <pre><code>VLLM_TARGET_DEVICE=cpu python setup.py install\n</code></pre> <p>See the vLLM documentation for more details, alternative installation methods (Docker) and performance tips.</p>"},{"location":"reference/models/vllm/#rocm","title":"ROCm","text":"<p>You will need to install vLLM from source. First install Pytorch on ROCm:</p> <pre><code>pip install torch==2.2.0.dev20231206+rocm5.7 --index-url https://download.pytorch.org/whl/nightly/rocm5.7 # tested version\n</code></pre> <p>You will then need to install flash attention for ROCm following these instructions. You can then install <code>xformers=0.0.23</code> and apply the patches needed to adapt Flash Attention for ROCm:</p> <pre><code>pip install xformers==0.0.23 --no-deps\nbash patch_xformers.rocm.sh\n</code></pre> <p>And finally build vLLM:</p> <pre><code>cd vllm\npip install -U -r requirements-rocm.txt\npython setup.py install # This may take 5-10 minutes.\n</code></pre> <p>See the vLLM documentation for alternative installation methods (Docker).</p>"},{"location":"reference/serve/lmstudio/","title":"Serve with LM Studio","text":"<p>Would rather not self-host?</p> <p>If you want to get started quickly with JSON-structured generation you can call instead .json, a .txt API that guarantees valid JSON.</p> <p>LM Studio is an application that runs local LLMs. It flexibly mixes GPU and CPU compute in hardware-constrained environments.</p> <p>As of LM Studio 0.3.4, it natively supports Outlines for structured text generation, using an OpenAI-compatible endpoint.</p>"},{"location":"reference/serve/lmstudio/#setup","title":"Setup","text":"<ol> <li>Install LM Studio by visiting their downloads page.</li> <li>Enable the LM Studio server functionality.</li> <li>Download a model.</li> <li>Install Python dependencies. <pre><code>pip install pydantic openai\n</code></pre></li> </ol>"},{"location":"reference/serve/lmstudio/#calling-the-server","title":"Calling the server","text":"<p>By default, LM Studio will serve from <code>http://localhost:1234</code>. If you are serving on a different port or host, make sure to change the <code>base_url</code> argument in <code>OpenAI</code> to the relevant location.</p> <pre><code>class Testing(BaseModel):\n    \"\"\"\n    A class representing a testing schema.\n    \"\"\"\n    name: str\n    age: int\n\nopenai_client = openai.OpenAI(\n    base_url=\"http://0.0.0.0:1234/v1\",\n    api_key=\"dopeness\"\n)\n\n# Make a request to the local LM Studio server\nresponse = openai_client.beta.chat.completions.parse(\n    model=\"hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are like so good at whatever you do.\"},\n        {\"role\": \"user\", \"content\": \"My name is Cameron and I am 28 years old. What's my name and age?\"}\n    ],\n    response_format=Testing\n)\n</code></pre> <p>You should receive a <code>ParsedChatCompletion[Testing]</code> object back:</p> <pre><code>ParsedChatCompletion[Testing](\n    id='chatcmpl-3hykyf0fxus7jc90k6gwlw',\n    choices=[\n        ParsedChoice[Testing](\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ParsedChatCompletionMessage[Testing](\n                content='{ \"age\": 28, \"name\": \"Cameron\" }',\n                refusal=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=[],\n                parsed=Testing(name='Cameron', age=28)\n            )\n        )\n    ],\n    created=1728595622,\n    model='lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint='lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF/Phi-3.1-mini-128k-instruct-\nQ4_K_M.gguf',\n    usage=CompletionUsage(\n        completion_tokens=17,\n        prompt_tokens=47,\n        total_tokens=64,\n        completion_tokens_details=None,\n        prompt_tokens_details=None\n    )\n)\n</code></pre> <p>You can retrieve your <code>Testing</code> object with</p> <pre><code>response.choices[0].message.parsed\n</code></pre>"},{"location":"reference/serve/vllm/","title":"Serve with vLLM","text":"<p>Would rather not self-host?</p> <p>If you want to get started quickly with JSON-structured generation you can call instead .json, a .txt API that guarantees valid JSON.</p> <p>Outlines can be deployed as an LLM service using the vLLM inference engine and a FastAPI server. vLLM is not installed by default so will need to install Outlines with:</p> <pre><code>pip install outlines[serve]\n</code></pre> <p>You can then start the server with:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used. The <code>--model</code> argument allows you to specify any model of your choosing.</p> <p>To run inference on multiple GPUs you must pass the <code>--tensor-parallel-size</code> argument when initializing the server. For instance, to run inference on 2 GPUs:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\" --tensor-parallel-size 2\n</code></pre>"},{"location":"reference/serve/vllm/#alternative-method-via-docker","title":"Alternative Method: Via Docker","text":"<p>You can install and run the server with Outlines' official Docker image using the command</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre>"},{"location":"reference/serve/vllm/#querying-endpoint","title":"Querying Endpoint","text":"<p>You can then query the model in shell by passing a prompt and either</p> <ol> <li>a JSON Schema specification or</li> <li>a Regex pattern</li> </ol> <p>with the <code>schema</code> or <code>regex</code> parameters, respectively, to the <code>/generate</code> endpoint. If both are specified, the schema will be used. If neither is specified, the generated text will be unconstrained.</p> <p>For example, to generate a string that matches the schema <code>{\"type\": \"string\"}</code> (any string):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is the capital of France?\",\n        \"schema\": {\"type\": \"string\", \"maxLength\": 5}\n        }'\n</code></pre> <p>To generate a string that matches the regex <code>(-)?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-][0-9]+)?</code> (a number):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is Pi? Give me the first 15 digits: \",\n        \"regex\": \"(-)?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-][0-9]+)?\"\n        }'\n</code></pre> <p>Instead of <code>curl</code>, you can also use the requests library from another python program.</p> <p>Please consult the vLLM documentation for details on additional request parameters. You can also read the code in case you need to customize the solution to your needs.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/roadmap/","title":"Roadmap","text":""}]}